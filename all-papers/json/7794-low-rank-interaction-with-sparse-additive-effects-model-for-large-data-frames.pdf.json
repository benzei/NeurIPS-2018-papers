{
    "filename": "7794-low-rank-interaction-with-sparse-additive-effects-model-for-large-data-frames.pdf",
    "metadata": {
        "title": "Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames",
        "author": "Genevi\u00e8ve Robin, Hoi-To Wai, Julie Josse, Olga Klopp, Eric Moulines",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7794-low-rank-interaction-with-sparse-additive-effects-model-for-large-data-frames.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Many applications of machine learning involve the analysis of large data frames \u2013 matrices collecting heterogeneous measurements (binary, numerical, counts, etc.) across samples \u2013 with missing values. Low-rank models, as studied by Udell et al. [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], are popular in this framework for tasks such as visualization, clustering and missing value imputation. Yet, available methods with statistical guarantees and efficient optimization do not allow explicit modeling of main additive effects such as row and column, or covariate effects. In this paper, we introduce a lowrank interaction and sparse additive effects (LORIS) model which combines matrix regression on a dictionary and low-rank design, to estimate main effects and interactions simultaneously. We provide statistical guarantees in the form of upper bounds on the estimation error of both components. Then, we introduce a mixed coordinate gradient descent (MCGD) method which provably converges sub-linearly to an optimal solution and is computationally efficient for large scale data sets. We show on simulated and survey data that the method has a clear advantage over current practices."
    },
    "keywords": [
        {
            "term": "principal component analysis",
            "url": "https://en.wikipedia.org/wiki/principal_component_analysis"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        }
    ],
    "highlights": [
        "A lot of effort has been devoted towards the efficient analysis of large data frames, a term coined by Udell et al [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "We propose in Section 3 a Mixed Coordinate Gradient Descent (MCGD) method to solve efficiently the LORIS estimation problem",
        "We show that the mixed coordinate gradient descent method converges to an -optimal solution in O(1/ ) iterations",
        "There are two differences: first, FW-T is focused on a quadratic loss which is a special case of the statistical estimation problem that we analyze; second, the per-iteration complexity of mixed coordinate gradient descent is lower as the update rules are simpler",
        "Using a new proof technique, we prove that the convergence rate of mixed coordinate gradient descent is strictly faster than FW-T",
        "Computing the Upper Bound RU We describe a strategy for computing a valid upper bound RU for Rand \u0398during the updates in the mixed coordinate gradient descent method"
    ],
    "key_statements": [
        "A lot of effort has been devoted towards the efficient analysis of large data frames, a term coined by Udell et al [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "We propose in Section 3 a Mixed Coordinate Gradient Descent (MCGD) method to solve efficiently the LORIS estimation problem",
        "We show that the mixed coordinate gradient descent method converges to an -optimal solution in O(1/ ) iterations",
        "There are two differences: first, FW-T is focused on a quadratic loss which is a special case of the statistical estimation problem that we analyze; second, the per-iteration complexity of mixed coordinate gradient descent is lower as the update rules are simpler",
        "Using a new proof technique, we prove that the convergence rate of mixed coordinate gradient descent is strictly faster than FW-T",
        "Computing the Upper Bound RU We describe a strategy for computing a valid upper bound RU for Rand \u0398during the updates in the mixed coordinate gradient descent method",
        "M ), 24\u03c3\u0398(RU(0B))2}. Such that Mis an upper bound to M (t), and the mixed coordinate gradient descent method converges to an -optimal solution to (5) in T iterations, i.e., F0(\u03b1(T ), \u0398(T )) \u2212 F0(\u03b1 , \u0398 ) \u2264 , where\n1 T 1 \u22121"
    ],
    "summary": [
        "A lot of effort has been devoted towards the efficient analysis of large data frames, a term coined by Udell et al [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "In the large-scale low-rank matrix estimation literature, available methods either do not take additive effects into account [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], or only handle the numerical data [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "We propose in Section 3 a Mixed Coordinate Gradient Descent (MCGD) method to solve efficiently the LORIS estimation problem.",
        "We show that the MCGD method converges to an -optimal solution in O(1/ ) iterations.",
        "There are two differences: first, FW-T is focused on a quadratic loss which is a special case of the statistical estimation problem that we analyze; second, the per-iteration complexity of MCGD is lower as the update rules are simpler.",
        "LOw-rank Interaction with Sparse additive effects (LORIS) model For every entry Yij, assume a vector of covariates xij \u2208 Rq is available, e.g., user information and item characteristics.",
        "The rate obtained for \u03980 is the sum of the standard low-rank matrix completion rate of order r max(n, p)/\u03c0, e.g., [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], and of a term which boils down to sparse vector estimation rate as long as maxk X(k) 1 = O(1).",
        "In (10), \u2207\u03b1L(\u00b7) is the gradient of the log-likelihood function taken w.r.t. \u03b1, \u03b3 > 0 is a pre-defined step size parameter and T\u03bb(x) := sign(x) (x \u2212 \u03bb1)+ is the component-wise soft thresholding operator.",
        "The per-iteration computation complexity of the MCGD method scales linearly with the problem dimension max{n, p} and |\u03a9|.",
        "Such that Mis an upper bound to M (t), and the MCGD method converges to an -optimal solution to (5) in T iterations, i.e., F0(\u03b1(T ), \u0398(T )) \u2212 F0(\u03b1 , \u0398 ) \u2264 , where",
        "(5) becomes the estimation problem solved in sparse plus low-rank matrix decomposition.",
        "Similar to the development of MCGD, a natural alternative is to apply algorithms based on the CG (a.k.a. Frank-Wolfe) method [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], whose iterations only require the computation of a top SVD.",
        "Distributed MCGD Optimization Consider the case where the observed data entries are stored across K workers, each of them communicating with a central server.",
        "As the CG update in line 5 essentially requires computing the top singular vectors of the gradient matrix \u2207\u0398L(\u03b1, \u0398) =",
        "We compare the results in terms of estimation error and computing time in Table 1, after letting the two methods converge to the same precision of 10\u22125.",
        "The two-step method is faster for small data sets, whereas for large data sizes LORIS is superior in computational time.",
        "Future work includes the incorporation of qualitative features with more than two categories and of missing values in the dictionary matrices"
    ],
    "headline": "We introduce a lowrank interaction and sparse additive effects  model which combines matrix regression on a dictionary and low-rank design, to estimate main effects and interactions simultaneously",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agresti. Categorical Data Analysis, 3rd Edition. Wiley, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agresti%2C%20A.%20Categorical%20Data%20Analysis%202013"
        },
        {
            "id": "2",
            "entry": "[2] A. Beck, E. Pauwels, and S. Sabach. The cyclic block conditional gradient method for convex optimization problems. SIAM Journal on Optimization, 25(4):2024\u20132049, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20A.%20Pauwels%2C%20E.%20Sabach%2C%20S.%20The%20cyclic%20block%20conditional%20gradient%20method%20for%20convex%20optimization%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20A.%20Pauwels%2C%20E.%20Sabach%2C%20S.%20The%20cyclic%20block%20conditional%20gradient%20method%20for%20convex%20optimization%20problems%202015"
        },
        {
            "id": "3",
            "entry": "[3] P. B\u00fchlmann and S. van de Geer. Statistics for High-Dimensional Data: Methods, Theory and Applications. Springer, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%BChlmann%2C%20P.%20van%20de%20Geer%2C%20S.%20Statistics%20for%20High-Dimensional%20Data%3A%20Methods%2C%20Theory%20and%20Applications%202011"
        },
        {
            "id": "4",
            "entry": "[4] E. J. Cand\u00e8s, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 58(3):11:1\u201311:37, June 2011. ISSN 0004-5411. doi: 10.1145/1970392.1970395. URL http://doi.acm.org/10.1145/1970392.1970395.",
            "crossref": "https://dx.doi.org/10.1145/1970392.1970395",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/1970392.1970395"
        },
        {
            "id": "5",
            "entry": "[5] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572\u2013596, 2011. doi: 10.1137/090761793. URL https://doi.org/10.1137/090761793.",
            "crossref": "https://dx.doi.org/10.1137/090761793",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/090761793"
        },
        {
            "id": "6",
            "entry": "[6] Y. Chen and M. J. Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees. CoRR, abs/1509.03025, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.03025"
        },
        {
            "id": "7",
            "entry": "[7] J. de Leeuw. Principal component analysis of binary data by iterated singular value decomposition. Comput. Stat. Data Anal., 50(1):21\u201339, Jan. 2006. ISSN 0167-9473. doi: 10.1016/j.csda.2004.07.010. URL http://dx.doi.org/10.1016/j.csda.2004.07.010.",
            "crossref": "https://dx.doi.org/10.1016/j.csda.2004.07.010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.csda.2004.07.010"
        },
        {
            "id": "8",
            "entry": "[8] A. Feuerverger, Y. He, and S. Khatri. Statistical significance of the netflix challenge. Statist. Sci., 27(2): 202\u2013231, 05 2012. doi: 10.1214/11-STS368. URL http://dx.doi.org/10.1214/11-STS368.",
            "crossref": "https://dx.doi.org/10.1214/11-STS368",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/11-STS368"
        },
        {
            "id": "9",
            "entry": "[9] W. Fithian and R. Mazumder. Flexible Low-Rank Statistical Modeling with Missing Data and Side Information. Statistical Science, 33(2):238\u2013260, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fithian%2C%20W.%20Mazumder%2C%20R.%20Flexible%20Low-Rank%20Statistical%20Modeling%20with%20Missing%20Data%20and%20Side%20Information%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fithian%2C%20W.%20Mazumder%2C%20R.%20Flexible%20Low-Rank%20Statistical%20Modeling%20with%20Missing%20Data%20and%20Side%20Information%202018"
        },
        {
            "id": "10",
            "entry": "[10] D. Garber, S. Sabach, and A. Kaplan. Fast generalized conditional gradient method with applications to matrix recovery problems. arXiv preprint arXiv:1802.05581, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05581"
        },
        {
            "id": "11",
            "entry": "[11] Q. Gu, Z. W. Wang, and H. Liu. Low-rank and sparse structure pursuit via alternating minimization. In A. Gretton and C. C. Robert, editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 600\u2013609, Cadiz, Spain, 09\u201311 May 2016. PMLR. URL http://proceedings.mlr.press/v51/gu16.html.",
            "url": "http://proceedings.mlr.press/v51/gu16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Q.%20Wang%2C%20Z.W.%20Liu%2C%20H.%20Low-rank%20and%20sparse%20structure%20pursuit%20via%20alternating%20minimization%202016-05"
        },
        {
            "id": "12",
            "entry": "[12] T. Hastie, R. Mazumder, J. Lee, and R. Zadeh. Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares. The Journal of Machine Learning Research, 16:3367\u20133402, jan 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastie%2C%20T.%20Mazumder%2C%20R.%20Lee%2C%20J.%20Zadeh%2C%20R.%20Matrix%20Completion%20and%20Low-Rank%20SVD%20via%20Fast%20Alternating%20Least%20Squares%202015-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastie%2C%20T.%20Mazumder%2C%20R.%20Lee%2C%20J.%20Zadeh%2C%20R.%20Matrix%20Completion%20and%20Low-Rank%20SVD%20via%20Fast%20Alternating%20Least%20Squares%202015-01"
        },
        {
            "id": "13",
            "entry": "[13] D. Hsu, S. M. Kakade, and T. Zhang. Robust matrix decomposition with sparse corruptions. EEE Transactions on Information Theory, 57(11):7221\u20137234, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20D.%20Kakade%2C%20S.M.%20Zhang%2C%20T.%20Robust%20matrix%20decomposition%20with%20sparse%20corruptions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20D.%20Kakade%2C%20S.M.%20Zhang%2C%20T.%20Robust%20matrix%20decomposition%20with%20sparse%20corruptions%202011"
        },
        {
            "id": "14",
            "entry": "[14] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML (1), pages 427\u2013435, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaggi%2C%20M.%20Revisiting%20frank-wolfe%3A%20Projection-free%20sparse%20convex%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaggi%2C%20M.%20Revisiting%20frank-wolfe%3A%20Projection-free%20sparse%20convex%20optimization%202013"
        },
        {
            "id": "15",
            "entry": "[15] H. A. L. Kiers. Simple structure in component analysis techniques for mixtures of qualitative and quantitative variables. Psychometrika, 56(2):197\u2013212, Jun 1991. ISSN 1860-0980. doi: 10.1007/ BF02294458. URL https://doi.org/10.1007/BF02294458.",
            "crossref": "https://dx.doi.org/10.1007/BF02294458",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/BF02294458"
        },
        {
            "id": "16",
            "entry": "[16] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 20(1):282\u2013303, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klopp%2C%20O.%20Noisy%20low-rank%20matrix%20completion%20with%20general%20sampling%20distribution%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klopp%2C%20O.%20Noisy%20low-rank%20matrix%20completion%20with%20general%20sampling%20distribution%202014"
        },
        {
            "id": "17",
            "entry": "[17] O. Klopp, K. Lounici, and A. B. Tsybakov. Robust matrix completion. Probability Theory and Related Fields, 169(1):523\u2013564, Oct 2017. doi: 10.1007/s00440-016-0736-y. URL https://doi.org/10.1007/s00440-016-0736-y.",
            "crossref": "https://dx.doi.org/10.1007/s00440-016-0736-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s00440-016-0736-y"
        },
        {
            "id": "18",
            "entry": "[18] N. K. Kumar and J. Schneider. Literature survey on low rank approximation of matrices. Linear and Multilinear Algebra, 65(11):2212\u20132244, 2017. doi: 10.1080/03081087.2016.1267104. URL https://doi.org/10.1080/03081087.2016.1267104.",
            "crossref": "https://dx.doi.org/10.1080/03081087.2016.1267104",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1080/03081087.2016.1267104"
        },
        {
            "id": "19",
            "entry": "[19] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate frank-wolfe optimization for structural svms. In Proceedings of the 30th International Conference on International Conference on Machine Learning-Volume 28, pages I\u201353. JMLR. org, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste-Julien%2C%20S.%20Jaggi%2C%20M.%20Schmidt%2C%20M.%20Pletscher%2C%20P.%20Block-coordinate%20frank-wolfe%20optimization%20for%20structural%20svms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lacoste-Julien%2C%20S.%20Jaggi%2C%20M.%20Schmidt%2C%20M.%20Pletscher%2C%20P.%20Block-coordinate%20frank-wolfe%20optimization%20for%20structural%20svms%202013"
        },
        {
            "id": "20",
            "entry": "[20] A. J. Landgraf and Y. Lee. Generalized principal component analysis: Projection of saturated model parameters. Technical report, The Ohio State University, Department of Statistics, 06 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Landgraf%2C%20A.J.%20Lee%2C%20Y.%20Generalized%20principal%20component%20analysis%3A%20Projection%20of%20saturated%20model%20parameters%202015"
        },
        {
            "id": "21",
            "entry": "[21] Z. Lin, R. Liu, and Z. Su. Linearized alternating direction method with adaptive penalty for low-rank representation. In Advances in neural information processing systems, pages 612\u2013620, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Z.%20Liu%2C%20R.%20Su%2C%20Z.%20Linearized%20alternating%20direction%20method%20with%20adaptive%20penalty%20for%20low-rank%20representation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Z.%20Liu%2C%20R.%20Su%2C%20Z.%20Linearized%20alternating%20direction%20method%20with%20adaptive%20penalty%20for%20low-rank%20representation%202011"
        },
        {
            "id": "22",
            "entry": "[22] L. T. Liu, E. Dobriban, and A. Singer. e pca: High dimensional exponential family pca. Annals of Applied Statistics, to appear, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20L.T.%20Dobriban%2C%20E.%20Singer%2C%20A.%20e%20pca%3A%20High%20dimensional%20exponential%20family%20pca%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20L.T.%20Dobriban%2C%20E.%20Singer%2C%20A.%20e%20pca%3A%20High%20dimensional%20exponential%20family%20pca%202018"
        },
        {
            "id": "23",
            "entry": "[23] C. Mu, Y. Zhang, J. Wright, and D. Goldfarb. Scalable robust matrix recovery: Frank\u2013wolfe meets proximal methods. SIAM Journal on Scientific Computing, 38(5):A3291\u2013A3317, 2016. doi: 10.1137/15M101628X. URL https://doi.org/10.1137/15M101628X.",
            "crossref": "https://dx.doi.org/10.1137/15M101628X",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/15M101628X"
        },
        {
            "id": "24",
            "entry": "[24] J. Pag\u00e8s. Multiple factor analysis by example using R. Chapman and Hall/CRC, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pag%C3%A8s%2C%20J.%20Multiple%20factor%20analysis%20by%20example%20using%20R%202014"
        },
        {
            "id": "25",
            "entry": "[25] M. Tao and X. Yuan. Recovering low-rank and sparse components of matrices from incomplete and noisy observations. SIAM Journal on Optimization, 21(1):57\u201381, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tao%2C%20M.%20Yuan%2C%20X.%20Recovering%20low-rank%20and%20sparse%20components%20of%20matrices%20from%20incomplete%20and%20noisy%20observations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tao%2C%20M.%20Yuan%2C%20X.%20Recovering%20low-rank%20and%20sparse%20components%20of%20matrices%20from%20incomplete%20and%20noisy%20observations%202011"
        },
        {
            "id": "26",
            "entry": "[26] M. Udell, C. Horn, R. Zadeh, and S. Boyd. Generalized low rank models. Foundations and Trends in Machine Learning, 9(1), 2016. doi: 10.1561/2200000055. URL http://dx.doi.org/10.1561/2200000055.",
            "crossref": "https://dx.doi.org/10.1561/2200000055",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1561/2200000055"
        },
        {
            "id": "27",
            "entry": "[27] H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. In Proceedings of the 23rd International Conference on Neural Information Processing Systems, NIPS\u201910, pages 2496\u20132504, USA, 2010. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id=2997046.2997174.",
            "url": "http://dl.acm.org/citation.cfm?id=2997046.2997174",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20H.%20Caramanis%2C%20C.%20Sanghavi%2C%20S.%20Robust%20pca%20via%20outlier%20pursuit%202010"
        },
        {
            "id": "28",
            "entry": "[28] X. Zhang, L. Wang, and Q. Gu. A unified framework for nonconvex low-rank plus sparse matrix recovery. In International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain, pages 1097\u20131107, 2018. URL http://proceedings.mlr.press/v84/zhang18c.html.",
            "url": "http://proceedings.mlr.press/v84/zhang18c.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20X.%20Wang%2C%20L.%20Gu%2C%20Q.%20A%20unified%20framework%20for%20nonconvex%20low-rank%20plus%20sparse%20matrix%20recovery%202018-04-09"
        },
        {
            "id": "29",
            "entry": "[29] W. Zheng, A. Bellet, and P. Gallinari. A distributed frank-wolfe framework for learning low-rank matrices with the trace norm. arXiv preprint arXiv:1712.07495, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1712.07495"
        }
    ]
}
