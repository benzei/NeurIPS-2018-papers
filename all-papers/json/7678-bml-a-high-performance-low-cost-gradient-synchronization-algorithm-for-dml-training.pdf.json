{
    "filename": "7678-bml-a-high-performance-low-cost-gradient-synchronization-algorithm-for-dml-training.pdf",
    "metadata": {
        "title": "BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training",
        "author": "Songtao Wang, Dan Li, Yang Cheng, Jinkun Geng, Yanshu Wang, Shuai Wang, Shu-Tao Xia, Jianping Wu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7678-bml-a-high-performance-low-cost-gradient-synchronization-algorithm-for-dml-training.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In distributed machine learning (DML), the network performance between machines significantly impacts the speed of iterative training. In this paper we propose BML, a new gradient synchronization algorithm with higher network performance and lower network cost than the current practice. BML runs on BCube network, instead of using the traditional Fat-Tree topology. BML algorithm is designed in such a way that, compared to the parameter server (PS) algorithm on a"
    },
    "keywords": [
        {
            "term": "network performance",
            "url": "https://en.wikipedia.org/wiki/network_performance"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "low cost",
            "url": "https://en.wikipedia.org/wiki/low_cost"
        },
        {
            "term": "topology",
            "url": "https://en.wikipedia.org/wiki/topology"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "distributed machine learning Models and Notations: distributed machine learning can run on multiple CPUs/GPUs in a machine or on multiple machines",
        "The synchronization algorithm of BML is designed in such a way that, compared to the parameter server algorithm running a Fat-Tree network connecting the same number of server machines, BML",
        "N denotes the total number of server machines in a distributed machine learning network, P denotes the size of full gradients for the trained model, and TF denotes the theoretical time to transmit the full gradients by full link speed",
        "Since we only focus on the training speed of distributed machine learning, we fix the number of iterations trained in each round of the experiment as 1000 and measure the job completion time(JCT) of the benchmark.\n4.3",
        "BML runs on BCube topology instead of the commonly-used Fat-Tree network in current data centers",
        "Compared with the parameter server algorithm running on a Fat-Tree network connecting the same number of servers, BML\n1 k of of the"
    ],
    "key_statements": [
        "distributed machine learning Models and Notations: distributed machine learning can run on multiple CPUs/GPUs in a machine or on multiple machines",
        "The synchronization algorithm of BML is designed in such a way that, compared to the parameter server algorithm running a Fat-Tree network connecting the same number of server machines, BML",
        "The experiment results show that, BML can reduce the job completion time of distributed machine learning training by up to 56.4% compared with the parameter server algorithm on Fat-Tree network",
        "distributed machine learning Models and Notations: distributed machine learning can run on multiple CPUs/GPUs in a machine or on multiple machines",
        "Based on splitting whether the training data or the model parameters onto multiple machines, distributed machine learning can be divided into data-parallel and model-parallel ones",
        "In data-parallel distributed machine learning, each machine uses a shard of training data to compute the gradients; while in model-parallel distributed machine learning, a machine computes gradients for part of the model",
        "In this work we focus on data-parallel distributed machine learning",
        "In this work we focus on BSP synchronization, which is widely used in modern Machine learning applications [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "N denotes the total number of server machines in a distributed machine learning network, P denotes the size of full gradients for the trained model, and TF denotes the theoretical time to transmit the full gradients by full link speed",
        "Motivation of BML Design: Fat-Tree achieves great success in providing uniform network performance to cloud computing applications, in this paper we argue that Fat-Tree does not well match the traffic pattern of distributed machine learning training",
        "Since we only focus on the training speed of distributed machine learning, we fix the number of iterations trained in each round of the experiment as 1000 and measure the job completion time(JCT) of the benchmark.\n4.3",
        "In the experiments we find that in some cases BML can reduce the job completion time by more than 50%",
        "BML runs on BCube topology instead of the commonly-used Fat-Tree network in current data centers",
        "Compared with the parameter server algorithm running on a Fat-Tree network connecting the same number of servers, BML\n1 k of of the"
    ],
    "summary": [
        "DML Models and Notations: DML can run on multiple CPUs/GPUs in a machine or on multiple machines.",
        "Fat-Tree network connecting the same number of server machines, BML achieves theoretically",
        "The synchronization algorithm of BML is designed in such a way that, compared to the PS algorithm running a Fat-Tree network connecting the same number of server machines, BML",
        "The experiment results show that, BML can reduce the job completion time of DML training by up to 56.4% compared with the PS algorithm on Fat-Tree network.",
        "N denotes the total number of server machines in a DML network, P denotes the size of full gradients for the trained model, and TF denotes the theoretical time to transmit the full gradients by full link speed.",
        "When running the PS algorithm for gradient synchronization in Fat-Tree, it is flexible to place the parameter servers and workers.",
        "Each gradient synchronization thread on a server runs the algorithm in two stages, namely, aggregation stage and broadcast stage.",
        "An Example of BML Algorithm in BCube(3,2): As shown by Fig. 4(a), in a BCube(3,2) network every server runs two gradient synchronization threads.",
        "Every synchronization thread further exchanges its partially-aggregated gradient pieces with 1-hop neighboring servers in another level, i.e., thread 0 taking level-1 links and thread 1 taking level-0 links.",
        "In the first step of the broadcast stage, every thread on a server broadcasts its fully-aggregated gradient piece to 1-hop neighboring servers, as shown in Fig.4(d).",
        "Every thread broadcasts its 3 fully-aggregated gradient pieces to 1-hop neighboring servers in another level, namely, thread 0 taking level-0 links while thread 1 taking level-1 links.",
        "We can run BML-like hierarchical synchronization algorithm on a Fat-Tree network, where servers are first grouped by edge switches, grouped by aggregation switches, and grouped by pods.",
        "We use the Fig.5 to demonstrate the GSTs when running PS-based algorithm and BML on both Fat-Tree and BCube networks.",
        "We run the P2P based PS algorithm for gradient synchronization in Fat-Tree, where each server plays as both a parameter server and a worker.",
        "With smaller sub-minibatch size on a server, the performance gap between BML and Fat-Tree is larger, because the communication cost has a higher weight in the whole training job.",
        "BML runs on BCube topology instead of the commonly-used Fat-Tree network in current data centers.",
        "Compared with the PS algorithm running on a Fat-Tree network connecting the same number of servers, BML",
        "Compared with the PS algorithm running on a Fat-Tree network connecting the same number of while using only k 5 switches.<br/><br/>The experiments of typical deep learning benchmarks on Tensorflow validate the performance gains of BML"
    ],
    "headline": "In this paper we propose BML, a new gradient synchronization algorithm with higher network performance and lower network cost than the current practice",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] caffe2 website. http://caffe2.ai/.",
            "url": "http://caffe2.ai/"
        },
        {
            "id": "2",
            "entry": "[2] Imagenet dataset. http://www.image-net.org/.",
            "url": "http://www.image-net.org/"
        },
        {
            "id": "3",
            "entry": "[3] The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "4",
            "entry": "[4] M. Abadi, P. Barham, J. Chen, et al. Tensorflow: A system for large-scale machine learning. In USENIX OSDI\u201916, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "5",
            "entry": "[5] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center network architecture. In ACM SIGCOMM\u201908, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Al-Fares%2C%20M.%20Loukissas%2C%20A.%20Vahdat%2C%20A.%20A%20scalable%2C%20commodity%20data%20center%20network%20architecture%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Fares%2C%20M.%20Loukissas%2C%20A.%20Vahdat%2C%20A.%20A%20scalable%2C%20commodity%20data%20center%20network%20architecture%202008"
        },
        {
            "id": "6",
            "entry": "[6] T. Chen, M. Li, Y. Li, et al. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015. http://arxiv.org/abs/1512.01274.",
            "url": "http://arxiv.org/abs/1512.01274",
            "arxiv_url": "https://arxiv.org/pdf/1512.01274"
        },
        {
            "id": "7",
            "entry": "[7] H.-T. Cheng, L. Koc, J. Harmsen, et al. Wide & deep learning for recommender systems. In DLRS\u201916, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20H.-T.%20Koc%2C%20L.%20Harmsen%2C%20J.%20Wide%20%26%20deep%20learning%20for%20recommender%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20H.-T.%20Koc%2C%20L.%20Harmsen%2C%20J.%20Wide%20%26%20deep%20learning%20for%20recommender%20systems%202016"
        },
        {
            "id": "8",
            "entry": "[8] R. D. S. Couto, S. Secci, M. E. M. Campista, and L. H. M. K. Costa. Reliability and survivability analysis of data center network topologies. CoRR, abs/1510.02735, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.02735"
        },
        {
            "id": "9",
            "entry": "[9] W. Dai, A. Kumar, J. Wei, et al. High-performance distributed ml at scale through parameter server consistency models. In AAAI, pages 79\u201387, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20W.%20Kumar%2C%20A.%20Wei%2C%20J.%20High-performance%20distributed%20ml%20at%20scale%20through%20parameter%20server%20consistency%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20W.%20Kumar%2C%20A.%20Wei%2C%20J.%20High-performance%20distributed%20ml%20at%20scale%20through%20parameter%20server%20consistency%20models%202015"
        },
        {
            "id": "10",
            "entry": "[10] J. Dean, G. Corrado, et al. Large scale distributed deep networks. In NIPS\u201912.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20J.%20Corrado%2C%20G.%20Large%20scale%20distributed%20deep%20networks.%20In%20NIPS%E2%80%9912"
        },
        {
            "id": "11",
            "entry": "[11] P. Goyal, P. Dollar, R. B. Girshick, et al. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. http://arxiv.org/abs/1706.02677.",
            "url": "http://arxiv.org/abs/1706.02677",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "12",
            "entry": "[12] C. Guo, G. Lu, D. Li, et al. Bcube: A high performance, server-centric network architecture for modular data centers. In ACM SIGCOMM\u201909, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20C.%20Lu%2C%20G.%20Li%2C%20D.%20Bcube%3A%20A%20high%20performance%2C%20server-centric%20network%20architecture%20for%20modular%20data%20centers%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20C.%20Lu%2C%20G.%20Li%2C%20D.%20Bcube%3A%20A%20high%20performance%2C%20server-centric%20network%20architecture%20for%20modular%20data%20centers%202009"
        },
        {
            "id": "13",
            "entry": "[13] C. Guo, H. Wu, K. Tan, et al. Dcell: A scalable and fault-tolerant network structure for data centers. In ACM SIGCOMM\u201908, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guo%2C%20C.%20Wu%2C%20H.%20Tan%2C%20K.%20Dcell%3A%20A%20scalable%20and%20fault-tolerant%20network%20structure%20for%20data%20centers%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20C.%20Wu%2C%20H.%20Tan%2C%20K.%20Dcell%3A%20A%20scalable%20and%20fault-tolerant%20network%20structure%20for%20data%20centers%202008"
        },
        {
            "id": "14",
            "entry": "[14] K. Hazelwood, S. Bird, D. Brooks, et al. Applied machine learning at facebook: A datacenter infrastructure perspective. In IEEE HPCA\u201918, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazelwood%2C%20K.%20Bird%2C%20S.%20Brooks%2C%20D.%20Applied%20machine%20learning%20at%20facebook%3A%20A%20datacenter%20infrastructure%20perspective%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazelwood%2C%20K.%20Bird%2C%20S.%20Brooks%2C%20D.%20Applied%20machine%20learning%20at%20facebook%3A%20A%20datacenter%20infrastructure%20perspective%202018"
        },
        {
            "id": "15",
            "entry": "[15] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, Nov 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lecun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lecun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11"
        },
        {
            "id": "16",
            "entry": "[16] D. Li, C. Guo, H. Wu, et al. Scalable and cost-effective interconnection of data-center servers using dual server ports. IEEE/ACM Transactions on Networking (TON), 19(1):102\u2013114, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20D.%20Guo%2C%20C.%20Wu%2C%20H.%20Scalable%20and%20cost-effective%20interconnection%20of%20data-center%20servers%20using%20dual%20server%20ports%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20D.%20Guo%2C%20C.%20Wu%2C%20H.%20Scalable%20and%20cost-effective%20interconnection%20of%20data-center%20servers%20using%20dual%20server%20ports%202011"
        },
        {
            "id": "17",
            "entry": "[17] M. Li, D. G. Andersen, J. W. Park, et al. Scaling distributed machine learning with the parameter server. In USENIX OSDI\u201914, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20M.%20Andersen%2C%20D.G.%20Park%2C%20J.W.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20M.%20Andersen%2C%20D.G.%20Park%2C%20J.W.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "18",
            "entry": "[18] R. McDonald, K. Hall, and G. Mann. Distributed training strategies for the structured perceptron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910, pages 456\u2013464, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R%20McDonald%20K%20Hall%20and%20G%20Mann%20Distributed%20training%20strategies%20for%20the%20structured%20perceptron%20In%20Human%20Language%20Technologies%20The%202010%20Annual%20Conference%20of%20the%20North%20American%20Chapter%20of%20the%20Association%20for%20Computational%20Linguistics%20HLT%2010%20pages%20456464%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R%20McDonald%20K%20Hall%20and%20G%20Mann%20Distributed%20training%20strategies%20for%20the%20structured%20perceptron%20In%20Human%20Language%20Technologies%20The%202010%20Annual%20Conference%20of%20the%20North%20American%20Chapter%20of%20the%20Association%20for%20Computational%20Linguistics%20HLT%2010%20pages%20456464%202010"
        },
        {
            "id": "19",
            "entry": "[19] P. Patarasuk and X. Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. Journal of Parallel and Distributed Computing, 69(2):117\u2013124, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patarasuk%2C%20P.%20Yuan%2C%20X.%20Bandwidth%20optimal%20all-reduce%20algorithms%20for%20clusters%20of%20workstations%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patarasuk%2C%20P.%20Yuan%2C%20X.%20Bandwidth%20optimal%20all-reduce%20algorithms%20for%20clusters%20of%20workstations%202009"
        },
        {
            "id": "20",
            "entry": "[20] D. E. RUMELHART. Learning representations by back propagation error. Nature, 323:533\u2013 536, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=RUMELHART%2C%20D.E.%20Learning%20representations%20by%20back%20propagation%20error%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=RUMELHART%2C%20D.E.%20Learning%20representations%20by%20back%20propagation%20error%201986"
        },
        {
            "id": "21",
            "entry": "[21] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696\u2013699. MIT Press, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Neurocomputing%3A%20Foundations%20of%20research.%20chapter%20Learning%20Representations%20by%20Back-propagating%20Errors%201988"
        },
        {
            "id": "22",
            "entry": "[22] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. http://arxiv.org/abs/1409.1556.",
            "url": "http://arxiv.org/abs/1409.1556",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "23",
            "entry": "[23] A. Smola. Machine learning - progress and opportunities. Speech on AI World 2017, 2017. https://goo.gl/emn8np.",
            "url": "https://goo.gl/emn8np"
        },
        {
            "id": "24",
            "entry": "[24] Y. You, Z. Zhang, C. Hsieh, et al. 100-epoch imagenet training with alexnet in 24 minutes. arXiv preprint arXiv:1709.05011, 2017. http://arxiv.org/abs/1709.05011.",
            "url": "http://arxiv.org/abs/1709.05011",
            "arxiv_url": "https://arxiv.org/pdf/1709.05011"
        },
        {
            "id": "25",
            "entry": "[25] H. Zhang, Z. Zheng, S. Xu, et al. Poseidon: An efficient communication architecture for distributed deep learning on gpu clusters. USENIX ATC\u201917, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Zheng%2C%20Z.%20Xu%2C%20S.%20Poseidon%3A%20An%20efficient%20communication%20architecture%20for%20distributed%20deep%20learning%20on%20gpu%20clusters%202017"
        },
        {
            "id": "26",
            "entry": "[26] H. Zhao and J. Canny. Butterfly mixing: Accelerating incremental-update algorithms on clusters. In SIAM SDM\u201913, pages 785\u2013793. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20H.%20Canny%2C%20J.%20Butterfly%20mixing%3A%20Accelerating%20incremental-update%20algorithms%20on%20clusters",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20H.%20Canny%2C%20J.%20Butterfly%20mixing%3A%20Accelerating%20incremental-update%20algorithms%20on%20clusters"
        }
    ]
}
