{
    "filename": "8243-learning-abstract-options.pdf",
    "metadata": {
        "title": "Learning Abstract Options",
        "author": "Matthew Riemer, Miao Liu, Gerald Tesauro",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8243-learning-abstract-options.pdf"
        },
        "abstract": "Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]. However, only recently in [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals. Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework."
    },
    "keywords": [
        {
            "term": "general purpose",
            "url": "https://en.wikipedia.org/wiki/general_purpose"
        },
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "policy gradient method",
            "url": "https://en.wikipedia.org/wiki/policy_gradient_method"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "primitive action",
            "url": "https://en.wikipedia.org/wiki/primitive_action"
        }
    ],
    "highlights": [
        "In reinforcement learning (RL), options [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] provide a general framework for defining temporally abstract courses of action for learning and planning",
        "Discovering these temporal abstractions autonomously has been the subject of extensive research [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] with approaches that can be used in continuous state and/or action spaces only recently becoming feasible [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Recent work on option-critic learning blurs the line between option discovery and option learning by providing policy gradient theorems for optimizing a two-level hierarchy of options and primitive actions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We extend option-critic to a novel hierarchical option-critic framework, presenting generalized policy gradient theorems that can be applied to an arbitrarily deep hierarchy of options",
        "In this work we propose the first policy gradient theorems to optimize an arbitrarily deep hierarchy of options to maximize the expected discounted return",
        "The architectures we explore in this paper have a fixed structure and fixed depth of abstraction for simplicity, the underlying theorems can guide learning for much more dynamic architectures that we hope to explore in future work"
    ],
    "key_statements": [
        "In reinforcement learning (RL), options [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] provide a general framework for defining temporally abstract courses of action for learning and planning",
        "Discovering these temporal abstractions autonomously has been the subject of extensive research [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] with approaches that can be used in continuous state and/or action spaces only recently becoming feasible [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Recent work on option-critic learning blurs the line between option discovery and option learning by providing policy gradient theorems for optimizing a two-level hierarchy of options and primitive actions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "These approaches have achieved success when applied to Q-learning on Atari games, but in continuous action spaces [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] and with asynchronous parallelization [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "We extend option-critic to a novel hierarchical option-critic framework, presenting generalized policy gradient theorems that can be applied to an arbitrarily deep hierarchy of options",
        "In this work we propose the first policy gradient theorems to optimize an arbitrarily deep hierarchy of options to maximize the expected discounted return",
        "The architectures we explore in this paper have a fixed structure and fixed depth of abstraction for simplicity, the underlying theorems can guide learning for much more dynamic architectures that we hope to explore in future work"
    ],
    "summary": [
        "In reinforcement learning (RL), options [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] provide a general framework for defining temporally abstract courses of action for learning and planning.",
        "Recent work on option-critic learning blurs the line between option discovery and option learning by providing policy gradient theorems for optimizing a two-level hierarchy of options and primitive actions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "We will start by reviewing related research and by describing the seminal work we build upon in this paper that first derived policy gradient theorems for learning with primitive actions [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] and options [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "We are the first to propose theorems along with a practical algorithm and architecture to train arbitrarily deep hierarchies of options end to end using policy gradients, maximizing the expected return.",
        "The option-critic architecture optimizes directly for the discounted return expected over trajectories starting at a designated state s0 and option o0: \u03c1(\u03a9, \u03b8 , \u03c6 , s0, o0) = E\u03a9,\u03c0\u03b8 ,\u03b2\u03c6 [\u2211t\u221e=0 \u03b3t rt+1|s0, o0].",
        "The hierarchical options framework that we introduce in this work considers an agent that learns using an N level hierarchy of policies, termination functions, and value functions.",
        "To enable comprehensive reasoning about the policies at each level of the option hierarchy, we need to maintain value functions that consider the state and every possible combination of active options and actions V\u03a9(s), Q\u03a9(s, o1), ..., Q\u03a9(s, o1:N\u22121, a).",
        "Like policy gradient methods and option-critic, the hierarchical options framework optimizes directly for the discounted return expected over all trajectories starting at a state s0 with active options o01:N\u22121:",
        "We consider the option value function for understanding reasoning about an option o at level 1 \u2264 \u2264 N based on the augmented state space (s, o1: \u22121):",
        "We seek to generalize the intra-option policy gradients theorem, deriving the update rule for a policy at an arbitrary level of abstraction \u03c0 by taking the gradient with respect to \u03b8 using the value function with the same augmented state space Q\u03a9(s, o1: \u22121).",
        "Our primitive action agents conduct A3C training using a convolutional network when Figure 3: Learning performance as a function of the there is image input followed by an LSTM abstraction level for a nonstationary four rooms domain to contextualize the state.",
        "In this work we propose the first policy gradient theorems to optimize an arbitrarily deep hierarchy of options to maximize the expected discounted return.",
        "While the performance of the hierarchical option-critic architecture is impressive, we envision our proposed policy gradient theorems eventually transcending it in overall impact.",
        "The architectures we explore in this paper have a fixed structure and fixed depth of abstraction for simplicity, the underlying theorems can guide learning for much more dynamic architectures that we hope to explore in future work"
    ],
    "headline": "We extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bacon%2C%20Pierre-Luc%20Harb%2C%20Jean%20Precup%2C%20Doina%20The%20option-critic%20architecture%202017"
        },
        {
            "id": "2",
            "entry": "[2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, jun 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-06"
        },
        {
            "id": "3",
            "entry": "[3] Christian Daniel, Herke Van Hoof, Jan Peters, and Gerhard Neumann. Probabilistic inference for determining options in reinforcement learning. Machine Learning, 104(2-3):337\u2013357, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20Christian%20Hoof%2C%20Herke%20Van%20Peters%2C%20Jan%20Neumann%2C%20Gerhard%20Probabilistic%20inference%20for%20determining%20options%20in%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%2C%20Christian%20Hoof%2C%20Herke%20Van%20Peters%2C%20Jan%20Neumann%2C%20Gerhard%20Probabilistic%20inference%20for%20determining%20options%20in%20reinforcement%20learning%202016"
        },
        {
            "id": "4",
            "entry": "[4] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural information processing systems, pages 271\u2013278, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20Peter%20Hinton%2C%20Geoffrey%20E.%20Feudal%20reinforcement%20learning%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20Peter%20Hinton%2C%20Geoffrey%20E.%20Feudal%20reinforcement%20learning%201993"
        },
        {
            "id": "5",
            "entry": "[5] Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level discovery of deep options. arXiv preprint arXiv:1703.08294, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.08294"
        },
        {
            "id": "6",
            "entry": "[6] Jean Harb, Pierre-Luc Bacon, Martin Klissarov, and Doina Precup. When waiting is not an option: Learning options with a deliberation cost. arXiv preprint arXiv:1709.04571, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04571"
        },
        {
            "id": "7",
            "entry": "[7] Martin Klissarov, Pierre-Luc Bacon, Jean Harb, and Doina Precup. Learnings options end-to-end for continuous action tasks. arXiv preprint arXiv:1712.00004, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00004"
        },
        {
            "id": "8",
            "entry": "[8] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pages 1008\u20131014, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konda%2C%20Vijay%20R.%20Tsitsiklis%2C%20John%20N.%20Actor-critic%20algorithms%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konda%2C%20Vijay%20R.%20Tsitsiklis%2C%20John%20N.%20Actor-critic%20algorithms%202000"
        },
        {
            "id": "9",
            "entry": "[9] George Konidaris, Scott Kuindersma, Roderic A Grupen, and Andrew G Barto. Autonomous skill acquisition on a mobile manipulator. In AAAI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konidaris%2C%20George%20Kuindersma%2C%20Scott%20Grupen%2C%20Roderic%20A.%20Barto%2C%20Andrew%20G.%20Autonomous%20skill%20acquisition%20on%20a%20mobile%20manipulator%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konidaris%2C%20George%20Kuindersma%2C%20Scott%20Grupen%2C%20Roderic%20A.%20Barto%2C%20Andrew%20G.%20Autonomous%20skill%20acquisition%20on%20a%20mobile%20manipulator%202011"
        },
        {
            "id": "10",
            "entry": "[10] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pages 3675\u20133683, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kulkarni%2C%20Tejas%20D.%20Narasimhan%2C%20Karthik%20Saeedi%2C%20Ardavan%20Tenenbaum%2C%20Josh%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kulkarni%2C%20Tejas%20D.%20Narasimhan%2C%20Karthik%20Saeedi%2C%20Ardavan%20Tenenbaum%2C%20Josh%20Hierarchical%20deep%20reinforcement%20learning%3A%20Integrating%20temporal%20abstraction%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "11",
            "entry": "[11] Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00948"
        },
        {
            "id": "12",
            "entry": "[12] Kfir Y Levy and Nahum Shimkin. Unified inter and intra options learning using policy gradient methods. In European Workshop on Reinforcement Learning, pages 153\u2013164.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20Kfir%20Y.%20Shimkin%2C%20Nahum%20Unified%20inter%20and%20intra%20options%20learning%20using%20policy%20gradient%20methods",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20Kfir%20Y.%20Shimkin%2C%20Nahum%20Unified%20inter%20and%20intra%20options%20learning%20using%20policy%20gradient%20methods"
        },
        {
            "id": "13",
            "entry": "[13] Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. arXiv preprint arXiv:1709.06009, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06009"
        },
        {
            "id": "14",
            "entry": "[14] Daniel J Mankowitz, Timothy A Mann, and Shie Mannor. Adaptive skills adaptive partitions (asap). In Advances in Neural Information Processing Systems, pages 1588\u20131596, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mankowitz%2C%20Daniel%20J.%20Mann%2C%20Timothy%20A.%20Mannor%2C%20Shie%20Adaptive%20skills%20adaptive%20partitions%20%28asap%29%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mankowitz%2C%20Daniel%20J.%20Mann%2C%20Timothy%20A.%20Mannor%2C%20Shie%20Adaptive%20skills%20adaptive%20partitions%20%28asap%29%202016"
        },
        {
            "id": "15",
            "entry": "[15] Timothy A Mann, Shie Mannor, and Doina Precup. Approximate value iteration with temporally extended actions. Journal of Artificial Intelligence Research, 53:375\u2013438, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mann%2C%20Timothy%20A.%20Mannor%2C%20Shie%20Precup%2C%20Doina%20Approximate%20value%20iteration%20with%20temporally%20extended%20actions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mann%2C%20Timothy%20A.%20Mannor%2C%20Shie%20Precup%2C%20Doina%20Approximate%20value%20iteration%20with%20temporally%20extended%20actions%202015"
        },
        {
            "id": "16",
            "entry": "[16] Amy McGovern and Andrew G Barto. Automatic discovery of subgoals in reinforcement learning using diverse density. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 361\u2013368. Morgan Kaufmann Publishers Inc., 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McGovern%2C%20Amy%20Barto%2C%20Andrew%20G.%20Automatic%20discovery%20of%20subgoals%20in%20reinforcement%20learning%20using%20diverse%20density%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McGovern%2C%20Amy%20Barto%2C%20Andrew%20G.%20Automatic%20discovery%20of%20subgoals%20in%20reinforcement%20learning%20using%20diverse%20density%202001"
        },
        {
            "id": "17",
            "entry": "[17] Ishai Menache, Shie Mannor, and Nahum Shimkin. Q-cut\u2014dynamic discovery of sub-goals in reinforcement learning. In European Conference on Machine Learning, pages 295\u2013306.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menache%2C%20Ishai%20Mannor%2C%20Shie%20Shimkin%2C%20Nahum%20Q-cut%E2%80%94dynamic%20discovery%20of%20sub-goals%20in%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menache%2C%20Ishai%20Mannor%2C%20Shie%20Shimkin%2C%20Nahum%20Q-cut%E2%80%94dynamic%20discovery%20of%20sub-goals%20in%20reinforcement%20learning"
        },
        {
            "id": "18",
            "entry": "[18] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "19",
            "entry": "[19] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "20",
            "entry": "[20] Scott D Niekum. Semantically grounded learning from unstructured demonstrations. University of Massachusetts Amherst, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niekum%2C%20Scott%20D.%20Semantically%20grounded%20learning%20from%20unstructured%20demonstrations%202013"
        },
        {
            "id": "21",
            "entry": "[21] Doina Precup. Temporal abstraction in reinforcement learning. University of Massachusetts Amherst, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Precup%2C%20Doina%20Temporal%20abstraction%20in%20reinforcement%20learning%202000"
        },
        {
            "id": "22",
            "entry": "[22] Martin L Puterman. Markov decision processes: Discrete dynamic stochastic programming, 92\u201393, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20Martin%20L.%20Markov%20decision%20processes%3A%20Discrete%20dynamic%20stochastic%20programming%201994"
        },
        {
            "id": "23",
            "entry": "[23] Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and Charles Isbell. Learning to compose skills. arXiv preprint arXiv:1711.11289, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11289"
        },
        {
            "id": "24",
            "entry": "[24] S. Sharma, A. Jha, P. Hegde, and B. Ravindran. Learning to multi-task by active sampling. arXiv preprint arXiv:1702.06053, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06053"
        },
        {
            "id": "25",
            "entry": "[25] Tianmin Shu, Caiming Xiong, and Richard Socher. Hierarchical and interpretable skill acquisition in multi-task reinforcement learning. arXiv preprint arXiv:1712.07294, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07294"
        },
        {
            "id": "26",
            "entry": "[26] David Silver and Kamil Ciosek. Compositional planning using optimal option models. arXiv preprint arXiv:1206.6473, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6473"
        },
        {
            "id": "27",
            "entry": "[27] \u00d6zg\u00fcr Simsek and Andrew G Barto. Skill characterization based on betweenness. In Advances in neural information processing systems, pages 1497\u20131504, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simsek%2C%20%C3%96zg%C3%BCr%20Barto%2C%20Andrew%20G.%20Skill%20characterization%20based%20on%20betweenness%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simsek%2C%20%C3%96zg%C3%BCr%20Barto%2C%20Andrew%20G.%20Skill%20characterization%20based%20on%20betweenness%202009"
        },
        {
            "id": "28",
            "entry": "[28] Martin Stolle and Doina Precup. Learning options in reinforcement learning. In International Symposium on abstraction, reformulation, and approximation, pages 212\u2013223.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stolle%2C%20Martin%20Precup%2C%20Doina%20Learning%20options%20in%20reinforcement%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stolle%2C%20Martin%20Precup%2C%20Doina%20Learning%20options%20in%20reinforcement%20learning"
        },
        {
            "id": "29",
            "entry": "[29] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2): 181\u2013211, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Precup%2C%20Doina%20Singh%2C%20Satinder%20Between%20mdps%20and%20semi-mdps%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning%201999"
        },
        {
            "id": "30",
            "entry": "[30] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "31",
            "entry": "[31] Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, et al. Strategic attentive writer for learning macro-actions. In Advances in neural information processing systems, pages 3486\u20133494, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vezhnevets%2C%20Alexander%20Mnih%2C%20Volodymyr%20Osindero%2C%20Simon%20Graves%2C%20Alex%20Strategic%20attentive%20writer%20for%20learning%20macro-actions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vezhnevets%2C%20Alexander%20Mnih%2C%20Volodymyr%20Osindero%2C%20Simon%20Graves%2C%20Alex%20Strategic%20attentive%20writer%20for%20learning%20macro-actions%202016"
        },
        {
            "id": "32",
            "entry": "[32] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1703.01161"
        }
    ]
}
