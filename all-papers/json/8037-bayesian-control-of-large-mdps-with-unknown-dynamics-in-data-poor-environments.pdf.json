{
    "filename": "8037-bayesian-control-of-large-mdps-with-unknown-dynamics-in-data-poor-environments.pdf",
    "metadata": {
        "title": "Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments",
        "author": "Mahdi Imani, Seyede Fatemeh Ghoreishi, Ulisses M. Braga-Neto",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8037-bayesian-control-of-large-mdps-with-unknown-dynamics-in-data-poor-environments.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose a Bayesian decision making framework for control of Markov Decision Processes (MDPs) with unknown dynamics and large, possibly continuous, state, action, and parameter spaces in data-poor environments. Most of the existing adaptive controllers for MDPs with unknown dynamics are based on the reinforcement learning framework and rely on large data sets acquired by sustained direct interaction with the system or via a simulator. This is not feasible in many applications, due to ethical, economic, and physical constraints. The proposed framework addresses the data poverty issue by decomposing the problem into an offline planning stage that does not rely on sustained direct interaction with the system or simulator and an online execution stage. In the offline process, parallel Gaussian process temporal difference (GPTD) learning techniques are employed for near-optimal Bayesian approximation of the expected discounted reward over a sample drawn from the prior distribution of unknown parameters. In the online stage, the action with the maximum expected return with respect to the posterior distribution of the parameters is selected. This is achieved by an approximation of the posterior distribution using a Markov Chain Monte Carlo (MCMC) algorithm, followed by constructing multiple Gaussian processes over the parameter space for efficient prediction of the means of the expected return at the MCMC sample. The effectiveness of the proposed framework is demonstrated using a simple dynamical system model with continuous state and action spaces, as well as a more complex model for a metastatic melanoma gene regulatory network observed through noisy synthetic gene expression data."
    },
    "keywords": [
        {
            "term": "Markov Decision Processes",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Processes"
        },
        {
            "term": "MCMC",
            "url": "https://en.wikipedia.org/wiki/MCMC"
        },
        {
            "term": "Dynamic programming",
            "url": "https://en.wikipedia.org/wiki/Dynamic_programming"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "markov chain",
            "url": "https://en.wikipedia.org/wiki/markov_chain"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "parameter space",
            "url": "https://en.wikipedia.org/wiki/parameter_space"
        }
    ],
    "highlights": [
        "These include parametric and nonparametric reinforcement learning (RL) techniques for approximating the expected discounted reward over large or continuous state and action spaces",
        "We propose to do this by using Gaussian process temporal difference (GPTD) learning [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "Let Q\u03b8(s, a) be the Gaussian process approximating the optimal Q-function for any s \u2208 S and a \u2208 A computed by a GP-SARSA algorithm associated with parameter \u03b8 \u2208 \u0398prior",
        "It can be seen that only a single offline sample point is in the area covered by the Markov Chain Monte-Carlo samples, which illustrates the advantage of the constructed Gaussian process for predicting the expected return over the posterior distribution",
        "The numerical experiments compare the performance of the proposed framework with two other methods: 1) Multiple Model-based reinforcement learning (MMRL) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]: the parameter space in this method is quantized into a finite set \u0398quant according to its prior distribution and the results of offline parallel GP-SARSA algorithms associated with this set are used for decision making during the execution process via: akMMRL = argmaxa\u2208A \u03b8\u2208\u0398quant Q\u03b8P (\u03b8 | s0:k, a0:k\u22121). 2) One-step lookahead policy [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]: this method selects the action with the highest expected immediate reward: akseq = argmaxa\u2208A E\u03b8|s0:k,a0:k\u22121 [R]",
        "We introduced a Bayesian decision making framework for control of Markov Decision Processes with unknown dynamics and large or continuous state, actions and parameter spaces in data-poor environments"
    ],
    "key_statements": [
        "These include parametric and nonparametric reinforcement learning (RL) techniques for approximating the expected discounted reward over large or continuous state and action spaces",
        "We propose to do this by using Gaussian process temporal difference (GPTD) learning [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "Let Q\u03b8(s, a) be the Gaussian process approximating the optimal Q-function for any s \u2208 S and a \u2208 A computed by a GP-SARSA algorithm associated with parameter \u03b8 \u2208 \u0398prior",
        "Let fsak = [Q\u03b81prior, . . . , Q\u03b8N prpiorrior]T and vsak = [cov\u03b81prior(,), . . ., cov\u03b8N prpiorrior(,)]T be the means and variances of the predicted expected returns computed based on the results of offline GP-SARSAs at current state sk for a given action a \u2208 A",
        "It can be seen that only a single offline sample point is in the area covered by the Markov Chain Monte-Carlo samples, which illustrates the advantage of the constructed Gaussian process for predicting the expected return over the posterior distribution",
        "The numerical experiments compare the performance of the proposed framework with two other methods: 1) Multiple Model-based reinforcement learning (MMRL) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]: the parameter space in this method is quantized into a finite set \u0398quant according to its prior distribution and the results of offline parallel GP-SARSA algorithms associated with this set are used for decision making during the execution process via: akMMRL = argmaxa\u2208A \u03b8\u2208\u0398quant Q\u03b8P (\u03b8 | s0:k, a0:k\u22121). 2) One-step lookahead policy [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]: this method selects the action with the highest expected immediate reward: akseq = argmaxa\u2208A E\u03b8|s0:k,a0:k\u22121 [R]",
        "We introduced a Bayesian decision making framework for control of Markov Decision Processes with unknown dynamics and large or continuous state, actions and parameter spaces in data-poor environments"
    ],
    "summary": [
        "These include parametric and nonparametric reinforcement learning (RL) techniques for approximating the expected discounted reward over large or continuous state and action spaces.",
        "Main Contributions: The goal of this paper is to develop a framework for Bayesian decision making for MDPs with unknown dynamics and large or continuous state, action and parameter spaces in data-poor environments.",
        "Parallel Gaussian process temporal difference (GPTD) learning algorithms are applied for Bayesian approximation of the expected discounted reward associated with these parameter samples.",
        "For an MDP with known dynamics and finite state and action spaces, planning algorithms such as Value Iteration or Policy Iteration [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] can be used to compute the optimal policy offline.",
        "Let Q\u03b8(s, a) be the Gaussian process approximating the optimal Q-function for any s \u2208 S and a \u2208 A computed by a GP-SARSA algorithm associated with parameter \u03b8 \u2208 \u0398prior.",
        ". ., cov\u03b8N prpiorrior(,)]T be the means and variances of the predicted expected returns computed based on the results of offline GP-SARSAs at current state sk for a given action a \u2208 A.",
        "It can be seen that only a single offline sample point is in the area covered by the MCMC samples, which illustrates the advantage of the constructed Gaussian process for predicting the expected return over the posterior distribution.",
        "The numerical experiments compare the performance of the proposed framework with two other methods: 1) Multiple Model-based RL (MMRL) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]: the parameter space in this method is quantized into a finite set \u0398quant according to its prior distribution and the results of offline parallel GP-SARSA algorithms associated with this set are used for decision making during the execution process via: akMMRL = argmaxa\u2208A \u03b8\u2208\u0398quant Q\u03b8P (\u03b8 | s0:k, a0:k\u22121).",
        "We introduced a Bayesian decision making framework for control of MDPs with unknown dynamics and large or continuous state, actions and parameter spaces in data-poor environments.",
        "The proposed framework does not require sustained direct interaction with the system or a simulator, but instead it plans offline over a finite sample of parameters from a prior distribution over the parameter space and transfers this knowledge efficiently to sample parameters from the posterior during the execution process.",
        "The methodology offers several benefits, including the possibility of handling large and possibly continuous state, action, and parameter spaces; data-poor environments; anytime planning; and dealing with risk in the decision making process."
    ],
    "headline": "We propose a Bayesian decision making framework for control of Markov Decision Processes  with unknown dynamics and large, possibly continuous, state, action, and parameter spaces in data-poor environments",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "2",
            "entry": "[2] A. Antos, C. Szepesv\u00e1ri, and R. Munos, \u201cFitted Q-iteration in continuous action-space MDPs,\u201d in Advances in neural information processing systems, pp. 9\u201316, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antos%2C%20A.%20Szepesv%C3%A1ri%2C%20C.%20Munos%2C%20R.%20%E2%80%9CFitted%20Q-iteration%20in%20continuous%20action-space%20MDPs%2C%E2%80%9D%20in%20Advances%20in%20neural%20information%20processing%20systems%202008"
        },
        {
            "id": "3",
            "entry": "[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, no. 7540, p. 529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%2C%202015"
        },
        {
            "id": "4",
            "entry": "[4] L. Busoniu, R. Babuska, B. De Schutter, and D. Ernst, Reinforcement learning and dynamic programming using function approximators, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Busoniu%2C%20L.%20Babuska%2C%20R.%20Schutter%2C%20B.De%20Ernst%2C%20D.%20Reinforcement%20learning%20and%20dynamic%20programming%20using%20function%20approximators"
        },
        {
            "id": "39",
            "entry": "39. CRC press, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CRC%20press%202010"
        },
        {
            "id": "5",
            "entry": "[5] Y. Engel, S. Mannor, and R. Meir, \u201cReinforcement learning with Gaussian processes,\u201d in Proceedings of the 22nd international conference on Machine learning, pp. 201\u2013208, ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Y.%20Mannor%2C%20S.%20Meir%2C%20R.%20Reinforcement%20learning%20with%20Gaussian%20processes%2C%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engel%2C%20Y.%20Mannor%2C%20S.%20Meir%2C%20R.%20Reinforcement%20learning%20with%20Gaussian%20processes%2C%202005"
        },
        {
            "id": "6",
            "entry": "[6] K. Doya, K. Samejima, K.-i. Katagiri, and M. Kawato, \u201cMultiple model-based reinforcement learning,\u201d Neural computation, vol. 14, no. 6, pp. 1347\u20131369, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doya%2C%20K.%20Samejima%2C%20K.%20Katagiri%2C%20K.-i%20Kawato%2C%20M.%20Multiple%20model-based%20reinforcement%20learning%2C%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doya%2C%20K.%20Samejima%2C%20K.%20Katagiri%2C%20K.-i%20Kawato%2C%20M.%20Multiple%20model-based%20reinforcement%20learning%2C%202002"
        },
        {
            "id": "7",
            "entry": "[7] M. Ghavamzadeh, S. Mannor, J. Pineau, A. Tamar, et al., \u201cBayesian reinforcement learning: A survey,\u201d Foundations and Trends R in Machine Learning, vol. 8, no. 5-6, pp. 359\u2013483, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghavamzadeh%2C%20M.%20Mannor%2C%20S.%20Pineau%2C%20J.%20Tamar%2C%20A.%20Bayesian%20reinforcement%20learning%3A%20A%20survey%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghavamzadeh%2C%20M.%20Mannor%2C%20S.%20Pineau%2C%20J.%20Tamar%2C%20A.%20Bayesian%20reinforcement%20learning%3A%20A%20survey%2C%202015"
        },
        {
            "id": "8",
            "entry": "[8] P. Poupart, N. Vlassis, J. Hoey, and K. Regan, \u201cAn analytic solution to discrete Bayesian reinforcement learning,\u201d in Proceedings of the 23rd international conference on Machine learning, pp. 697\u2013704, ACM, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poupart%2C%20P.%20Vlassis%2C%20N.%20Hoey%2C%20J.%20Regan%2C%20K.%20An%20analytic%20solution%20to%20discrete%20Bayesian%20reinforcement%20learning%2C%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poupart%2C%20P.%20Vlassis%2C%20N.%20Hoey%2C%20J.%20Regan%2C%20K.%20An%20analytic%20solution%20to%20discrete%20Bayesian%20reinforcement%20learning%2C%202006"
        },
        {
            "id": "9",
            "entry": "[9] A. Guez, D. Silver, and P. Dayan, \u201cEfficient Bayes-adaptive reinforcement learning using sample-based search,\u201d in Advances in Neural Information Processing Systems, pp. 1025\u20131033, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guez%2C%20A.%20Silver%2C%20D.%20Dayan%2C%20P.%20Efficient%20Bayes-adaptive%20reinforcement%20learning%20using%20sample-based%20search%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guez%2C%20A.%20Silver%2C%20D.%20Dayan%2C%20P.%20Efficient%20Bayes-adaptive%20reinforcement%20learning%20using%20sample-based%20search%2C%202012"
        },
        {
            "id": "10",
            "entry": "[10] J. Asmuth and M. L. Littman, \u201cLearning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search,\u201d arXiv preprint arXiv:1202.3699, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1202.3699"
        },
        {
            "id": "11",
            "entry": "[11] Y. Wang, K. S. Won, D. Hsu, and W. S. Lee, \u201cMonte Carlo Bayesian reinforcement learning,\u201d arXiv preprint arXiv:1206.6449, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6449"
        },
        {
            "id": "12",
            "entry": "[12] N. A. Vien, W. Ertel, V.-H. Dang, and T. Chung, \u201cMonte-Carlo tree search for Bayesian reinforcement learning,\u201d Applied intelligence, vol. 39, no. 2, pp. 345\u2013353, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vien%2C%20N.A.%20Ertel%2C%20W.%20Dang%2C%20V.-H.%20Chung%2C%20T.%20Monte-Carlo%20tree%20search%20for%20Bayesian%20reinforcement%20learning%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vien%2C%20N.A.%20Ertel%2C%20W.%20Dang%2C%20V.-H.%20Chung%2C%20T.%20Monte-Carlo%20tree%20search%20for%20Bayesian%20reinforcement%20learning%2C%202013"
        },
        {
            "id": "13",
            "entry": "[13] S. Ross and J. Pineau, \u201cModel-based Bayesian reinforcement learning in large structured domains,\u201d in Uncertainty in artificial intelligence: proceedings of the... conference. Conference on Uncertainty in Artificial Intelligence, vol. 2008, p. 476, NIH Public Access, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ross%2C%20S.%20Pineau%2C%20J.%20Model-based%20Bayesian%20reinforcement%20learning%20in%20large%20structured%20domains%2C%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ross%2C%20S.%20Pineau%2C%20J.%20Model-based%20Bayesian%20reinforcement%20learning%20in%20large%20structured%20domains%2C%202008"
        },
        {
            "id": "14",
            "entry": "[14] T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans, \u201cBayesian sparse sampling for on-line reward optimization,\u201d in Proceedings of the 22nd international conference on Machine learning, pp. 956\u2013963, ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20T.%20Lizotte%2C%20D.%20Bowling%2C%20M.%20Schuurmans%2C%20D.%20Bayesian%20sparse%20sampling%20for%20on-line%20reward%20optimization%2C%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20T.%20Lizotte%2C%20D.%20Bowling%2C%20M.%20Schuurmans%2C%20D.%20Bayesian%20sparse%20sampling%20for%20on-line%20reward%20optimization%2C%202005"
        },
        {
            "id": "15",
            "entry": "[15] R. Fonteneau, L. Busoniu, and R. Munos, \u201cOptimistic planning for belief-augmented Markov decision processes,\u201d in Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2013 IEEE Symposium on, pp. 77\u201384, IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fonteneau%2C%20R.%20Busoniu%2C%20L.%20Munos%2C%20R.%20%E2%80%9COptimistic%20planning%20for%20belief-augmented%20Markov%20decision%20processes%2C%E2%80%9D%20in%20Adaptive%20Dynamic%20Programming%20And%20Reinforcement%20Learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fonteneau%2C%20R.%20Busoniu%2C%20L.%20Munos%2C%20R.%20%E2%80%9COptimistic%20planning%20for%20belief-augmented%20Markov%20decision%20processes%2C%E2%80%9D%20in%20Adaptive%20Dynamic%20Programming%20And%20Reinforcement%20Learning%202013"
        },
        {
            "id": "16",
            "entry": "[16] A. Guez, D. Silver, and P. Dayan, \u201cScalable and efficient Bayes-adaptive reinforcement learning based on Monte-Carlo tree search,\u201d Journal of Artificial Intelligence Research, vol. 48, pp. 841\u2013883, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guez%2C%20A.%20Silver%2C%20D.%20Dayan%2C%20P.%20Scalable%20and%20efficient%20Bayes-adaptive%20reinforcement%20learning%20based%20on%20Monte-Carlo%20tree%20search%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guez%2C%20A.%20Silver%2C%20D.%20Dayan%2C%20P.%20Scalable%20and%20efficient%20Bayes-adaptive%20reinforcement%20learning%20based%20on%20Monte-Carlo%20tree%20search%2C%202013"
        },
        {
            "id": "17",
            "entry": "[17] W. B. Powell and I. O. Ryzhov, Optimal learning, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=W%20B%20Powell%20and%20I%20O%20Ryzhov%20Optimal%20learning%20vol",
            "oa_query": "https://api.scholarcy.com/oa_version?query=W%20B%20Powell%20and%20I%20O%20Ryzhov%20Optimal%20learning%20vol"
        },
        {
            "id": "841",
            "entry": "841. John Wiley & Sons, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=John%20Wiley%20%20Sons%202012"
        },
        {
            "id": "18",
            "entry": "[18] N. Drougard, F. Teichteil-K\u00f6nigsbuch, J.-L. Farges, and D. Dubois, \u201cStructured possibilistic planning using decision diagrams.,\u201d in AAAI, pp. 2257\u20132263, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drougard%2C%20N.%20Teichteil-K%C3%B6nigsbuch%2C%20F.%20Farges%2C%20J.-L.%20Dubois%2C%20D.%20Structured%20possibilistic%20planning%20using%20decision%20diagrams.%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drougard%2C%20N.%20Teichteil-K%C3%B6nigsbuch%2C%20F.%20Farges%2C%20J.-L.%20Dubois%2C%20D.%20Structured%20possibilistic%20planning%20using%20decision%20diagrams.%2C%202014"
        },
        {
            "id": "19",
            "entry": "[19] F. W. Trevizan, F. G. Cozman, and L. N. de Barros, \u201cPlanning under risk and Knightian uncertainty.,\u201d in IJCAI, vol. 2007, pp. 2023\u20132028, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Trevizan%2C%20F.W.%20Cozman%2C%20F.G.%20de%20Barros%2C%20L.N.%20Planning%20under%20risk%20and%20Knightian%20uncertainty.%2C%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Trevizan%2C%20F.W.%20Cozman%2C%20F.G.%20de%20Barros%2C%20L.N.%20Planning%20under%20risk%20and%20Knightian%20uncertainty.%2C%202007"
        },
        {
            "id": "20",
            "entry": "[20] D. P. Bertsekas, Dynamic programming and optimal control, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Dynamic%20programming%20and%20optimal",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20D.P.%20Dynamic%20programming%20and%20optimal"
        },
        {
            "id": "1",
            "entry": "1. Athena scientific Belmont, MA, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Athena%20scientific%201995"
        },
        {
            "id": "21",
            "entry": "[21] C. E. Rasmussen and C. Williams, Gaussian processes for machine learning. MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.%20Gaussian%20processes%20for%20machine%20learning%202006"
        },
        {
            "id": "22",
            "entry": "[22] M. Gasic and S. Young, \u201cGaussian processes for POMDP-based dialogue manager optimization,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 1, pp. 28\u201340, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gasic%2C%20M.%20Young%2C%20S.%20Gaussian%20processes%20for%20POMDP-based%20dialogue%20manager%20optimization%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gasic%2C%20M.%20Young%2C%20S.%20Gaussian%20processes%20for%20POMDP-based%20dialogue%20manager%20optimization%2C%202014"
        },
        {
            "id": "23",
            "entry": "[23] W. K. Hastings, \u201cMonte Carlo sampling methods using Markov chains and their applications,\u201d Biometrika, vol. 57, no. 1, pp. 97\u2013109, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hastings%2C%20W.K.%20Monte%20Carlo%20sampling%20methods%20using%20Markov%20chains%20and%20their%20applications%2C%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hastings%2C%20W.K.%20Monte%20Carlo%20sampling%20methods%20using%20Markov%20chains%20and%20their%20applications%2C%201970"
        },
        {
            "id": "24",
            "entry": "[24] W. R. Gilks, S. Richardson, and D. Spiegelhalter, Markov chain Monte Carlo in practice. CRC press, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilks%2C%20W.R.%20Richardson%2C%20S.%20Spiegelhalter%2C%20D.%20Markov%20chain%20Monte%20Carlo%20in%20practice%201995"
        },
        {
            "id": "25",
            "entry": "[25] U. Braga-Neto, \u201cOptimal state estimation for Boolean dynamical systems,\u201d in Signals, Systems and Computers (ASILOMAR), 2011 Conference Record of the Forty Fifth Asilomar Conference on, pp. 1050\u20131054, IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Braga-Neto%2C%20U.%20%E2%80%9COptimal%20state%20estimation%20for%20Boolean%20dynamical%20systems%2C%E2%80%9D%20in%20Signals%2C%20Systems%20and%20Computers%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Braga-Neto%2C%20U.%20%E2%80%9COptimal%20state%20estimation%20for%20Boolean%20dynamical%20systems%2C%E2%80%9D%20in%20Signals%2C%20Systems%20and%20Computers%202011"
        },
        {
            "id": "26",
            "entry": "[26] M. Imani and U. Braga-Neto, \u201cMaximum-likelihood adaptive filter for partially-observed Boolean dynamical systems,\u201d IEEE Transactions on Signal Processing, vol. 65, no. 2, pp. 359\u2013371, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Imani%2C%20M.%20Braga-Neto%2C%20U.%20Maximum-likelihood%20adaptive%20filter%20for%20partially-observed%20Boolean%20dynamical%20systems%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Imani%2C%20M.%20Braga-Neto%2C%20U.%20Maximum-likelihood%20adaptive%20filter%20for%20partially-observed%20Boolean%20dynamical%20systems%2C%202017"
        },
        {
            "id": "27",
            "entry": "[27] M. Imani and U. M. Braga-Neto, \u201cPoint-based methodology to monitor and control gene regulatory networks via noisy measurements,\u201d IEEE Transactions on Control Systems Technology, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Imani%2C%20M.%20Braga-Neto%2C%20U.M.%20Point-based%20methodology%20to%20monitor%20and%20control%20gene%20regulatory%20networks%20via%20noisy%20measurements%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Imani%2C%20M.%20Braga-Neto%2C%20U.M.%20Point-based%20methodology%20to%20monitor%20and%20control%20gene%20regulatory%20networks%20via%20noisy%20measurements%2C%202018"
        },
        {
            "id": "28",
            "entry": "[28] M. Imani and U. M. Braga-Neto, \u201cFinite-horizon LQR controller for partially-observed Boolean dynamical systems,\u201d Automatica, vol. 95, pp. 172\u2013179, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Imani%2C%20M.%20Braga-Neto%2C%20U.M.%20Finite-horizon%20LQR%20controller%20for%20partially-observed%20Boolean%20dynamical%20systems%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Imani%2C%20M.%20Braga-Neto%2C%20U.M.%20Finite-horizon%20LQR%20controller%20for%20partially-observed%20Boolean%20dynamical%20systems%2C%202018"
        },
        {
            "id": "29",
            "entry": "[29] M. Bittner, P. Meltzer, Y. Chen, Y. Jiang, E. Seftor, M. Hendrix, M. Radmacher, R. Simon, Z. Yakhini, A. Ben-Dor, et al., \u201cMolecular classification of cutaneous malignant melanoma by gene expression profiling,\u201d Nature, vol. 406, no. 6795, pp. 536\u2013540, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bittner%2C%20M.%20Meltzer%2C%20P.%20Chen%2C%20Y.%20Jiang%2C%20Y.%20Molecular%20classification%20of%20cutaneous%20malignant%20melanoma%20by%20gene%20expression%20profiling%2C%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bittner%2C%20M.%20Meltzer%2C%20P.%20Chen%2C%20Y.%20Jiang%2C%20Y.%20Molecular%20classification%20of%20cutaneous%20malignant%20melanoma%20by%20gene%20expression%20profiling%2C%202000"
        },
        {
            "id": "30",
            "entry": "[30] A. T. Weeraratna, Y. Jiang, G. Hostetter, K. Rosenblatt, P. Duray, M. Bittner, and J. M. Trent, \u201cWnt5a signaling directly affects cell motility and invasion of metastatic melanoma,\u201d Cancer cell, vol. 1, no. 3, pp. 279\u2013288, 2002. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weeraratna%2C%20A.T.%20Jiang%2C%20Y.%20Hostetter%2C%20G.%20Rosenblatt%2C%20K.%20Wnt5a%20signaling%20directly%20affects%20cell%20motility%20and%20invasion%20of%20metastatic%20melanoma%2C%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weeraratna%2C%20A.T.%20Jiang%2C%20Y.%20Hostetter%2C%20G.%20Rosenblatt%2C%20K.%20Wnt5a%20signaling%20directly%20affects%20cell%20motility%20and%20invasion%20of%20metastatic%20melanoma%2C%202002"
        }
    ]
}
