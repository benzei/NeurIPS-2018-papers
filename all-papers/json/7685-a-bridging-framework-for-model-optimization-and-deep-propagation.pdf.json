{
    "filename": "7685-a-bridging-framework-for-model-optimization-and-deep-propagation.pdf",
    "metadata": {
        "title": "A Bridging Framework for Model Optimization and Deep Propagation",
        "author": "Risheng Liu1,2,\u2217 Shichao Cheng3, Xiaokun Liu1, Long Ma1, Xin Fan1,2, Zhongxuan Luo2,3 1International School of Information Science & Engineering, Dalian University of Technology",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7685-a-bridging-framework-for-model-optimization-and-deep-propagation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Optimizing task-related mathematical model is one of the most fundamental methodologies in statistic and learning areas. However, generally designed schematic iterations may hard to investigate complex data distributions in real-world applications. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular tasks. Unfortunately, existing networks are often built in heuristic manners, thus lack of principled interpretations and solid theoretical supports. In this work, we provide a new paradigm, named Propagation and Optimization based Deep Model (PODM), to bridge the gaps between these different mechanisms (i.e., model optimization and deep propagation). On the one hand, we utilize PODM as a deeply trained solver for model optimization. Different from these existing network based iterations, which often lack theoretical investigations, we provide strict convergence analysis for PODM in the challenging nonconvex and nonsmooth scenarios. On the other hand, by relaxing the model constraints and performing end-to-end training, we also develop a PODM based strategy to integrate domain knowledge (formulated as models) and real data distributions (learned by networks), resulting in a generic ensemble framework for challenging real-world applications. Extensive experiments verify our theoretical results and demonstrate the superiority of PODM against these state-of-the-art approaches."
    },
    "keywords": [
        {
            "term": "structural similarity",
            "url": "https://en.wikipedia.org/wiki/structural_similarity"
        },
        {
            "term": "domain knowledge",
            "url": "https://en.wikipedia.org/wiki/domain_knowledge"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        }
    ],
    "highlights": [
        "In the last several decades, many machine learning and computer vision tasks have been formulated as the problems of solving mathematically designed optimization models",
        "To partially break through these limitations, this paper proposes a theoretically guaranteed paradigm, named Propagation and Optimization based Deep Model (PODM), to incorporate knowledge-driven schematic iterations and data-dependent network architectures to address both model optimization and learning tasks",
        "By introducing an optimality error checking condition together with a proximal feedback mechanism, we prove in theory that the propagation generated by PODM is globally2 convergent to the critical point of the given optimization model",
        "We prove that PODM provides a novel strategy to iteratively guide the propagations of deep networks toward the critical point of the given nonconvex optimization model, leading to a fast and accurate numerical solver.\n5Notice that both \u03b8Al and \u03b8H l are learnable in the Relaxed PODM, which will be introduced in Section 4.2",
        "This paper proposed Propagation and Optimization based Deep Model (PODM), a new paradigm to integrate principled domain knowledge and trainable architectures to build deep propagations for model optimization and machine learning",
        "As a learning based numerical solver, we proved in theory that the sequences generated by PODM can successfully converge to the critical point of the given nonconvex and nonsmooth optimization model"
    ],
    "key_statements": [
        "In the last several decades, many machine learning and computer vision tasks have been formulated as the problems of solving mathematically designed optimization models",
        "Generally designed optimization models [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] may be lack of flexibility and robustness leading to severe corruptions and errors, which are commonly existed in real-world scenarios",
        "To partially break through these limitations, this paper proposes a theoretically guaranteed paradigm, named Propagation and Optimization based Deep Model (PODM), to incorporate knowledge-driven schematic iterations and data-dependent network architectures to address both model optimization and learning tasks",
        "Compared with these naive unrolling based methods (e.g., [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]), the main advantage of PODM is that we can generate iterations, which strictly converge to the critical point of the given optimization model, even in the complex nonconvex and nonsmooth scenarios",
        "By introducing an optimality error checking condition together with a proximal feedback mechanism, we prove in theory that the propagation generated by PODM is globally2 convergent to the critical point of the given optimization model",
        "We review existing training based iterative methods for model optimization",
        "We demonstrate how to apply PODM for fast and accurate nonconvex optimization",
        "It should be emphasized that different from most existing trainable iteration methods, which either incorporate networks into the iterations in heuristic manner (e.g., [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]) or directly estimate data-dependent descent directions using networks (e.g., [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]), PODM provides a nice mechanism with optimality error to control the training based propagations",
        "We prove that PODM provides a novel strategy to iteratively guide the propagations of deep networks toward the critical point of the given nonconvex optimization model, leading to a fast and accurate numerical solver.\n5Notice that both \u03b8Al and \u03b8H l are learnable in the Relaxed PODM, which will be introduced in Section 4.2",
        "All the experiments are conducted on a PC with Intel Core i7 CPU @ 3.6",
        "We explored the optimality error of our PODM",
        "Real Blurry Images: we evaluated Relaxed PODM on the real-world blurry images [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "This paper proposed Propagation and Optimization based Deep Model (PODM), a new paradigm to integrate principled domain knowledge and trainable architectures to build deep propagations for model optimization and machine learning",
        "As a learning based numerical solver, we proved in theory that the sequences generated by PODM can successfully converge to the critical point of the given nonconvex and nonsmooth optimization model"
    ],
    "summary": [
        "In the last several decades, many machine learning and computer vision tasks have been formulated as the problems of solving mathematically designed optimization models.",
        "To partially break through these limitations, this paper proposes a theoretically guaranteed paradigm, named Propagation and Optimization based Deep Model (PODM), to incorporate knowledge-driven schematic iterations and data-dependent network architectures to address both model optimization and learning tasks.",
        "Compared with these naive unrolling based methods (e.g., [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]), the main advantage of PODM is that we can generate iterations, which strictly converge to the critical point of the given optimization model, even in the complex nonconvex and nonsmooth scenarios.",
        "The recurrent [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], unrolling [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and reinforcement [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] learning strategies have been introduced to train network based iterations for model optimization.",
        "We establish two fundamental iterative modules as our trainable architectures for both model optimization and deep propagation.",
        "We discuss how to establish end-to-end type PODM with relaxed optimality error to perform practical ensemble learning for challenging real-world applications.",
        "It should be emphasized that different from most existing trainable iteration methods, which either incorporate networks into the iterations in heuristic manner (e.g., [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]) or directly estimate data-dependent descent directions using networks (e.g., [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]), PODM provides a nice mechanism with optimality error to control the training based propagations.",
        "Based on the above results, it is ready to establish the convergence results for our PODM when considering it as a numerical solver for nonconvex model optimization.",
        "We prove that PODM provides a novel strategy to iteratively guide the propagations of deep networks toward the critical point of the given nonconvex optimization model, leading to a fast and accurate numerical solver.",
        "To verify the convergence properties of our framework, we plotted the iteration behaviors of PODM on example images from the commonly used \u201cSet5\u201d super-resolution benchmark [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] and compared it with the most popular numerical solvers (e.g., FISTA [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]) and the recently proposed representative network based iteration methods (e.g., IRCNN [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]).",
        "As for the running time, we observed that RPODM is much faster than most conventional optimization based approaches and recently proposed learning based iteration methods (i.e., TV, EPLL, RTF, MLP and IRCNN).",
        "As a learning based numerical solver, we proved in theory that the sequences generated by PODM can successfully converge to the critical point of the given nonconvex and nonsmooth optimization model.",
        "By relaxing the optimality error, we obtain a plug-and-play, collaborative, interpretable, and end-to-end deep model for real-world complex tasks.",
        "Extensive experimental results verified our theoretical investigations and demonstrated the effectiveness of the proposed framework"
    ],
    "headline": "We provide a new paradigm, named Propagation and Optimization based Deep Model , to bridge the gaps between these different mechanisms ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis. In ICML, pages 1247\u20131255, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrew%2C%20Galen%20Arora%2C%20Raman%20Bilmes%2C%20Jeff%20Livescu%2C%20Karen%20Deep%20canonical%20correlation%20analysis.%20In%20ICML%202013"
        },
        {
            "id": "2",
            "entry": "[2] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In NIPS, pages 3981\u20133989, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "3",
            "entry": "[3] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "4",
            "entry": "[4] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In BMVC, pages 1\u201310, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bevilacqua%2C%20Marco%20Roumy%2C%20Aline%20Guillemot%2C%20Christine%20Alberi-Morel%2C%20Marie%20Line%20Low-complexity%20single-image%20super-resolution%20based%20on%20nonnegative%20neighbor%20embedding%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bevilacqua%2C%20Marco%20Roumy%2C%20Aline%20Guillemot%2C%20Christine%20Alberi-Morel%2C%20Marie%20Line%20Low-complexity%20single-image%20super-resolution%20based%20on%20nonnegative%20neighbor%20embedding%202012"
        },
        {
            "id": "5",
            "entry": "[5] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in Machine Learning, 3(1):1\u2013122, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Parikh%2C%20Neal%20Chu%2C%20Eric%20Peleato%2C%20Borja%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20Stephen%20Parikh%2C%20Neal%20Chu%2C%20Eric%20Peleato%2C%20Borja%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011"
        },
        {
            "id": "6",
            "entry": "[6] Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07289"
        },
        {
            "id": "7",
            "entry": "[7] Steven Diamond, Vincent Sitzmann, Felix Heide, and Gordon Wetzstein. Unrolled optimization with deep priors. arXiv preprint arXiv:1705.08041, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08041"
        },
        {
            "id": "8",
            "entry": "[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "9",
            "entry": "[9] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In ICML, pages 399\u2013406, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gregor%2C%20Karol%20LeCun%2C%20Yann%20Learning%20fast%20approximations%20of%20sparse%20coding%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gregor%2C%20Karol%20LeCun%2C%20Yann%20Learning%20fast%20approximations%20of%20sparse%20coding%202010"
        },
        {
            "id": "10",
            "entry": "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "11",
            "entry": "[11] Rolf K\u00f6hler, Michael Hirsch, Betty Mohler, Bernhard Sch\u00f6lkopf, and Stefan Harmeling. Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database. In ECCV, pages 27\u201340, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%C3%B6hler%2C%20Rolf%20Hirsch%2C%20Michael%20Mohler%2C%20Betty%20Sch%C3%B6lkopf%2C%20Bernhard%20Recording%20and%20playback%20of%20camera%20shake%3A%20Benchmarking%20blind%20deconvolution%20with%20a%20real-world%20database%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%C3%B6hler%2C%20Rolf%20Hirsch%2C%20Michael%20Mohler%2C%20Betty%20Sch%C3%B6lkopf%2C%20Bernhard%20Recording%20and%20playback%20of%20camera%20shake%3A%20Benchmarking%20blind%20deconvolution%20with%20a%20real-world%20database%202012"
        },
        {
            "id": "12",
            "entry": "[12] Dilip Krishnan and Rob Fergus. Fast image deconvolution using hyper-laplacian priors. In NIPS, pages 1033\u20131041, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnan%2C%20Dilip%20Fergus%2C%20Rob%20Fast%20image%20deconvolution%20using%20hyper-laplacian%20priors%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishnan%2C%20Dilip%20Fergus%2C%20Rob%20Fast%20image%20deconvolution%20using%20hyper-laplacian%20priors%202009"
        },
        {
            "id": "13",
            "entry": "[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "14",
            "entry": "[14] Jakob Kruse, Carsten Rother, and Uwe Schmidt. Learning to push the limits of efficient fft-based image deconvolution. In ICCV, pages 4586\u20134594, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kruse%2C%20Jakob%20Rother%2C%20Carsten%20Schmidt%2C%20Uwe%20Learning%20to%20push%20the%20limits%20of%20efficient%20fft-based%20image%20deconvolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kruse%2C%20Jakob%20Rother%2C%20Carsten%20Schmidt%2C%20Uwe%20Learning%20to%20push%20the%20limits%20of%20efficient%20fft-based%20image%20deconvolution%202017"
        },
        {
            "id": "15",
            "entry": "[15] Anat Levin, Yair Weiss, Fredo Durand, and William T. Freeman. Understanding and evaluating blind deconvolution algorithms. In CVPR, pages 1964\u20131971, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levin%2C%20Anat%20Weiss%2C%20Yair%20Durand%2C%20Fredo%20Freeman%2C%20William%20T.%20Understanding%20and%20evaluating%20blind%20deconvolution%20algorithms%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levin%2C%20Anat%20Weiss%2C%20Yair%20Durand%2C%20Fredo%20Freeman%2C%20William%20T.%20Understanding%20and%20evaluating%20blind%20deconvolution%20algorithms%202009"
        },
        {
            "id": "16",
            "entry": "[16] Chengbo Li, Wotao Yin, Hong Jiang, and Yin Zhang. An efficient augmented lagrangian method with applications to total variation minimization. Computational Optimization & Applications, 56(3):507\u2013530, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chengbo%20Yin%2C%20Wotao%20Jiang%2C%20Hong%20Zhang%2C%20Yin%20An%20efficient%20augmented%20lagrangian%20method%20with%20applications%20to%20total%20variation%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chengbo%20Yin%2C%20Wotao%20Jiang%2C%20Hong%20Zhang%2C%20Yin%20An%20efficient%20augmented%20lagrangian%20method%20with%20applications%20to%20total%20variation%20minimization%202013"
        },
        {
            "id": "17",
            "entry": "[17] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01885"
        },
        {
            "id": "18",
            "entry": "[18] Risheng Liu, Shichao Cheng, Yi He, Xin Fan, Zhouchen Lin, and Zhongxuan Luo. On the convergence of learning-based iterative methods for nonconvex inverse problems. arXiv preprint arXiv:1808.05331, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.05331"
        },
        {
            "id": "19",
            "entry": "[19] Risheng Liu, Xin Fan, Shichao Cheng, Xiangyu Wang, and Zhongxuan Luo. Proximal alternating direction network: A globally converged deep unrolling framework. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Risheng%20Fan%2C%20Xin%20Cheng%2C%20Shichao%20Wang%2C%20Xiangyu%20Proximal%20alternating%20direction%20network%3A%20A%20globally%20converged%20deep%20unrolling%20framework%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Risheng%20Fan%2C%20Xin%20Cheng%2C%20Shichao%20Wang%2C%20Xiangyu%20Proximal%20alternating%20direction%20network%3A%20A%20globally%20converged%20deep%20unrolling%20framework%202018"
        },
        {
            "id": "20",
            "entry": "[20] Risheng Liu, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan Luo, and Lei Zhang. Learning aggregated transmission propagation networks for haze removal and beyond. IEEE TNNLS, (99):1\u201314, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Risheng%20Fan%2C%20Xin%20Hou%2C%20Minjun%20Jiang%2C%20Zhiying%20Learning%20aggregated%20transmission%20propagation%20networks%20for%20haze%20removal%20and%20beyond%202018"
        },
        {
            "id": "21",
            "entry": "[21] Risheng Liu, Zhouchen Lin, and Zhixun Su. Learning markov random walks for robust subspace clustering and estimation. Neural Networks, 59:1\u201315, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Risheng%20Lin%2C%20Zhouchen%20Su%2C%20Zhixun%20Learning%20markov%20random%20walks%20for%20robust%20subspace%20clustering%20and%20estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Risheng%20Lin%2C%20Zhouchen%20Su%2C%20Zhixun%20Learning%20markov%20random%20walks%20for%20robust%20subspace%20clustering%20and%20estimation%202014"
        },
        {
            "id": "22",
            "entry": "[22] Risheng Liu, long Ma, Yiyang Wang, and Lei Zhang. Learning converged propagations with deep prior ensemble for image enhancement. arXiv preprint arXiv:1810.04012v1, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.04012v1"
        },
        {
            "id": "23",
            "entry": "[23] Jinshan Pan, Zhouchen Lin, Zhixun Su, and Ming-Hsuan Yang. Robust kernel estimation with outliers handling for image deblurring. In CVPR, pages 2800\u20132808, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20Jinshan%20Lin%2C%20Zhouchen%20Su%2C%20Zhixun%20Yang%2C%20Ming-Hsuan%20Robust%20kernel%20estimation%20with%20outliers%20handling%20for%20image%20deblurring%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20Jinshan%20Lin%2C%20Zhouchen%20Su%2C%20Zhixun%20Yang%2C%20Ming-Hsuan%20Robust%20kernel%20estimation%20with%20outliers%20handling%20for%20image%20deblurring%202016"
        },
        {
            "id": "24",
            "entry": "[24] Uwe Schmidt, Jeremy Jancsary, Sebastian Nowozin, Stefan Roth, and Carsten Rother. Cascades of regression tree fields for image restoration. IEEE TPAMI, 38(4):677\u2013689, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20Uwe%20Jancsary%2C%20Jeremy%20Nowozin%2C%20Sebastian%20Roth%2C%20Stefan%20Cascades%20of%20regression%20tree%20fields%20for%20image%20restoration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20Uwe%20Jancsary%2C%20Jeremy%20Nowozin%2C%20Sebastian%20Roth%2C%20Stefan%20Cascades%20of%20regression%20tree%20fields%20for%20image%20restoration%202016"
        },
        {
            "id": "25",
            "entry": "[25] Uwe Schmidt and Stefan Roth. Shrinkage fields for effective image restoration. In CVPR, pages 2774\u20132781, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20Uwe%20Roth%2C%20Stefan%20Shrinkage%20fields%20for%20effective%20image%20restoration%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20Uwe%20Roth%2C%20Stefan%20Shrinkage%20fields%20for%20effective%20image%20restoration%202014"
        },
        {
            "id": "26",
            "entry": "[26] Christian J Schuler, Harold Christopher Burger, Stefan Harmeling, and Bernhard Scholkopf. A machine learning approach for non-blind image deconvolution. In CVPR, pages 1067\u20131074, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schuler%2C%20Christian%20J.%20Burger%2C%20Harold%20Christopher%20Harmeling%2C%20Stefan%20Scholkopf%2C%20Bernhard%20A%20machine%20learning%20approach%20for%20non-blind%20image%20deconvolution%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schuler%2C%20Christian%20J.%20Burger%2C%20Harold%20Christopher%20Harmeling%2C%20Stefan%20Scholkopf%2C%20Bernhard%20A%20machine%20learning%20approach%20for%20non-blind%20image%20deconvolution%202013"
        },
        {
            "id": "27",
            "entry": "[27] Pablo Sprechmann, Alexander M Bronstein, and Guillermo Sapiro. Learning efficient sparse and low rank models. IEEE TPAMI, 37(9):1821\u20131833, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sprechmann%2C%20Pablo%20Bronstein%2C%20Alexander%20M.%20Sapiro%2C%20Guillermo%20Learning%20efficient%20sparse%20and%20low%20rank%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sprechmann%2C%20Pablo%20Bronstein%2C%20Alexander%20M.%20Sapiro%2C%20Guillermo%20Learning%20efficient%20sparse%20and%20low%20rank%20models%202015"
        },
        {
            "id": "28",
            "entry": "[28] Libin Sun, Sunghyun Cho, Jue Wang, and James Hays. Edge-based blur kernel estimation using patch priors. In ICCP, pages 1\u20138, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Libin%20Cho%2C%20Sunghyun%20Wang%2C%20Jue%20Hays%2C%20James%20Edge-based%20blur%20kernel%20estimation%20using%20patch%20priors%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Libin%20Cho%2C%20Sunghyun%20Wang%2C%20Jue%20Hays%2C%20James%20Edge-based%20blur%20kernel%20estimation%20using%20patch%20priors%202013"
        },
        {
            "id": "29",
            "entry": "[29] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "30",
            "entry": "[30] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. arXiv preprint arXiv:1711.10925, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10925"
        },
        {
            "id": "31",
            "entry": "[31] Jingyu Yan and Marc Pollefeys. A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate. In ECCV, pages 94\u2013106, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20Jingyu%20Pollefeys%2C%20Marc%20A%20general%20framework%20for%20motion%20segmentation%3A%20Independent%2C%20articulated%2C%20rigid%2C%20non-rigid%2C%20degenerate%20and%20non-degenerate%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20Jingyu%20Pollefeys%2C%20Marc%20A%20general%20framework%20for%20motion%20segmentation%3A%20Independent%2C%20articulated%2C%20rigid%2C%20non-rigid%2C%20degenerate%20and%20non-degenerate%202006"
        },
        {
            "id": "32",
            "entry": "[32] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In International conference on curves and surfaces, pages 711\u2013730, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeyde%2C%20Roman%20Elad%2C%20Michael%20Protter%2C%20Matan%20On%20single%20image%20scale-up%20using%20sparse-representations%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeyde%2C%20Roman%20Elad%2C%20Michael%20Protter%2C%20Matan%20On%20single%20image%20scale-up%20using%20sparse-representations%202010"
        },
        {
            "id": "33",
            "entry": "[33] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In CVPR, pages 2808\u20132817, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Kai%20Zuo%2C%20Wangmeng%20Gu%2C%20Shuhang%20Zhang%2C%20Lei%20Learning%20deep%20cnn%20denoiser%20prior%20for%20image%20restoration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Kai%20Zuo%2C%20Wangmeng%20Gu%2C%20Shuhang%20Zhang%2C%20Lei%20Learning%20deep%20cnn%20denoiser%20prior%20for%20image%20restoration%202017"
        },
        {
            "id": "34",
            "entry": "[34] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration. In ICCV, pages 479\u2013486, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoran%2C%20Daniel%20Weiss%2C%20Yair%20From%20learning%20models%20of%20natural%20image%20patches%20to%20whole%20image%20restoration%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoran%2C%20Daniel%20Weiss%2C%20Yair%20From%20learning%20models%20of%20natural%20image%20patches%20to%20whole%20image%20restoration%202011"
        }
    ]
}
