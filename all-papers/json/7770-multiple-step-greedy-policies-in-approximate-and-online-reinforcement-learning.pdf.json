{
    "filename": "7770-multiple-step-greedy-policies-in-approximate-and-online-reinforcement-learning.pdf",
    "metadata": {
        "title": "Multiple-Step Greedy Policies in Approximate and Online Reinforcement Learning",
        "author": "Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7770-multiple-step-greedy-policies-in-approximate-and-online-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator."
    },
    "keywords": [
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "actor critic",
            "url": "https://en.wikipedia.org/wiki/actor_critic"
        },
        {
            "term": "dynamic programming",
            "url": "https://en.wikipedia.org/wiki/dynamic_programming"
        },
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "model predictive control",
            "url": "https://en.wikipedia.org/wiki/model_predictive_control"
        }
    ],
    "highlights": [
        "The use of the 1-step policy improvement in Reinforcement Learning (RL) was theoretically investigated under several frameworks, e.g., Policy Iteration (PI) [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], approximate Policy Iteration [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], and Actor-Critic [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]; its practical uses are abundant [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We find this property intriguing since monotonic improvement is guaranteed in the case of soft updates w.r.t. the 1-step greedy policy, and is central to the analysis of many Reinforcement Learning algorithms [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "Let \u03c0k be the policy at the k-th iteration of \u03ba-Approximate Policy Iteration and \u03b4 be the error as defined in Definition 2",
        "Let \u03c3\u03ba,k be the policy at the k-th iteration of \u03ba-Policy Search by Dynamic Programming and \u03b4 be the error as defined in Definition 2",
        "We introduced and analyzed online and approximate Policy Iteration methods, generalized to the \u03ba-greedy policy, an instance of a multiple-step greedy policy",
        "The second property we find intriguing stemmed from analyzing \u03ba generalizations of known approximate hard-update Policy Iteration methods"
    ],
    "key_statements": [
        "The use of the 1-step policy improvement in Reinforcement Learning (RL) was theoretically investigated under several frameworks, e.g., Policy Iteration (PI) [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], approximate Policy Iteration [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], and Actor-Critic [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]; its practical uses are abundant [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We find this property intriguing since monotonic improvement is guaranteed in the case of soft updates w.r.t. the 1-step greedy policy, and is central to the analysis of many Reinforcement Learning algorithms [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "We introduce two new concentrability coefficients suitable for bounding the worst-case performance of Policy Iteration algorithms with approximate \u03ba-greedy policies",
        "Let \u03c0k be the policy at the k-th iteration of \u03ba-Approximate Policy Iteration and \u03b4 be the error as defined in Definition 2",
        "Let \u03c3\u03ba,k be the policy at the k-th iteration of \u03ba-Policy Search by Dynamic Programming and \u03b4 be the error as defined in Definition 2",
        "We introduced and analyzed online and approximate Policy Iteration methods, generalized to the \u03ba-greedy policy, an instance of a multiple-step greedy policy",
        "The second property we find intriguing stemmed from analyzing \u03ba generalizations of known approximate hard-update Policy Iteration methods"
    ],
    "summary": [
        "The use of the 1-step policy improvement in Reinforcement Learning (RL) was theoretically investigated under several frameworks, e.g., Policy Iteration (PI) [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], approximate PI [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], and Actor-Critic [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]; its practical uses are abundant [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>].",
        "Recent work [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] provided guarantees on the performance of the multiple-step greedy policy and generalizations of it in PI.",
        "We find this property intriguing since monotonic improvement is guaranteed in the case of soft updates w.r.t. the 1-step greedy policy, and is central to the analysis of many RL algorithms [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "These algorithms assume access to a generative model (Section 5) or to an approximate multiple-step greedy policy (Section 6).",
        "The set of \u03ba-greedy policies w.r.t. a value function v, G\u03ba(v), is defined using the following operators:",
        "We focus on policy improvement of multiple-step greedy policies, performed with soft updates.",
        "Soft updates of the 1-step greedy policy have proved necessary and beneficial in prominent algorithms [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "Theorem 1 guarantees monotonic improvement for the 1-step greedy policy as a special case when \u03ba = 0.",
        "To better understand this operator, first notice that in Stochastic Approximation methods such as Algorithm 1, the policy is improved using soft updates with decaying stepsizes.",
        "Theorem 1 establishes the conditions required for guaranteed monotonic improvement of softlyupdated multiple-step greedy policies.",
        "We introduce two new concentrability coefficients suitable for bounding the worst-case performance of PI algorithms with approximate \u03ba-greedy policies.",
        "A natural generalization of API [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] to the multiple-step greedy policy is \u03ba-API, as given in Algorithm 2.",
        "Let \u03c0k be the policy at the k-th iteration of \u03ba-API and \u03b4 be the error as defined in Definition 2.",
        "As for the other extreme, \u03ba = 1, we first remind that in the non-approximate case, applying T\u03ba=1 amounts to solving the original \u03b3-discounted MDP in a single step [5, Remark 4].",
        "Let \u03c3\u03ba,k be the policy at the k-th iteration of \u03ba-PSDP and \u03b4 be the error as defined in Definition 2.",
        "Compared to \u03ba-API from the previous section, the \u03ba-PSDP bound consists of a different fixed approximation error and a shared geometrically decaying term.",
        "We introduced and analyzed online and approximate PI methods, generalized to the \u03ba-greedy policy, an instance of a multiple-step greedy policy.",
        "We discovered two intriguing properties compared to the well-studied 1-step greedy policy, which we believe can be impactful in designing state-of-the-art algorithms.",
        "Successive application of multiple-step greedy policies with a soft, stepsize-based update does not guarantee improvement; see Theorem 1.",
        "In the online case, Lemma 5.4 in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] does not hold with multiple-step greedy policies."
    ],
    "headline": "We study multiple-step greedy algorithms in more practical setups",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J Andrew Bagnell, Sham M Kakade, Jeff G Schneider, and Andrew Y Ng. Policy search by dynamic programming. In Advances in neural information processing systems, pages 831\u2013838, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bagnell%2C%20J.Andrew%20Kakade%2C%20Sham%20M.%20Schneider%2C%20Jeff%20G.%20and%20Andrew%20Y%20Ng.%20Policy%20search%20by%20dynamic%20programming%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bagnell%2C%20J.Andrew%20Kakade%2C%20Sham%20M.%20Schneider%2C%20Jeff%20G.%20and%20Andrew%20Y%20Ng.%20Policy%20search%20by%20dynamic%20programming%202004"
        },
        {
            "id": "2",
            "entry": "[2] Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, volume 1, pages 560\u2013564. IEEE, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Tsitsiklis%2C%20John%20N.%20Neuro-dynamic%20programming%3A%20an%20overview%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20Dimitri%20P.%20Tsitsiklis%2C%20John%20N.%20Neuro-dynamic%20programming%3A%20an%20overview%201995"
        },
        {
            "id": "3",
            "entry": "[3] Bruno Bouzy and Bernard Helmstetter. Monte-carlo go developments. In Advances in computer games, pages 159\u2013174.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bouzy%2C%20Bruno%20Helmstetter%2C%20Bernard%20Monte-carlo%20go%20developments",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bouzy%2C%20Bruno%20Helmstetter%2C%20Bernard%20Monte-carlo%20go%20developments"
        },
        {
            "id": "4",
            "entry": "[4] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1\u201343, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Browne%2C%20Cameron%20B.%20Powley%2C%20Edward%20Whitehouse%2C%20Daniel%20Lucas%2C%20Simon%20M.%20A%20survey%20of%20monte%20carlo%20tree%20search%20methods%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Browne%2C%20Cameron%20B.%20Powley%2C%20Edward%20Whitehouse%2C%20Daniel%20Lucas%2C%20Simon%20M.%20A%20survey%20of%20monte%20carlo%20tree%20search%20methods%202012"
        },
        {
            "id": "5",
            "entry": "[5] Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor. Beyond the one step greedy approach in reinforcement learning. arXiv preprint arXiv:1802.03654, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03654"
        },
        {
            "id": "6",
            "entry": "[6] Damien Ernst, Mevludin Glavic, Florin Capitanescu, and Louis Wehenkel. Reinforcement learning versus model predictive control: a comparison on a power system problem. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):517\u2013529, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ernst%2C%20Damien%20Glavic%2C%20Mevludin%20Capitanescu%2C%20Florin%20Wehenkel%2C%20Louis%20Reinforcement%20learning%20versus%20model%20predictive%20control%3A%20a%20comparison%20on%20a%20power%20system%20problem%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ernst%2C%20Damien%20Glavic%2C%20Mevludin%20Capitanescu%2C%20Florin%20Wehenkel%2C%20Louis%20Reinforcement%20learning%20versus%20model%20predictive%20control%3A%20a%20comparison%20on%20a%20power%20system%20problem%202009"
        },
        {
            "id": "7",
            "entry": "[7] Amir-massoud Farahmand, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Error propagation for approximate policy and value iteration. In Advances in Neural Information Processing Systems, pages 568\u2013576, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amir-massoud%20Farahmand%2C%20Csaba%20Szepesv%C3%A1ri%20Munos%2C%20R%C3%A9mi%20Error%20propagation%20for%20approximate%20policy%20and%20value%20iteration%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amir-massoud%20Farahmand%2C%20Csaba%20Szepesv%C3%A1ri%20Munos%2C%20R%C3%A9mi%20Error%20propagation%20for%20approximate%20policy%20and%20value%20iteration%202010"
        },
        {
            "id": "8",
            "entry": "[8] Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 1181\u20131189. International Foundation for Autonomous Agents and Multiagent Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20Nan%20Kulesza%2C%20Alex%20Singh%2C%20Satinder%20Lewis%2C%20Richard%20The%20dependence%20of%20effective%20planning%20horizon%20on%20model%20accuracy%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20Nan%20Kulesza%2C%20Alex%20Singh%2C%20Satinder%20Lewis%2C%20Richard%20The%20dependence%20of%20effective%20planning%20horizon%20on%20model%20accuracy%202015"
        },
        {
            "id": "9",
            "entry": "[9] S.M. Kakade and J. Langford. Approximately Optimal Approximate Reinforcement Learning. In International Conference on Machine Learning, pages 267\u2013274, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20S.M.%20Langford%2C%20J.%20Approximately%20Optimal%20Approximate%20Reinforcement%20Learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20S.M.%20Langford%2C%20J.%20Approximately%20Optimal%20Approximate%20Reinforcement%20Learning%202002"
        },
        {
            "id": "10",
            "entry": "[10] Vijaymohan R Konda and Vivek S Borkar. Actor-critic\u2013type learning algorithms for markov decision processes. SIAM Journal on control and Optimization, 38(1):94\u2013123, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konda%2C%20Vijaymohan%20R.%20Borkar%2C%20Vivek%20S.%20Actor-critic%E2%80%93type%20learning%20algorithms%20for%20markov%20decision%20processes%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konda%2C%20Vijaymohan%20R.%20Borkar%2C%20Vivek%20S.%20Actor-critic%E2%80%93type%20learning%20algorithms%20for%20markov%20decision%20processes%201999"
        },
        {
            "id": "11",
            "entry": "[11] Alessandro Lazaric, Mohammad Ghavamzadeh, and R\u00e9mi Munos. Analysis of classificationbased policy iteration algorithms. The Journal of Machine Learning Research, 17(1):583\u2013612, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alessandro%20Lazaric%2C%20Mohammad%20Ghavamzadeh%20Munos%2C%20R%C3%A9mi%20Analysis%20of%20classificationbased%20policy%20iteration%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alessandro%20Lazaric%2C%20Mohammad%20Ghavamzadeh%20Munos%2C%20R%C3%A9mi%20Analysis%20of%20classificationbased%20policy%20iteration%20algorithms%202016"
        },
        {
            "id": "12",
            "entry": "[12] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "13",
            "entry": "[13] R\u00e9mi Munos. Error bounds for approximate policy iteration. In Proceedings of the Twentieth International Conference on International Conference on Machine Learning, pages 560\u2013567. AAAI Press, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R%C3%A9mi%20Error%20bounds%20for%20approximate%20policy%20iteration%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R%C3%A9mi%20Error%20bounds%20for%20approximate%20policy%20iteration%202003"
        },
        {
            "id": "14",
            "entry": "[14] R\u00e9mi Munos. Performance bounds in l_p-norm for approximate value iteration. SIAM journal on control and optimization, 46(2):541\u2013561, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Munos%2C%20R%C3%A9mi%20Performance%20bounds%20in%20l_p-norm%20for%20approximate%20value%20iteration%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Munos%2C%20R%C3%A9mi%20Performance%20bounds%20in%20l_p-norm%20for%20approximate%20value%20iteration%202007"
        },
        {
            "id": "15",
            "entry": "[15] Rudy R Negenborn, Bart De Schutter, Marco A Wiering, and Hans Hellendoorn. Learningbased model predictive control for markov decision processes. IFAC Proceedings Volumes, 38(1):354\u2013359, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negenborn%2C%20Rudy%20R.%20Schutter%2C%20Bart%20De%20Wiering%2C%20Marco%20A.%20Hellendoorn%2C%20Hans%20Learningbased%20model%20predictive%20control%20for%20markov%20decision%20processes%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negenborn%2C%20Rudy%20R.%20Schutter%2C%20Bart%20De%20Wiering%2C%20Marco%20A.%20Hellendoorn%2C%20Hans%20Learningbased%20model%20predictive%20control%20for%20markov%20decision%20processes%202005"
        },
        {
            "id": "16",
            "entry": "[16] Steven Perkins and David S Leslie. Asynchronous stochastic approximation with differential inclusions. Stochastic Systems, 2(2):409\u2013446, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perkins%2C%20Steven%20Leslie%2C%20David%20S.%20Asynchronous%20stochastic%20approximation%20with%20differential%20inclusions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perkins%2C%20Steven%20Leslie%2C%20David%20S.%20Asynchronous%20stochastic%20approximation%20with%20differential%20inclusions%202013"
        },
        {
            "id": "17",
            "entry": "[17] Marek Petrik and Bruno Scherrer. Biasing approximate dynamic programming with a lower discount factor. In Advances in neural information processing systems, pages 1265\u20131272, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petrik%2C%20Marek%20Scherrer%2C%20Bruno%20Biasing%20approximate%20dynamic%20programming%20with%20a%20lower%20discount%20factor%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petrik%2C%20Marek%20Scherrer%2C%20Bruno%20Biasing%20approximate%20dynamic%20programming%20with%20a%20lower%20discount%20factor%202009"
        },
        {
            "id": "18",
            "entry": "[18] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20Martin%20L.%20Markov%20decision%20processes%3A%20discrete%20stochastic%20dynamic%20programming%201994"
        },
        {
            "id": "19",
            "entry": "[19] Bruno Scherrer. Approximate policy iteration schemes: a comparison. In International Conference on Machine Learning, pages 1314\u20131322, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scherrer%2C%20Bruno%20Approximate%20policy%20iteration%20schemes%3A%20a%20comparison%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scherrer%2C%20Bruno%20Approximate%20policy%20iteration%20schemes%3A%20a%20comparison%202014"
        },
        {
            "id": "20",
            "entry": "[20] Bruno Scherrer. Improved and Generalized Upper Bounds on the Complexity of Policy Iteration. INFORMS, February 2016. Markov decision processes ; Dynamic Programming ; Analysis of Algorithms.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scherrer%2C%20Bruno%20Improved%20and%20Generalized%20Upper%20Bounds%20on%20the%20Complexity%20of%20Policy%20Iteration%202016-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scherrer%2C%20Bruno%20Improved%20and%20Generalized%20Upper%20Bounds%20on%20the%20Complexity%20of%20Policy%20Iteration%202016-02"
        },
        {
            "id": "21",
            "entry": "[21] Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative policy iteration as boosted policy search. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 35\u201350.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scherrer%2C%20Bruno%20Geist%2C%20Matthieu%20Local%20policy%20search%20in%20a%20convex%20space%20and%20conservative%20policy%20iteration%20as%20boosted%20policy%20search",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scherrer%2C%20Bruno%20Geist%2C%20Matthieu%20Local%20policy%20search%20in%20a%20convex%20space%20and%20conservative%20policy%20iteration%20as%20boosted%20policy%20search"
        },
        {
            "id": "22",
            "entry": "[22] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20John%20Levine%2C%20Sergey%20Abbeel%2C%20Pieter%20Jordan%2C%20Michael%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "23",
            "entry": "[23] Brian Sheppard. World-championship-caliber scrabble. Artificial Intelligence, 134(1-2):241\u2013 275, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sheppard%2C%20Brian%20World-championship-caliber%20scrabble%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sheppard%2C%20Brian%20World-championship-caliber%20scrabble%202002"
        },
        {
            "id": "24",
            "entry": "[24] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01815"
        },
        {
            "id": "25",
            "entry": "[25] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "26",
            "entry": "[26] Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finite mdps: Pac analysis. Journal of Machine Learning Research, 10(Nov):2413\u20132444, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Littman%2C%20Michael%20L.%20Reinforcement%20learning%20in%20finite%20mdps%3A%20Pac%20analysis%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strehl%2C%20Alexander%20L.%20Li%2C%20Lihong%20Littman%2C%20Michael%20L.%20Reinforcement%20learning%20in%20finite%20mdps%3A%20Pac%20analysis%202009"
        },
        {
            "id": "27",
            "entry": "[27] Aviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, and Pieter Abbeel. Learning from the hindsight plan\u2014episodic mpc improvement. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 336\u2013343. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tamar%2C%20Aviv%20Thomas%2C%20Garrett%20Zhang%2C%20Tianhao%20Levine%2C%20Sergey%20Learning%20from%20the%20hindsight%20plan%E2%80%94episodic%20mpc%20improvement%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tamar%2C%20Aviv%20Thomas%2C%20Garrett%20Zhang%2C%20Tianhao%20Levine%2C%20Sergey%20Learning%20from%20the%20hindsight%20plan%E2%80%94episodic%20mpc%20improvement%202017"
        },
        {
            "id": "28",
            "entry": "[28] Gerald Tesauro and Gregory R Galperin. On-line policy improvement using monte-carlo search. In Advances in Neural Information Processing Systems, pages 1068\u20131074, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tesauro%2C%20Gerald%20Galperin%2C%20Gregory%20R.%20On-line%20policy%20improvement%20using%20monte-carlo%20search%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tesauro%2C%20Gerald%20Galperin%2C%20Gregory%20R.%20On-line%20policy%20improvement%20using%20monte-carlo%20search%201997"
        },
        {
            "id": "29",
            "entry": "[29] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279\u2013292, 1992. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christopher%20JCH%20Watkins%20and%20Peter%20Dayan%20Qlearning%20Machine%20learning%20834279292%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christopher%20JCH%20Watkins%20and%20Peter%20Dayan%20Qlearning%20Machine%20learning%20834279292%201992"
        }
    ]
}
