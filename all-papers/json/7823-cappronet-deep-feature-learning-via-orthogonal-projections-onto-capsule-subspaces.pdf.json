{
    "filename": "7823-cappronet-deep-feature-learning-via-orthogonal-projections-onto-capsule-subspaces.pdf",
    "metadata": {
        "title": "CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces",
        "author": "Liheng Zhang, Marzieh Edraki, Guo-Jun Qi",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7823-cappronet-deep-feature-learning-via-orthogonal-projections-onto-capsule-subspaces.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes. We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class. We will also show that the capsule projection can be viewed as normalizing the multiple columns of the weight matrix simultaneously to form an orthogonal basis, which makes it more effective in incorporating novel components of input features to update capsule representations. In other words, the capsule projection can be viewed as a multi-dimensional weight normalization in capsule subspaces, where the conventional weight normalization is simply a special case of the capsule projection onto 1D lines. Only a small negligible computing overhead is incurred to train the network in low-dimensional capsule subspaces or through an alternative hyper-power iteration to estimate the normalization matrix. Experiment results on image datasets show the presented model can greatly improve the performance of the state-of-the-art ResNet backbones by 10 \u2212 20% and that of the Densenet by 5 \u2212 7% respectively at the same level of computing and memory expenses. The CapProNet establishes the competitive state-of-the-art performance for the family of capsule nets by significantly reducing test errors on the benchmark datasets."
    },
    "keywords": [
        {
            "term": "different class",
            "url": "https://en.wikipedia.org/wiki/Different_Class"
        },
        {
            "term": "new generation",
            "url": "https://en.wikipedia.org/wiki/New_Generation"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "orthogonal projection",
            "url": "https://en.wikipedia.org/wiki/orthogonal_projection"
        }
    ],
    "highlights": [
        "Since the idea of capsule net [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] was proposed, many efforts [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] have been made to seek better capsule architectures as the generation of deep network structures",
        "While these efforts have greatly revolutionized the idea of building a new generation of deep networks, there are still a large room to improve the state of the art for capsule nets",
        "We present a novel capsule project network by learning a group of capsule subspaces for different classes",
        "The parameters of an orthogonal projection is learned for each class and the lengths of projected capsules are used to predict the entity class for a given input feature vector",
        "The training continues until the capsule subspaces contain input feature vectors of corresponding classes or the back-propagated error vanishes",
        "Experiment results on real image datasets show that the proposed CapProNet+X could greatly improve the performance of backbone network without incurring large computing and memory overheads"
    ],
    "key_statements": [
        "Since the idea of capsule net [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] was proposed, many efforts [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] have been made to seek better capsule architectures as the generation of deep network structures",
        "While these efforts have greatly revolutionized the idea of building a new generation of deep networks, there are still a large room to improve the state of the art for capsule nets",
        "By analyzing the gradient through the capsule projection, one can show that a capsule subspace is iteratively updated along the complement component that contains the novel characteristics of the input feature vector",
        "In resent the novel information in input data, and one training iteration, two basis vectors w1 and the orthogonal decomposition enables us w2 are updated to w1 and w2 along the orthogonal to update capsule subspaces by more effectively direction x\u22a5, where x\u22a5 is viewed as containing incorporating novel characteristics/components novel characteristics of an entity not yet contained than the classic capsule nets",
        "We note that the CapProNet significantly advances the capsule net performance [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] by reducing its test error from 10.3% and 4.3% on CIFAR10 and Street View House Number to 3.64% and 1.54% respectively based on the chosen backbones",
        "We present a novel capsule project network by learning a group of capsule subspaces for different classes",
        "The parameters of an orthogonal projection is learned for each class and the lengths of projected capsules are used to predict the entity class for a given input feature vector",
        "The training continues until the capsule subspaces contain input feature vectors of corresponding classes or the back-propagated error vanishes",
        "Experiment results on real image datasets show that the proposed CapProNet+X could greatly improve the performance of backbone network without incurring large computing and memory overheads"
    ],
    "summary": [
        "Since the idea of capsule net [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] was proposed, many efforts [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] have been made to seek better capsule architectures as the generation of deep network structures.",
        "A capsule projection is performed by projecting input feature vectors fed from a backbone network onto the capsule subspace.",
        "By analyzing the gradient through the capsule projection, one can show that a capsule subspace is iteratively updated along the complement component that contains the novel characteristics of the input feature vector.",
        "We formally present the orthogonal projection of input feature vectors onto multiple capsule subspaces where capsule lengths are separated from their orientations to score the presence of entities belonging to different classes.",
        "We analyze the gradient of the resultant capsule projection by showing how capsule subspaces are updated iteratively to adopt novel characteristics of input feature vectors through back-propagation.",
        "In resent the novel information in input data, and one training iteration, two basis vectors w1 and the orthogonal decomposition enables us w2 are updated to w1 and w2 along the orthogonal to update capsule subspaces by more effectively direction x\u22a5, where x\u22a5 is viewed as containing incorporating novel characteristics/components novel characteristics of an entity not yet contained than the classic capsule nets.",
        "By increasing the dimension c, the capsule projection can be viewed as normalizing the multiple columns of the weight matrix Wl to form an orthogonal basis, and as we explained above, this can more effectively incorporate novel components of input feature vectors to update capsule representations.",
        "By performing an orthogonal projection of an input feature vector onto a capsule subspace, one can find the best direction revealing these properties.",
        "We note that the CapProNet significantly advances the capsule net performance [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] by reducing its test error from 10.3% and 4.3% on CIFAR10 and SVHN to 3.64% and 1.54% respectively based on the chosen backbones.",
        "This is probably because both ResNet backbones have a much smaller input dimension d = 64 of feature vectors into the capsule projection than that of WideResNet backbone where d = 128 and d = 160 with 16 and 28 layers, respectively.",
        "The parameters of an orthogonal projection is learned for each class and the lengths of projected capsules are used to predict the entity class for a given input feature vector.",
        "The training continues until the capsule subspaces contain input feature vectors of corresponding classes or the back-propagated error vanishes.",
        "Experiment results on real image datasets show that the proposed CapProNet+X could greatly improve the performance of backbone network without incurring large computing and memory overheads.",
        "While we only test the capsule projection as the output layer in this paper, we will attempt to insert it into intermediate layers of backbone networks as well, and hope this could give rise to a new generation of capsule networks with more discriminative architectures in future"
    ],
    "headline": "We propose to learn a group of capsule subspaces onto which an input feature vector is projected",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mohammad Taha Bahadori. Spectral capsule networks. 6th International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahadori%2C%20Mohammad%20Taha%20Spectral%20capsule%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahadori%2C%20Mohammad%20Taha%20Spectral%20capsule%20networks%202018"
        },
        {
            "id": "2",
            "entry": "[2] Adi Ben-Israel. An iterative method for computing the generalized inverse of an arbitrary matrix. Mathematics of Computation, 19(91):452\u2013455, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-Israel%2C%20Adi%20An%20iterative%20method%20for%20computing%20the%20generalized%20inverse%20of%20an%20arbitrary%20matrix%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-Israel%2C%20Adi%20An%20iterative%20method%20for%20computing%20the%20generalized%20inverse%20of%20an%20arbitrary%20matrix%201965"
        },
        {
            "id": "3",
            "entry": "[3] Adi Ben-Israel and Dan Cohen. On iterative computation of generalized inverses and associated projections. SIAM Journal on Numerical Analysis, 3(3):410\u2013419, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-Israel%2C%20Adi%20Cohen%2C%20Dan%20On%20iterative%20computation%20of%20generalized%20inverses%20and%20associated%20projections%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-Israel%2C%20Adi%20Cohen%2C%20Dan%20On%20iterative%20computation%20of%20generalized%20inverses%20and%20associated%20projections%201966"
        },
        {
            "id": "4",
            "entry": "[4] Adi Ben-Israel and Thomas NE Greville. Generalized inverses: theory and applications, volume 15. Springer Science & Business Media, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adi%20BenIsrael%20and%20Thomas%20NE%20Greville%20Generalized%20inverses%20theory%20and%20applications%20volume%2015%20Springer%20Science%20%20Business%20Media%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adi%20BenIsrael%20and%20Thomas%20NE%20Greville%20Generalized%20inverses%20theory%20and%20applications%20volume%2015%20Springer%20Science%20%20Business%20Media%202003"
        },
        {
            "id": "5",
            "entry": "[5] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1302.4389"
        },
        {
            "id": "6",
            "entry": "[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "7",
            "entry": "[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pages 630\u2013645.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks"
        },
        {
            "id": "8",
            "entry": "[8] Geoffrey Hinton, Nicholas Frosst, and Sara Sabour. Matrix capsules with em routing. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Frosst%2C%20Nicholas%20Sabour%2C%20Sara%20Matrix%20capsules%20with%20em%20routing%202018"
        },
        {
            "id": "9",
            "entry": "[9] Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In International Conference on Artificial Neural Networks, pages 44\u201351.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Wang%2C%20Sida%20D.%20Transforming%20auto-encoders",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Wang%2C%20Sida%20D.%20Transforming%20auto-encoders"
        },
        {
            "id": "10",
            "entry": "[10] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, page 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "11",
            "entry": "[11] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, pages 646\u2013661.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Sun%2C%20Yu%20Liu%2C%20Zhuang%20Sedra%2C%20Daniel%20Deep%20networks%20with%20stochastic%20depth",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Sun%2C%20Yu%20Liu%2C%20Zhuang%20Sedra%2C%20Daniel%20Deep%20networks%20with%20stochastic%20depth"
        },
        {
            "id": "12",
            "entry": "[12] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised nets. In Artificial Intelligence and Statistics, pages 562\u2013570, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Chen-Yu%20Xie%2C%20Saining%20Gallagher%2C%20Patrick%20Zhang%2C%20Zhengyou%20Deeply-supervised%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Chen-Yu%20Xie%2C%20Saining%20Gallagher%2C%20Patrick%20Zhang%2C%20Zhengyou%20Deeply-supervised%20nets%202015"
        },
        {
            "id": "13",
            "entry": "[13] Kaare Brandt Petersen et al. The matrix cookbook.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petersen%2C%20Kaare%20Brandt%20The%20matrix%20cookbook"
        },
        {
            "id": "14",
            "entry": "[14] David Rawlinson, Abdelrahman Ahmed, and Gideon Kowadlo. Sparse unsupervised capsules generalize better. arXiv preprint arXiv:1804.06094, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06094"
        },
        {
            "id": "15",
            "entry": "[15] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in Neural Information Processing Systems, pages 3859\u20133869, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017"
        },
        {
            "id": "16",
            "entry": "[16] Pierre Sermanet, Soumith Chintala, and Yann LeCun. Convolutional neural networks applied to house numbers digit classification. In Pattern Recognition (ICPR), 2012 21st International Conference on, pages 3288\u20133291. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sermanet%2C%20Pierre%20Chintala%2C%20Soumith%20LeCun%2C%20Yann%20Convolutional%20neural%20networks%20applied%20to%20house%20numbers%20digit%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sermanet%2C%20Pierre%20Chintala%2C%20Soumith%20LeCun%2C%20Yann%20Convolutional%20neural%20networks%20applied%20to%20house%20numbers%20digit%20classification%202012"
        },
        {
            "id": "17",
            "entry": "[17] Dilin Wang and Qiang Liu. An optimization view on dynamic routing between capsules. 6th International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Dilin%20Liu%2C%20Qiang%20An%20optimization%20view%20on%20dynamic%20routing%20between%20capsules%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Dilin%20Liu%2C%20Qiang%20An%20optimization%20view%20on%20dynamic%20routing%20between%20capsules%202018"
        },
        {
            "id": "18",
            "entry": "[18] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        }
    ]
}
