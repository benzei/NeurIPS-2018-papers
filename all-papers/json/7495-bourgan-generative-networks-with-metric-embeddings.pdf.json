{
    "filename": "7495-bourgan-generative-networks-with-metric-embeddings.pdf",
    "metadata": {
        "title": "BourGAN: Generative Networks with Metric Embeddings",
        "author": "Chang Xiao, Peilin Zhong, Changxi Zheng",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7495-bourgan-generative-networks-with-metric-embeddings.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into the 2 space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features."
    },
    "keywords": [
        {
            "term": "metric space",
            "url": "https://en.wikipedia.org/wiki/metric_space"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "finite metric space",
            "url": "https://en.wikipedia.org/wiki/finite_metric_space"
        },
        {
            "term": "metric embedding",
            "url": "https://en.wikipedia.org/wiki/metric_embedding"
        }
    ],
    "highlights": [
        "Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] is by far one of the most widely used methods for training deep generative models",
        "To exploit the constructed Gaussian mixture for addressing mode collapse, we propose a simple extension to the Generative Adversarial Networks objective that encourages the pairwise 2 distance of latent-space random vectors to match the distance of the generated data samples in the metric space",
        "We start with an overview of our experiments. i) On synthetic datasets, we quantitatively compare our method with four types of Generative Adversarial Networks, including the original Generative Adversarial Networks [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and more recent VEEGAN [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], Unrolled Generative Adversarial Networks [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], and PacGAN [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], following the evaluation metrics used by those methods (Appendix F.2). ii) We examine in each mode how well the distribution of generated samples matches the data distribution (Appendix F.2) \u2013 a new test not presented previously. iii) We compare the training convergence rate of our method with existing Generative Adversarial Networks (Appendix F.2), examining to what extent the Gaussian mixture sampling is beneficial. iv) We challenge our method with the difficult stacked MNIST dataset (Appendix F.3), testing how many modes it can cover. v) Most notably, we examine if there are \u201cfalse positive\u201d samples generated by our method and others (Figure 4)",
        "This paper introduces BourGAN, a new Generative Adversarial Networks variant aiming to address mode collapse in generator networks",
        "In contrast to previous approaches, we draw latent space vectors using a Gaussian mixture, which is constructed through metric embeddings",
        "Supported by theoretical analysis and experiments, our method enables a well-posed mapping between latent space and multi-modal data distributions"
    ],
    "key_statements": [
        "Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] is by far one of the most widely used methods for training deep generative models",
        "We address the problem of mode collapse in a general metric space",
        "We introduce BourGAN, an enhancement of Generative Adversarial Networks to avoid mode collapse in any metric space",
        "To exploit the constructed Gaussian mixture for addressing mode collapse, we propose a simple extension to the Generative Adversarial Networks objective that encourages the pairwise 2 distance of latent-space random vectors to match the distance of the generated data samples in the metric space",
        "Through a series of theoretical analyses, we show that if BourGAN is fully optimized, the logarithmic pairwise distance distribution of its generated samples closely match the logarithmic pairwise distance distribution of the real data items",
        "We show that our method outperforms several recent Generative Adversarial Networks variants in terms of generated data diversity",
        "Instead of using a standard Gaussian to sample latent space, we propose to use a Gaussian mixture model constructed using metric embeddings",
        "When training a BourGAN, we encourage the geometric structure embodied in the latent-space Gaussian mixture to be preserved by the generator network",
        "Before delving into our method, we introduce a few theoretical tools to concretize the geometric structure in a data distribution, paving the way toward understanding our algorithmic details and subsequent theoretical analysis",
        "logarithmic pairwise distance distribution of the distributions generated at various steps of our method will be measured in Wasserstein-1 distance",
        "We start with an overview of our experiments. i) On synthetic datasets, we quantitatively compare our method with four types of Generative Adversarial Networks, including the original Generative Adversarial Networks [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and more recent VEEGAN [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], Unrolled Generative Adversarial Networks [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], and PacGAN [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], following the evaluation metrics used by those methods (Appendix F.2). ii) We examine in each mode how well the distribution of generated samples matches the data distribution (Appendix F.2) \u2013 a new test not presented previously. iii) We compare the training convergence rate of our method with existing Generative Adversarial Networks (Appendix F.2), examining to what extent the Gaussian mixture sampling is beneficial. iv) We challenge our method with the difficult stacked MNIST dataset (Appendix F.3), testing how many modes it can cover. v) Most notably, we examine if there are \u201cfalse positive\u201d samples generated by our method and others (Figure 4)",
        "We find that all the four existing Generative Adversarial Networks suffer from this problem, because they use Gaussian to draw latent vectors",
        "This paper introduces BourGAN, a new Generative Adversarial Networks variant aiming to address mode collapse in generator networks",
        "In contrast to previous approaches, we draw latent space vectors using a Gaussian mixture, which is constructed through metric embeddings",
        "Supported by theoretical analysis and experiments, our method enables a well-posed mapping between latent space and multi-modal data distributions"
    ],
    "summary": [
        "Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] is by far one of the most widely used methods for training deep generative models.",
        "To exploit the constructed Gaussian mixture for addressing mode collapse, we propose a simple extension to the GAN objective that encourages the pairwise 2 distance of latent-space random vectors to match the distance of the generated data samples in the metric space.",
        "A GAN uses a neural network, called generator G, to map a low-dimensional latent-space vector z \u2208 Rd, drawn from a standard distribution Z (e.g., a Gaussian or uniform distribution), to generate data items in a space of interest such as natural images and text.",
        "Instead of using a standard Gaussian to sample latent space, we propose to use a Gaussian mixture model constructed using metric embeddings.",
        "As long as the Gaussian mixture is able to mirror the mode structure of the given dataset, the problem of mapping it to the data distribution becomes well-posed (Figure 1-c).",
        "We construct a Gaussian mixture in the 2 space, regardless of the distance metric for the data items.",
        "When training a BourGAN, we encourage the geometric structure embodied in the latent-space Gaussian mixture to be preserved by the generator network.",
        "We propose to use the pairwise distance distribution of data items to reflect the mode structure in a dataset (Figure 2-top).",
        "LPDD of the distributions generated at various steps of our method will be measured in Wasserstein-1 distance.",
        "Iii) Logarithmic distances ease our theoretical analysis, which, as we will formalize in Section 4, states that when Eq (3) is optimized, the distribution of generated samples will closely resemble the real distribution X .",
        "Proved in Appendix G.3, this theorem states that we only need m(\u039b/\u03bb)) subsamples to form a multiset Y that well captures the mode structure in the real data.",
        "Iii) We compare the training convergence rate of our method with existing GANs (Appendix F.2), examining to what extent the Gaussian mixture sampling is beneficial.",
        "We show that vi) our method is able to incorporate different distance metrics, ones that lead to different mode interpretations (Appendix F.3); and vii) our pre-training step further accelerates the training convergence in real datasets (Appendix F.2).",
        "In contrast to previous approaches, we draw latent space vectors using a Gaussian mixture, which is constructed through metric embeddings.",
        "Supported by theoretical analysis and experiments, our method enables a well-posed mapping between latent space and multi-modal data distributions.",
        "Our embedding and Gaussian mixture sampling can be readily combined with other GAN variants and even other generative models to leverage their advantages."
    ],
    "headline": "This paper addresses the mode collapse for generative adversarial networks ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "2",
            "entry": "[2] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pages 271\u2013279, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Cseke%2C%20Botond%20Tomioka%2C%20Ryota%20f-gan%3A%20Training%20generative%20neural%20samplers%20using%20variational%20divergence%20minimization%202016"
        },
        {
            "id": "3",
            "entry": "[3] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "4",
            "entry": "[4] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.07875"
        },
        {
            "id": "5",
            "entry": "[5] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00704"
        },
        {
            "id": "6",
            "entry": "[6] Jeff Donahue, Philipp Kr\u00e4henb\u00fchl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09782"
        },
        {
            "id": "7",
            "entry": "[7] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.05396"
        },
        {
            "id": "8",
            "entry": "[8] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06434"
        },
        {
            "id": "9",
            "entry": "[9] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02163"
        },
        {
            "id": "10",
            "entry": "[10] Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing Systems, pages 3310\u20133320, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Akash%20Valkoz%2C%20Lazar%20Russell%2C%20Chris%20Gutmann%2C%20Michael%20U.%20Veegan%3A%20Reducing%20mode%20collapse%20in%20gans%20using%20implicit%20variational%20learning%202017"
        },
        {
            "id": "11",
            "entry": "[11] Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04086"
        },
        {
            "id": "12",
            "entry": "[12] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.02136"
        },
        {
            "id": "13",
            "entry": "[13] Jean Bourgain. On lipschitz embedding of finite metric spaces in hilbert space. Israel Journal of Mathematics, 52(1-2):46\u201352, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bourgain%2C%20Jean%20On%20lipschitz%20embedding%20of%20finite%20metric%20spaces%20in%20hilbert%20space%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bourgain%2C%20Jean%20On%20lipschitz%20embedding%20of%20finite%20metric%20spaces%20in%20hilbert%20space%201985"
        },
        {
            "id": "14",
            "entry": "[14] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Duan%2C%20Yan%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "15",
            "entry": "[15] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "16",
            "entry": "[16] Ashish Bora, Eric Price, and Alexandros G Dimakis. Ambientgan: Generative models from lossy measurements. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bora%2C%20Ashish%20Price%2C%20Eric%20Dimakis%2C%20Alexandros%20G.%20Ambientgan%3A%20Generative%20models%20from%20lossy%20measurements%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bora%2C%20Ashish%20Price%2C%20Eric%20Dimakis%2C%20Alexandros%20G.%20Ambientgan%3A%20Generative%20models%20from%20lossy%20measurements%202018"
        },
        {
            "id": "17",
            "entry": "[17] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative models. arXiv preprint arXiv:1703.03208, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03208"
        },
        {
            "id": "18",
            "entry": "[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20Phillip%20Zhu%2C%20Jun-Yan%20Zhou%2C%20Tinghui%20and%20Alexei%20A%20Efros.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks.%20arXiv%20p%202017"
        },
        {
            "id": "19",
            "entry": "[19] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.10593"
        },
        {
            "id": "20",
            "entry": "[20] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledig%2C%20Christian%20Theis%2C%20Lucas%20Husz%C3%A1r%2C%20Ferenc%20Caballero%2C%20Jose%20Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network.%20arXiv%20p%202016"
        },
        {
            "id": "21",
            "entry": "[21] Jun-Yan Zhu, Philipp Kr\u00e4henb\u00fchl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, pages 597\u2013613.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun-Yan%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Shechtman%2C%20Eli%20and%20Alexei%20A%20Efros.%20Generative%20visual%20manipulation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun-Yan%20Kr%C3%A4henb%C3%BChl%2C%20Philipp%20Shechtman%2C%20Eli%20and%20Alexei%20A%20Efros.%20Generative%20visual%20manipulation"
        },
        {
            "id": "22",
            "entry": "[22] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In Advances In Neural Information Processing Systems, pages 613\u2013621, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Torralba%2C%20Antonio%20Generating%20videos%20with%20scene%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20Carl%20Pirsiavash%2C%20Hamed%20Torralba%2C%20Antonio%20Generating%20videos%20with%20scene%20dynamics%202016"
        },
        {
            "id": "23",
            "entry": "[23] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural Information Processing Systems, pages 82\u201390, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Jiajun%20Zhang%2C%20Chengkai%20Xue%2C%20Tianfan%20Freeman%2C%20Bill%20Learning%20a%20probabilistic%20latent%20space%20of%20object%20shapes%20via%203d%20generative-adversarial%20modeling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Jiajun%20Zhang%2C%20Chengkai%20Xue%2C%20Tianfan%20Freeman%2C%20Bill%20Learning%20a%20probabilistic%20latent%20space%20of%20object%20shapes%20via%203d%20generative-adversarial%20modeling%202016"
        },
        {
            "id": "24",
            "entry": "[24] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2813\u20132821. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "25",
            "entry": "[25] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pages 5769\u20135779, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "26",
            "entry": "[26] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03126"
        },
        {
            "id": "27",
            "entry": "[27] Yunus Saatci and Andrew G Wilson. Bayesian gan. In Advances in neural information processing systems, pages 3622\u20133631, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yunus%20Saatci%20and%20Andrew%20G%20Wilson%20Bayesian%20gan%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%2036223631%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yunus%20Saatci%20and%20Andrew%20G%20Wilson%20Bayesian%20gan%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%2036223631%202017"
        },
        {
            "id": "28",
            "entry": "[28] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00573"
        },
        {
            "id": "29",
            "entry": "[29] Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.09300"
        },
        {
            "id": "30",
            "entry": "[30] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "31",
            "entry": "[31] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "32",
            "entry": "[32] Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space of generative networks. arXiv preprint arXiv:1707.05776, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.05776"
        },
        {
            "id": "33",
            "entry": "[33] Jir\u00ed Matou\u0161ek. Embedding finite metric spaces into normed spaces. In Lectures on Discrete Geometry, pages 355\u2013400.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matou%C5%A1ek%2C%20Jir%C3%AD%20Embedding%20finite%20metric%20spaces%20into%20normed%20spaces",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matou%C5%A1ek%2C%20Jir%C3%AD%20Embedding%20finite%20metric%20spaces%20into%20normed%20spaces"
        },
        {
            "id": "34",
            "entry": "[34] Nicolas Courty, R\u00e9mi Flamary, and M\u00e9lanie Ducoffe. Learning wasserstein embeddings. arXiv preprint arXiv:1710.07457, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07457"
        },
        {
            "id": "35",
            "entry": "[35] Piotr Indyk and Jir\u0131 Matou\u0161ek. Low-distortion embeddings of finite metric spaces. Handbook of discrete and computational geometry, 37:46, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Indyk%2C%20Piotr%20Matou%C5%A1ek%2C%20Jir%C4%B1%20Low-distortion%20embeddings%20of%20finite%20metric%20spaces.%20Handbook%20of%20discrete%20and%20computational%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Indyk%2C%20Piotr%20Matou%C5%A1ek%2C%20Jir%C4%B1%20Low-distortion%20embeddings%20of%20finite%20metric%20spaces.%20Handbook%20of%20discrete%20and%20computational%202004"
        },
        {
            "id": "36",
            "entry": "[36] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "37",
            "entry": "[37] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In Advances in Neural Information Processing Systems, pages 5591\u20135600, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagarajan%2C%20Vaishnavh%20Kolter%2C%20J.Zico%20Gradient%20descent%20gan%20optimization%20is%20locally%20stable%202017"
        },
        {
            "id": "38",
            "entry": "[38] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20William%20B.%20Lindenstrauss%2C%20Joram%20Extensions%20of%20lipschitz%20mappings%20into%20a%20hilbert%20space%201984"
        },
        {
            "id": "39",
            "entry": "[39] Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs and some of its algorithmic applications. Combinatorica, 15(2):215\u2013245, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linial%2C%20Nathan%20London%2C%20Eran%20Rabinovich%2C%20Yuri%20The%20geometry%20of%20graphs%20and%20some%20of%20its%20algorithmic%20applications%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linial%2C%20Nathan%20London%2C%20Eran%20Rabinovich%2C%20Yuri%20The%20geometry%20of%20graphs%20and%20some%20of%20its%20algorithmic%20applications%201995"
        },
        {
            "id": "40",
            "entry": "[40] Tom Leighton and Satish Rao. An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms. In Foundations of Computer Science, 1988., 29th Annual Symposium on, pages 422\u2013431. IEEE, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leighton%2C%20Tom%20Rao%2C%20Satish%20An%20approximate%20max-flow%20min-cut%20theorem%20for%20uniform%20multicommodity%20flow%20problems%20with%20applications%20to%20approximation%20algorithms%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leighton%2C%20Tom%20Rao%2C%20Satish%20An%20approximate%20max-flow%20min-cut%20theorem%20for%20uniform%20multicommodity%20flow%20problems%20with%20applications%20to%20approximation%20algorithms%201988"
        },
        {
            "id": "41",
            "entry": "[41] Lucas Theis, A\u00e4ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.01844"
        },
        {
            "id": "42",
            "entry": "[42] Ali Borji. Pros and cons of gan evaluation measures. arXiv preprint arXiv:1802.03446, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03446"
        },
        {
            "id": "43",
            "entry": "[43] Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab\u00e1s P\u00f3czos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pages 2200\u20132210, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Chun-Liang%20Chang%2C%20Wei-Cheng%20Cheng%2C%20Yu%20Yang%2C%20Yiming%20Mmd%20gan%3A%20Towards%20deeper%20understanding%20of%20moment%20matching%20network%202017"
        },
        {
            "id": "44",
            "entry": "[44] Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Sch\u00f6lkopf. Adagan: Boosting generative models. In Advances in Neural Information Processing Systems, pages 5430\u20135439, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tolstikhin%2C%20Ilya%20O.%20Gelly%2C%20Sylvain%20Bousquet%2C%20Olivier%20Simon-Gabriel%2C%20Carl-Johann%20Adagan%3A%20Boosting%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tolstikhin%2C%20Ilya%20O.%20Gelly%2C%20Sylvain%20Bousquet%2C%20Olivier%20Simon-Gabriel%2C%20Carl-Johann%20Adagan%3A%20Boosting%20generative%20models%202017"
        },
        {
            "id": "45",
            "entry": "[45] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "46",
            "entry": "[46] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "47",
            "entry": "[47] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "48",
            "entry": "[48] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "49",
            "entry": "[49] Friedrich Pukelsheim. The three sigma rule. The American Statistician, 48(2):88\u201391, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pukelsheim%2C%20Friedrich%20The%20three%20sigma%20rule%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pukelsheim%2C%20Friedrich%20The%20three%20sigma%20rule%201994"
        },
        {
            "id": "50",
            "entry": "[50] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "51",
            "entry": "[51] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        }
    ]
}
