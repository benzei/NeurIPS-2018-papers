{
    "filename": "7560-optimization-over-continuous-and-multi-dimensional-decisions-with-observational-data.pdf",
    "metadata": {
        "title": "Optimization over Continuous and Multi-dimensional Decisions with Observational Data",
        "author": "Dimitris Bertsimas, Christopher McCord",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7560-optimization-over-continuous-and-multi-dimensional-decisions-with-observational-data.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider the optimization of an uncertain objective over continuous and multidimensional decision spaces in problems in which we are only provided with observational data. We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems. Our approach leverages predictive machine learning methods and incorporates information on the uncertainty of the predicted outcomes for the purpose of prescribing decisions. We demonstrate the efficacy of our method on examples involving both synthetic and real data sets."
    },
    "keywords": [
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "causal inference",
            "url": "https://en.wikipedia.org/wiki/causal_inference"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "decision maker",
            "url": "https://en.wikipedia.org/wiki/decision_maker"
        }
    ],
    "highlights": [
        "We study the general problem in which a decision maker seeks to optimize a known objective function that depends on an uncertain quantity",
        "The uncertain quantity has an unknown distribution, which may be affected by the action chosen by the decision maker",
        "The general problem we study is characterized by the following components:",
        "We train a random forest model on the validation set, and we select the model that minimizes the sum of the mean squared error and the predicted cost on the validation data",
        "We introduced a data-driven framework that combines ideas from predictive machine learning and causal inference to optimize an uncertain objective using observational data"
    ],
    "key_statements": [
        "We study the general problem in which a decision maker seeks to optimize a known objective function that depends on an uncertain quantity",
        "The uncertain quantity has an unknown distribution, which may be affected by the action chosen by the decision maker",
        "The general problem we study is characterized by the following components:",
        "We train a random forest model on the validation set, and we select the model that minimizes the sum of the mean squared error and the predicted cost on the validation data",
        "We introduced a data-driven framework that combines ideas from predictive machine learning and causal inference to optimize an uncertain objective using observational data"
    ],
    "summary": [
        "We study the general problem in which a decision maker seeks to optimize a known objective function that depends on an uncertain quantity.",
        "Bertsimas and Kallus [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] developed a framework that uses nonparametric machine learning methods to solve data-driven optimization problems in the presence of auxiliary covariates.",
        "Our primary contribution is an algorithmic framework for observational data driven optimization that allows the decision variable to take values on continuous and multidimensional sets.",
        "We denote the prediction of the ML algorithm as a linear combination of the cost function evaluated at the training examples, n \u03bc(x, z) := wi(x, z)c(z; Yi).",
        "We train a random forest model on the validation set, and we select the model that minimizes the sum of the mean squared error and the predicted cost on the validation data.",
        "CART, RF, and Lasso refer to the direct methods of training, respectively, a decision tree, a random forest, and a lasso regression [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to predict revenue, as a function of the auxiliary covariates and prices, and choosing prices, for each vector of auxiliary covariates in the test set, that maximize predicted revenue.",
        "We restrict all prices to be at most 50 to ensure the optimization problems are bounded.) UP-CART, UP-RF, and UP-Lasso refer to the uncertainty penalized analogues in which the variance and bias terms are included in the objective.",
        "(Note: for CART with no penalization, when multiple doses give the same optimal predicted response, we select the mean.) when we compare the random forest and Lasso methods with their uncertainty-penalizing analogues, we again see consistent improvements in MSE.",
        "We train a random forest to predict the optimal dose as a function of the patient covariates.",
        "We allow their method access to the true propensity scores that generated the data and optimize over all regularized linear policies for which the proposed dose is a linear function of the auxiliary covariates.",
        "With the maximal training set size of 4000, the improvements of the CART, random forest, and lasso uncertainty penalized methods over their unpenalized analogues (2.2%, 8.6%, 0.5% respectively) are all statistically significant at the 0.05 family-wise error rate level by the Wilcoxon signed-rank test with Bonferroni correction.",
        "Most existing algorithms, our approach handles continuous and multi-dimensional decision variables by introducing terms that penalize the uncertainty associated with the predicted costs.",
        "We proved finite sample generalization and regret bounds and provided a sufficient set of conditions under which the resulting decisions are asymptotically optimal.",
        "Both theoretically and with real-world examples, the tractability of the approach and the benefit of the approach over unpenalized predicted cost minimization"
    ],
    "headline": "We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Athey, Susan, Stefan Wager. 2017. Efficient policy learning. arXiv preprint arXiv:1702.02896 .",
            "arxiv_url": "https://arxiv.org/pdf/1702.02896"
        },
        {
            "id": "2",
            "entry": "[2] Bertsimas, Dimitris, Jack Dunn. 2017. Optimal classification trees. Machine Learning 1\u201344.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Dunn%2C%20Jack%20Optimal%20classification%20trees.%20Machine%20Learning%201%E2%80%9344%202017"
        },
        {
            "id": "3",
            "entry": "[3] Bertsimas, Dimitris, Jack Dunn, Nishanth Mundru. 2018. Optimal prescriptive trees. Under review .",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Dunn%2C%20Jack%20Mundru%2C%20Nishanth%20Optimal%20prescriptive%20trees%202018"
        },
        {
            "id": "4",
            "entry": "[4] Bertsimas, Dimitris, Nathan Kallus. 2014. From predictive to prescriptive analytics. arXiv preprint arXiv:1402.5481 .",
            "arxiv_url": "https://arxiv.org/pdf/1402.5481"
        },
        {
            "id": "5",
            "entry": "[5] Bertsimas, Dimitris, Nathan Kallus. 2017. The power and limits of predictive approaches to observationaldata-driven optimization .",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Kallus%2C%20Nathan%20The%20power%20and%20limits%20of%20predictive%20approaches%20to%20observationaldata-driven%20optimization%202017"
        },
        {
            "id": "6",
            "entry": "[6] Breiman, Leo. 2001. Random forests. Machine learning 45(1) 5\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Random%20forests%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Random%20forests%202001"
        },
        {
            "id": "7",
            "entry": "[7] Breiman, Leo, Jerome Friedman, Charles J Stone, Richard A Olshen. 1984. Classification and regression trees. CRC press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Friedman%2C%20Jerome%20Stone%2C%20Charles%20J.%20Olshen%2C%20Richard%20A.%20Classification%20and%20regression%20trees%201984"
        },
        {
            "id": "8",
            "entry": "[8] Bubeck, S\u00e9bastien, Nicolo Cesa-Bianchi, et al. 2012. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends R in Machine Learning 5(1) 1\u2013122.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Cesa-Bianchi%2C%20Nicolo%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "9",
            "entry": "[9] Chen, Tianqi, Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. ACM, 785\u2013794.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Tianqi%20Guestrin%2C%20Carlos%20Xgboost%3A%20A%20scalable%20tree%20boosting%20system%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Tianqi%20Guestrin%2C%20Carlos%20Xgboost%3A%20A%20scalable%20tree%20boosting%20system%202016"
        },
        {
            "id": "10",
            "entry": "[10] Consortium, International Warfarin Pharmacogenetics, et al. 2009. Estimation of the warfarin dose with clinical and pharmacogenetic data. N Engl J Med 2009(360) 753\u2013764.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Consortium%2C%20International%20Warfarin%20Pharmacogenetics%20Estimation%20of%20the%20warfarin%20dose%20with%20clinical%20and%20pharmacogenetic%20data%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Consortium%2C%20International%20Warfarin%20Pharmacogenetics%20Estimation%20of%20the%20warfarin%20dose%20with%20clinical%20and%20pharmacogenetic%20data%202009"
        },
        {
            "id": "11",
            "entry": "[11] Elmachtoub, Adam N, Paul Grigas. 2017. Smart\" predict, then optimize\". arXiv preprint arXiv:1710.08005 .",
            "arxiv_url": "https://arxiv.org/pdf/1710.08005"
        },
        {
            "id": "12",
            "entry": "[12] Flores, Carlos A. 2005. Estimation of dose-response functions and optimal doses with a continuous treatment .",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flores%2C%20Carlos%20A.%20Estimation%20of%20dose-response%20functions%20and%20optimal%20doses%20with%20a%20continuous%20treatment%202005"
        },
        {
            "id": "13",
            "entry": "[13] Hirano, Keisuke, Guido W Imbens. 2004. The propensity score with continuous treatments. Applied Bayesian modeling and causal inference from incomplete-data perspectives 226164 73\u201384.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hirano%2C%20Keisuke%20Imbens%2C%20Guido%20W.%20The%20propensity%20score%20with%20continuous%20treatments.%20Applied%20Bayesian%20modeling%20and%20causal%20inference%20from%20incomplete-data%20perspectives%20226164%2073%E2%80%9384%202004"
        },
        {
            "id": "14",
            "entry": "[14] Kallus, Nathan. 2017. Balanced policy evaluation and learning. arXiv preprint arXiv:1705.07384 .",
            "arxiv_url": "https://arxiv.org/pdf/1705.07384"
        },
        {
            "id": "15",
            "entry": "[15] Kallus, Nathan. 2017. Recursive partitioning for personalization using observational data. International Conference on Machine Learning. 1789\u20131798.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kallus%2C%20Nathan%20Recursive%20partitioning%20for%20personalization%20using%20observational%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kallus%2C%20Nathan%20Recursive%20partitioning%20for%20personalization%20using%20observational%20data%202017"
        },
        {
            "id": "16",
            "entry": "[16] Kallus, Nathan, Angela Zhou. 2018. Policy evaluation and optimization with continuous treatments. arXiv preprint arXiv:1802.06037 .",
            "arxiv_url": "https://arxiv.org/pdf/1802.06037"
        },
        {
            "id": "17",
            "entry": "[17] Kao, Yi-hao, Benjamin V Roy, Xiang Yan. 2009. Directed regression. Advances in Neural Information Processing Systems. 889\u2013897.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kao%20Yihao%20Benjamin%20V%20Roy%20Xiang%20Yan%202009%20Directed%20regression%20Advances%20in%20Neural%20Information%20Processing%20Systems%20889897",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kao%20Yihao%20Benjamin%20V%20Roy%20Xiang%20Yan%202009%20Directed%20regression%20Advances%20in%20Neural%20Information%20Processing%20Systems%20889897"
        },
        {
            "id": "18",
            "entry": "[18] Maurer, Andreas, Massimiliano Pontil. 2009. Empirical bernstein bounds and sample variance penalization. arXiv preprint arXiv:0907.3740 .",
            "arxiv_url": "https://arxiv.org/pdf/0907.3740"
        },
        {
            "id": "19",
            "entry": "[19] Misic, Velibor V. 2017. Optimization of tree ensembles. arXiv preprint arXiv:1705.10883 .",
            "arxiv_url": "https://arxiv.org/pdf/1705.10883"
        },
        {
            "id": "20",
            "entry": "[20] Rosenbaum, Paul R. 2002. Observational studies. Observational studies. Springer, 1\u201317.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rosenbaum%2C%20Paul%20R.%20Observational%20studies.%20Observational%20studies%202002"
        },
        {
            "id": "21",
            "entry": "[21] Swaminathan, Adith, Thorsten Joachims. 2015. Counterfactual risk minimization. Proceedings of the 24th International Conference on World Wide Web. ACM, 939\u2013941.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Swaminathan%2C%20Adith%20Joachims%2C%20Thorsten%20Counterfactual%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Swaminathan%2C%20Adith%20Joachims%2C%20Thorsten%20Counterfactual%20risk%20minimization%202015"
        },
        {
            "id": "22",
            "entry": "[22] Tibshirani, Robert. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) 267\u2013288.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "23",
            "entry": "[23] Wager, Stefan, Susan Athey. 2017. Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association (just-accepted). ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wager%2C%20Stefan%20Athey%2C%20Susan%20Estimation%20and%20inference%20of%20heterogeneous%20treatment%20effects%20using%20random%20forests%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wager%2C%20Stefan%20Athey%2C%20Susan%20Estimation%20and%20inference%20of%20heterogeneous%20treatment%20effects%20using%20random%20forests%202017"
        }
    ]
}
