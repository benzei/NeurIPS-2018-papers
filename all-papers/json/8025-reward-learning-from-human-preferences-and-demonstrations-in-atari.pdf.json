{
    "filename": "8025-reward-learning-from-human-preferences-and-demonstrations-in-atari.pdf",
    "metadata": {
        "title": "Reward learning from human preferences and demonstrations in Atari",
        "author": "Jan Leike, Borja Ibarz, Dario Amodei, Geoffrey Irving, Shane Legg, Tobias Pohlen",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8025-reward-learning-from-human-preferences-and-demonstrations-in-atari.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we need humans to communicate an objective to the agent directly. In this work, we combine two approaches to this problem: learning from expert demonstrations and learning from trajectory preferences. We use both to train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games. Additionally, we investigate the fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels."
    },
    "keywords": [
        {
            "term": "TAMER",
            "url": "https://en.wikipedia.org/wiki/Tamer"
        },
        {
            "term": "policy model",
            "url": "https://en.wikipedia.org/wiki/policy_model"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        },
        {
            "term": "Inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Inverse_reinforcement_learning"
        }
    ],
    "highlights": [
        "Reinforcement learning (RL) has recently been very successful in solving large problems in domains with well-specified reward functions (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a></a>, 2016; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>)",
        "A policy model is trained to imitate a human demonstrator on the task (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho and Ermon, 2016</a>; <a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\">Hester et al, 2018</a>)",
        "We focus on reward learning from trajectory preferences in the same way as <a class=\"ref-link\" id=\"cChristiano_et+al_2017_a\" href=\"#rChristiano_et+al_2017_a\">Christiano et al (2017</a>)",
        "Using trajectory preferences and expert demonstrations, we train a reward model that lets us improve on the policy learned from imitation",
        "Some versions of our method make use of the demonstrations for reward function training\u2014specifically, our autolabel experiments label the demonstrations as preferred to the agent policy, which is closely related to generative adversarial imitation learning (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho and Ermon, 2016</a>), a form of Inverse reinforcement learning"
    ],
    "key_statements": [
        "Reinforcement learning (RL) has recently been very successful in solving large problems in domains with well-specified reward functions (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a></a>, 2016; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>)",
        "A policy model is trained to imitate a human demonstrator on the task (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho and Ermon, 2016</a>; <a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\">Hester et al, 2018</a>)",
        "We focus on reward learning from trajectory preferences in the same way as <a class=\"ref-link\" id=\"cChristiano_et+al_2017_a\" href=\"#rChristiano_et+al_2017_a\">Christiano et al (2017</a>)",
        "Using trajectory preferences and expert demonstrations, we train a reward model that lets us improve on the policy learned from imitation",
        "Some versions of our method make use of the demonstrations for reward function training\u2014specifically, our autolabel experiments label the demonstrations as preferred to the agent policy, which is closely related to generative adversarial imitation learning (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho and Ermon, 2016</a>), a form of Inverse reinforcement learning"
    ],
    "summary": [
        "Reinforcement learning (RL) has recently been very successful in solving large problems in domains with well-specified reward functions (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a></a>, 2016; <a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\"><a class=\"ref-link\" id=\"cSilver_et+al_2016_a\" href=\"#rSilver_et+al_2016_a\">Silver et al, 2016</a></a>).",
        "A policy model is trained to imitate a human demonstrator on the task (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\"><a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho and Ermon, 2016</a></a>; <a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\"><a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\"><a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\">Hester et al, 2018</a></a></a>).",
        "Using trajectory preferences and expert demonstrations, we train a reward model that lets us improve on the policy learned from imitation.",
        "Some versions of our method make use of the demonstrations for reward function training\u2014specifically, our autolabel experiments label the demonstrations as preferred to the agent policy, which is closely related to generative adversarial imitation learning (<a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\"><a class=\"ref-link\" id=\"cHo_2016_a\" href=\"#rHo_2016_a\">Ho and Ermon, 2016</a></a>), a form of IRL.",
        "This action-value function is learned from demonstrations and from agent experience, both stored in a replay buffer (<a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\"><a class=\"ref-link\" id=\"cMnih_et+al_2015_a\" href=\"#rMnih_et+al_2015_a\">Mnih et al, 2015</a></a>) in the form of transitions, where is the reward discount factor.",
        "The bad performance of imitation learning in most Atari tasks is a known problem (<a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\"><a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\"><a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\">Hester et al, 2018</a></a></a>) and in the absence of a reward function preference feedback offers an excellent complement.",
        "Private Eye is a stark exception: imitation is hard to beat even with access to reward (<a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\"><a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\"><a class=\"ref-link\" id=\"cHester_et+al_2018_a\" href=\"#rHester_et+al_2018_a\">Hester et al, 2018</a></a></a>), and in our setting preference feedback is seriously damaging, except when the demonstrations themselves are leveraged for labeling.",
        "In these cases the drop in performance happens when the agent learns to exploit undesired loopholes in the reward function (Figure 4, right), dramatically increasing the predicted reward with behaviors that diminish the true score.2 These loopholes can be fixed interactively when the model is trained online with the agent, since exploitative behaviors that do not lead to good scores can be annotated as soon as they feature significantly in the agent\u2019s policy, similar to adversarial training (<a class=\"ref-link\" id=\"cGoodfellow_et+al_2014_a\" href=\"#rGoodfellow_et+al_2014_a\">Goodfellow et al, 2014</a>).",
        "Our results show that combining demonstrations with preference feedback is an effective way to provide guidance to an agent in the absence of explicit reward (Figure 1).",
        "In addition to the experiments presented here, we were unsuccessful at achieving significant performance improvements from a variety of other ideas: distributional RL (Bellemare et al, 2017), quantile distributional RL (<a class=\"ref-link\" id=\"cDabney_et+al_2017_a\" href=\"#rDabney_et+al_2017_a\">Dabney et al, 2017</a>), weight sharing between reward model and policy, supplying the actions as input to the reward model, pretrained convolutional layers or semi-supervised training of the reward model, phasing out of the large-margin supervised loss along training, and other strategies of annotation from demos.",
        "This suggests that our method, keeping a human in the training loop who provides online feedback to the agent, is effective in preventing reward hacking in Atari games."
    ],
    "headline": "We investigate the fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels",
    "reference_links": [
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning, pages 1\u20138, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "Akrour_et+al_2012_a",
            "entry": "Riad Akrour, Marc Schoenauer, and Mich\u00e8le Sebag. April: Active preference learning-based reinforcement learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 116\u2013131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Akrour%2C%20Riad%20Schoenauer%2C%20Marc%20Sebag%2C%20Mich%C3%A8le%20April%3A%20Active%20preference%20learning-based%20reinforcement%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Akrour%2C%20Riad%20Schoenauer%2C%20Marc%20Sebag%2C%20Mich%C3%A8le%20April%3A%20Active%20preference%20learning-based%20reinforcement%20learning%202012"
        },
        {
            "id": "Amodei_et+al_2016_a",
            "entry": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "Andrychowicz_et+al_2017_a",
            "entry": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048\u20135058, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Wolski%2C%20Filip%20Ray%2C%20Alex%20Schneider%2C%20Jonas%20Hindsight%20experience%20replay%202017"
        },
        {
            "id": "Bellemare_et+al_2016_a",
            "entry": "Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471\u20131479, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20Arcade%20Learning%20Environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20Arcade%20Learning%20Environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "Marc_2017_a",
            "entry": "Marc G Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pages 449\u2013458, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marc%20G%20Bellemare%2C%20Will%20Dabney%20Munos%2C%20R%C3%A9mi%20A%20distributional%20perspective%20on%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marc%20G%20Bellemare%2C%20Will%20Dabney%20Munos%2C%20R%C3%A9mi%20A%20distributional%20perspective%20on%20reinforcement%20learning%202017"
        },
        {
            "id": "Bradley_1952_a",
            "entry": "Ralph A Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bradley%2C%20Ralph%20A.%20Terry%2C%20Milton%20E.%20Rank%20analysis%20of%20incomplete%20block%20designs%3A%20I.%20The%20method%20of%20paired%20comparisons%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bradley%2C%20Ralph%20A.%20Terry%2C%20Milton%20E.%20Rank%20analysis%20of%20incomplete%20block%20designs%3A%20I.%20The%20method%20of%20paired%20comparisons%201952"
        },
        {
            "id": "Chentanez_et+al_2005_a",
            "entry": "Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems, pages 1281\u20131288, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chentanez%2C%20Nuttapong%20Barto%2C%20Andrew%20G.%20Singh%2C%20Satinder%20P.%20Intrinsically%20motivated%20reinforcement%20learning%202005"
        },
        {
            "id": "Christiano_et+al_2017_a",
            "entry": "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pages 4302\u20134310, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christiano%2C%20Paul%20F.%20Leike%2C%20Jan%20Brown%2C%20Tom%20Martic%2C%20Miljan%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christiano%2C%20Paul%20F.%20Leike%2C%20Jan%20Brown%2C%20Tom%20Martic%2C%20Miljan%20Deep%20reinforcement%20learning%20from%20human%20preferences%202017"
        },
        {
            "id": "Dabney_et+al_2017_a",
            "entry": "Will Dabney, Mark Rowland, Marc G Bellemare, and R\u00e9mi Munos. Distributional reinforcement learning with quantile regression. arXiv preprint arXiv:1710.10044, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10044"
        },
        {
            "id": "Daniel_et+al_2015_a",
            "entry": "Christian Daniel, Oliver Kroemer, Malte Viering, Jan Metz, and Jan Peters. Active reward learning with a novel acquisition function. Autonomous Robots, 39(3):389\u2013405, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniel%2C%20Christian%20Kroemer%2C%20Oliver%20Viering%2C%20Malte%20Metz%2C%20Jan%20Active%20reward%20learning%20with%20a%20novel%20acquisition%20function%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniel%2C%20Christian%20Kroemer%2C%20Oliver%20Viering%2C%20Malte%20Metz%2C%20Jan%20Active%20reward%20learning%20with%20a%20novel%20acquisition%20function%202015"
        },
        {
            "id": "Asri_et+al_2016_a",
            "entry": "Layla El Asri, Bilal Piot, Matthieu Geist, Romain Laroche, and Olivier Pietquin. Score-based inverse reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems, pages 457\u2013465, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Asri%2C%20Layla%20El%20Piot%2C%20Bilal%20Geist%2C%20Matthieu%20Laroche%2C%20Romain%20Score-based%20inverse%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Asri%2C%20Layla%20El%20Piot%2C%20Bilal%20Geist%2C%20Matthieu%20Laroche%2C%20Romain%20Score-based%20inverse%20reinforcement%20learning%202016"
        },
        {
            "id": "Elo_1978_a",
            "entry": "Arpad Elo. The Rating of Chessplayers, Past and Present. Arco Pub., 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elo%2C%20Arpad%20The%20Rating%20of%20Chessplayers%2C%20Past%20and%20Present%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elo%2C%20Arpad%20The%20Rating%20of%20Chessplayers%2C%20Past%20and%20Present%201978"
        },
        {
            "id": "Everitt_2018_a",
            "entry": "Tom Everitt. Towards Safe Artificial General Intelligence. PhD thesis, Australian National University, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Everitt%2C%20Tom%20Towards%20Safe%20Artificial%20General%20Intelligence%202018"
        },
        {
            "id": "Eysenbach_et+al_2018_a",
            "entry": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06070"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "Gregor_et+al_2016_a",
            "entry": "Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.07507"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Qlearning. In AAAI, pages 2094\u20132100, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20Hasselt%2C%20Hado%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20Qlearning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20Hasselt%2C%20Hado%20Guez%2C%20Arthur%20Silver%2C%20David%20Deep%20reinforcement%20learning%20with%20double%20Qlearning%202016"
        },
        {
            "id": "Hester_et+al_2018_a",
            "entry": "Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z Leibo, and Audrunas Gruslys. Deep Q-learning from demonstrations. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hester%2C%20Todd%20Vecerik%2C%20Matej%20Pietquin%2C%20Olivier%20Lanctot%2C%20Marc%20Deep%20Q-learning%20from%20demonstrations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hester%2C%20Todd%20Vecerik%2C%20Matej%20Pietquin%2C%20Olivier%20Lanctot%2C%20Marc%20Deep%20Q-learning%20from%20demonstrations%202018"
        },
        {
            "id": "Ho_2016_a",
            "entry": "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pages 4565\u20134573, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Jonathan%20Ermon%2C%20Stefano%20Generative%20adversarial%20imitation%20learning%202016"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jaderberg_et+al_2017_a",
            "entry": "Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Mnih%2C%20Volodymyr%20Czarnecki%2C%20Wojciech%20Marian%20Schaul%2C%20Tom%20Reinforcement%20learning%20with%20unsupervised%20auxiliary%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Mnih%2C%20Volodymyr%20Czarnecki%2C%20Wojciech%20Marian%20Schaul%2C%20Tom%20Reinforcement%20learning%20with%20unsupervised%20auxiliary%20tasks%202017"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "Knox_2009_a",
            "entry": "W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The TAMER framework. In International Conference on Knowledge Capture, pages 9\u201316, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Knox%2C%20W.Bradley%20Stone%2C%20Peter%20Interactively%20shaping%20agents%20via%20human%20reinforcement%3A%20The%20TAMER%20framework%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Knox%2C%20W.Bradley%20Stone%2C%20Peter%20Interactively%20shaping%20agents%20via%20human%20reinforcement%3A%20The%20TAMER%20framework%202009"
        },
        {
            "id": "Lehman_et+al_2018_a",
            "entry": "Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Julie Beaulieu, Peter J Bentley, Samuel Bernard, Guillaume Belson, David M Bryson, Nick Cheney, et al. The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. arXiv preprint arXiv:1803.03453, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03453"
        },
        {
            "id": "Lin_et+al_2017_a",
            "entry": "Zhiyu Lin, Brent Harrison, Aaron Keech, and Mark O Riedl. Explore, exploit or listen: Combining human feedback and policy model to speed up deep reinforcement learning in 3D worlds. arXiv preprint arXiv:1709.03969, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03969"
        },
        {
            "id": "Macglashan_et+al_2017_a",
            "entry": "James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, David Roberts, Matthew E Taylor, and Michael L Littman. Interactive learning from policy-dependent human feedback. In International Conference on Machine Learning, pages 2285\u20132294, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacGlashan%2C%20James%20Ho%2C%20Mark%20K.%20Loftin%2C%20Robert%20Peng%2C%20Bei%20Interactive%20learning%20from%20policy-dependent%20human%20feedback%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacGlashan%2C%20James%20Ho%2C%20Mark%20K.%20Loftin%2C%20Robert%20Peng%2C%20Bei%20Interactive%20learning%20from%20policy-dependent%20human%20feedback%202017"
        },
        {
            "id": "Mathewson_2017_a",
            "entry": "Kory Mathewson and Patrick Pilarski. Actor-critic reinforcement learning with simultaneous human control and feedback. arXiv preprint arXiv:1703.01274, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01274"
        },
        {
            "id": "Mnih_et+al_2013_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "Mnih_et+al_2015_a",
            "entry": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Mohamed_2015_a",
            "entry": "Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems, pages 2125\u20132133, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Variational%20information%20maximisation%20for%20intrinsically%20motivated%20reinforcement%20learning%202015"
        },
        {
            "id": "Nair_et+al_2018_a",
            "entry": "Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In International Conference on Robotics and Automation, pages 6292\u20136299, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nair%2C%20Ashvin%20McGrew%2C%20Bob%20Andrychowicz%2C%20Marcin%20Zaremba%2C%20Wojciech%20Overcoming%20exploration%20in%20reinforcement%20learning%20with%20demonstrations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nair%2C%20Ashvin%20McGrew%2C%20Bob%20Andrychowicz%2C%20Marcin%20Zaremba%2C%20Wojciech%20Overcoming%20exploration%20in%20reinforcement%20learning%20with%20demonstrations%202018"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, pages 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Orseau_et+al_2013_a",
            "entry": "Laurent Orseau, Tor Lattimore, and Marcus Hutter. Universal knowledge-seeking agents for stochastic environments. In Algorithmic Learning Theory, pages 158\u2013172, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orseau%2C%20Laurent%20Lattimore%2C%20Tor%20Hutter%2C%20Marcus%20Universal%20knowledge-seeking%20agents%20for%20stochastic%20environments%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orseau%2C%20Laurent%20Lattimore%2C%20Tor%20Hutter%2C%20Marcus%20Universal%20knowledge-seeking%20agents%20for%20stochastic%20environments%202013"
        },
        {
            "id": "Pathak_et+al_2017_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, pages 2778\u20132787, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction%202017"
        },
        {
            "id": "Pilarski_et+al_2011_a",
            "entry": "Patrick M Pilarski, Michael R Dawson, Thomas Degris, Farbod Fahimi, Jason P Carey, and Richard Sutton. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning. In International Conference on Rehabilitation Robotics, pages 1\u20137, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilarski%2C%20Patrick%20M.%20Dawson%2C%20Michael%20R.%20Degris%2C%20Thomas%20Fahimi%2C%20Farbod%20Online%20human%20training%20of%20a%20myoelectric%20prosthesis%20controller%20via%20actor-critic%20reinforcement%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilarski%2C%20Patrick%20M.%20Dawson%2C%20Michael%20R.%20Degris%2C%20Thomas%20Fahimi%2C%20Farbod%20Online%20human%20training%20of%20a%20myoelectric%20prosthesis%20controller%20via%20actor-critic%20reinforcement%20learning%202011"
        },
        {
            "id": "Saunders_et+al_2018_a",
            "entry": "William Saunders, Girish Sastry, Andreas Stuhlmueller, and Owain Evans. Trial without error: Towards safe reinforcement learning via human intervention. In International Conference on Autonomous Agents and MultiAgent Systems, pages 2067\u20132069, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saunders%2C%20William%20Sastry%2C%20Girish%20Stuhlmueller%2C%20Andreas%20Evans%2C%20Owain%20Trial%20without%20error%3A%20Towards%20safe%20reinforcement%20learning%20via%20human%20intervention%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saunders%2C%20William%20Sastry%2C%20Girish%20Stuhlmueller%2C%20Andreas%20Evans%2C%20Owain%20Trial%20without%20error%3A%20Towards%20safe%20reinforcement%20learning%20via%20human%20intervention%202018"
        },
        {
            "id": "Schaul_et+al_2015_a",
            "entry": "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. CoRR, abs/1511.05952, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05952"
        },
        {
            "id": "Schmidhuber_2006_a",
            "entry": "J\u00fcrgen Schmidhuber. Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Connection Science, 18(2):173\u2013187, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20Developmental%20robotics%2C%20optimal%20artificial%20curiosity%2C%20creativity%2C%20music%2C%20and%20the%20fine%20arts%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidhuber%2C%20J%C3%BCrgen%20Developmental%20robotics%2C%20optimal%20artificial%20curiosity%2C%20creativity%2C%20music%2C%20and%20the%20fine%20arts%202006"
        },
        {
            "id": "Silver_et+al_2016_a",
            "entry": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Srivastava_et+al_2014_a",
            "entry": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "Jan_1995_a",
            "entry": "Jan Storck, Sepp Hochreiter, and J\u00fcrgen Schmidhuber. Reinforcement driven information acquisition in non-deterministic environments. In International Conference on Artificial Neural Networks, pages 159\u2013164, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jan%20Storck%2C%20Sepp%20Hochreiter%20Schmidhuber%2C%20J%C3%BCrgen%20Reinforcement%20driven%20information%20acquisition%20in%20non-deterministic%20environments%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jan%20Storck%2C%20Sepp%20Hochreiter%20Schmidhuber%2C%20J%C3%BCrgen%20Reinforcement%20driven%20information%20acquisition%20in%20non-deterministic%20environments%201995"
        },
        {
            "id": "Sutton_2018_a",
            "entry": "Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT press, 2nd edition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20Barto%2C%20Andrew%20Reinforcement%20Learning%3A%20An%20Introduction%202018"
        },
        {
            "id": "Tarvainen_2017_a",
            "entry": "Antti Tarvainen and Harri Valpola. Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01780"
        },
        {
            "id": "Vecer_et+al_2017_a",
            "entry": "Matej Vecer\u00edk, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08817"
        },
        {
            "id": "Wang_et+al_1995_a",
            "entry": "Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995\u20132003, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20Hasselt%2C%20Hado%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Ziyu%20Schaul%2C%20Tom%20Hessel%2C%20Matteo%20Hasselt%2C%20Hado%20Dueling%20network%20architectures%20for%20deep%20reinforcement%20learning%201995"
        },
        {
            "id": "Warnell_et+al_2017_a",
            "entry": "Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep TAMER: Interactive agent shaping in high-dimensional state spaces. arXiv preprint arXiv:1709.10163, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.10163"
        },
        {
            "id": "Wilson_et+al_2012_a",
            "entry": "Aaron Wilson, Alan Fern, and Prasad Tadepalli. A Bayesian approach for policy learning from trajectory preference queries. In Advances in Neural Information Processing Systems, pages 1133\u20131141, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wilson%2C%20Aaron%20Fern%2C%20Alan%20Tadepalli%2C%20Prasad%20A%20Bayesian%20approach%20for%20policy%20learning%20from%20trajectory%20preference%20queries%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wilson%2C%20Aaron%20Fern%2C%20Alan%20Tadepalli%2C%20Prasad%20A%20Bayesian%20approach%20for%20policy%20learning%20from%20trajectory%20preference%20queries%202012"
        },
        {
            "id": "Wirth_2013_a",
            "entry": "Christian Wirth and Johannes F\u00fcrnkranz. Preference-based reinforcement learning: A preliminary survey. In ECML/PKDD Workshop on Reinforcement Learning from Generalized Feedback: Beyond Numeric Rewards, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wirth%2C%20Christian%20F%C3%BCrnkranz%2C%20Johannes%20Preference-based%20reinforcement%20learning%3A%20A%20preliminary%20survey.%20In%20ECML/PKDD%20Workshop%20on%20Reinforcement%20Learning%20from%20Generalized%20Feedback%3A%20Beyond%20Numeric%20Rewards%202013"
        },
        {
            "id": "Christian_2016_a",
            "entry": "Christian Wirth, J F\u00fcrnkranz, and Gerhard Neumann. Model-free preference-based reinforcement learning. In AAAI, pages 2222\u20132228, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christian%20Wirth%2C%20J.F%C3%BCrnkranz%20Neumann%2C%20Gerhard%20Model-free%20preference-based%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christian%20Wirth%2C%20J.F%C3%BCrnkranz%20Neumann%2C%20Gerhard%20Model-free%20preference-based%20reinforcement%20learning%202016"
        },
        {
            "id": "Wirth_et+al_2017_a",
            "entry": "Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F\u00fcrnkranz. A survey of preferencebased reinforcement learning methods. The Journal of Machine Learning Research, 18(1):4945\u2013 4990, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wirth%2C%20Christian%20Akrour%2C%20Riad%20Neumann%2C%20Gerhard%20and%20Johannes%20F%C3%BCrnkranz.%20A%20survey%20of%20preferencebased%20reinforcement%20learning%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wirth%2C%20Christian%20Akrour%2C%20Riad%20Neumann%2C%20Gerhard%20and%20Johannes%20F%C3%BCrnkranz.%20A%20survey%20of%20preferencebased%20reinforcement%20learning%20methods%202017"
        },
        {
            "id": "Zhang_2018_a",
            "entry": "Xiaoqin Zhang and Huimin Ma. Pretraining deep actor-critic reinforcement learning algorithms with expert demonstrations. arXiv preprint arXiv:1801.10459, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.10459"
        },
        {
            "id": "Zhu_et+al_2018_a",
            "entry": "Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvunakool, J\u00e1nos Kram\u00e1r, Raia Hadsell, Nando de Freitas, and Nicolas Heess. Reinforcement and imitation learning for diverse visuomotor skills. arXiv preprint arXiv:1802.09564, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09564"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, pages 1433\u20131438, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Maas%2C%20Andrew%20L.%20Bagnell%2C%20J.Andrew%20Dey%2C%20Anind%20K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        }
    ]
}
