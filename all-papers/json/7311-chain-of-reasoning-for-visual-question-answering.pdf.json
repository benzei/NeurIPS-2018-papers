{
    "filename": "7311-chain-of-reasoning-for-visual-question-answering.pdf",
    "metadata": {
        "title": "Chain of Reasoning for Visual Question Answering",
        "author": "Chenfei Wu, Jinlai Liu, Xiaojie Wang, Xuan Dong",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7311-chain-of-reasoning-for-visual-question-answering.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Reasoning plays an essential role in Visual Question Answering (VQA). Multi-step and dynamic reasoning is often necessary for answering complex questions. For example, a question \u201cWhat is placed next to the bus on the right of the picture?\u201d talks about a compound object \u201cbus on the right,\u201d which is generated by the relation <bus, on the right of, picture>. Furthermore, a new relation including this compound object <sign, next to, bus on the right> is then required to infer the answer. However, previous methods support either one-step or static reasoning, without updating relations or generating compound objects. This paper proposes a novel reasoning model for addressing these problems. A chain of reasoning (CoR) is constructed for supporting multi-step and dynamic reasoning on changed relations and objects. In detail, iteratively, the relational reasoning operations form new relations between objects, and the object refining operations generate new compound objects from relations. We achieve new state-of-the-art results on four publicly available datasets. The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step by step."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "compound object",
            "url": "https://en.wikipedia.org/wiki/compound_object"
        },
        {
            "term": "long short-term memory",
            "url": "https://en.wikipedia.org/wiki/long_short-term_memory"
        },
        {
            "term": "complex question",
            "url": "https://en.wikipedia.org/wiki/complex_question"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "multi-layer perceptrons",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptrons"
        }
    ],
    "highlights": [
        "Visual question answering (VQA) aims to select an answer given an image and a related question",
        "We introduce a new Visual question answering model that performs a chain of reasoning, which generates new relations and compound objects dynamically to infer the answer",
        "We visualize the chain of reasoning, which shows the progress that the chain of reasoning generates new compound objects dynamically that lead to the answer of the question step by step.\n2 Related Work",
        "We propose a novel chain of reasoning model for Visual question answering task",
        "Experimental results on four publicly available datasets show that chain of reasoning outperforms state-of-the-art approaches",
        "The visualization of the chain of reasoning illustrates the progress that the chain of reasoning generates new compound objects that lead to the answer of the question step-by-step"
    ],
    "key_statements": [
        "Visual question answering (VQA) aims to select an answer given an image and a related question",
        "We introduce a new Visual question answering model that performs a chain of reasoning, which generates new relations and compound objects dynamically to infer the answer",
        "We visualize the chain of reasoning, which shows the progress that the chain of reasoning generates new compound objects dynamically that lead to the answer of the question step by step.\n2 Related Work",
        "Starting from outputs of Data Embedding, relational reasoning on initial objects forms new relations, and object refining generates new compound objects based on the new relations",
        "We propose a novel chain of reasoning model for Visual question answering task",
        "The reasoning procedure is viewed as the alternate updating of objects and relations",
        "Experimental results on four publicly available datasets show that chain of reasoning outperforms state-of-the-art approaches",
        "The visualization of the chain of reasoning illustrates the progress that the chain of reasoning generates new compound objects that lead to the answer of the question step-by-step",
        "We plan to apply chain of reasoning to other tasks that require reasoning like reading comprehension question answering or video question answering"
    ],
    "summary": [
        "Visual question answering (VQA) aims to select an answer given an image and a related question.",
        "One-step relational reasoning can only construct pairwise relations between initial objects, which is not always sufficient for complex questions.",
        "Attention-based methods [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] view reasoning procedure as to update the attention distribution on objects, such as image regions or bounding boxes, so as to gradually infer the answer.",
        "We introduce a new VQA model that performs a chain of reasoning, which generates new relations and compound objects dynamically to infer the answer.",
        "We visualize the chain of reasoning, which shows the progress that the CoR generates new compound objects dynamically that lead to the answer of the question step by step.",
        "RN uses full arrangement to model all the interactions between objects in the image and performs multi-layer perceptrons (MLPs) to calculate all the relations.",
        "Our model lowers the computational complexity and makes the multi-step reasoning possible.",
        "[<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] proposed one-step attention to locate relevant objects of images.",
        "[<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] proposed multi-step attention to update relevant objects of images and infer the answer progressively.",
        "Our model pay attentions on not only objects in original input features but new compound objects generated dynamically during reasoning.",
        "Starting from outputs of Data Embedding, relational reasoning on initial objects forms new relations, and object refining generates new compound objects based on the new relations.",
        "These two operations on updated relations and objects build the chain of reasoning, which outputs a series of results.",
        "Starting from initial objects O(1) = V defined in Eq (1), a chain of reasoning consists of a series of sub-chains and an output at each time, which is explained in Fig. 3.",
        "This is in line with the reasoning procedure \u2014 the question decides what the model should do for the intermediate conclusions it already got and the initial premises.",
        "In order to avoid the exponential complexity of multi-step reasoning, we refine these relations to m new compound objects, each denoted in Eq (8): m",
        "As shown in Tab. 5, the chain structure not only significantly improves the performance of attention models but superior to their stack or parallel structures.",
        "This shows using initial premises Oj(1) at each step is crucial and may avoid \u201cover-reasoning\u201d by modeling very complex relations between compound objects.",
        "The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step-by-step.",
        "We plan to apply CoR to other tasks that require reasoning like reading comprehension question answering or video question answering"
    ],
    "headline": "This paper proposes a novel reasoning model for addressing these problems",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Edward A. Feigenbaum. The art of artificial intelligence.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feigenbaum%2C%20Edward%20A.%20The%20art%20of%20artificial%20intelligence"
        },
        {
            "id": "1",
            "entry": "1. Themes and case studies of knowledge engineering. Technical report, Stanford Univ CA Dept of Computer Science, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Themes%20and%20case%20studies%20of%20knowledge%20engineering%201977"
        },
        {
            "id": "2",
            "entry": "[2] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "3",
            "entry": "[3] Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541\u2013551, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Boser%2C%20Bernhard%20Denker%2C%20John%20S.%20Henderson%2C%20Donnie%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Boser%2C%20Bernhard%20Denker%2C%20John%20S.%20Henderson%2C%20Donnie%20Backpropagation%20applied%20to%20handwritten%20zip%20code%20recognition%201989"
        },
        {
            "id": "4",
            "entry": "[4] Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Raposo%2C%20David%20Barrett%2C%20David%20G.T.%20Malinowski%2C%20Mateusz%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Raposo%2C%20David%20Barrett%2C%20David%20G.T.%20Malinowski%2C%20Mateusz%20A%20simple%20neural%20network%20module%20for%20relational%20reasoning%202017"
        },
        {
            "id": "5",
            "entry": "[5] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zichao%20He%2C%20Xiaodong%20Gao%2C%20Jianfeng%20Deng%2C%20Li%20Stacked%20attention%20networks%20for%20image%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zichao%20He%2C%20Xiaodong%20Gao%2C%20Jianfeng%20Deng%2C%20Li%20Stacked%20attention%20networks%20for%20image%20question%20answering%202016"
        },
        {
            "id": "6",
            "entry": "[6] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In ECCV, pages 451\u2013466, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huijuan%20Ask%2C%20Kate%20Saenko%20attend%20and%20answer%3A%20Exploring%20question-guided%20spatial%20attention%20for%20visual%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huijuan%20Ask%2C%20Kate%20Saenko%20attend%20and%20answer%3A%20Exploring%20question-guided%20spatial%20attention%20for%20visual%20question%20answering%202016"
        },
        {
            "id": "7",
            "entry": "[7] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, pages 39\u201348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%20Andreas%20Marcus%20Rohrbach%20Trevor%20Darrell%20and%20Dan%20Klein%20Neural%20module%20networks%20In%20CVPR%20pages%203948%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%20Andreas%20Marcus%20Rohrbach%20Trevor%20Darrell%20and%20Dan%20Klein%20Neural%20module%20networks%20In%20CVPR%20pages%203948%202016"
        },
        {
            "id": "8",
            "entry": "[8] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. In NAACL, pages 1545\u20131554, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Learning%20to%20compose%20neural%20networks%20for%20question%20answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Learning%20to%20compose%20neural%20networks%20for%20question%20answering%202016"
        },
        {
            "id": "9",
            "entry": "[9] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Ronghang%20Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Learning%20to%20reason%3A%20End-to-end%20module%20networks%20for%20visual%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Ronghang%20Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Learning%20to%20reason%3A%20End-to-end%20module%20networks%20for%20visual%20question%20answering%202017"
        },
        {
            "id": "10",
            "entry": "[10] Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia. ABC-CNN: An attention based convolutional neural network for visual question answering. arXiv:1511.05960, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05960"
        },
        {
            "id": "11",
            "entry": "[11] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical Question-Image CoAttention for Visual Question Answering. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Jiasen%20Yang%2C%20Jianwei%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Hierarchical%20Question-Image%20CoAttention%20for%20Visual%20Question%20Answering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Jiasen%20Yang%2C%20Jianwei%20Batra%2C%20Dhruv%20Parikh%2C%20Devi%20Hierarchical%20Question-Image%20CoAttention%20for%20Visual%20Question%20Answering%202016"
        },
        {
            "id": "12",
            "entry": "[12] Idan Schwartz, Alexander G. Schwing, and Tamir Hazan. High-Order Attention Models for Visual Question Answering. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwartz%2C%20Idan%20Schwing%2C%20Alexander%20G.%20Hazan%2C%20Tamir%20High-Order%20Attention%20Models%20for%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwartz%2C%20Idan%20Schwing%2C%20Alexander%20G.%20Hazan%2C%20Tamir%20High-Order%20Attention%20Models%20for%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "13",
            "entry": "[13] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. In EMNLP, pages 457\u2013468, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fukui%2C%20Akira%20Park%2C%20Dong%20Huk%20Yang%2C%20Daylen%20Rohrbach%2C%20Anna%20Multimodal%20compact%20bilinear%20pooling%20for%20visual%20question%20answering%20and%20visual%20grounding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fukui%2C%20Akira%20Park%2C%20Dong%20Huk%20Yang%2C%20Daylen%20Rohrbach%2C%20Anna%20Multimodal%20compact%20bilinear%20pooling%20for%20visual%20question%20answering%20and%20visual%20grounding%202016"
        },
        {
            "id": "14",
            "entry": "[14] Jin-Hwa Kim, Kyoung-Woon On, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard product for low-rank bilinear pooling. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Jin-Hwa%20On%2C%20Kyoung-Woon%20Kim%2C%20Jeonghee%20Ha%2C%20Jung-Woo%20Hadamard%20product%20for%20low-rank%20bilinear%20pooling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Jin-Hwa%20On%2C%20Kyoung-Woon%20Kim%2C%20Jeonghee%20Ha%2C%20Jung-Woo%20Hadamard%20product%20for%20low-rank%20bilinear%20pooling%202017"
        },
        {
            "id": "15",
            "entry": "[15] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for Visual Question Answering. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Zhou%20Yu%2C%20Jun%20Fan%2C%20Jianping%20Tao%2C%20Dacheng%20Multi-modal%20Factorized%20Bilinear%20Pooling%20with%20Co-Attention%20Learning%20for%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Zhou%20Yu%2C%20Jun%20Fan%2C%20Jianping%20Tao%2C%20Dacheng%20Multi-modal%20Factorized%20Bilinear%20Pooling%20with%20Co-Attention%20Learning%20for%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "16",
            "entry": "[16] Hedi Ben-younes, R\u00e9mi Cadene, Matthieu Cord, and Nicolas Thome. MUTAN: Multimodal Tucker Fusion for Visual Question Answering. In ICCV, pages 2631\u20132639, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-younes%2C%20Hedi%20Cadene%2C%20R%C3%A9mi%20Cord%2C%20Matthieu%20Thome%2C%20Nicolas%20MUTAN%3A%20Multimodal%20Tucker%20Fusion%20for%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-younes%2C%20Hedi%20Cadene%2C%20R%C3%A9mi%20Cord%2C%20Matthieu%20Thome%2C%20Nicolas%20MUTAN%3A%20Multimodal%20Tucker%20Fusion%20for%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "17",
            "entry": "[17] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, pages 91\u201399, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20r-cnn%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%202015"
        },
        {
            "id": "18",
            "entry": "[18] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, volume 3, page 6, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20Peter%20He%2C%20Xiaodong%20Buehler%2C%20Chris%20Teney%2C%20Damien%20Bottom-up%20and%20top-down%20attention%20for%20image%20captioning%20and%20visual%20question%20answering%202018"
        },
        {
            "id": "19",
            "entry": "[19] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. arXiv preprint arXiv:1409.1259, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1259"
        },
        {
            "id": "20",
            "entry": "[20] Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In NIPS, pages 3294\u20133302, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiros%2C%20Ryan%20Zhu%2C%20Yukun%20Salakhutdinov%2C%20Ruslan%20R.%20Zemel%2C%20Richard%20Skip-thought%20vectors%202015"
        },
        {
            "id": "21",
            "entry": "[21] Pan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang, and Xiaogang Wang. Co-attending Freeform Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Pan%20Li%2C%20Hongsheng%20Zhang%2C%20Wei%20Wang%2C%20Jianyong%20Co-attending%20Freeform%20Regions%20and%20Detections%20with%20Multi-modal%20Multiplicative%20Feature%20Embedding%20for%20Visual%20Question%20Answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Pan%20Li%2C%20Hongsheng%20Zhang%2C%20Wei%20Wang%2C%20Jianyong%20Co-attending%20Freeform%20Regions%20and%20Detections%20with%20Multi-modal%20Multiplicative%20Feature%20Embedding%20for%20Visual%20Question%20Answering%202018"
        },
        {
            "id": "22",
            "entry": "[22] Ilija Ilievski and Jiashi Feng. Multimodal Learning and Reasoning for Visual Question Answering. In NIPS, pages 551\u2013562, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ilievski%2C%20Ilija%20Feng%2C%20Jiashi%20Multimodal%20Learning%20and%20Reasoning%20for%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ilievski%2C%20Ilija%20Feng%2C%20Jiashi%20Multimodal%20Learning%20and%20Reasoning%20for%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "23",
            "entry": "[23] Chen Zhu, Yanpeng Zhao, Shuaiyi Huang, Kewei Tu, and Yi Ma. Structured Attentions for Visual Question Answering. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Chen%20Zhao%2C%20Yanpeng%20Huang%2C%20Shuaiyi%20Tu%2C%20Kewei%20Structured%20Attentions%20for%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Chen%20Zhao%2C%20Yanpeng%20Huang%2C%20Shuaiyi%20Tu%2C%20Kewei%20Structured%20Attentions%20for%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "24",
            "entry": "[24] Damien Teney, Peter Anderson, Xiaodong He, and Anton van den Hengel. Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Damien%20Teney%20Peter%20Anderson%20Xiaodong%20He%20and%20Anton%20van%20den%20Hengel%20Tips%20and%20Tricks%20for%20Visual%20Question%20Answering%20Learnings%20from%20the%202017%20Challenge%20In%20CVPR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Damien%20Teney%20Peter%20Anderson%20Xiaodong%20He%20and%20Anton%20van%20den%20Hengel%20Tips%20and%20Tricks%20for%20Visual%20Question%20Answering%20Learnings%20from%20the%202017%20Challenge%20In%20CVPR%202018"
        },
        {
            "id": "25",
            "entry": "[25] Yan Zhang, Jonathon Hare, and Adam Pr\u00fcgel-Bennett. Learning to Count Objects in Natural Images for Visual Question Answering. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yan%20Hare%2C%20Jonathon%20Pr%C3%BCgel-Bennett%2C%20Adam%20Learning%20to%20Count%20Objects%20in%20Natural%20Images%20for%20Visual%20Question%20Answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yan%20Hare%2C%20Jonathon%20Pr%C3%BCgel-Bennett%2C%20Adam%20Learning%20to%20Count%20Objects%20in%20Natural%20Images%20for%20Visual%20Question%20Answering%202018"
        },
        {
            "id": "26",
            "entry": "[26] Ruiyu Li and Jiaya Jia. Visual question answering with question representation update (qru). In NIPS, pages 4655\u20134663, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Ruiyu%20Jia%2C%20Jiaya%20Visual%20question%20answering%20with%20question%20representation%20update%20%28qru%29%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Ruiyu%20Jia%2C%20Jiaya%20Visual%20question%20answering%20with%20question%20representation%20update%20%28qru%29%202016"
        },
        {
            "id": "27",
            "entry": "[27] Kushal Kafle and Christopher Kanan. An Analysis of Visual Question Answering Algorithms. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kafle%2C%20Kushal%20Kanan%2C%20Christopher%20An%20Analysis%20of%20Visual%20Question%20Answering%20Algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kafle%2C%20Kushal%20Kanan%2C%20Christopher%20An%20Analysis%20of%20Visual%20Question%20Answering%20Algorithms%202017"
        },
        {
            "id": "28",
            "entry": "[28] Yang Shi, Tommaso Furlanello, Sheng Zha, and Animashree Anandkumar. Question Type Guided Attention in Visual Question Answering. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Yang%20Furlanello%2C%20Tommaso%20Zha%2C%20Sheng%20Anandkumar%2C%20Animashree%20Question%20Type%20Guided%20Attention%20in%20Visual%20Question%20Answering%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Yang%20Furlanello%2C%20Tommaso%20Zha%2C%20Sheng%20Anandkumar%2C%20Animashree%20Question%20Type%20Guided%20Attention%20in%20Visual%20Question%20Answering%202018"
        },
        {
            "id": "29",
            "entry": "[29] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, pages 2425\u20132433, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Antol%2C%20Stanislaw%20Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Mitchell%2C%20Margaret%20Vqa%3A%20Visual%20question%20answering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Antol%2C%20Stanislaw%20Agrawal%2C%20Aishwarya%20Lu%2C%20Jiasen%20Mitchell%2C%20Margaret%20Vqa%3A%20Visual%20question%20answering%202015"
        },
        {
            "id": "30",
            "entry": "[30] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In CVPR, volume 1, page 9, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Yash%20Khot%2C%20Tejas%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Making%20the%20V%20in%20VQA%20matter%3A%20Elevating%20the%20role%20of%20image%20understanding%20in%20Visual%20Question%20Answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Yash%20Khot%2C%20Tejas%20Summers-Stay%2C%20Douglas%20Batra%2C%20Dhruv%20Making%20the%20V%20in%20VQA%20matter%3A%20Elevating%20the%20role%20of%20image%20understanding%20in%20Visual%20Question%20Answering%202017"
        },
        {
            "id": "31",
            "entry": "[31] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering. In NIPS, pages 2953\u20132961, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Mengye%20Kiros%2C%20Ryan%20Zemel%2C%20Richard%20Exploring%20models%20and%20data%20for%20image%20question%20answering%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Mengye%20Kiros%2C%20Ryan%20Zemel%2C%20Richard%20Exploring%20models%20and%20data%20for%20image%20question%20answering%202015"
        },
        {
            "id": "32",
            "entry": "[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740\u2013755, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Doll%C3%A1r%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%20pages%20740755%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=TsungYi%20Lin%20Michael%20Maire%20Serge%20Belongie%20James%20Hays%20Pietro%20Perona%20Deva%20Ramanan%20Piotr%20Doll%C3%A1r%20and%20C%20Lawrence%20Zitnick%20Microsoft%20coco%20Common%20objects%20in%20context%20In%20ECCV%20pages%20740755%202014"
        },
        {
            "id": "33",
            "entry": "[33] Mateusz Malinowski and Mario Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In NIPS, pages 1682\u20131690, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malinowski%2C%20Mateusz%20Fritz%2C%20Mario%20A%20multi-world%20approach%20to%20question%20answering%20about%20real-world%20scenes%20based%20on%20uncertain%20input%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malinowski%2C%20Mateusz%20Fritz%2C%20Mario%20A%20multi-world%20approach%20to%20question%20answering%20about%20real-world%20scenes%20based%20on%20uncertain%20input%202014"
        },
        {
            "id": "34",
            "entry": "[34] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "35",
            "entry": "[35] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "36",
            "entry": "[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \\Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998\u20136008, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%20pages%2059986008%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20Lukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20NIPS%20pages%2059986008%202017"
        },
        {
            "id": "37",
            "entry": "[37] J. Johnson, B. Hariharan, L. v d Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In CVPR, pages 1988\u20131997, July 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.%20Hariharan%2C%20B.%20L.%20v%201988-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20J.%20Hariharan%2C%20B.%20L.%20v%201988-07"
        }
    ]
}
