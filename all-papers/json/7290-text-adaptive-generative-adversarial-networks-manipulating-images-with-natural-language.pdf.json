{
    "filename": "7290-text-adaptive-generative-adversarial-networks-manipulating-images-with-natural-language.pdf",
    "metadata": {
        "title": "Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language",
        "author": "Seonghyeon Nam, Yunji Kim, Seon Joo Kim",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7290-text-adaptive-generative-adversarial-networks-manipulating-images-with-natural-language.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve textirrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word-level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs."
    },
    "keywords": [
        {
            "term": "mobile device",
            "url": "https://en.wikipedia.org/wiki/mobile_device"
        },
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "ICCV",
            "url": "https://en.wikipedia.org/wiki/ICCV"
        },
        {
            "term": "ECCV",
            "url": "https://en.wikipedia.org/wiki/ECCV"
        },
        {
            "term": "CVPR",
            "url": "https://en.wikipedia.org/wiki/CVPR"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        }
    ],
    "highlights": [
        "Taking pictures has become a big part of people\u2019s life ever since the emergence of smartphones as the everyday device",
        "We propose a novel text-conditioned visual attribute manipulation method called the Text-Adaptive Generative Adversarial Network (TAGAN)",
        "Experimental results show that our method outperforms existing methods on CUB [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] and Oxford102 [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] datasets both quantitatively and qualitatively",
        "Our extensive analysis shows that the Text-Adaptive Generative Adversarial Network effectively disentangles visual attributes and accurately manipulates images according to the text, while preserving text-irrelevant contents in the original image.\n2 Related Work",
        "We proposed a text-adaptive generative adversarial network to semantically manipulate images using natural language description",
        "Our text-adaptive discriminator disentangles fine-grained visual attributes in the text using word-level local discriminators created on the fly according to the text"
    ],
    "key_statements": [
        "Taking pictures has become a big part of people\u2019s life ever since the emergence of smartphones as the everyday device",
        "We specifically focus on modifying visual attributes of an object, where the visual attributes are characterized by the color and the texture of an object",
        "In [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], both approaches train generative adversarial networks (GANs) using the encoded image and the sentence vector pretrained for visual-semantic similarity [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "We propose a novel text-conditioned visual attribute manipulation method called the Text-Adaptive Generative Adversarial Network (TAGAN)",
        "Experimental results show that our method outperforms existing methods on CUB [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] and Oxford102 [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] datasets both quantitatively and qualitatively",
        "Our extensive analysis shows that the Text-Adaptive Generative Adversarial Network effectively disentangles visual attributes and accurately manipulates images according to the text, while preserving text-irrelevant contents in the original image.\n2 Related Work",
        "We describe the Text-Adaptive Generative Adversarial Network in detail",
        "We proposed a text-adaptive generative adversarial network to semantically manipulate images using natural language description",
        "Our text-adaptive discriminator disentangles fine-grained visual attributes in the text using word-level local discriminators created on the fly according to the text",
        "Experimental results show that our method outperforms existing methods both quantitatively and qualitatively"
    ],
    "summary": [
        "Taking pictures has become a big part of people\u2019s life ever since the emergence of smartphones as the everyday device.",
        "In [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], both approaches train generative adversarial networks (GANs) using the encoded image and the sentence vector pretrained for visual-semantic similarity [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>].",
        "We propose a novel text-conditioned visual attribute manipulation method called the Text-Adaptive Generative Adversarial Network (TAGAN).",
        "With the text-adaptive discriminator, our generator learns to change the parts of the image while preserving other contents.",
        "Our extensive analysis shows that the TAGAN effectively disentangles visual attributes and accurately manipulates images according to the text, while preserving text-irrelevant contents in the original image.",
        "While it uses a word-level visual-semantic attention similar to our method, it fundamentally relies on a sentence vector to generate images.",
        "In [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], the authors briefly showed an image manipulation method as an application by learning an auxiliary image encoder to reconstruct a latent noise vector that contains text-irrelevant information of image.",
        "Their method trains a conditional GAN to synthesize a manipulated version of image given an original image and a target text description.",
        "We tackle the problem in a different manner by making a fine-grained discriminator to learn to disentangle visual attributes from the image and the text.",
        "Our method generates realistic images, where the visual attributes are manipulated accurately while preserving text-irrelevant contents of the original image.",
        "The methods are likely to generate a new image based on the original layout and do not preserve the contents that are not relevant to the text.",
        "The methods generate similar images because the sentence is highly correlated to a particular class of birds or flowers.",
        "As shown in the rightmost column in Fig. 4, our method fails to generate the exact shape described in the text on the original image.",
        "The network generates more details when trained using conv3, but the quality is not satisfying as various scales of visual attributes are hard to learn within a single scale layer.",
        "Our multi-scale network effectively learns to generate both coarse and fine-grained visual attributes by separating different scales of the attributes in the latent space.",
        "As shown in Fig. 7, our method generates reasonable images of interpolated text while preserving the contents of original image.",
        "We proposed a text-adaptive generative adversarial network to semantically manipulate images using natural language description.",
        "Our generator learns to generate particular visual attributes while preserving irrelevant contents in the original image.",
        "Experimental results show that our method outperforms existing methods both quantitatively and qualitatively"
    ],
    "headline": "This paper addresses the problem of manipulating images using natural language description",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. Zhang, P. Isola, and A. A. Efros, \u201cColorful image colorization,\u201d in ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20R.%20Isola%2C%20P.%20Efros%2C%20A.A.%20%E2%80%9CColorful%20image%20colorization%2C%E2%80%9D%20in%20ECCV%202016"
        },
        {
            "id": "2",
            "entry": "[2] L. A. Gatys, A. S. Ecker, and M. Bethge, \u201cImage style transfer using convolutional neural networks,\u201d in CVPR, pp. 2414\u20132423, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Image%20style%20transfer%20using%20convolutional%20neural%20networks%2C%202016"
        },
        {
            "id": "3",
            "entry": "[3] X. Liang, H. Zhang, and E. P. Xing, \u201cGenerative semantic manipulation with contrasting gan,\u201d arXiv preprint arXiv:1708.00315, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.00315"
        },
        {
            "id": "4",
            "entry": "[4] J. Johnson, A. Alahi, and L. Fei-Fei, \u201cPerceptual losses for real-time style transfer and super-resolution,\u201d in ECCV, pp. 694\u2013711, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%2C%202016"
        },
        {
            "id": "5",
            "entry": "[5] F. Luan, S. Paris, E. Shechtman, and K. Bala, \u201cDeep photo style transfer,\u201d in CVPR, pp. 6997\u20137005, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luan%2C%20F.%20Paris%2C%20S.%20Shechtman%2C%20E.%20Bala%2C%20K.%20Deep%20photo%20style%20transfer%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luan%2C%20F.%20Paris%2C%20S.%20Shechtman%2C%20E.%20Bala%2C%20K.%20Deep%20photo%20style%20transfer%2C%202017"
        },
        {
            "id": "6",
            "entry": "[6] J.-Y. Zhu, P. Kr\u00e4henb\u00fchl, E. Shechtman, and A. A. Efros, \u201cGenerative visual manipulation on the natural image manifold,\u201d in ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Kr%C3%A4henb%C3%BChl%2C%20P.%20Shechtman%2C%20E.%20Efros%2C%20A.A.%20%E2%80%9CGenerative%20visual%20manipulation%20on%20the%20natural%20image%20manifold%2C%E2%80%9D%20in%20ECCV%202016"
        },
        {
            "id": "7",
            "entry": "[7] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, \u201cNeural photo editing with introspective adversarial networks,\u201d arXiv preprint arXiv:1609.07093, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07093"
        },
        {
            "id": "8",
            "entry": "[8] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer, et al., \u201cFader networks: Manipulating images by sliding attributes,\u201d in NIPS, pp. 5969\u20135978, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20G.%20Zeghidour%2C%20N.%20Usunier%2C%20N.%20Bordes%2C%20A.%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lample%2C%20G.%20Zeghidour%2C%20N.%20Usunier%2C%20N.%20Bordes%2C%20A.%20Fader%20networks%3A%20Manipulating%20images%20by%20sliding%20attributes%2C%202017"
        },
        {
            "id": "9",
            "entry": "[9] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \u201cImage-to-image translation with conditional adversarial networks,\u201d in CVPR, pp. 1125\u20131134, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.-Y.%20Zhou%2C%20T.%20Efros%2C%20A.A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%2C%202017"
        },
        {
            "id": "10",
            "entry": "[10] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \u201cUnpaired image-to-image translation using cycle-consistent adversarial networks,\u201d in ECCV, pp. 2223\u20132232, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%2C%202017"
        },
        {
            "id": "11",
            "entry": "[11] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, \u201cGenerative adversarial text-to-image synthesis,\u201d in ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Akata%2C%20Z.%20Yan%2C%20X.%20Logeswaran%2C%20L.%20%E2%80%9CGenerative%20adversarial%20text-to-image%20synthesis%2C%E2%80%9D%20in%20ICML%202016"
        },
        {
            "id": "12",
            "entry": "[12] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. Metaxas, \u201cStackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks,\u201d in ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Xu%2C%20T.%20Li%2C%20H.%20Zhang%2C%20S.%20%E2%80%9CStackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%2C%E2%80%9D%20in%20ICCV%202017"
        },
        {
            "id": "13",
            "entry": "[13] Q. H. H. Z. Z. G. X. H. X. H. Tao Xu, Pengchuan Zhang, \u201cAttngan: Fine-grained text to image generation with attentional generative adversarial networks,\u201d in CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Q.H.H.Z.Z.G.X.H.X.H.Tao%20Zhang%2C%20Pengchuan%20%E2%80%9CAttngan%3A%20Fine-grained%20text%20to%20image%20generation%20with%20attentional%20generative%20adversarial%20networks%2C%E2%80%9D%20in%20CVPR%202018"
        },
        {
            "id": "14",
            "entry": "[14] Z. Zhang, Y. Xie, and L. Yang, \u201cPhotographic text-to-image synthesis with a hierarchically-nested adversarial network,\u201d in CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Z.%20Xie%2C%20Y.%20Yang%2C%20L.%20%E2%80%9CPhotographic%20text-to-image%20synthesis%20with%20a%20hierarchically-nested%20adversarial%20network%2C%E2%80%9D%20in%20CVPR%202018"
        },
        {
            "id": "15",
            "entry": "[15] H. Dong, S. Yu, C. Wu, and Y. Guo, \u201cSemantic image synthesis via adversarial learning,\u201d in ICCV, Oct 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20H.%20Yu%2C%20S.%20Wu%2C%20C.%20Guo%2C%20Y.%20%E2%80%9CSemantic%20image%20synthesis%20via%20adversarial%20learning%2C%E2%80%9D%20in%20ICCV%202017-10"
        },
        {
            "id": "16",
            "entry": "[16] R. Kiros, R. Salakhutdinov, and R. S. Zemel, \u201cUnifying visual-semantic embeddings with multimodal neural language models,\u201d arXiv preprint arXiv:1411.2539, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.2539"
        },
        {
            "id": "17",
            "entry": "[17] S. E. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee, \u201cLearning what and where to draw,\u201d in NIPS, pp. 217\u2013225, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.E.%20Akata%2C%20Z.%20Mohan%2C%20S.%20Tenka%2C%20S.%20Learning%20what%20and%20where%20to%20draw%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.E.%20Akata%2C%20Z.%20Mohan%2C%20S.%20Tenka%2C%20S.%20Learning%20what%20and%20where%20to%20draw%2C%202016"
        },
        {
            "id": "18",
            "entry": "[18] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, \u201cThe Caltech-UCSD Birds-200-2011 Dataset,\u201d Tech. Rep. CNS-TR-2011-001, California Institute of Technology, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wah%2C%20C.%20Branson%2C%20S.%20Welinder%2C%20P.%20Perona%2C%20P.%20The%20Caltech-UCSD%20Birds-200-2011%20Dataset%2C%202011"
        },
        {
            "id": "19",
            "entry": "[19] M.-E. Nilsback and A. Zisserman, \u201cAutomated flower classification over a large number of classes,\u201d in ICCVGIP, Dec 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nilsback%2C%20M.-E.%20Zisserman%2C%20A.%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%2C%202008-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nilsback%2C%20M.-E.%20Zisserman%2C%20A.%20Automated%20flower%20classification%20over%20a%20large%20number%20of%20classes%2C%202008-12"
        },
        {
            "id": "20",
            "entry": "[20] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "21",
            "entry": "[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d in NIPS, pp. 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%2C%202014"
        },
        {
            "id": "22",
            "entry": "[22] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, \u201cSemi-supervised learning with deep generative models,\u201d in NIPS, pp. 3581\u20133589, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20M.%20Welling%2C%20%E2%80%9CSemi-supervised%20learning%20with%20deep%20generative%20models%2C%E2%80%9D%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Mohamed%2C%20S.%20Rezende%2C%20D.J.%20M.%20Welling%2C%20%E2%80%9CSemi-supervised%20learning%20with%20deep%20generative%20models%2C%E2%80%9D%202014"
        },
        {
            "id": "23",
            "entry": "[23] M. Mirza and S. Osindero, \u201cConditional generative adversarial nets,\u201d arXiv preprint arXiv:1411.1784, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1784"
        },
        {
            "id": "24",
            "entry": "[24] X. Yan, J. Yang, K. Sohn, and H. Lee, \u201cAttribute2image: Conditional image generation from visual attributes,\u201d in ECCV, pp. 776\u2013791, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20X.%20Yang%2C%20J.%20Sohn%2C%20K.%20Lee%2C%20H.%20Attribute2image%3A%20Conditional%20image%20generation%20from%20visual%20attributes%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20X.%20Yang%2C%20J.%20Sohn%2C%20K.%20Lee%2C%20H.%20Attribute2image%3A%20Conditional%20image%20generation%20from%20visual%20attributes%2C%202016"
        },
        {
            "id": "25",
            "entry": "[25] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, \u201cInfogan: Interpretable representation learning by information maximizing generative adversarial nets,\u201d in NIPS, pp. 2172\u20132180, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Infogan%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%2C%202016"
        },
        {
            "id": "26",
            "entry": "[26] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. Metaxas, \u201cStackgan++: Realistic image synthesis with stacked generative adversarial networks,\u201d arXiv: 1710.10916, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10916"
        },
        {
            "id": "27",
            "entry": "[27] Z. He, W. Zuo, M. Kan, S. Shan, and X. Chen, \u201cArbitrary facial attribute editing: Only change what you want,\u201d arXiv preprint arXiv:1711.10678, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10678"
        },
        {
            "id": "28",
            "entry": "[28] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, \u201cEnriching word vectors with subword information,\u201d arXiv preprint arXiv:1607.04606, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.04606"
        },
        {
            "id": "29",
            "entry": "[29] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "30",
            "entry": "[30] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, \u201cImproved techniques for training gans,\u201d in NIPS, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Cheung%2C%20V.%20Improved%20techniques%20for%20training%20gans%2C%202016"
        },
        {
            "id": "31",
            "entry": "[31] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, \u201cLearning deep features for discriminative localization,\u201d in CVPR, pp. 2921\u20132929, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20B.%20Khosla%2C%20A.%20Lapedriza%2C%20A.%20Oliva%2C%20A.%20Learning%20deep%20features%20for%20discriminative%20localization%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20B.%20Khosla%2C%20A.%20Lapedriza%2C%20A.%20Oliva%2C%20A.%20Learning%20deep%20features%20for%20discriminative%20localization%2C%202016"
        },
        {
            "id": "32",
            "entry": "[32] S. Li, T. Xiao, H. Li, W. Yang, and X. Wang, \u201cIdentity-aware textual-visual matching with latent coattention,\u201d in CVPR, pp. 1890\u20131899, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20S.%20Xiao%2C%20T.%20Li%2C%20H.%20Yang%2C%20W.%20Identity-aware%20textual-visual%20matching%20with%20latent%20coattention%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20S.%20Xiao%2C%20T.%20Li%2C%20H.%20Yang%2C%20W.%20Identity-aware%20textual-visual%20matching%20with%20latent%20coattention%2C%202017"
        },
        {
            "id": "33",
            "entry": "[33] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking the inception architecture for computer vision,\u201d in CVPR, pp. 2818\u20132826, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20inception%20architecture%20for%20computer%20vision%2C%202016"
        },
        {
            "id": "34",
            "entry": "[34] S. Reed, Z. Akata, H. Lee, and B. Schiele, \u201cLearning deep representations of fine-grained visual descriptions,\u201d in CVPR, pp. 49\u201358, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reed%2C%20S.%20Akata%2C%20Z.%20Lee%2C%20H.%20Schiele%2C%20B.%20Learning%20deep%20representations%20of%20fine-grained%20visual%20descriptions%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reed%2C%20S.%20Akata%2C%20Z.%20Lee%2C%20H.%20Schiele%2C%20B.%20Learning%20deep%20representations%20of%20fine-grained%20visual%20descriptions%2C%202016"
        }
    ]
}
