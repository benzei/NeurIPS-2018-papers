{
    "filename": "7875-visualizing-the-loss-landscape-of-neural-nets.pdf",
    "metadata": {
        "title": "Visualizing the Loss Landscape of Neural Nets",
        "author": "Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Neural network training relies on our ability to find \u201cgood\u201d minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \u201cfilter normalization\u201d method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers."
    },
    "keywords": [
        {
            "term": "National Science Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Training neural networks requires minimizing a high-dimensional non-convex loss function \u2013 a task that is hard in theory, but sometimes easy in practice",
        "Why are we able to minimize highly non-convex neural loss functions? And why do the resulting minima generalize? To clarify these questions, we use high-resolution visualizations to provide an empirical characterization of neural loss functions, and explore how different network architecture choices affect the loss landscape",
        "We explore how the non-convex structure of neural loss functions relates to their trainability, and how the geometry of neural minimizers, affects their generalization properties",
        "Because of the computational burden of 2D plotting, these methods generally result in low-resolution plots of small regions that have not captured the complex non-convexity of loss surfaces",
        "We presented a visualization technique that provides insights into the consequences of a variety of choices facing the neural network practitioner, including network architecture, optimizer selection, and batch size"
    ],
    "key_statements": [
        "Training neural networks requires minimizing a high-dimensional non-convex loss function \u2013 a task that is hard in theory, but sometimes easy in practice",
        "Why are we able to minimize highly non-convex neural loss functions? And why do the resulting minima generalize? To clarify these questions, we use high-resolution visualizations to provide an empirical characterization of neural loss functions, and explore how different network architecture choices affect the loss landscape",
        "We explore how the non-convex structure of neural loss functions relates to their trainability, and how the geometry of neural minimizers, affects their generalization properties",
        "Using 2D methods, that some loss functions have extreme non-convexities, and that these non-convexities correlate with the difference in generalization between different network architectures",
        "Because of the computational burden of 2D plotting, these methods generally result in low-resolution plots of small regions that have not captured the complex non-convexity of loss surfaces",
        "We do an empirical study of neural architectures to explore why the non-convexity of loss functions seems to be problematic in some situations, but not in others",
        "Experimental Setup To understand the effects of network architecture on non-convexity, we trained a number of networks, and plotted the landscape around the obtained minimizers using the filter-normalized random direction method described in Section 4",
        "We presented a visualization technique that provides insights into the consequences of a variety of choices facing the neural network practitioner, including network architecture, optimizer selection, and batch size"
    ],
    "summary": [
        "Training neural networks requires minimizing a high-dimensional non-convex loss function \u2013 a task that is hard in theory, but sometimes easy in practice.",
        "We use high-resolution visualizations to provide an empirical characterization of neural loss functions, and explore how different network architecture choices affect the loss landscape.",
        "We explore how the non-convex structure of neural loss functions relates to their trainability, and how the geometry of neural minimizers, affects their generalization properties.",
        "We use visualizations to explore sharpness/flatness of minimizers found by different methods, as well as the effect of network architecture choices on the loss landscape.",
        "We present a simple visualization method based on \u201cfilter normalization.\u201d The sharpness of minimizers correlates well with generalization error when this normalization is used, even when making comparisons across disparate network architectures and training methods.",
        "Using 2D methods, that some loss functions have extreme non-convexities, and that these non-convexities correlate with the difference in generalization between different network architectures.",
        "Because of the computational burden of 2D plotting, these methods generally result in low-resolution plots of small regions that have not captured the complex non-convexity of loss surfaces.",
        "To remove this scaling effect, we plot loss functions using filter-wise normalized directions.",
        "The results, presented in Figure 3, still show differences in sharpness between small batch and large batch minima, these differences are much more subtle than it would appear in the un-normalized plots.",
        "Using the filter-normalized plots in Figure 3, we can make side-by-side comparisons between minimizers, and we see that sharpness correlates well with generalization error.",
        "We do an empirical study of neural architectures to explore why the non-convexity of loss functions seems to be problematic in some situations, but not in others.",
        "Experimental Setup To understand the effects of network architecture on non-convexity, we trained a number of networks, and plotted the landscape around the obtained minimizers using the filter-normalized random direction method described in Section 4.",
        "ResNet-56-noshort has dramatic non-convexities and large regions where the gradient directions do not point towards the minimizer at the center.",
        "Note that visually flatter minimizers consistently correspond to lower test error, which further strengthens our assertion that filter normalization is a natural way to visualize loss function geometry.",
        "For convex-looking surfaces like DenseNet, the negative eigenvalues remain extremely small over a large region of the plot.",
        "Our hope is that effective visualization, when coupled with continued advances in theory, can result in faster training, simpler models, and better generalization"
    ],
    "headline": "We explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balduzzi%2C%20David%20Frean%2C%20Marcus%20Lennox%20Leary%2C%20J.P.Lewis%20Ma%2C%20Kurt%20Wan-Duo%20The%20shattered%20gradients%20problem%3A%20If%20resnets%20are%20the%20answer%2C%20then%20what%20is%20the%20question%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balduzzi%2C%20David%20Frean%2C%20Marcus%20Lennox%20Leary%2C%20J.P.Lewis%20Ma%2C%20Kurt%20Wan-Duo%20The%20shattered%20gradients%20problem%3A%20If%20resnets%20are%20the%20answer%2C%20then%20what%20is%20the%20question%3F%202017"
        },
        {
            "id": "2",
            "entry": "[2] Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In NIPS, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Rivest%2C%20Ronald%20L.%20Training%20a%203-node%20neural%20network%20is%20np-complete%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Rivest%2C%20Ronald%20L.%20Training%20a%203-node%20neural%20network%20is%20np-complete%201989"
        },
        {
            "id": "3",
            "entry": "[3] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing gradient descent into wide valleys. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhari%2C%20Pratik%20Choromanska%2C%20Anna%20Soatto%2C%20Stefano%20LeCun%2C%20Yann%20Entropy-sgd%3A%20Biasing%20gradient%20descent%20into%20wide%20valleys%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhari%2C%20Pratik%20Choromanska%2C%20Anna%20Soatto%2C%20Stefano%20LeCun%2C%20Yann%20Entropy-sgd%3A%20Biasing%20gradient%20descent%20into%20wide%20valleys%202017"
        },
        {
            "id": "4",
            "entry": "[4] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "5",
            "entry": "[5] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "6",
            "entry": "[6] Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. Automated inference with adaptive batches. In AISTATS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=De%2C%20Soham%20Yadav%2C%20Abhay%20Jacobs%2C%20David%20Goldstein%2C%20Tom%20Automated%20inference%20with%20adaptive%20batches%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=De%2C%20Soham%20Yadav%2C%20Abhay%20Jacobs%2C%20David%20Goldstein%2C%20Tom%20Automated%20inference%20with%20adaptive%20batches%202017"
        },
        {
            "id": "7",
            "entry": "[7] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dinh%2C%20Laurent%20Pascanu%2C%20Razvan%20Bengio%2C%20Samy%20Bengio%2C%20Yoshua%20Sharp%20minima%20can%20generalize%20for%20deep%20nets%202017"
        },
        {
            "id": "8",
            "entry": "[8] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In UAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017"
        },
        {
            "id": "9",
            "entry": "[9] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freeman%2C%20C.Daniel%20Bruna%2C%20Joan%20Topology%20and%20geometry%20of%20half-rectified%20network%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freeman%2C%20C.Daniel%20Bruna%2C%20Joan%20Topology%20and%20geometry%20of%20half-rectified%20network%20optimization%202017"
        },
        {
            "id": "10",
            "entry": "[10] Marcus Gallagher and Tom Downs. Visualization of learning in multilayer perceptron networks using principal component analysis. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 33(1):28\u201334, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gallagher%2C%20Marcus%20Downs%2C%20Tom%20Visualization%20of%20learning%20in%20multilayer%20perceptron%20networks%20using%20principal%20component%20analysis%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gallagher%2C%20Marcus%20Downs%2C%20Tom%20Visualization%20of%20learning%20in%20multilayer%20perceptron%20networks%20using%20principal%20component%20analysis%202003"
        },
        {
            "id": "11",
            "entry": "[11] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "12",
            "entry": "[12] Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. arXiv preprint arXiv:1610.07531, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.07531"
        },
        {
            "id": "13",
            "entry": "[13] Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Vinyals%2C%20Oriol%20Saxe%2C%20Andrew%20M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Vinyals%2C%20Oriol%20Saxe%2C%20Andrew%20M.%20Qualitatively%20characterizing%20neural%20network%20optimization%20problems%202015"
        },
        {
            "id": "14",
            "entry": "[14] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "15",
            "entry": "[15] Benjamin D Haeffele and Ren\u00e9 Vidal. Global optimality in neural network training. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haeffele%2C%20Benjamin%20D.%20Vidal%2C%20Ren%C3%A9%20Global%20optimality%20in%20neural%20network%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haeffele%2C%20Benjamin%20D.%20Vidal%2C%20Ren%C3%A9%20Global%20optimality%20in%20neural%20network%20training%202017"
        },
        {
            "id": "16",
            "entry": "[16] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Identity%20matters%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Ma%2C%20Tengyu%20Identity%20matters%20in%20deep%20learning%202017"
        },
        {
            "id": "17",
            "entry": "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20Residual%20Learning%20for%20Image%20Recognition%202016"
        },
        {
            "id": "18",
            "entry": "[18] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural Computation, 9(1):1\u201342, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Flat%20minima%201997"
        },
        {
            "id": "19",
            "entry": "[19] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "20",
            "entry": "[20] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] Daniel Jiwoong Im, Michael Tao, and Kristin Branson. An empirical analysis of deep network loss surfaces. arXiv preprint arXiv:1612.04010, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04010"
        },
        {
            "id": "22",
            "entry": "[22] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20Normalization%3A%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20Normalization%3A%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%202015"
        },
        {
            "id": "23",
            "entry": "[23] Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05468"
        },
        {
            "id": "24",
            "entry": "[24] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20Nitish%20Shirish%20Mudigere%2C%20Dheevatsa%20Nocedal%2C%20Jorge%20Smelyanskiy%2C%20Mikhail%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017"
        },
        {
            "id": "25",
            "entry": "[25] Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In NIPS, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%2C%20Anders%20Krogh%20John%20A%20Hertz.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=and%2C%20Anders%20Krogh%20John%20A%20Hertz.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992"
        },
        {
            "id": "26",
            "entry": "[26] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arXiv preprint arXiv:1705.09886, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09886"
        },
        {
            "id": "27",
            "entry": "[27] Qianli Liao and Tomaso Poggio. Theory of deep learning ii: Landscape of the empirical risk in deep learning. arXiv preprint arXiv:1703.09833, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09833"
        },
        {
            "id": "28",
            "entry": "[28] Zachary C Lipton. Stuck in a what? adventures in weight space. In ICLR Workshop, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lipton%2C%20Zachary%20C.%20Stuck%20in%20a%20what%3F%20adventures%20in%20weight%20space%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lipton%2C%20Zachary%20C.%20Stuck%20in%20a%20what%3F%20adventures%20in%20weight%20space%202016"
        },
        {
            "id": "29",
            "entry": "[29] Eliana Lorch. Visualizing deep network training trajectories with pca. In ICML Workshop on Visualization for Deep Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lorch%2C%20Eliana%20Visualizing%20deep%20network%20training%20trajectories%20with%20pca%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lorch%2C%20Eliana%20Visualizing%20deep%20network%20training%20trajectories%20with%20pca%202016"
        },
        {
            "id": "30",
            "entry": "[30] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20Behnam%20Bhojanapalli%2C%20Srinadh%20McAllester%2C%20David%20Srebro%2C%20Nati%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "31",
            "entry": "[31] Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017"
        },
        {
            "id": "32",
            "entry": "[32] Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016"
        },
        {
            "id": "33",
            "entry": "[33] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition%202015"
        },
        {
            "id": "34",
            "entry": "[34] Leslie N Smith and Nicholay Topin. Exploring loss function topology with cyclical learning rates. arXiv preprint arXiv:1702.04283, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04283"
        },
        {
            "id": "35",
            "entry": "[35] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04926"
        },
        {
            "id": "36",
            "entry": "[36] Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv preprint arXiv:1702.05777, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05777"
        },
        {
            "id": "37",
            "entry": "[37] Grzegorz Swirszcz, Wojciech Marian Czarnecki, and Razvan Pascanu. Local minima in training of deep networks. arXiv preprint arXiv:1611.06310, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06310"
        },
        {
            "id": "38",
            "entry": "[38] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20relu%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tian%2C%20Yuandong%20An%20analytical%20formula%20of%20population%20gradient%20for%20two-layered%20relu%20network%20and%20its%20applications%20in%20convergence%20and%20critical%20point%20analysis%202017"
        },
        {
            "id": "39",
            "entry": "[39] Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In AISTATS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Bo%20Liang%2C%20Yingyu%20Song%2C%20Le%20Diverse%20neural%20network%20learns%20true%20target%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Bo%20Liang%2C%20Yingyu%20Song%2C%20Le%20Diverse%20neural%20network%20learns%20true%20target%20functions%202017"
        },
        {
            "id": "40",
            "entry": "[40] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yun%2C%20Chulhee%20Sra%2C%20Suvrit%20Jadbabaie%2C%20Ali%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yun%2C%20Chulhee%20Sra%2C%20Suvrit%20Jadbabaie%2C%20Ali%20Global%20optimality%20conditions%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "41",
            "entry": "[41] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20Sergey%20Komodakis%2C%20Nikos%20Wide%20residual%20networks%202016"
        },
        {
            "id": "42",
            "entry": "[42] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        }
    ]
}
