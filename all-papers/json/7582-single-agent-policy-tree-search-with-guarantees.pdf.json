{
    "filename": "7582-single-agent-policy-tree-search-with-guarantees.pdf",
    "metadata": {
        "title": "Single-Agent Policy Tree Search With Guarantees",
        "author": "Laurent Orseau, Levi Lelis, Tor Lattimore, Theophane Weber",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7582-single-agent-policy-tree-search-with-guarantees.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce two novel tree search algorithms that use a policy to guide search. The first algorithm is a best-first enumeration that uses a cost function that allows us to provide an upper bound on the number of nodes to be expanded before reaching a goal state. We show that this best-first algorithm is particularly well suited for \u201cneedle-in-a-haystack\u201d problems. The second algorithm, which is based on sampling, provides an upper bound on the expected number of nodes to be expanded before reaching a set of goal states. We show that this algorithm is better suited for problems where many paths lead to a goal. We validate these tree search algorithms on 1,000 computer-generated levels of Sokoban, where the policy used to guide search comes from a neural network trained using A3C. Our results show that the policy tree search algorithms we introduce are competitive with a state-of-the-art domain-independent planner that uses heuristic search."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "cost function",
            "url": "https://en.wikipedia.org/wiki/cost_function"
        },
        {
            "term": "upper bound",
            "url": "https://en.wikipedia.org/wiki/upper_bound"
        },
        {
            "term": "tree search algorithm",
            "url": "https://en.wikipedia.org/wiki/tree_search_algorithm"
        },
        {
            "term": "heuristic search",
            "url": "https://en.wikipedia.org/wiki/heuristic_search"
        },
        {
            "term": "Monte-Carlo tree search",
            "url": "https://en.wikipedia.org/wiki/Monte-Carlo_tree_search"
        },
        {
            "term": "MCTS",
            "url": "https://en.wikipedia.org/wiki/MCTS"
        }
    ],
    "highlights": [
        "Monte-Carlo tree search (MCTS) algorithms [<a class=\"ref-link\" id=\"cCoulom_2007_a\" href=\"#rCoulom_2007_a\"><a class=\"ref-link\" id=\"cCoulom_2007_a\" href=\"#rCoulom_2007_a\">Coulom, 2007</a></a>, <a class=\"ref-link\" id=\"cBrowne_et+al_2012_a\" href=\"#rBrowne_et+al_2012_a\"><a class=\"ref-link\" id=\"cBrowne_et+al_2012_a\" href=\"#rBrowne_et+al_2012_a\">Browne et al, 2012</a></a>] have been recently applied with great success to several problems such as Go, Chess, and Shogi [Silver et al, 2016, 2017]",
        "The sampling procedure used in Monte-Carlo tree search algorithms is not well-suited for other kinds of problems [<a class=\"ref-link\" id=\"cNakhost_2013_a\" href=\"#rNakhost_2013_a\">Nakhost, 2013</a>], such as deterministic single-agent problems where the objective is to find any solution at all",
        "In practice such algorithms can be guided by a heuristic but, to the best of our knowledge, no bound is known that depends on the quality of the heuristic. For such cases one may use instead other traditional search approaches such as A* [<a class=\"ref-link\" id=\"cHart_et+al_1968_a\" href=\"#rHart_et+al_1968_a\">Hart et al, 1968</a>] and Greedy Best-First Search (GBFS) [<a class=\"ref-link\" id=\"cDoran_1966_a\" href=\"#rDoran_1966_a\">Doran and Michie, 1966</a>], which are guided by a heuristic cost function",
        "We introduced two novel tree search algorithms for single-agent problems that are guided by a policy: Levin tree search and LubyTS",
        "The results on the computer-generated Sokoban problems using a pre-trained neural network show that these algorithms can largely improve through tree search upon the score of the network during training",
        "Our results showed that Levin tree search is able to solve most of the levels used in our experiment while expanding many fewer nodes than a state-of-the-art heuristic search planner"
    ],
    "key_statements": [
        "Monte-Carlo tree search (MCTS) algorithms [<a class=\"ref-link\" id=\"cCoulom_2007_a\" href=\"#rCoulom_2007_a\"><a class=\"ref-link\" id=\"cCoulom_2007_a\" href=\"#rCoulom_2007_a\">Coulom, 2007</a></a>, <a class=\"ref-link\" id=\"cBrowne_et+al_2012_a\" href=\"#rBrowne_et+al_2012_a\"><a class=\"ref-link\" id=\"cBrowne_et+al_2012_a\" href=\"#rBrowne_et+al_2012_a\">Browne et al, 2012</a></a>] have been recently applied with great success to several problems such as Go, Chess, and Shogi [Silver et al, 2016, 2017]",
        "The sampling procedure used in Monte-Carlo tree search algorithms is not well-suited for other kinds of problems [<a class=\"ref-link\" id=\"cNakhost_2013_a\" href=\"#rNakhost_2013_a\">Nakhost, 2013</a>], such as deterministic single-agent problems where the objective is to find any solution at all",
        "In practice such algorithms can be guided by a heuristic but, to the best of our knowledge, no bound is known that depends on the quality of the heuristic. For such cases one may use instead other traditional search approaches such as A* [<a class=\"ref-link\" id=\"cHart_et+al_1968_a\" href=\"#rHart_et+al_1968_a\">Hart et al, 1968</a>] and Greedy Best-First Search (GBFS) [<a class=\"ref-link\" id=\"cDoran_1966_a\" href=\"#rDoran_1966_a\">Doran and Michie, 1966</a>], which are guided by a heuristic cost function",
        "We focus on deterministic environments, Levin tree search and LubyTS can be extended to stochastic environments with a known model",
        "We introduced two novel tree search algorithms for single-agent problems that are guided by a policy: Levin tree search and LubyTS",
        "The results on the computer-generated Sokoban problems using a pre-trained neural network show that these algorithms can largely improve through tree search upon the score of the network during training",
        "Our results showed that Levin tree search is able to solve most of the levels used in our experiment while expanding many fewer nodes than a state-of-the-art heuristic search planner"
    ],
    "summary": [
        "Monte-Carlo tree search (MCTS) algorithms [<a class=\"ref-link\" id=\"cCoulom_2007_a\" href=\"#rCoulom_2007_a\"><a class=\"ref-link\" id=\"cCoulom_2007_a\" href=\"#rCoulom_2007_a\">Coulom, 2007</a></a>, <a class=\"ref-link\" id=\"cBrowne_et+al_2012_a\" href=\"#rBrowne_et+al_2012_a\"><a class=\"ref-link\" id=\"cBrowne_et+al_2012_a\" href=\"#rBrowne_et+al_2012_a\">Browne et al, 2012</a></a>] have been recently applied with great success to several problems such as Go, Chess, and Shogi [Silver et al, 2016, 2017].",
        "LevinTS and LubyTS are the first policy tree search algorithms with such guarantees.",
        "The set of expanded nodes is a search tree.",
        "The new Levin tree search (LevinTS) algorithm is a version of TS in which nodes are expanded in order of increasing costs d0(n)/\u03c0(n).",
        "Remembering that a tree search algorithm does not expand children of target nodes, the result follows from observing that E[N] is the expectation of a geometric distribution with success probability \u03c0+",
        "Let N g be the set of target nodes, LubyTS(\u221e, 1) with a policy \u03c0 ensures that the expected number of node expansions before reaching a target node is bounded by",
        "This is a straightforward application of Theorem 5: The randomized program samples a sequence of actions from the policy \u03c0, the running time t becomes the depth d0(n) of a node n, the probability distribution p over halting times becomes the probability of reaching a target node of depth t, p(t) = {n\u2208N g,d0(n)=t} \u03c0(n), and the cumulative distribution function q becomes \u03c0d+.",
        "Because the cumulative probability of the target nodes at depth d is 1/2, LubyTS finds a solution in O(d log d) node expansions, which is an exponential gain over LevinTS.",
        "If the target set contains only one node at some depth d, even when following a uniform policy, LevinTS expands only those 2d nodes.",
        "For both LevinTS and LubyTS, if the provided policy \u03c0 incorrectly assigns a probability too close to 0 to some sequences of actions, the algorithm may never find the solution.",
        "The Uniform searcher (LevinTS with a uniform policy) with maximum 100,000 node expansions per level\u2014and still with state cuts\u2014can solve no more than 9% of the levels, which shows that the problem is not trivial.",
        "LevinTS has to expand a large number of nodes for a small number of levels likely due to the training procedure used to derive the policy.",
        "We introduced two novel tree search algorithms for single-agent problems that are guided by a policy: LevinTS and LubyTS.",
        "LevinTS and LubyTS depart from the traditional heuristic approach to tree search by employing a policy instead of a heuristic function to guide search while still offering important guarantees.",
        "Our results showed that LevinTS is able to solve most of the levels used in our experiment while expanding many fewer nodes than a state-of-the-art heuristic search planner.",
        "LevinTS was able to find considerably shorter solutions than the planner"
    ],
    "headline": "We introduce two novel tree search algorithms that use a policy to guide search",
    "reference_links": [
        {
            "id": "Browne_et+al_2012_a",
            "entry": "C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1\u201343, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Browne%2C%20C.B.%20Powley%2C%20E.%20Whitehouse%2C%20D.%20Lucas%2C%20S.M.%20A%20survey%20of%20monte%20carlo%20tree%20search%20methods%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Browne%2C%20C.B.%20Powley%2C%20E.%20Whitehouse%2C%20D.%20Lucas%2C%20S.M.%20A%20survey%20of%20monte%20carlo%20tree%20search%20methods%202012"
        },
        {
            "id": "Coulom_2007_a",
            "entry": "R. Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In Computers and Games, pages 72\u201383. Springer Berlin Heidelberg, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coulom%2C%20R.%20Efficient%20selectivity%20and%20backup%20operators%20in%20monte-carlo%20tree%20search%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coulom%2C%20R.%20Efficient%20selectivity%20and%20backup%20operators%20in%20monte-carlo%20tree%20search%202007"
        },
        {
            "id": "Culberson_1999_a",
            "entry": "J. C. Culberson. Sokoban is PSPACE-Complete. In Fun With Algorithms, pages 65\u201376, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Culberson%2C%20J.C.%20Sokoban%20is%20PSPACE-Complete.%20In%20Fun%20With%20Algorithms%201999"
        },
        {
            "id": "Doran_1966_a",
            "entry": "J. E. Doran and D. Michie. Experiments with the graph traverser program. In Royal Society of London A, volume 294, pages 235\u2013259. The Royal Society, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doran%2C%20J.E.%20Michie%2C%20D.%20Experiments%20with%20the%20graph%20traverser%20program%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doran%2C%20J.E.%20Michie%2C%20D.%20Experiments%20with%20the%20graph%20traverser%20program%201966"
        },
        {
            "id": "Hart_et+al_1968_a",
            "entry": "P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, SSC-4(2):100\u2013107, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hart%2C%20P.E.%20Nilsson%2C%20N.J.%20Raphael%2C%20B.%20A%20formal%20basis%20for%20the%20heuristic%20determination%20of%20minimum%20cost%20paths%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hart%2C%20P.E.%20Nilsson%2C%20N.J.%20Raphael%2C%20B.%20A%20formal%20basis%20for%20the%20heuristic%20determination%20of%20minimum%20cost%20paths%201968"
        },
        {
            "id": "Helmert_2006_a",
            "entry": "M. Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26: 191\u2013246, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Helmert%2C%20M.%20The%20fast%20downward%20planning%20system%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Helmert%2C%20M.%20The%20fast%20downward%20planning%20system%202006"
        },
        {
            "id": "Hoffmann_2001_a",
            "entry": "J. Hoffmann and B. Nebel. The FF planning system: Fast plan generation through heuristic search. Journal of Artificial Intelligence Research, 14:253\u2013302, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffmann%2C%20J.%20Nebel%2C%20B.%20The%20FF%20planning%20system%3A%20Fast%20plan%20generation%20through%20heuristic%20search%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffmann%2C%20J.%20Nebel%2C%20B.%20The%20FF%20planning%20system%3A%20Fast%20plan%20generation%20through%20heuristic%20search%202001"
        },
        {
            "id": "Korf_1985_a",
            "entry": "R. E. Korf. Depth-first iterative-deepening. Artificial Intelligence, 27(1):97 \u2013 109, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Korf%2C%20R.E.%20Depth-first%20iterative-deepening%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Korf%2C%20R.E.%20Depth-first%20iterative-deepening%201985"
        },
        {
            "id": "Levin_1973_a",
            "entry": "L. A. Levin. Universal sequential search problems. Problems of Information Transmission, 9(3): 265\u2013266, 1973.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levin%2C%20L.A.%20Universal%20sequential%20search%20problems.%20Problems%20of%201973",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levin%2C%20L.A.%20Universal%20sequential%20search%20problems.%20Problems%20of%201973"
        },
        {
            "id": "Luby_et+al_1993_a",
            "entry": "M. Luby, A. Sinclair, and D. Zuckerman. Optimal speedup of Las Vegas algorithms. Inf. Process. Lett., 47(4):173\u2013180, Sept. 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luby%2C%20M.%20Sinclair%2C%20A.%20Zuckerman%2C%20D.%20Optimal%20speedup%20of%20Las%20Vegas%20algorithms%201993-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luby%2C%20M.%20Sinclair%2C%20A.%20Zuckerman%2C%20D.%20Optimal%20speedup%20of%20Las%20Vegas%20algorithms%201993-09"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 1928\u20131937. PMLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Nakhost_2013_a",
            "entry": "H. Nakhost. Random Walk Planning: Theory, Practice, and Application. PhD thesis, University of Alberta, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nakhost%2C%20H.%20Random%20Walk%20Planning%3A%20Theory%2C%20Practice%2C%20and%20Application%202013"
        },
        {
            "id": "Orseau_et+al_2017_a",
            "entry": "L. Orseau, T. Lattimore, and S. Legg. Soft-bayes: Prod for mixtures of experts with log-loss. In Proceedings of the 28th International Conference on Algorithmic Learning Theory, volume 76 of Proceedings of Machine Learning Research, pages 372\u2013399, Kyoto University, Kyoto, Japan, 15\u201317 Oct 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orseau%2C%20L.%20Lattimore%2C%20T.%20Legg%2C%20S.%20Soft-bayes%3A%20Prod%20for%20mixtures%20of%20experts%20with%20log-loss%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orseau%2C%20L.%20Lattimore%2C%20T.%20Legg%2C%20S.%20Soft-bayes%3A%20Prod%20for%20mixtures%20of%20experts%20with%20log-loss%202017-10"
        },
        {
            "id": "Racani_et+al_2017_a",
            "entry": "S. Racani\u00e8re, T. Weber, D. Reichert, L. Buesing, A. Guez, D. Jimenez Rezende, A. Puigdom\u00e8nech Badia, O. Vinyals, N. Heess, Y. Li, R. Pascanu, P. Battaglia, D. Hassabis, D. Silver, and D. Wierstra. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems 30, pages 5690\u20135701. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Racani%C3%A8re%2C%20S.%20Weber%2C%20T.%20Reichert%2C%20D.%20Buesing%2C%20L.%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Racani%C3%A8re%2C%20S.%20Weber%2C%20T.%20Reichert%2C%20D.%20Buesing%2C%20L.%20Imagination-augmented%20agents%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Reinefeld_1994_a",
            "entry": "A. Reinefeld and T. A. Marsland. Enhanced iterative-deepening search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(7):701\u2013710, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reinefeld%2C%20A.%20Marsland%2C%20T.A.%20Enhanced%20iterative-deepening%20search%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reinefeld%2C%20A.%20Marsland%2C%20T.A.%20Enhanced%20iterative-deepening%20search%201994"
        },
        {
            "id": "Richter_2010_a",
            "entry": "S. Richter and M. Westphal. The lama planner: Guiding cost-based anytime planning with landmarks. Journal of Artificial Intelligence Research, 39(1):127\u2013177, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richter%2C%20S.%20Westphal%2C%20M.%20The%20lama%20planner%3A%20Guiding%20cost-based%20anytime%20planning%20with%20landmarks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richter%2C%20S.%20Westphal%2C%20M.%20The%20lama%20planner%3A%20Guiding%20cost-based%20anytime%20planning%20with%20landmarks%202010"
        },
        {
            "id": "D_2016_a",
            "entry": "D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "Silver_et+al_2017_a",
            "entry": "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.",
            "url": "http://arxiv.org/abs/1712.01815",
            "arxiv_url": "https://arxiv.org/pdf/1712.01815"
        },
        {
            "id": "Solomonoff_1984_a",
            "entry": "R. J. Solomonoff. Optimum sequential search. Oxbridge Research, 1984. T. Tieleman and G. Hinton. Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012. B. A. Trakhtenbrot. A survey of Russian approaches to Perebor (brute-force searches) algorithms.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Solomonoff%2C%20R.J.%20Optimum%20sequential%20search%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Solomonoff%2C%20R.J.%20Optimum%20sequential%20search%201984"
        },
        {
            "id": "Annals_1984_a",
            "entry": "Annals of the History of Computing, 6(4):384\u2013400, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Annals%20of%20the%20History%20of%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Annals%20of%20the%20History%20of%201984"
        },
        {
            "id": "Xie_et+al_2012_a",
            "entry": "F. Xie, H. Nakhost, and M. M\u00fcller. Planning via random walk-driven local search. In Proceedings of the Twenty-Second International Conference on Automated Planning and Scheduling, pages 315\u2013322, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20F.%20Nakhost%2C%20H.%20M%C3%BCller%2C%20M.%20Planning%20via%20random%20walk-driven%20local%20search%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20F.%20Nakhost%2C%20H.%20M%C3%BCller%2C%20M.%20Planning%20via%20random%20walk-driven%20local%20search%202012"
        }
    ]
}
