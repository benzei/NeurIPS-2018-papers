{
    "filename": "8182-generalization-bounds-for-uniformly-stable-algorithms.pdf",
    "metadata": {
        "title": "Generalization Bounds for Uniformly Stable Algorithms",
        "author": "Vitaly Feldman, Jan Vondrak",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8182-generalization-bounds-for-uniformly-stable-algorithms.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Uniform stability of a learning algorithm is a classical notion of algorithmic stability introduced to derive high-probability bounds on the generalization error (Bousquet and Elisseeff, 2002). Specifically, for a loss function with range bounded in [0, 1], the generalization error of \u03b3-uniformly stable learning algorithm on n samples is known to be at most O((\u03b3 + 1/n) n log(1/\u03b4)) with probability at least 1 \u2212 \u03b4. Unfortunately, this bound does not le\u221aad to meaningful generalization bounds in many common settings where \u03b3 \u2265 1/ n. At the same time the bound is known to be tight only when \u03b3 = O(1/n). Here we prove substantially stronger generalization bounds for uniformly stable algorithms without any additional assumptions. First, we show that the generalization error in this setting is at most O( (\u03b3 + 1/n) log(1/\u03b4)) with probability at least 1 \u2212 \u03b4. In addition, we prove a tight bound of O(\u03b32 + 1/n) on the second moment of the generalization error. The best previous bound on the second moment of the generalization error is O(\u03b3 + 1/n). Our proofs are based on new analysis techniques and our results imply substantially stronger generalization guarantees for several well-studied algorithms."
    },
    "keywords": [
        {
            "term": "cross validation",
            "url": "https://en.wikipedia.org/wiki/cross_validation"
        },
        {
            "term": "uniform convergence",
            "url": "https://en.wikipedia.org/wiki/uniform_convergence"
        },
        {
            "term": "generalization error",
            "url": "https://en.wikipedia.org/wiki/generalization_error"
        },
        {
            "term": "algorithmic stability",
            "url": "https://en.wikipedia.org/wiki/algorithmic_stability"
        },
        {
            "term": "second moment",
            "url": "https://en.wikipedia.org/wiki/second_moment"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "We consider the basic problem of estimating the generalization error of learning algorithms",
        "There exist empirical risk minimizing algorithms whose generalization error is d times larger than the generalization error of stochastic gradient descent, where d is the dimension of the problem [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]. This disparity stems from the fact that uniform convergence bounds largely ignore the way in which the model output by the algorithm depends on the data",
        "We note that in the restricted setting of generalized linear models one can obtain tight generalization bounds via uniform convergence [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]. Another classical approach to proving generalization bounds is to analyze the stability of the learning algorithm to changes in the dataset",
        "A is said to have uniform stability \u03b3n with respect to if for any pair of datasets S, S \u2208 Zn that differ in a single element and every z \u2208 Z, | (A(S), z) \u2212 (A(S ), z)| \u2264 \u03b3n",
        "We show that from multiple executions of M on independently chosen datasets it is possible to select the execution of M with approximately the largest generalization error",
        "Our approach to proving the high-probability generalization bounds is based on the technique introduced by [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to show that differentially private algorithm have strong generalization properties"
    ],
    "key_statements": [
        "We consider the basic problem of estimating the generalization error of learning algorithms",
        "There exist empirical risk minimizing algorithms whose generalization error is d times larger than the generalization error of stochastic gradient descent, where d is the dimension of the problem [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]. This disparity stems from the fact that uniform convergence bounds largely ignore the way in which the model output by the algorithm depends on the data",
        "We note that in the restricted setting of generalized linear models one can obtain tight generalization bounds via uniform convergence [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]. Another classical approach to proving generalization bounds is to analyze the stability of the learning algorithm to changes in the dataset",
        "A is said to have uniform stability \u03b3n with respect to if for any pair of datasets S, S \u2208 Zn that differ in a single element and every z \u2208 Z, | (A(S), z) \u2212 (A(S ), z)| \u2264 \u03b3n",
        "We summarize the generalization properties of uniform stability in the below (all proved in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "We show that from multiple executions of M on independently chosen datasets it is possible to select the execution of M with approximately the largest generalization error",
        "We show that our results can be used to improve the recent bounds on generalization error of learning algorithms with differentially private prediction",
        "Our approach to proving the high-probability generalization bounds is based on the technique introduced by [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to show that differentially private algorithm have strong generalization properties",
        "L has uniform stability of at most 2\u03b3 since for two datasets S and S that differ in a single element, Observe that"
    ],
    "summary": [
        "We consider the basic problem of estimating the generalization error of learning algorithms.",
        "The standard generalization error bounds for stochastic gradient descent (SGD) on convex Lipschitz functions cannot be obtained by proving uniform convergence for all empirical risk minimizers (ERM) [1\u221a3, 26].",
        "Another classical approach to proving generalization bounds is to analyze the stability of the learning algorithm to changes in the dataset.",
        "Let A : Zn \u2192 F be a learning algorithm that has uniform stability \u03b3n with respect to a loss function : F \u00d7 Z \u2192 [0, 1].",
        "Our techniques The high-probability generalization result in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]) is based on a simple observation that as a function of S, \u2206P\u2212S(M ) has the bounded differences property.",
        "Applications We apply our bounds on the generalization error to several known uniformly stable algorithms in a straightforward way.",
        "We note that for the stability-based analysis in this\u221acase even \u201clow-probability\" generalization bounds were not known for the optimal error rate of 1/ n.",
        "We show that our results can be used to improve the recent bounds on generalization error of learning algorithms with differentially private prediction.",
        "For an -differentially private prediction algorithm, every loss function : Y \u00d7 Y \u2192 [0, 1], two datasets S, S \u2208 (X \u00d7 Y )n that differ in a single element and (x, y) \u2208 X \u00d7 Y : E[ (M (S, x), y)] \u2212 E[ (M (S , x), y)] \u2264 e \u2212 1.",
        "Work on stability focused on extensions of these results to other \u201clocal\u201d algorithms and estimators and focused primarily on variance (a notable exception is [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] where high probability bounds on the generalization error of k-NN are proved).",
        "The results in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] on uniform stability and their applications to generalization properties of strongly convex ERM algorithms have been extended and generalized in several directions (e.g.",
        "Uniform stability plays an important role in privacy-preserving learning since a differentially private learning algorithm can usually be obtained one by adding noise to the output of a uniformly stable one (e.g.",
        "Our approach to proving the high-probability generalization bounds is based on the technique introduced by [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to show that differentially private algorithm have strong generalization properties.",
        "The final ingredient of our proof is a bound on the expected generalization error of any uniformly stable algorithm on a sub-dataset chosen in a differentially private way.",
        "Let L : Zn \u00d7 Z \u2192 [\u22121, 1] be a data-dependent function with uniform stability \u03b3 and P be an arbitrary distribution over Z."
    ],
    "headline": "We prove substantially stronger generalization bounds for uniformly stable algorithms without any additional assumptions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Karim T. Abou-Moustafa and Csaba Szepesv\u00e1ri. An exponential tail bound for lq stable learning rules. application to k-folds cross-validation. In ISAIM, 2018. URL http://isaim2018.cs.virginia.edu/papers/ISAIM2018_Abou-Moustafa_Szepesvari.pdf.",
            "url": "http://isaim2018.cs.virginia.edu/papers/ISAIM2018_Abou-Moustafa_Szepesvari.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karim%2C%20T.%20Abou-Moustafa%20and%20Csaba%20Szepesv%C3%A1ri.%20An%20exponential%20tail%20bound%20for%20lq%20stable%20learning%20rules.%20application%20to%20k-folds%20cross-validation%202018"
        },
        {
            "id": "2",
            "entry": "[2] Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan Ullman. Algorithmic stability for adaptive data analysis. In STOC, pages 1046\u20131059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bassily%2C%20Raef%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20D.%20Steinke%2C%20Thomas%20Algorithmic%20stability%20for%20adaptive%20data%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bassily%2C%20Raef%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20D.%20Steinke%2C%20Thomas%20Algorithmic%20stability%20for%20adaptive%20data%20analysis%202016"
        },
        {
            "id": "3",
            "entry": "[3] Avrim Blum, Adam Kalai, and John Langford. Beating the hold-out: Bounds for k-fold and progressive cross-validation. In COLT, pages 203\u2013208, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Kalai%2C%20Adam%20Langford%2C%20John%20Beating%20the%20hold-out%3A%20Bounds%20for%20k-fold%20and%20progressive%20cross-validation%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Kalai%2C%20Adam%20Langford%2C%20John%20Beating%20the%20hold-out%3A%20Bounds%20for%20k-fold%20and%20progressive%20cross-validation%201999"
        },
        {
            "id": "4",
            "entry": "[4] Olivier Bousquet and Andr\u00e9 Elisseeff. Stability and generalization. JMLR, 2:499\u2013526, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andr%C3%A9%20Stability%20and%20generalization%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andr%C3%A9%20Stability%20and%20generalization%202002"
        },
        {
            "id": "5",
            "entry": "[5] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20N.%20Conconi%2C%20A.%20Gentile%2C%20C.%20On%20the%20generalization%20ability%20of%20on-line%20learning%20algorithms%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20N.%20Conconi%2C%20A.%20Gentile%2C%20C.%20On%20the%20generalization%20ability%20of%20on-line%20learning%20algorithms%202004"
        },
        {
            "id": "6",
            "entry": "[6] Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12:1069\u20131109, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chaudhuri%2C%20Kamalika%20Monteleoni%2C%20Claire%20Sarwate%2C%20Anand%20D.%20Differentially%20private%20empirical%20risk%20minimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chaudhuri%2C%20Kamalika%20Monteleoni%2C%20Claire%20Sarwate%2C%20Anand%20D.%20Differentially%20private%20empirical%20risk%20minimization%202011"
        },
        {
            "id": "7",
            "entry": "[7] L. Devroye, L. Gy\u00f6rfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devroye%2C%20L.%20Gy%C3%B6rfi%2C%20L.%20Lugosi%2C%20G.%20A%20Probabilistic%20Theory%20of%20Pattern%20Recognition%201996"
        },
        {
            "id": "8",
            "entry": "[8] Luc Devroye and Terry J. Wagner. Distribution-free inequalities for the deleted and holdout error estimates. IEEE Trans. Information Theory, 25(2):202\u2013207, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devroye%2C%20Luc%20Wagner%2C%20Terry%20J.%20Distribution-free%20inequalities%20for%20the%20deleted%20and%20holdout%20error%20estimates%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devroye%2C%20Luc%20Wagner%2C%20Terry%20J.%20Distribution-free%20inequalities%20for%20the%20deleted%20and%20holdout%20error%20estimates%201979"
        },
        {
            "id": "9",
            "entry": "[9] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In TCC, pages 265\u2013284, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20C.%20McSherry%2C%20F.%20Nissim%2C%20K.%20Smith%2C%20A.%20Calibrating%20noise%20to%20sensitivity%20in%20private%20data%20analysis%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20C.%20McSherry%2C%20F.%20Nissim%2C%20K.%20Smith%2C%20A.%20Calibrating%20noise%20to%20sensitivity%20in%20private%20data%20analysis%202006"
        },
        {
            "id": "10",
            "entry": "[10] Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. CoRR, abs/1803.10266, 2018. URL http://arxiv.org/abs/1803.10266. Extended abstract in COLT 2018.",
            "url": "http://arxiv.org/abs/1803.10266",
            "arxiv_url": "https://arxiv.org/pdf/1803.10266"
        },
        {
            "id": "11",
            "entry": "[11] Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1411.2664"
        },
        {
            "id": "12",
            "entry": "[12] Andr\u00e9 Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning algorithms. Journal of Machine Learning Research, 6:55\u201379, 2005. URL http://www.jmlr.org/papers/v6/elisseeff05a.html.",
            "url": "http://www.jmlr.org/papers/v6/elisseeff05a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elisseeff%2C%20Andr%C3%A9%20Evgeniou%2C%20Theodoros%20Pontil%2C%20Massimiliano%20Stability%20of%20randomized%20learning%20algorithms%202005"
        },
        {
            "id": "13",
            "entry": "[13] Vitaly Feldman. Generalization of ERM in stochastic convex optimization: The dimension strikes back. CoRR, abs/1608.04414, 2016. URL http://arxiv.org/abs/1608.04414. Extended abstract in NIPS 2016.",
            "url": "http://arxiv.org/abs/1608.04414",
            "arxiv_url": "https://arxiv.org/pdf/1608.04414"
        },
        {
            "id": "14",
            "entry": "[14] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In ICML, pages 1225\u20131234, 2016. URL http://jmlr.org/proceedings/papers/v48/hardt16.html.",
            "url": "http://jmlr.org/proceedings/papers/v48/hardt16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "15",
            "entry": "[15] S. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In NIPS, pages 793\u2013800, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20S.%20Sridharan%2C%20K.%20Tewari%2C%20A.%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20S.%20Sridharan%2C%20K.%20Tewari%2C%20A.%20On%20the%20complexity%20of%20linear%20prediction%3A%20Risk%20bounds%2C%20margin%20bounds%2C%20and%20regularization%202008"
        },
        {
            "id": "16",
            "entry": "[16] Satyen Kale, Ravi Kumar, and Sergei Vassilvitskii. Cross-validation and mean-square stability. In Innovations in Computer Science - ICS, pages 487\u2013495, 2011. URL http://conference.itcs.tsinghua.edu.cn/ICS2011/content/papers/31.html.",
            "url": "http://conference.itcs.tsinghua.edu.cn/ICS2011/content/papers/31.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kale%2C%20Satyen%20Kumar%2C%20Ravi%20Vassilvitskii%2C%20Sergei%20Cross-validation%20and%20mean-square%20stability%202011"
        },
        {
            "id": "17",
            "entry": "[17] Ravi Kumar, Daniel Lokshtanov, Sergei Vassilvitskii, and Andrea Vattani. Near-optimal bounds for cross-validation via loss stability. In ICML, pages 27\u201335, 2013. URL http://jmlr.org/proceedings/papers/v28/kumar13a.html.",
            "url": "http://jmlr.org/proceedings/papers/v28/kumar13a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20Ravi%20Lokshtanov%2C%20Daniel%20Vassilvitskii%2C%20Sergei%20Vattani%2C%20Andrea%20Near-optimal%20bounds%20for%20cross-validation%20via%20loss%20stability%202013"
        },
        {
            "id": "18",
            "entry": "[18] Tongliang Liu, G\u00e1bor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis complexity. In ICML, pages 2159\u20132167, 2017. URL http://proceedings.mlr.press/v70/liu17c.html.",
            "url": "http://proceedings.mlr.press/v70/liu17c.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Tongliang%20Lugosi%2C%20G%C3%A1bor%20Neu%2C%20Gergely%20Tao%2C%20Dacheng%20Algorithmic%20stability%20and%20hypothesis%20complexity%202017"
        },
        {
            "id": "19",
            "entry": "[19] Ben London. A pac-bayesian analysis of randomized learning with application to stochastic gradient descent. In NIPS, pages 2935\u20132944, 2017. URL http://papers.nips.cc/paper/6886-a-pac-bayesian-analysis-of-randomized-learning-with-application-to-stochastic-gradient-",
            "url": "http://papers.nips.cc/paper/6886-a-pac-bayesian-analysis-of-randomized-learning-with-application-to-stochastic-gradient-",
            "oa_query": "https://api.scholarcy.com/oa_version?query=London%2C%20Ben%20A%20pac-bayesian%20analysis%20of%20randomized%20learning%20with%20application%20to%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "20",
            "entry": "[20] Andreas Maurer. A second-order look at stability and generalization. In COLT, pages 1461\u2013 1475, 2017. URL http://proceedings.mlr.press/v65/maurer17a.html.",
            "url": "http://proceedings.mlr.press/v65/maurer17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20A%20second-order%20look%20at%20stability%20and%20generalization%202017"
        },
        {
            "id": "21",
            "entry": "[21] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS, pages 94\u2013103, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McSherry%2C%20Frank%20Talwar%2C%20Kunal%20Mechanism%20design%20via%20differential%20privacy%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McSherry%2C%20Frank%20Talwar%2C%20Kunal%20Mechanism%20design%20via%20differential%20privacy%202007"
        },
        {
            "id": "22",
            "entry": "[22] Kobbi Nissim and Uri Stemmer. On the generalization properties of differential privacy. CoRR, abs/1504.05800, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.05800"
        },
        {
            "id": "23",
            "entry": "[23] Kobbi Nissim and Uri Stemmer. Concentration bounds for high sensitivity functions through differential privacy. CoRR, abs/1703.01970, 2017. URL http://arxiv.org/abs/1703.01970.",
            "url": "http://arxiv.org/abs/1703.01970",
            "arxiv_url": "https://arxiv.org/pdf/1703.01970"
        },
        {
            "id": "24",
            "entry": "[24] Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for predictivity in learning theory. Nature, 428(6981):419\u2013422, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poggio%2C%20Tomaso%20Rifkin%2C%20Ryan%20Mukherjee%2C%20Sayan%20Niyogi%2C%20Partha%20General%20conditions%20for%20predictivity%20in%20learning%20theory%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poggio%2C%20Tomaso%20Rifkin%2C%20Ryan%20Mukherjee%2C%20Sayan%20Niyogi%2C%20Partha%20General%20conditions%20for%20predictivity%20in%20learning%20theory%202004"
        },
        {
            "id": "25",
            "entry": "[25] W. H. Rogers and T. J. Wagner. A finite sample distribution-free performance bound for local discrimination rules. The Annals of Statistics, 6(3):506\u2013514, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rogers%2C%20W.H.%20Wagner%2C%20T.J.%20A%20finite%20sample%20distribution-free%20performance%20bound%20for%20local%20discrimination%20rules%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rogers%2C%20W.H.%20Wagner%2C%20T.J.%20A%20finite%20sample%20distribution-free%20performance%20bound%20for%20local%20discrimination%20rules%201978"
        },
        {
            "id": "26",
            "entry": "[26] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635\u20132670, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20Srebro%2C%20Nathan%20Sridharan%2C%20Karthik%20Learnability%2C%20stability%20and%20uniform%20convergence%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20Srebro%2C%20Nathan%20Sridharan%2C%20Karthik%20Learnability%2C%20stability%20and%20uniform%20convergence%202010"
        },
        {
            "id": "27",
            "entry": "[27] Thomas Steinke and Jonathan Ullman. Subgaussian tail bounds via stability arguments. arXiv preprint arXiv:1701.03493, 2017. URL https://arxiv.org/abs/1701.03493.",
            "url": "https://arxiv.org/abs/1701.03493",
            "arxiv_url": "https://arxiv.org/pdf/1701.03493"
        },
        {
            "id": "28",
            "entry": "[28] Rosasco Lorenzo Wibisono, Andre and Tomaso Poggio. Sufficient conditions for uniform stability of regularization algorithms. Technical Report MIT-CSAIL-TR-2009-060, MIT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wibisono%2C%20Rosasco%20Lorenzo%20Andre%20Poggio%2C%20Tomaso%20Sufficient%20conditions%20for%20uniform%20stability%20of%20regularization%20algorithms%202009"
        },
        {
            "id": "29",
            "entry": "[29] Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolton differential privacy for scalable stochastic gradient descent-based analytics. In (SIGMOD), pages 1307\u20131322, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Xi%20Li%2C%20Fengan%20Kumar%2C%20Arun%20Chaudhuri%2C%20Kamalika%20Bolton%20differential%20privacy%20for%20scalable%20stochastic%20gradient%20descent-based%20analytics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Xi%20Li%2C%20Fengan%20Kumar%2C%20Arun%20Chaudhuri%2C%20Kamalika%20Bolton%20differential%20privacy%20for%20scalable%20stochastic%20gradient%20descent-based%20analytics%202017"
        },
        {
            "id": "30",
            "entry": "[30] Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397\u20131437, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Leave-one-out%20bounds%20for%20kernel%20methods%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Leave-one-out%20bounds%20for%20kernel%20methods%202003"
        }
    ]
}
