{
    "filename": "7870-sequential-test-for-the-lowest-mean-from-thompson-to-murphy-sampling.pdf",
    "metadata": {
        "date": 2018,
        "title": "Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling",
        "author": "Emilie Kaufmann1 Wouter M. Koolen2 Aur\u00e9lien Garivier3 1 CNRS & U. Lille, CRIStAL / SequeL Inria Lille, emilie.kaufmann@univ-lille.fr",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7870-sequential-test-for-the-lowest-mean-from-thompson-to-murphy-sampling.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-task in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "finite set",
            "url": "https://en.wikipedia.org/wiki/finite_set"
        },
        {
            "term": "thompson sampling",
            "url": "https://en.wikipedia.org/wiki/thompson_sampling"
        },
        {
            "term": "confidence interval",
            "url": "https://en.wikipedia.org/wiki/confidence_interval"
        },
        {
            "term": "tree search",
            "url": "https://en.wikipedia.org/wiki/tree_search"
        }
    ],
    "highlights": [
        "We consider a collection of core problems related to minimums of means",
        "Knowing about minima/maxima is crucial in reinforcement learning or game-playing, where the value of a state for an agent is the maximum over actions of the successor state value or the minimum over adversary moves of the state value",
        "The open problem of designing asymptotically optimal algorithms for Monte Carlo Tree Search led us to isolate one core difficulty that we study here, namely the construction of confidence intervals and associated sampling/stopping rules for learning minima",
        "We investigate the combined use of three sampling rules, MS, LCB and Thompson Sampling with three stopping rules, \u03c4 Agg, \u03c4 Box and \u03c4 Generalized Likelihood Ratio Tests",
        "We propose new sampling and stopping rules for sequentially testing the minimum of means",
        "We make first contributions in both directions: we prove tighter sample complexity lower bounds for symmetric algorithms (Proposition 2, Theorem 3) and we design aggregating confidence intervals which are tighter in practice (Figure 2)"
    ],
    "key_statements": [
        "We consider a collection of core problems related to minimums of means",
        "For a given finite collection of probability distributions parameterized by their means \u03bc1, . . . , \u03bcK, we are interested in learning about \u03bc\u2217 = mina \u03bca from adaptive samples Xt \u223c \u03bcAt , where anytime sampling strategy indicates the distribution sampled at time t",
        "Knowing about minima/maxima is crucial in reinforcement learning or game-playing, where the value of a state for an agent is the maximum over actions of the successor state value or the minimum over adversary moves of the state value",
        "The open problem of designing asymptotically optimal algorithms for Monte Carlo Tree Search led us to isolate one core difficulty that we study here, namely the construction of confidence intervals and associated sampling/stopping rules for learning minima",
        "Our second contribution is a new sampling rule for the minimum testing problem, under which the empirical fraction of selections converges to the optimal allocation without forced exploration",
        "In Section 2 we introduce our notation and formally define our objective",
        "In Section 4 we introduce Murphy sampling, and prove its optimality in conjunction with a simple stopping rule",
        "In light of the lower bound in Lemma 1, we investigate the design of optimal learning algorithms",
        "We show that a simple algorithm, called LCB, can do that for all \u03bc \u2208 H>",
        "In Appendix E we prove (Proposition 15) that LCB is optimal for \u03bc \u2208 H> we show (Proposition 16) that on instances of H< it draws all arms a \u2260 a\u2217 too much and cannot match our lower bound",
        "The literature on regret minimization in bandit models provides candidate algorithms that have this type of behavior, and we propose to use the",
        "We presented a simple stopping rule, \u03c4Box, that can be asymptotically optimal for every \u03bc \u2208 H< if it is used in combination with Thompson Sampling and for \u03bc \u2208 H> if it is used in combination with LCB",
        "We propose a new Thompson Sampling like algorithm that ensures the right exploration under both H< and H>",
        "In Section 5, we further present an improved stopping rule that may stop significantly earlier than \u03c4Box on instances of H<, by aggregating samples from multiple arms that look small",
        "We argue that ensuring the sampling proportions converge to w\u2217 is sufficient for reaching the optimal sample complexity, at least in an asymptotic sense",
        "We introduce a new sampling rule called Murphy Sampling after Murphy\u2019s Law, as it performs some conditionning to the \u201cworst event\u201d (\u03bc \u2208 H<): MS: Sample \u03b8t \u223c \u03a0t\u22121 (\u22c5 H<), play anytime sampling strategy = a\u2217",
        "We argue that the MS sampling proportions converge to the oracle weights of Lemma 1",
        "We investigate the combined use of three sampling rules, MS, LCB and Thompson Sampling with three stopping rules, \u03c4 Agg, \u03c4 Box and \u03c4 Generalized Likelihood Ratio Tests",
        "We propose new sampling and stopping rules for sequentially testing the minimum of means",
        "We make first contributions in both directions: we prove tighter sample complexity lower bounds for symmetric algorithms (Proposition 2, Theorem 3) and we design aggregating confidence intervals which are tighter in practice (Figure 2)"
    ],
    "summary": [
        "We consider a collection of core problems related to minimums of means.",
        "The open problem of designing asymptotically optimal algorithms for MCTS led us to isolate one core difficulty that we study here, namely the construction of confidence intervals and associated sampling/stopping rules for learning minima.",
        "Our second contribution is a new sampling rule for the minimum testing problem, under which the empirical fraction of selections converges to the optimal allocation without forced exploration.",
        "In Section 3, we present lower bounds on the sample complexity of sequential tests for minima.",
        "In Section 4 we introduce Murphy sampling, and prove its optimality in conjunction with a simple stopping rule.",
        "We study information-theoretic sample complexity lower bounds, in particular to find out what the problem tells us about the behavior of oracle algorithms.",
        "In Appendix E we prove (Proposition 15) that LCB is optimal for \u03bc \u2208 H> we show (Proposition 16) that on instances of H< it draws all arms a \u2260 a\u2217 too much and cannot match our lower bound.",
        "As TS is an anytime sampling strategy, Lemma 4 below permits to justify that on every instance of H< with a unique optimal arm, under this algorithm \u03c4Box \u2243 (1 d(\u03bc1, \u03b8)) ln(1 \u03b4).",
        "In Section 5, we further present an improved stopping rule that may stop significantly earlier than \u03c4Box on instances of H<, by aggregating samples from multiple arms that look small.",
        "Given the convergence of the weights, the asymptotic optimality in terms of sample complexity follows by Lemma 4, if MS is used with an appropriate stopping rule (Box (2) or the improved Aggregate stopping rule discussed in Section 5).",
        "First it can be observed that picking \u03c0 that is uniform over subset of size 1, i.e. \u03c0(S) = K\u221211( S = 1), one obtain a \u03b4-correct \u03c4Box stopping rule with thresholds functions satisfying the assumptions of Lemma 4.",
        "The larger the number of arms close to minimum is, the more UCB Aggregate beats UCB Box. Observe that using inequality (7) in Theorem 7 allows to derive tighter lower confidence bounds on the maximum of several means.",
        "We propose new sampling and stopping rules for sequentially testing the minimum of means.",
        "We make first contributions in both directions: we prove tighter sample complexity lower bounds for symmetric algorithms (Proposition 2, Theorem 3) and we design aggregating confidence intervals which are tighter in practice (Figure 2)."
    ],
    "headline": "We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Agrawal and N. Goyal. Analysis of Thompson Sampling for the multi-armed bandit problem. In Proceedings of the 25th Conference On Learning Theory, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20S.%20Goyal%2C%20N.%20Analysis%20of%20Thompson%20Sampling%20for%20the%20multi-armed%20bandit%20problem%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20S.%20Goyal%2C%20N.%20Analysis%20of%20Thompson%20Sampling%20for%20the%20multi-armed%20bandit%20problem%202012"
        },
        {
            "id": "2",
            "entry": "[2] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Fondations and Trends in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "3",
            "entry": "[3] O. Capp\u00e9, A. Garivier, O.-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper confidence bounds for optimal sequential allocation. Annals of Statistics, 41(3):1516\u20131541, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Capp%C3%A9%2C%20O.%20Garivier%2C%20A.%20Maillard%2C%20O.-A.%20Munos%2C%20R.%20Kullback-Leibler%20upper%20confidence%20bounds%20for%20optimal%20sequential%20allocation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Capp%C3%A9%2C%20O.%20Garivier%2C%20A.%20Maillard%2C%20O.-A.%20Munos%2C%20R.%20Kullback-Leibler%20upper%20confidence%20bounds%20for%20optimal%20sequential%20allocation%202013"
        },
        {
            "id": "4",
            "entry": "[4] L. Chen, A. Gupta, J. Li, M. Qiao, and R. Wang. Nearly optimal sampling algorithms for combinatorial pure exploration. In Proceedings of the 30th Conference on Learning Theory (COLT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20L.%20Gupta%2C%20A.%20Li%2C%20J.%20Qiao%2C%20M.%20Nearly%20optimal%20sampling%20algorithms%20for%20combinatorial%20pure%20exploration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20L.%20Gupta%2C%20A.%20Li%2C%20J.%20Qiao%2C%20M.%20Nearly%20optimal%20sampling%20algorithms%20for%20combinatorial%20pure%20exploration%202017"
        },
        {
            "id": "5",
            "entry": "[5] H. Chernoff. Sequential design of Experiments. The Annals of Mathematical Statistics, 30(3): 755\u2013770, 1959.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chernoff%2C%20H.%20Sequential%20design%20of%20Experiments%201959",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chernoff%2C%20H.%20Sequential%20design%20of%20Experiments%201959"
        },
        {
            "id": "6",
            "entry": "[6] V. H. de la Pe\u00f1a, T. L. Lai, and S. Q. Self-normalized processes. Limit Theory and Statistical applications. Springer, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20la%20Pe%C3%B1a%2C%20V.H.%20Lai%2C%20T.L.%20Q%2C%20S.%20Self-normalized%20processes.%20Limit%20Theory%20and%20Statistical%20applications%202009"
        },
        {
            "id": "7",
            "entry": "[7] C. D\u2019Eramo, M. Restelli, and A. Nuara. Estimating maximum expected value through Gaussian approximation. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%E2%80%99Eramo%2C%20C.%20Restelli%2C%20M.%20Nuara%2C%20A.%20Estimating%20maximum%20expected%20value%20through%20Gaussian%20approximation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%E2%80%99Eramo%2C%20C.%20Restelli%2C%20M.%20Nuara%2C%20A.%20Estimating%20maximum%20expected%20value%20through%20Gaussian%20approximation%202016"
        },
        {
            "id": "8",
            "entry": "[8] C. D\u2019Eramo, A. Nuara, M. Pirotta, and M. Restelli. Estimating the maximum expected value in continuous reinforcement learning problems. In AAAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%E2%80%99Eramo%2C%20C.%20Nuara%2C%20A.%20Pirotta%2C%20M.%20Restelli%2C%20M.%20Estimating%20the%20maximum%20expected%20value%20in%20continuous%20reinforcement%20learning%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D%E2%80%99Eramo%2C%20C.%20Nuara%2C%20A.%20Pirotta%2C%20M.%20Restelli%2C%20M.%20Estimating%20the%20maximum%20expected%20value%20in%20continuous%20reinforcement%20learning%20problems%202017"
        },
        {
            "id": "9",
            "entry": "[9] E. Even-Dar, S. Mannor, and Y. Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Research, 7:1079\u20131105, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Even-Dar%2C%20E.%20Mannor%2C%20S.%20Mansour%2C%20Y.%20Action%20Elimination%20and%20Stopping%20Conditions%20for%20the%20Multi-Armed%20Bandit%20and%20Reinforcement%20Learning%20Problems%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Even-Dar%2C%20E.%20Mannor%2C%20S.%20Mansour%2C%20Y.%20Action%20Elimination%20and%20Stopping%20Conditions%20for%20the%20Multi-Armed%20Bandit%20and%20Reinforcement%20Learning%20Problems%202006"
        },
        {
            "id": "10",
            "entry": "[10] A. Garivier and E. Kaufmann. Optimal best arm identification with fixed confidence. In Proceedings of the 29th Conference On Learning Theory (COLT), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garivier%2C%20A.%20Kaufmann%2C%20E.%20Optimal%20best%20arm%20identification%20with%20fixed%20confidence%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garivier%2C%20A.%20Kaufmann%2C%20E.%20Optimal%20best%20arm%20identification%20with%20fixed%20confidence%202016"
        },
        {
            "id": "11",
            "entry": "[11] A. Garivier, E. Kaufmann, and W. M. Koolen. Maximin action identification: A new bandit framework for games. In Proceedings of the 29th Conference On Learning Theory (COLT), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garivier%2C%20A.%20Kaufmann%2C%20E.%20Koolen%2C%20W.M.%20Maximin%20action%20identification%3A%20A%20new%20bandit%20framework%20for%20games%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garivier%2C%20A.%20Kaufmann%2C%20E.%20Koolen%2C%20W.M.%20Maximin%20action%20identification%3A%20A%20new%20bandit%20framework%20for%20games%202016"
        },
        {
            "id": "12",
            "entry": "[12] A. Garivier, P. M\u00e9nard, and L. Rossi. Thresholding bandit for dose-ranging: The impact of monotonicity. arXiv:1711.04454, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04454"
        },
        {
            "id": "13",
            "entry": "[13] A. Garivier, P. M\u00e9nard, and G. Stoltz. Explore first, exploit next: The true shape of regret in bandit problems. Mathematics of Operations Research, Jun. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garivier%2C%20A.%20M%C3%A9nard%2C%20P.%20Stoltz%2C%20G.%20Explore%20first%2C%20exploit%20next%3A%20The%20true%20shape%20of%20regret%20in%20bandit%20problems%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garivier%2C%20A.%20M%C3%A9nard%2C%20P.%20Stoltz%2C%20G.%20Explore%20first%2C%20exploit%20next%3A%20The%20true%20shape%20of%20regret%20in%20bandit%20problems%202018-06"
        },
        {
            "id": "14",
            "entry": "[14] D. Goldsman and B. L. Nelson. Comparing Systems via Simulation, Chapter 9 in the Handbook of Simulation. Wiley, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldsman%2C%20D.%20Nelson%2C%20B.L.%20Comparing%20Systems%20via%20Simulation%2C%20Chapter%209%20in%20the%20Handbook%20of%20Simulation%201998"
        },
        {
            "id": "15",
            "entry": "[15] J.-B. Grill, M. Valko, and R. Munos. Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning. In Advances in Neural Information Processing Systems (NIPS), pages 4680\u20134688, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grill%2C%20J.-B.%20Valko%2C%20M.%20Munos%2C%20R.%20Blazing%20the%20trails%20before%20beating%20the%20path%3A%20Sample-efficient%20Monte-Carlo%20planning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grill%2C%20J.-B.%20Valko%2C%20M.%20Munos%2C%20R.%20Blazing%20the%20trails%20before%20beating%20the%20path%3A%20Sample-efficient%20Monte-Carlo%20planning%202016"
        },
        {
            "id": "16",
            "entry": "[16] P. D. Gr\u00fcnwald. The minimum description length principle. MIT press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%BCnwald%2C%20P.D.%20The%20minimum%20description%20length%20principle%202007"
        },
        {
            "id": "17",
            "entry": "[17] R. Huang, M. M. Ajallooeian, C. Szepesv\u00e1ri, and M. M\u00fcller. Structured best arm identification with fixed confidence. In International Conference on Algorithmic Learning Theory (ALT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20R.%20Ajallooeian%2C%20M.M.%20Szepesv%C3%A1ri%2C%20C.%20M%C3%BCller%2C%20M.%20Structured%20best%20arm%20identification%20with%20fixed%20confidence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20R.%20Ajallooeian%2C%20M.M.%20Szepesv%C3%A1ri%2C%20C.%20M%C3%BCller%2C%20M.%20Structured%20best%20arm%20identification%20with%20fixed%20confidence%202017"
        },
        {
            "id": "18",
            "entry": "[18] K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lil\u2019UCB: an Optimal Exploration Algorithm for Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jamieson%2C%20K.%20Malloy%2C%20M.%20Nowak%2C%20R.%20Bubeck%2C%20S.%20lil%E2%80%99UCB%3A%20an%20Optimal%20Exploration%20Algorithm%20for%20Multi-Armed%20Bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jamieson%2C%20K.%20Malloy%2C%20M.%20Nowak%2C%20R.%20Bubeck%2C%20S.%20lil%E2%80%99UCB%3A%20an%20Optimal%20Exploration%20Algorithm%20for%20Multi-Armed%20Bandits%202014"
        },
        {
            "id": "19",
            "entry": "[19] S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. PAC subset selection in stochastic multi-armed bandits. In International Conference on Machine Learning (ICML), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalyanakrishnan%2C%20S.%20Tewari%2C%20A.%20Auer%2C%20P.%20Stone%2C%20P.%20PAC%20subset%20selection%20in%20stochastic%20multi-armed%20bandits%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalyanakrishnan%2C%20S.%20Tewari%2C%20A.%20Auer%2C%20P.%20Stone%2C%20P.%20PAC%20subset%20selection%20in%20stochastic%20multi-armed%20bandits%202012"
        },
        {
            "id": "20",
            "entry": "[20] E. Kaufmann and W. M. Koolen. Monte-Carlo tree search by best arm identification. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaufmann%2C%20E.%20Koolen%2C%20W.M.%20Monte-Carlo%20tree%20search%20by%20best%20arm%20identification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaufmann%2C%20E.%20Koolen%2C%20W.M.%20Monte-Carlo%20tree%20search%20by%20best%20arm%20identification%202017"
        },
        {
            "id": "21",
            "entry": "[21] E. Kaufmann and W. M. Koolen. Mixture Martingales Revisited with Applications to Sequential Tests and Confidence Intervals. Preprint, 2018. URL https://hal.archives-ouvertes.fr/hal-01886612v1/.",
            "url": "https://hal.archives-ouvertes.fr/hal-01886612v1/"
        },
        {
            "id": "22",
            "entry": "[22] E. Kaufmann, N. Korda, and R. Munos. Thompson Sampling : an Asymptotically Optimal Finite-Time Analysis. In Proceedings of the 23rd conference on Algorithmic Learning Theory, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaufmann%2C%20E.%20Korda%2C%20N.%20Munos%2C%20R.%20Thompson%20Sampling%20%3A%20an%20Asymptotically%20Optimal%20Finite-Time%20Analysis%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaufmann%2C%20E.%20Korda%2C%20N.%20Munos%2C%20R.%20Thompson%20Sampling%20%3A%20an%20Asymptotically%20Optimal%20Finite-Time%20Analysis%202012"
        },
        {
            "id": "23",
            "entry": "[23] E. Kaufmann, O. Capp\u00e9, and A. Garivier. On the Complexity of Best Arm Identification in Multi-Armed Bandit Models. Journal of Machine Learning Research, 17(1):1\u201342, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=E.%20Kaufmann%2C%20O.%20Capp%C3%A9%20Garivier%2C%20A.%20On%20the%20Complexity%20of%20Best%20Arm%20Identification%20in%20Multi-Armed%20Bandit%20Models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=E.%20Kaufmann%2C%20O.%20Capp%C3%A9%20Garivier%2C%20A.%20On%20the%20Complexity%20of%20Best%20Arm%20Identification%20in%20Multi-Armed%20Bandit%20Models%202016"
        },
        {
            "id": "24",
            "entry": "[24] S.-H. Kim. Comparison with a standard via fully sequential procedures. ACM Transactions on Modeling and Computer Simulation (TOMACS), 15(2):155\u2013174, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.-H.%20Comparison%20with%20a%20standard%20via%20fully%20sequential%20procedures%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20S.-H.%20Comparison%20with%20a%20standard%20via%20fully%20sequential%20procedures%202005"
        },
        {
            "id": "25",
            "entry": "[25] L. Kocsis and C. Szepesv\u00e1ri. Bandit based Monte-Carlo planning. In Proceedings of the 17th European Conference on Machine Learning, ECML\u201906, pages 282\u2013293, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN 3-540-45375-X, 978-3-540-45375-8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kocsis%2C%20L.%20Szepesv%C3%A1ri%2C%20C.%20Bandit%20based%20Monte-Carlo%20planning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kocsis%2C%20L.%20Szepesv%C3%A1ri%2C%20C.%20Bandit%20based%20Monte-Carlo%20planning"
        },
        {
            "id": "26",
            "entry": "[26] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4\u201322, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985"
        },
        {
            "id": "27",
            "entry": "[27] A. Locatelli, M. Gutzeit, and A. Carpentier. An optimal algorithm for the thresholding bandit problem. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Locatelli%2C%20A.%20Gutzeit%2C%20M.%20Carpentier%2C%20A.%20An%20optimal%20algorithm%20for%20the%20thresholding%20bandit%20problem%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Locatelli%2C%20A.%20Gutzeit%2C%20M.%20Carpentier%2C%20A.%20An%20optimal%20algorithm%20for%20the%20thresholding%20bandit%20problem%202016"
        },
        {
            "id": "28",
            "entry": "[28] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527\u2013535, 1952.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Some%20aspects%20of%20the%20sequential%20design%20of%20experiments%201952"
        },
        {
            "id": "29",
            "entry": "[29] D. Russo. Simple Bayesian algorithms for best arm identification. CoRR, abs/1602.08448, 2016. URL http://arxiv.org/abs/1602.08448.",
            "url": "http://arxiv.org/abs/1602.08448",
            "arxiv_url": "https://arxiv.org/pdf/1602.08448"
        },
        {
            "id": "30",
            "entry": "[30] M. Simchowitz, K. Jamieson, and B. Recht. The simulator: Understanding adaptive sampling in the moderate-confidence regime. In Proceedings of the 30th Conference on Learning Theory (COLT), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simchowitz%2C%20M.%20Jamieson%2C%20K.%20Recht%2C%20B.%20The%20simulator%3A%20Understanding%20adaptive%20sampling%20in%20the%20moderate-confidence%20regime%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simchowitz%2C%20M.%20Jamieson%2C%20K.%20Recht%2C%20B.%20The%20simulator%3A%20Understanding%20adaptive%20sampling%20in%20the%20moderate-confidence%20regime%202017"
        },
        {
            "id": "31",
            "entry": "[31] I. Takahisa and T. Kaneko. Estimating the maximum expected value through upper confidence bound of likelihood. In Conference on Technologies and Applications of Artificial Intelligence (TAAI), pages 202\u2013207. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takahisa%2C%20I.%20Kaneko%2C%20T.%20Estimating%20the%20maximum%20expected%20value%20through%20upper%20confidence%20bound%20of%20likelihood%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takahisa%2C%20I.%20Kaneko%2C%20T.%20Estimating%20the%20maximum%20expected%20value%20through%20upper%20confidence%20bound%20of%20likelihood%202017"
        },
        {
            "id": "32",
            "entry": "[32] K. Teraoka, K. Hatano, and E. Takimoto. Efficient sampling method for Monte Carlo tree search problem. IEICE Transactions on Infomation and Systems, pages 392\u2013398, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teraoka%2C%20K.%20Hatano%2C%20K.%20Takimoto%2C%20E.%20Efficient%20sampling%20method%20for%20Monte%20Carlo%20tree%20search%20problem%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teraoka%2C%20K.%20Hatano%2C%20K.%20Takimoto%2C%20E.%20Efficient%20sampling%20method%20for%20Monte%20Carlo%20tree%20search%20problem%202014"
        },
        {
            "id": "33",
            "entry": "[33] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25:285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "34",
            "entry": "[34] H. van Hasselt. Estimating the maximum expected value: An analysis of (nested) cross validation and the maximum sample average. CoRR, abs/1302.7175, 2013. URL http://arxiv.org/abs/1302.7175. ",
            "url": "http://arxiv.org/abs/1302.7175",
            "arxiv_url": "https://arxiv.org/pdf/1302.7175"
        }
    ]
}
