{
    "filename": "8283-smoothed-analysis-of-discrete-tensor-decomposition-and-assemblies-of-neurons.pdf",
    "metadata": {
        "title": "Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons",
        "author": "Nima Anari, Constantinos Daskalakis, Wolfgang Maass, Christos Papadimitriou, Amin Saberi, Santosh Vempala",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8283-smoothed-analysis-of-discrete-tensor-decomposition-and-assemblies-of-neurons.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon Bhaskara et al. [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] but allows for a wider range of perturbation models, including discrete ones. We give an application to recovering assemblies of neurons. Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies. This suggests that an animal\u2019s memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data. Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their -wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram."
    },
    "keywords": [
        {
            "term": "power method",
            "url": "https://en.wikipedia.org/wiki/power_method"
        },
        {
            "term": "average case",
            "url": "https://en.wikipedia.org/wiki/average_case"
        },
        {
            "term": "hidden markov model",
            "url": "https://en.wikipedia.org/wiki/hidden_markov_model"
        },
        {
            "term": "venn diagram",
            "url": "https://en.wikipedia.org/wiki/venn_diagram"
        },
        {
            "term": "tensor decomposition",
            "url": "https://en.wikipedia.org/wiki/tensor_decomposition"
        }
    ],
    "highlights": [
        "Tensor decomposition is one of the key algorithmic tools for learning many latent variable models [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "Tensor decomposition methods based on gradient descent and power method have been observed to work well [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "Determining the minimum number of rank one components in the tensor decomposition is known to be NP-hard in the worst case [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], so usually tensor decomposition is analyzed in the average case",
        "Several algorithms have been analyzed in the average case, where the input tensor is produced according to some probabilistic model, for example see Bhaskara et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], De Lathauwer et al [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], Goyal et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] as well as sum-of-squares-based algorithms like Barak et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Ge and Ma [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], Hopkins et al [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], Ma et al [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "The main departing point of our work is our smoothed analysis of linear independence, which we base on a new notion we call echelon trees, a generalization of Gaussian elimination and echelon form to high-order tensors, which might be of independent interest"
    ],
    "key_statements": [
        "Tensor decomposition is one of the key algorithmic tools for learning many latent variable models [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "Tensor decomposition methods based on gradient descent and power method have been observed to work well [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "Determining the minimum number of rank one components in the tensor decomposition is known to be NP-hard in the worst case [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], so usually tensor decomposition is analyzed in the average case",
        "Several algorithms have been analyzed in the average case, where the input tensor is produced according to some probabilistic model, for example see Bhaskara et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], De Lathauwer et al [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], Goyal et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] as well as sum-of-squares-based algorithms like Barak et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], Ge and Ma [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], Hopkins et al [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], Ma et al [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "The main departing point of our work is our smoothed analysis of linear independence, which we base on a new notion we call echelon trees, a generalization of Gaussian elimination and echelon form to high-order tensors, which might be of independent interest"
    ],
    "summary": [
        "Tensor decomposition is one of the key algorithmic tools for learning many latent variable models [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>].",
        "They either assume components of the tensor are fully random, i.e., generated from a known distribution (e.g., Gaussian), or they follow a smoothed analysis setting where some adversarially chosen instance is perturbed by random noise, see for example Bhaskara et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], Goyal et al [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], Ma et al [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>].",
        "The main departing point of our work is our smoothed analysis of linear independence, which we base on a new notion we call echelon trees, a generalization of Gaussian elimination and echelon form to high-order tensors, which might be of independent interest.",
        "Repeated applications of fact 9 on the echelon tree would produce a height 1 echelon tree, and we have already observed that the vectors assigned to the leaves in such a tree must be linearly independent.",
        "The number of nodes at level \u2212 1 is at most n \u22121, so by a union bound, we get that with probability at least 1 \u2212 n \u22121p(1\u2212c)n, the tree produced at the level is \u03b4x-large.",
        "Let us fix \u03b2 \u2208 [0, 1] such that the premise of the induction hypothesis holds and we can get an echelon tree of height \u2212 1 with fractional branching (\u03b2, \u03b13, .",
        "We can again apply the induction hypothesis to this space and as long as the premise holds obtain an echelon tree of height \u2212 1 with fractional branching (\u03b2, \u03b13, .",
        "Suppose we have repeated this procedure \u03b3n1 \u2212 1 many times and currently have a height echelon tree with fractional branching (\u03b3, \u03b2, \u03b13, .",
        "If we perturb our cell assemblies according to example 3, i.e., flip assembly memberships for each neuron class and assembly pair with probability q, how large of a q do we need for the conditions of theorem 5 and [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] to be satisfied?",
        "We wish to create sets of expected size K representing the nodes, starting from the universe of neurons [N ] and executing instructions of the following form: A \u2190 C1 \u222a C2, A \u2190 C1 \u2229 C2, A \u2190 C1 \u2212 C2, A \u2190 S(C, p), where by S(C, p) we denote the result of sampling each node in set C with probability p \u2014 a simple and realistic enough primitive.",
        "The question is, which graphs can be realized in such a way that the intended relations between the nodes and their intersections are not corrupted, with high enough probability, by the randomness of the process?"
    ],
    "headline": "We study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their -wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture models and hidden markov models. In Conference on Learning Theory, pages 33\u20131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20A%20method%20of%20moments%20for%20mixture%20models%20and%20hidden%20markov%20models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20A%20method%20of%20moments%20for%20mixture%20models%20and%20hidden%20markov%20models%202012"
        },
        {
            "id": "2",
            "entry": "[2] Boaz Barak, Jonathan A Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 143\u2013151. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barak%2C%20Boaz%20Kelner%2C%20Jonathan%20A.%20Steurer%2C%20David%20Dictionary%20learning%20and%20tensor%20decomposition%20via%20the%20sum-of-squares%20method%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barak%2C%20Boaz%20Kelner%2C%20Jonathan%20A.%20Steurer%2C%20David%20Dictionary%20learning%20and%20tensor%20decomposition%20via%20the%20sum-of-squares%20method%202015"
        },
        {
            "id": "3",
            "entry": "[3] Aditya Bhaskara, Moses Charikar, Ankur Moitra, and Aravindan Vijayaraghavan. Smoothed analysis of tensor decompositions. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 594\u2013603. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhaskara%2C%20Aditya%20Charikar%2C%20Moses%20Moitra%2C%20Ankur%20Vijayaraghavan%2C%20Aravindan%20Smoothed%20analysis%20of%20tensor%20decompositions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhaskara%2C%20Aditya%20Charikar%2C%20Moses%20Moitra%2C%20Ankur%20Vijayaraghavan%2C%20Aravindan%20Smoothed%20analysis%20of%20tensor%20decompositions%202014"
        },
        {
            "id": "4",
            "entry": "[4] Gy\u00f6rgy Buzs\u00e1ki. Neural syntax: cell assemblies, synapsembles, and readers. Neuron, 68(3): 362\u2013385, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buzs%C3%A1ki%2C%20Gy%C3%B6rgy%20Neural%20syntax%3A%20cell%20assemblies%2C%20synapsembles%2C%20and%20readers%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buzs%C3%A1ki%2C%20Gy%C3%B6rgy%20Neural%20syntax%3A%20cell%20assemblies%2C%20synapsembles%2C%20and%20readers%202010"
        },
        {
            "id": "5",
            "entry": "[5] Joseph T Chang. Full reconstruction of markov models on evolutionary trees: identifiability and consistency. Mathematical biosciences, 137(1):51\u201373, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20Joseph%20T.%20Full%20reconstruction%20of%20markov%20models%20on%20evolutionary%20trees%3A%20identifiability%20and%20consistency%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20Joseph%20T.%20Full%20reconstruction%20of%20markov%20models%20on%20evolutionary%20trees%3A%20identifiability%20and%20consistency%201996"
        },
        {
            "id": "6",
            "entry": "[6] Emanuela De Falco, Matias J Ison, Itzhak Fried, and Rodrigo Quian Quiroga. Long-term coding of personal and universal associations underlying the memory web in the human brain. Nature communications, 7:13408, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Falco%2C%20Emanuela%20De%20Ison%2C%20Matias%20J.%20Fried%2C%20Itzhak%20Quiroga%2C%20Rodrigo%20Quian%20Long-term%20coding%20of%20personal%20and%20universal%20associations%20underlying%20the%20memory%20web%20in%20the%20human%20brain%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Falco%2C%20Emanuela%20De%20Ison%2C%20Matias%20J.%20Fried%2C%20Itzhak%20Quiroga%2C%20Rodrigo%20Quian%20Long-term%20coding%20of%20personal%20and%20universal%20associations%20underlying%20the%20memory%20web%20in%20the%20human%20brain%202016"
        },
        {
            "id": "7",
            "entry": "[7] Lieven De Lathauwer, Josphine Castaing, and Jean-Franois Cardoso. Fourth-order cumulantbased blind identification of underdetermined mixtures. IEEE Transactions on Signal Processing, 55(6):2965\u20132973, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lathauwer%2C%20Lieven%20De%20Castaing%2C%20Josphine%20Cardoso%2C%20Jean-Franois%20Fourth-order%20cumulantbased%20blind%20identification%20of%20underdetermined%20mixtures%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lathauwer%2C%20Lieven%20De%20Castaing%2C%20Josphine%20Cardoso%2C%20Jean-Franois%20Fourth-order%20cumulantbased%20blind%20identification%20of%20underdetermined%20mixtures%202007"
        },
        {
            "id": "8",
            "entry": "[8] Rong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms. arXiv preprint arXiv:1504.05287, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.05287"
        },
        {
            "id": "9",
            "entry": "[9] Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. In Advances in Neural Information Processing Systems, pages 3656\u20133666, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Ma%2C%20Tengyu%20On%20the%20optimization%20landscape%20of%20tensor%20decompositions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Ma%2C%20Tengyu%20On%20the%20optimization%20landscape%20of%20tensor%20decompositions%202017"
        },
        {
            "id": "10",
            "entry": "[10] Navin Goyal, Santosh Vempala, and Ying Xiao. Fourier pca and robust tensor decomposition. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 584\u2013593. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20Navin%20Vempala%2C%20Santosh%20Xiao%2C%20Ying%20Fourier%20pca%20and%20robust%20tensor%20decomposition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goyal%2C%20Navin%20Vempala%2C%20Santosh%20Xiao%2C%20Ying%20Fourier%20pca%20and%20robust%20tensor%20decomposition%202014"
        },
        {
            "id": "11",
            "entry": "[11] Johan H\u00e5stad. Tensor rank is np-complete. Journal of Algorithms, 11(4):644\u2013654, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H%C3%A5stad%2C%20Johan%20Tensor%20rank%20is%20np-complete%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H%C3%A5stad%2C%20Johan%20Tensor%20rank%20is%20np-complete%201990"
        },
        {
            "id": "12",
            "entry": "[12] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM (JACM), 60(6):45, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hillar%2C%20Christopher%20J.%20Lim%2C%20Lek-Heng%20Most%20tensor%20problems%20are%20np-hard%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hillar%2C%20Christopher%20J.%20Lim%2C%20Lek-Heng%20Most%20tensor%20problems%20are%20np-hard%202013"
        },
        {
            "id": "13",
            "entry": "[13] Samuel B Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 178\u2013191. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hopkins%2C%20Samuel%20B.%20Schramm%2C%20Tselil%20Shi%2C%20Jonathan%20Steurer%2C%20David%20Fast%20spectral%20algorithms%20from%20sum-of-squares%20proofs%3A%20tensor%20decomposition%20and%20planted%20sparse%20vectors%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hopkins%2C%20Samuel%20B.%20Schramm%2C%20Tselil%20Shi%2C%20Jonathan%20Steurer%2C%20David%20Fast%20spectral%20algorithms%20from%20sum-of-squares%20proofs%3A%20tensor%20decomposition%20and%20planted%20sparse%20vectors%202016"
        },
        {
            "id": "14",
            "entry": "[14] Daniel Hsu, Sham M Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460\u20131480, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20A%20spectral%20algorithm%20for%20learning%20hidden%20markov%20models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Zhang%2C%20Tong%20A%20spectral%20algorithm%20for%20learning%20hidden%20markov%20models%202012"
        },
        {
            "id": "15",
            "entry": "[15] Matias J Ison, Rodrigo Quian Quiroga, and Itzhak Fried. Rapid encoding of new memories by individual neurons in the human brain. Neuron, 87(1):220\u2013230, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ison%2C%20Matias%20J.%20Quiroga%2C%20Rodrigo%20Quian%20Fried%2C%20Itzhak%20Rapid%20encoding%20of%20new%20memories%20by%20individual%20neurons%20in%20the%20human%20brain%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ison%2C%20Matias%20J.%20Quiroga%2C%20Rodrigo%20Quian%20Fried%2C%20Itzhak%20Rapid%20encoding%20of%20new%20memories%20by%20individual%20neurons%20in%20the%20human%20brain%202015"
        },
        {
            "id": "16",
            "entry": "[16] Tamara G Kolda and Jackson R Mayo. Shifted power method for computing tensor eigenpairs. SIAM Journal on Matrix Analysis and Applications, 32(4):1095\u20131124, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Mayo%2C%20Jackson%20R.%20Shifted%20power%20method%20for%20computing%20tensor%20eigenpairs%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20Tamara%20G.%20Mayo%2C%20Jackson%20R.%20Shifted%20power%20method%20for%20computing%20tensor%20eigenpairs%202011"
        },
        {
            "id": "17",
            "entry": "[17] SE Leurgans, RT Ross, and RB Abel. A decomposition for three-way arrays. SIAM Journal on Matrix Analysis and Applications, 14(4):1064\u20131083, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leurgans%2C%20S.E.%20Ross%2C%20R.T.%20Abel%2C%20R.B.%20A%20decomposition%20for%20three-way%20arrays%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leurgans%2C%20S.E.%20Ross%2C%20R.T.%20Abel%2C%20R.B.%20A%20decomposition%20for%20three-way%20arrays%201993"
        },
        {
            "id": "18",
            "entry": "[18] Tengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with sumof-squares. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pages 438\u2013446. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Tengyu%20Shi%2C%20Jonathan%20Steurer%2C%20David%20Polynomial-time%20tensor%20decompositions%20with%20sumof-squares%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Tengyu%20Shi%2C%20Jonathan%20Steurer%2C%20David%20Polynomial-time%20tensor%20decompositions%20with%20sumof-squares%202016"
        },
        {
            "id": "19",
            "entry": "[19] Elchanan Mossel and S\u00e9bastien Roch. Learning nonsingular phylogenies and hidden markov models. In Proceedings of the thirty-seventh annual ACM symposium on theory of computing, pages 366\u2013375. ACM, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mossel%2C%20Elchanan%20Roch%2C%20S%C3%A9bastien%20Learning%20nonsingular%20phylogenies%20and%20hidden%20markov%20models%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mossel%2C%20Elchanan%20Roch%2C%20S%C3%A9bastien%20Learning%20nonsingular%20phylogenies%20and%20hidden%20markov%20models%202005"
        },
        {
            "id": "20",
            "entry": "[20] Itamar Pitowsky. Correlation polytopes: their geometry and complexity. Mathematical Programming, 50(1):395\u2013414, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pitowsky%2C%20Itamar%20Correlation%20polytopes%3A%20their%20geometry%20and%20complexity%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pitowsky%2C%20Itamar%20Correlation%20polytopes%3A%20their%20geometry%20and%20complexity%201991"
        },
        {
            "id": "21",
            "entry": "[21] Rodrigo Quian Quiroga. Concept cells: the building blocks of declarative memory functions. Nature reviews. Neuroscience, 13(8):587, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quiroga%2C%20Rodrigo%20Quian%20Concept%20cells%3A%20the%20building%20blocks%20of%20declarative%20memory%20functions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quiroga%2C%20Rodrigo%20Quian%20Concept%20cells%3A%20the%20building%20blocks%20of%20declarative%20memory%20functions%202012"
        }
    ]
}
