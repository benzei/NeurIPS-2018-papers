{
    "filename": "7882-learning-to-teach-with-dynamic-loss-functions.pdf",
    "metadata": {
        "title": "Learning to Teach with Dynamic Loss Functions",
        "author": "Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, Tie-Yan Liu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7882-learning-to-teach-with-dynamic-loss-functions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Teaching is critical to human society: it is with teaching that prospective students are educated and human civilization can be inherited and advanced. A good teacher not only provides his/her students with qualified teaching materials (e.g., textbooks), but also sets up appropriate learning objectives (e.g., course projects and exams) considering different situations of a student. When it comes to artificial intelligence, treating machine learning models as students, the loss functions that are optimized act as perfect counterparts of the learning objective set by the teacher. In this work, we explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models. Different from typical learning settings in which the loss function of a machine learning model is predefined and fixed, in our framework, the loss function of a machine learning model (we call it student) is defined by another machine learning model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its student model at different training stages. We develop an efficient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as \u201clearning to teach with dynamic loss functions\u201d (L2T-DLF for short). Extensive experiments on real world tasks including image classification and neural machine translation demonstrate that our method significantly improves the quality of various student models."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "human civilization",
            "url": "https://en.wikipedia.org/wiki/human_civilization"
        },
        {
            "term": "DDPG",
            "url": "https://en.wikipedia.org/wiki/DDPG"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "human society",
            "url": "https://en.wikipedia.org/wiki/human_society"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "artificial intelligence",
            "url": "https://en.wikipedia.org/wiki/artificial_intelligence"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "Teaching, which aims to help students learn new knowledge or skills effectively and efficiently, is important to advance modern human civilization",
        "We demonstrate the effectiveness of L2T-DLF on various realworld tasks including image classification and neural machine translation with different student models such as multi-layer perputted via teacher model at different phases of student model training",
        "On all the three tasks, the dynamic loss functions outputted via teacher model help to cultivate better student model",
        "We can clearly observe the dynamic loss functions outputted via our teacher model can guide the student model to have superior performance compared with other specially designed loss functions",
        "In contrast to expert designed and fixed loss functions in conventional machine learning systems, we in this paper study how to learn dynamic loss functions so as to better teach a student machine learning model",
        "Since loss functions provided by the teacher model dynamically change with respect to the growth of the student model and the teacher model is trained through end-to-end optimization, the quality of the student model gets improved significantly, as shown in our experiments"
    ],
    "key_statements": [
        "Teaching, which aims to help students learn new knowledge or skills effectively and efficiently, is important to advance modern human civilization",
        "We demonstrate the effectiveness of L2T-DLF on various realworld tasks including image classification and neural machine translation with different student models such as multi-layer perputted via teacher model at different phases of student model training",
        "In the training process of teacher model, similar to qualified human teachers are good at improving the quality of exams, the teacher model in L2T-DLF refines the loss function it sets up via optimizing its own \u03b8",
        "Another possible way to conduct teacher model optimization is through deep reinforcement learning",
        "L2T-DLF is more general in that at each timestep: 1) the teacher model considers the overall status of the student model for the sake of optimizing its parameters, rather than the instant action; 2) the teacher model outputs a loss function with the goal of maximizing, but not approximating the future reward",
        "On all the three tasks, the dynamic loss functions outputted via teacher model help to cultivate better student model",
        "In Appendix we provide the details of the LSTM/Transformer student models and the training settings of student/teacher models",
        "We can clearly observe the dynamic loss functions outputted via our teacher model can guide the student model to have superior performance compared with other specially designed loss functions",
        "In contrast to expert designed and fixed loss functions in conventional machine learning systems, we in this paper study how to learn dynamic loss functions so as to better teach a student machine learning model",
        "Since loss functions provided by the teacher model dynamically change with respect to the growth of the student model and the teacher model is trained through end-to-end optimization, the quality of the student model gets improved significantly, as shown in our experiments"
    ],
    "summary": [
        "Teaching, which aims to help students learn new knowledge or skills effectively and efficiently, is important to advance modern human civilization.",
        "L2T-DLF differs from prior works in that: 1) the loss functions are automatically learned, covering a large space and without the demand of heuristic understanding for task specific objective and optimization process; 2) the loss function dynamically evolves during the training process, leading to a more coherent interaction between loss and student model.",
        "The loss function l(y, y), taking the model prediction y = f\u03c9(x) and ground-truth y as inputs, acts as the surrogate of m to evaluate the student model f\u03c9 during its training process, just as the exams in real-world human teaching.",
        "In the training process of teacher model, similar to qualified human teachers are good at improving the quality of exams, the teacher model in L2T-DLF refines the loss function it sets up via optimizing its own \u03b8.",
        "L2T-DLF is more general in that at each timestep: 1) the teacher model considers the overall status of the student model for the sake of optimizing its parameters, rather than the instant action; 2) the teacher model outputs a loss function with the goal of maximizing, but not approximating the future reward.",
        "On all the three tasks, the dynamic loss functions outputted via teacher model help to cultivate better student model.",
        "The loss functions we leverage to train student models include: 1) Cross entropy loss Lce to perform maximum likelihood estimation (MLE) for training LSTM/Transformer model with teacher forcing [<a class=\"ref-link\" id=\"c52\" href=\"#r52\">52</a>]; 2) The reinforcement learning (RL) loss Lrl, a.k.a, sequence level training [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] or minimum risk training [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>], targets at directly optimizing the BLEU scores for NMT models.",
        "A typical RL loss is Lrl, y) = \u2212 y\u2217\u2208Y log p\u03c9(y\u2217|x)(BLEU (y\u2217, y) \u2212 b), where b is the reward baseline and Y is the candidate subset; 3) The loss specified via actor-critic (AC) algorithm Lac [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], which approximates the BLEU score via a critic network; 4) The softmax-margin loss, which is empirically shown to be the most effective structural prediction loss for NMT [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]; 5) The loss function discovered via our L2T-DLF.",
        "We can clearly observe the dynamic loss functions outputted via our teacher model can guide the student model to have superior performance compared with other specially designed loss functions.",
        "We are interested in trying more complicated teacher models such as deeper neural networks"
    ],
    "headline": "We explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] John R Anderson, C Franklin Boyle, and Brian J Reiser. Intelligent tutoring systems. Science, 228(4698):456\u2013462, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20John%20R.%20Boyle%2C%20C.Franklin%20Reiser%2C%20Brian%20J.%20Intelligent%20tutoring%20systems%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20John%20R.%20Boyle%2C%20C.Franklin%20Reiser%2C%20Brian%20J.%20Intelligent%20tutoring%20systems%201985"
        },
        {
            "id": "2",
            "entry": "[2] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pages 3981\u20133989, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "3",
            "entry": "[3] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.07086"
        },
        {
            "id": "4",
            "entry": "[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "5",
            "entry": "[5] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834\u2013846, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barto%2C%20Andrew%20G.%20Sutton%2C%20Richard%20S.%20Anderson%2C%20Charles%20W.%20Neuronlike%20adaptive%20elements%20that%20can%20solve%20difficult%20learning%20control%20problems.%20IEEE%20transactions%20on%20systems%201983"
        },
        {
            "id": "6",
            "entry": "[6] Atilim Gunes Baydin and Barak A Pearlmutter. Automatic differentiation of algorithms for machine learning. arXiv preprint arXiv:1404.7456, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1404.7456"
        },
        {
            "id": "7",
            "entry": "[7] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889\u20131900, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Gradient-based%20optimization%20of%20hyperparameters%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Gradient-based%20optimization%20of%20hyperparameters%202000"
        },
        {
            "id": "8",
            "entry": "[8] Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th ICML, pages 41\u201348. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "9",
            "entry": "[9] M Cettolo, J Niehues, S St\u00fcker, L Bentivogli, and M Federico. Report on the 11th iwslt evaluation campaign, iwslt 2014. In IWSLT-International Workshop on Spoken Language Processing, pages 2\u201317. Marcello Federico, Sebastian St\u00fcker, Fran\u00e7ois Yvon, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Cettolo%20J%20Niehues%20S%20St%C3%BCker%20L%20Bentivogli%20and%20M%20Federico%20Report%20on%20the%2011th%20iwslt%20evaluation%20campaign%20iwslt%202014%20In%20IWSLTInternational%20Workshop%20on%20Spoken%20Language%20Processing%20pages%20217%20Marcello%20Federico%20Sebastian%20St%C3%BCker%20Fran%C3%A7ois%20Yvon%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Cettolo%20J%20Niehues%20S%20St%C3%BCker%20L%20Bentivogli%20and%20M%20Federico%20Report%20on%20the%2011th%20iwslt%20evaluation%20campaign%20iwslt%202014%20In%20IWSLTInternational%20Workshop%20on%20Spoken%20Language%20Processing%20pages%20217%20Marcello%20Federico%20Sebastian%20St%C3%BCker%20Fran%C3%A7ois%20Yvon%202014"
        },
        {
            "id": "10",
            "entry": "[10] Yutian Chen, Matthew W Hoffman, Sergio G\u00f3mez Colmenarejo, Misha Denil, Timothy P Lillicrap, Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. arXiv preprint arXiv:1611.03824, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03824"
        },
        {
            "id": "11",
            "entry": "[11] Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19\u201367, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boer%2C%20Pieter-Tjerk%20De%20Kroese%2C%20Dirk%20P.%20Mannor%2C%20Shie%20Rubinstein%2C%20Reuven%20Y.%20A%20tutorial%20on%20the%20cross-entropy%20method%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boer%2C%20Pieter-Tjerk%20De%20Kroese%2C%20Dirk%20P.%20Mannor%2C%20Shie%20Rubinstein%2C%20Reuven%20Y.%20A%20tutorial%20on%20the%20cross-entropy%20method%202005"
        },
        {
            "id": "12",
            "entry": "[12] Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc\u2019Aurelio Ranzato. Classical structured prediction losses for sequence to sequence learning. arXiv preprint arXiv:1711.04956, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04956"
        },
        {
            "id": "13",
            "entry": "[13] Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Yang%20Tian%2C%20Fei%20Qin%2C%20Tao%20Li%2C%20Xiang-Yang%20Learning%20to%20teach%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Yang%20Tian%2C%20Fei%20Qin%2C%20Tao%20Li%2C%20Xiang-Yang%20Learning%20to%20teach%202018"
        },
        {
            "id": "14",
            "entry": "[14] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126\u20131135, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017-08"
        },
        {
            "id": "15",
            "entry": "[15] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1165\u20131173, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Franceschi%2C%20Luca%20Donini%2C%20Michele%20Frasconi%2C%20Paolo%20Pontil%2C%20Massimiliano%20Forward%20and%20reverse%20gradient-based%20hyperparameter%20optimization%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Franceschi%2C%20Luca%20Donini%2C%20Michele%20Frasconi%2C%20Paolo%20Pontil%2C%20Massimiliano%20Forward%20and%20reverse%20gradient-based%20hyperparameter%20optimization%202017-08"
        },
        {
            "id": "16",
            "entry": "[16] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning, pages 1243\u2013 1252, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gehring%2C%20Jonas%20Auli%2C%20Michael%20Grangier%2C%20David%20Yarats%2C%20Denis%20Convolutional%20sequence%20to%20sequence%20learning%202017"
        },
        {
            "id": "17",
            "entry": "[17] S.A. Goldman and M.J. Kearns. On the complexity of teaching. J. Comput. Syst. Sci., 50(1):20\u2013 31, February 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldman%2C%20S.A.%20Kearns%2C%20M.J.%20On%20the%20complexity%20of%20teaching%201995-02-31",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldman%2C%20S.A.%20Kearns%2C%20M.J.%20On%20the%20complexity%20of%20teaching%201995-02-31"
        },
        {
            "id": "18",
            "entry": "[18] Chen Gong, Dacheng Tao, Wei Liu, Liu Liu, and Jie Yang. Label propagation via teachingto-learn and learning-to-teach. IEEE transactions on neural networks and learning systems, 28(6):1452\u20131465, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20Chen%20Tao%2C%20Dacheng%20Liu%2C%20Wei%20Liu%2C%20Liu%20Label%20propagation%20via%20teachingto-learn%20and%20learning-to-teach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20Chen%20Tao%2C%20Dacheng%20Liu%2C%20Wei%20Liu%2C%20Liu%20Label%20propagation%20via%20teachingto-learn%20and%20learning-to-teach%202017"
        },
        {
            "id": "19",
            "entry": "[19] Chen Gong, Dacheng Tao, Jie Yang, and Wei Liu. Teaching-to-learn and learning-to-teach for multi-label propagation. In AAAI 2016, pages 1610\u20131616, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20Chen%20Tao%2C%20Dacheng%20Yang%2C%20Jie%20Liu%2C%20Wei%20Teaching-to-learn%20and%20learning-to-teach%20for%20multi-label%20propagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20Chen%20Tao%2C%20Dacheng%20Yang%2C%20Jie%20Liu%2C%20Wei%20Teaching-to-learn%20and%20learning-to-teach%20for%20multi-label%20propagation%202016"
        },
        {
            "id": "20",
            "entry": "[20] Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic non-convex problems. In International Conference on Machine Learning, pages 1833\u20131841, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Yehuda%20Shalev-Shwartz%2C%20Shai%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Yehuda%20Shalev-Shwartz%2C%20Shai%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016"
        },
        {
            "id": "21",
            "entry": "[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "22",
            "entry": "[22] Paul Henderson and Vittorio Ferrari. End-to-end training of object class detectors for mean average precision. In Asian Conference on Computer Vision, pages 198\u2013213.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henderson%2C%20Paul%20Ferrari%2C%20Vittorio%20End-to-end%20training%20of%20object%20class%20detectors%20for%20mean%20average%20precision",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henderson%2C%20Paul%20Ferrari%2C%20Vittorio%20End-to-end%20training%20of%20object%20class%20detectors%20for%20mean%20average%20precision"
        },
        {
            "id": "23",
            "entry": "[23] Mark K Ho, Michael Littman, James MacGlashan, Fiery Cushman, and Joseph L Austerweil. Showing versus doing: Teaching by demonstration. In Advances in Neural Information Processing Systems 29, pages 3027\u20133035. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Mark%20K.%20Littman%2C%20Michael%20MacGlashan%2C%20James%20Cushman%2C%20Fiery%20Showing%20versus%20doing%3A%20Teaching%20by%20demonstration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Mark%20K.%20Littman%2C%20Michael%20MacGlashan%2C%20James%20Cushman%2C%20Fiery%20Showing%20versus%20doing%3A%20Teaching%20by%20demonstration%202016"
        },
        {
            "id": "24",
            "entry": "[24] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Maaten%2C%20Laurens%20Van%20Der%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "25",
            "entry": "[25] Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, and Li Deng. Towards neural phrase-based machine translation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Po-Sen%20Wang%2C%20Chong%20Huang%2C%20Sitao%20Zhou%2C%20Dengyong%20Towards%20neural%20phrase-based%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Po-Sen%20Wang%2C%20Chong%20Huang%2C%20Sitao%20Zhou%2C%20Dengyong%20Towards%20neural%20phrase-based%20machine%20translation%202018"
        },
        {
            "id": "26",
            "entry": "[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "27",
            "entry": "[27] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems, pages 1189\u20131197, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-paced%20learning%20for%20latent%20variable%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-paced%20learning%20for%20latent%20variable%20models%202010"
        },
        {
            "id": "28",
            "entry": "[28] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "29",
            "entry": "[29] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01885"
        },
        {
            "id": "30",
            "entry": "[30] Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-Taylor, and Jaz S. Kandola. The perceptron algorithm with uneven margins. In Proceedings of the Nineteenth International Conference on Machine Learning, ICML \u201902, pages 379\u2013386, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yaoyong%20Zaragoza%2C%20Hugo%20Herbrich%2C%20Ralf%20Shawe-Taylor%2C%20John%20The%20perceptron%20algorithm%20with%20uneven%20margins%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yaoyong%20Zaragoza%2C%20Hugo%20Herbrich%2C%20Ralf%20Shawe-Taylor%2C%20John%20The%20perceptron%20algorithm%20with%20uneven%20margins%202002"
        },
        {
            "id": "31",
            "entry": "[31] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "32",
            "entry": "[32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. arXiv preprint arXiv:1708.02002, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02002"
        },
        {
            "id": "33",
            "entry": "[33] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Zhouhan%20Feng%2C%20Minwei%20dos%20Santos%2C%20Cicero%20Nogueira%20Yu%2C%20Mo%20and%20Yoshua%20Bengio.%20A%20structured%20self-attentive%20sentence%20embedding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Zhouhan%20Feng%2C%20Minwei%20dos%20Santos%2C%20Cicero%20Nogueira%20Yu%2C%20Mo%20and%20Yoshua%20Bengio.%20A%20structured%20self-attentive%20sentence%20embedding%202017"
        },
        {
            "id": "34",
            "entry": "[34] Ji Liu and Xiaojin Zhu. The teaching dimension of linear learners. Journal of Machine Learning Research, 17(162):1\u201325, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ji%20Zhu%2C%20Xiaojin%20The%20teaching%20dimension%20of%20linear%20learners%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ji%20Zhu%2C%20Xiaojin%20The%20teaching%20dimension%20of%20linear%20learners%202016"
        },
        {
            "id": "35",
            "entry": "[35] Weiyang Liu, Bo Dai, James Rehg, and Le Song. Iterative machine teaching. In Proceedings of the 34st International Conference on Machine Learning (ICML-17), pages 1188\u20131196, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Dai%2C%20Bo%20Rehg%2C%20James%20Song%2C%20Le%20Iterative%20machine%20teaching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Dai%2C%20Bo%20Rehg%2C%20James%20Song%2C%20Le%20Iterative%20machine%20teaching%202017"
        },
        {
            "id": "36",
            "entry": "[36] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In International Conference on Machine Learning, pages 507\u2013516, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Weiyang%20Wen%2C%20Yandong%20Yu%2C%20Zhiding%20Yang%2C%20Meng%20Large-margin%20softmax%20loss%20for%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "37",
            "entry": "[37] Renqian Luo, Fei Tian, Tao Qin, and Tie-Yan Liu. Neural architecture optimization. arXiv preprint arXiv:1808.07233, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.07233"
        },
        {
            "id": "38",
            "entry": "[38] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning, pages 2113\u20132122, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20Gradient-based%20hyperparameter%20optimization%20through%20reversible%20learning%202015"
        },
        {
            "id": "39",
            "entry": "[39] Smitha Milli, Pieter Abbeel, and Igor Mordatch. Interpretable and pedagogical examples. arXiv preprint arXiv:1711.00694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00694"
        },
        {
            "id": "40",
            "entry": "[40] Tan Nguyen and Scott Sanner. Algorithms for direct 0\u20131 loss optimization in binary classification. In International Conference on Machine Learning, pages 1085\u20131093, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Tan%20Sanner%2C%20Scott%20Algorithms%20for%20direct%200%E2%80%931%20loss%20optimization%20in%20binary%20classification%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Tan%20Sanner%2C%20Scott%20Algorithms%20for%20direct%200%E2%80%931%20loss%20optimization%20in%20binary%20classification%202013"
        },
        {
            "id": "41",
            "entry": "[41] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papineni%2C%20Kishore%20Roukos%2C%20Salim%20Ward%2C%20Todd%20Zhu%2C%20Wei-Jing%20Bleu%3A%20a%20method%20for%20automatic%20evaluation%20of%20machine%20translation%202002"
        },
        {
            "id": "42",
            "entry": "[42] Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06732"
        },
        {
            "id": "43",
            "entry": "[43] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20Jurgen%20Evolutionary%20principles%20in%20self-referential%20learning.%20Diploma%201987"
        },
        {
            "id": "44",
            "entry": "[44] Patrick Shafto, Noah D Goodman, and Thomas L Griffiths. A rational account of pedagogical reasoning: Teaching by, and learning from, examples. Cognitive psychology, 71:55\u201389, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shafto%2C%20Patrick%20Goodman%2C%20Noah%20D.%20Griffiths%2C%20Thomas%20L.%20A%20rational%20account%20of%20pedagogical%20reasoning%3A%20Teaching%20by%2C%20and%20learning%20from%2C%20examples%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shafto%2C%20Patrick%20Goodman%2C%20Noah%20D.%20Griffiths%2C%20Thomas%20L.%20A%20rational%20account%20of%20pedagogical%20reasoning%3A%20Teaching%20by%2C%20and%20learning%20from%2C%20examples%202014"
        },
        {
            "id": "45",
            "entry": "[45] Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum risk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683\u20131692. Association for Computational Linguistics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Shiqi%20Cheng%2C%20Yong%20He%2C%20Zhongjun%20He%2C%20Wei%20Minimum%20risk%20training%20for%20neural%20machine%20translation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Shiqi%20Cheng%2C%20Yong%20He%2C%20Zhongjun%20He%2C%20Wei%20Minimum%20risk%20training%20for%20neural%20machine%20translation%202016"
        },
        {
            "id": "46",
            "entry": "[46] Yang Song, Alexander Schwing, Raquel Urtasun, et al. Training deep neural networks via direct loss minimization. In International Conference on Machine Learning, pages 2169\u20132177, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Yang%20Schwing%2C%20Alexander%20Urtasun%2C%20Raquel%20Training%20deep%20neural%20networks%20via%20direct%20loss%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Yang%20Schwing%2C%20Alexander%20Urtasun%2C%20Raquel%20Training%20deep%20neural%20networks%20via%20direct%20loss%20minimization%202016"
        },
        {
            "id": "47",
            "entry": "[47] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "48",
            "entry": "[48] Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20Stuart%20Temporal%20credit%20assignment%20in%20reinforcement%20learning%201984"
        },
        {
            "id": "49",
            "entry": "[49] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. Softrank: optimizing nonsmooth rank metrics. In Proceedings of the 2008 International Conference on Web Search and Data Mining, pages 77\u201386. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20Michael%20Guiver%2C%20John%20Robertson%2C%20Stephen%20Minka%2C%20Tom%20Softrank%3A%20optimizing%20nonsmooth%20rank%20metrics%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%2C%20Michael%20Guiver%2C%20John%20Robertson%2C%20Stephen%20Minka%2C%20Tom%20Softrank%3A%20optimizing%20nonsmooth%20rank%20metrics%202008"
        },
        {
            "id": "50",
            "entry": "[50] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20Sebastian%20Pratt%2C%20Lorien%20Learning%20to%20learn%202012"
        },
        {
            "id": "51",
            "entry": "[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashish%20Vaswani%20Noam%20Shazeer%20Niki%20Parmar%20Jakob%20Uszkoreit%20Llion%20Jones%20Aidan%20N%20Gomez%20%C5%81ukasz%20Kaiser%20and%20Illia%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "52",
            "entry": "[52] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270\u2013280, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Zipser%2C%20David%20A%20learning%20algorithm%20for%20continually%20running%20fully%20recurrent%20neural%20networks%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Zipser%2C%20David%20A%20learning%20algorithm%20for%20continually%20running%20fully%20recurrent%20neural%20networks%201989"
        },
        {
            "id": "53",
            "entry": "[53] Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning for neural machine translation. In EMNLP, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Lijun%20Tian%2C%20Fei%20Qin%2C%20Tao%20Lai%2C%20Jianhuang%20A%20study%20of%20reinforcement%20learning%20for%20neural%20machine%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Lijun%20Tian%2C%20Fei%20Qin%2C%20Tao%20Lai%2C%20Jianhuang%20A%20study%20of%20reinforcement%20learning%20for%20neural%20machine%20translation%202018"
        },
        {
            "id": "54",
            "entry": "[54] Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Adversarial neural machine translation. In ACML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lijun%20Wu%20Yingce%20Xia%20Li%20Zhao%20Fei%20Tian%20Tao%20Qin%20Jianhuang%20Lai%20and%20TieYan%20Liu%20Adversarial%20neural%20machine%20translation%20In%20ACML%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lijun%20Wu%20Yingce%20Xia%20Li%20Zhao%20Fei%20Tian%20Tao%20Qin%20Jianhuang%20Lai%20and%20TieYan%20Liu%20Adversarial%20neural%20machine%20translation%20In%20ACML%202018"
        },
        {
            "id": "55",
            "entry": "[55] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "56",
            "entry": "[56] Xiaojin Zhu. Machine teaching: An inverse problem to machine learning and an approach toward optimal education. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI\u201915, pages 4083\u20134087. AAAI Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Xiaojin%20Machine%20teaching%3A%20An%20inverse%20problem%20to%20machine%20learning%20and%20an%20approach%20toward%20optimal%20education%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Xiaojin%20Machine%20teaching%3A%20An%20inverse%20problem%20to%20machine%20learning%20and%20an%20approach%20toward%20optimal%20education%202015"
        },
        {
            "id": "57",
            "entry": "[57] Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%2C%20Barret%20Le%2C%20Quoc%20Neural%20architecture%20search%20with%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoph%2C%20Barret%20Le%2C%20Quoc%20Neural%20architecture%20search%20with%20reinforcement%20learning%202017"
        }
    ]
}
