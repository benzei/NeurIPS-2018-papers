{
    "filename": "7548-factored-bandits.pdf",
    "metadata": {
        "title": "Julian Zimmert University of Copenhagen zimmert@di.ku.dk",
        "author": "Yevgeny Seldin University of Copenhagen seldin@di.ku.dk",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7548-factored-bandits.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce the factored bandits model, which is a framework for learning with limited (bandit) feedback, where actions can be decomposed into a Cartesian product of atomic actions. Factored bandits incorporate rank-1 bandits as a special case, but significantly relax the assumptions on the form of the reward function. We provide an anytime algorithm for stochastic factored bandits and up to constants matching upper and lower regret bounds for the problem. Furthermore, we show how a slight modification enables the proposed algorithm to be applied to utilitybased dueling bandits. We obtain an improvement in the additive terms of the regret bound compared to state-of-the-art algorithms (the additive terms are dominating up to time horizons that are exponential in the number of arms)."
    },
    "keywords": [
        {
            "term": "cartesian product",
            "url": "https://en.wikipedia.org/wiki/cartesian_product"
        },
        {
            "term": "atomic action",
            "url": "https://en.wikipedia.org/wiki/atomic_action"
        }
    ],
    "highlights": [
        "We introduce factored bandits, which is a bandit learning model, where actions can be decomposed into a Cartesian product of atomic actions",
        "We presented an algorithm for playing stochastic factored bandits with uniformly identifiable actions and provided matching upper and lower bounds for the problem up to constant factors",
        "Our algorithm and proofs might serve as a template to turn other elimination style algorithms into improved anytime algorithms",
        "We improve the additive constants in the regret bound compared to state-of-the-art algorithms for utility-based dueling bandits"
    ],
    "key_statements": [
        "We introduce factored bandits, which is a bandit learning model, where actions can be decomposed into a Cartesian product of atomic actions",
        "We presented an algorithm for playing stochastic factored bandits with uniformly identifiable actions and provided matching upper and lower bounds for the problem up to constant factors",
        "Our algorithm and proofs might serve as a template to turn other elimination style algorithms into improved anytime algorithms",
        "We improve the additive constants in the regret bound compared to state-of-the-art algorithms for utility-based dueling bandits"
    ],
    "summary": [
        "We introduce factored bandits, which is a bandit learning model, where actions can be decomposed into a Cartesian product of atomic actions.",
        "Factored bandits generalize the above example to an arbitrary number of atomic actions and arbitrary reward functions satisfying some mild assumptions.",
        "Factored bandits generalize this to an arbitrary number of actions and significantly relax the assumption on the form of the reward function.",
        "3. We provide an anytime algorithm for playing factored bandits under uniform identifiability assumption in stochastic environments and analyze its regret.",
        "5. We show that the algorithm can be applied to utility-based dueling bandits, where the additive factor in the regret bound is reduced by a multiplicative factor of K compared to state-of-the-art.",
        "In theory an asymptotically optimal algorithm for any structured bandit problem was presented in [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], for factored bandits this algorithm does not only require solving an intractable semiinfinite linear program at every round, but it suffers from additive constants which are exponential in the number of atomic actions L.",
        "We start this section with the main theorem, which bounds the number of times the TEM pulls sub-optimal arms.",
        "A regret bound for the Factored Bandit TEA algorithm, Algorithm 1, is provided in the following theorem.",
        "A regret bound for the Dueling Bandit TEA algorithm (DBTEA), Algorithm 2, is provided in the following theorem.",
        "Both algorithms have a regret bound of problem independent multiplicative factors hidden under O are smaller for TEA, even without considering that rank-1 Elimination requires a doubling trick for anytime applications.",
        "[<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] based on Thompson Sampling shows one of the best empirical performances, but its regret bound includes an additive constant of order OpK3q.",
        "To show that there exists a regime where the improved constants gain an advantage over RMED, we conducted a second experiment in Figure 4, where we set the winning probability to 0.952 and significantly increase the number of arms.",
        "The evaluation shows that the additive terms are non-negligible and that Dueling Bandit TEA outperforms all baseline algorithms when the number of arms is sufficiently large.",
        "We presented an algorithm for playing stochastic factored bandits with uniformly identifiable actions and provided matching upper and lower bounds for the problem up to constant factors.",
        "We improve the additive constants in the regret bound compared to state-of-the-art algorithms for utility-based dueling bandits.",
        "One example mentioned in the text is the possibility of improving the regret bound when additional restrictions on the form of the reward function are introduced or improvements of the lower bound when algorithms are restricted in computational or memory complexity."
    ],
    "headline": "We introduce the factored bandits model, which is a framework for learning with limited  feedback, where actions can be decomposed into a Cartesian product of atomic actions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Y. Abbasi-Yadkori, D. P\u00e1l, and C. Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312\u20132320, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Y.%20P%C3%A1l%2C%20D.%20Szepesv%C3%A1ri%2C%20C.%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Y.%20P%C3%A1l%2C%20D.%20Szepesv%C3%A1ri%2C%20C.%20Improved%20algorithms%20for%20linear%20stochastic%20bandits%202011"
        },
        {
            "id": "2",
            "entry": "[2] N. Ailon, Z. Karnin, and T. Joachims. Reducing dueling bandits to cardinal bandits. In International Conference on Machine Learning, pages 856\u2013864, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ailon%2C%20N.%20Karnin%2C%20Z.%20Joachims%2C%20T.%20Reducing%20dueling%20bandits%20to%20cardinal%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ailon%2C%20N.%20Karnin%2C%20Z.%20Joachims%2C%20T.%20Reducing%20dueling%20bandits%20to%20cardinal%20bandits%202014"
        },
        {
            "id": "3",
            "entry": "[3] N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System Sciences, 78(5):1404\u20131422, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20N.%20Lugosi%2C%20G.%20Combinatorial%20bandits%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cesa-Bianchi%2C%20N.%20Lugosi%2C%20G.%20Combinatorial%20bandits%202012"
        },
        {
            "id": "4",
            "entry": "[4] W. Chen, Y. Wang, Y. Yuan, and Q. Wang. Combinatorial multi-armed bandit and its extension to probabilistically triggered arms. The Journal of Machine Learning Research, 17(1):1746\u20131778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20W.%20Wang%2C%20Y.%20Yuan%2C%20Y.%20Wang%2C%20Q.%20Combinatorial%20multi-armed%20bandit%20and%20its%20extension%20to%20probabilistically%20triggered%20arms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20W.%20Wang%2C%20Y.%20Yuan%2C%20Y.%20Wang%2C%20Q.%20Combinatorial%20multi-armed%20bandit%20and%20its%20extension%20to%20probabilistically%20triggered%20arms%202016"
        },
        {
            "id": "5",
            "entry": "[5] R. Combes, S. Magureanu, and A. Proutiere. Minimal exploration in structured stochastic bandits. In Advances in Neural Information Processing Systems, pages 1761\u20131769, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combes%2C%20R.%20Magureanu%2C%20S.%20Proutiere%2C%20A.%20Minimal%20exploration%20in%20structured%20stochastic%20bandits%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combes%2C%20R.%20Magureanu%2C%20S.%20Proutiere%2C%20A.%20Minimal%20exploration%20in%20structured%20stochastic%20bandits%202017"
        },
        {
            "id": "6",
            "entry": "[6] S. Filippi, O. Cappe, A. Garivier, and C. Szepesv\u00e1ri. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586\u2013594, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Filippi%2C%20S.%20Cappe%2C%20O.%20Garivier%2C%20A.%20Szepesv%C3%A1ri%2C%20C.%20Parametric%20bandits%3A%20The%20generalized%20linear%20case%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Filippi%2C%20S.%20Cappe%2C%20O.%20Garivier%2C%20A.%20Szepesv%C3%A1ri%2C%20C.%20Parametric%20bandits%3A%20The%20generalized%20linear%20case%202010"
        },
        {
            "id": "7",
            "entry": "[7] S. Katariya, B. Kveton, C. Szepesv\u00e1ri, C. Vernade, and Z. Wen. Stochastic rank-1 bandits (long version). In AISTATS, volume 54 of PMLR, pages 392\u2013401, April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Katariya%20B%20Kveton%20C%20Szepesv%C3%A1ri%20C%20Vernade%20and%20Z%20Wen%20Stochastic%20rank1%20bandits%20long%20version%20In%20AISTATS%20volume%2054%20of%20PMLR%20pages%20392401%20April%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%20Katariya%20B%20Kveton%20C%20Szepesv%C3%A1ri%20C%20Vernade%20and%20Z%20Wen%20Stochastic%20rank1%20bandits%20long%20version%20In%20AISTATS%20volume%2054%20of%20PMLR%20pages%20392401%20April%202017"
        },
        {
            "id": "8",
            "entry": "[8] S. Katariya, B. Kveton, C. Szepesv\u00e1ri, C. Vernade, and Z. Wen. Bernoulli rank-1 bandits for click feedback. International Joint Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Katariya%2C%20S.%20Kveton%2C%20B.%20Szepesv%C3%A1ri%2C%20C.%20Vernade%2C%20C.%20Bernoulli%20rank-1%20bandits%20for%20click%20feedback%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Katariya%2C%20S.%20Kveton%2C%20B.%20Szepesv%C3%A1ri%2C%20C.%20Vernade%2C%20C.%20Bernoulli%20rank-1%20bandits%20for%20click%20feedback%202017"
        },
        {
            "id": "9",
            "entry": "[9] J. Komiyama, J. Honda, H. Kashima, and H. Nakagawa. Regret lower bound and optimal algorithm in dueling bandit problem. In Conference on Learning Theory, pages 1141\u20131154, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Komiyama%2C%20J.%20Honda%2C%20J.%20Kashima%2C%20H.%20Nakagawa%2C%20H.%20Regret%20lower%20bound%20and%20optimal%20algorithm%20in%20dueling%20bandit%20problem%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Komiyama%2C%20J.%20Honda%2C%20J.%20Kashima%2C%20H.%20Nakagawa%2C%20H.%20Regret%20lower%20bound%20and%20optimal%20algorithm%20in%20dueling%20bandit%20problem%202015"
        },
        {
            "id": "10",
            "entry": "[10] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4\u201322, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20T.L.%20Robbins%2C%20H.%20Asymptotically%20efficient%20adaptive%20allocation%20rules%201985"
        },
        {
            "id": "11",
            "entry": "[11] T. Lattimore and C. Szepesv\u00e1ri. The end of optimism? An asymptotic analysis of finite-armed linear bandits (long version). In AISTATS, volume 54 of PMLR, pages 728\u2013737, April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lattimore%2C%20T.%20Szepesv%C3%A1ri%2C%20C.%20The%20end%20of%20optimism%3F%20An%20asymptotic%20analysis%20of%20finite-armed%20linear%20bandits%20%28long%20version%29%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lattimore%2C%20T.%20Szepesv%C3%A1ri%2C%20C.%20The%20end%20of%20optimism%3F%20An%20asymptotic%20analysis%20of%20finite-armed%20linear%20bandits%20%28long%20version%29%202017-04"
        },
        {
            "id": "12",
            "entry": "[12] T. Urvoy, F. Clerot, R. F\u00e9raud, and S. Naamane. Generic exploration and k-armed voting bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 91\u201399, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Urvoy%2C%20T.%20Clerot%2C%20F.%20F%C3%A9raud%2C%20R.%20Naamane%2C%20S.%20Generic%20exploration%20and%20k-armed%20voting%20bandits%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Urvoy%2C%20T.%20Clerot%2C%20F.%20F%C3%A9raud%2C%20R.%20Naamane%2C%20S.%20Generic%20exploration%20and%20k-armed%20voting%20bandits%202013"
        },
        {
            "id": "13",
            "entry": "[13] H. Wu and X. Liu. Double thompson sampling for dueling bandits. In Advances in Neural Information Processing Systems, pages 649\u2013657, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20H.%20Liu%2C%20X.%20Double%20thompson%20sampling%20for%20dueling%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20H.%20Liu%2C%20X.%20Double%20thompson%20sampling%20for%20dueling%20bandits%202016"
        },
        {
            "id": "14",
            "entry": "[14] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1201\u20131208. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yue%2C%20Y.%20Joachims%2C%20T.%20Interactively%20optimizing%20information%20retrieval%20systems%20as%20a%20dueling%20bandits%20problem%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yue%2C%20Y.%20Joachims%2C%20T.%20Interactively%20optimizing%20information%20retrieval%20systems%20as%20a%20dueling%20bandits%20problem%202009"
        },
        {
            "id": "15",
            "entry": "[15] Y. Yue and T. Joachims. Beat the mean bandit. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 241\u2013248, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yue%2C%20Y.%20Joachims%2C%20T.%20Beat%20the%20mean%20bandit%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yue%2C%20Y.%20Joachims%2C%20T.%20Beat%20the%20mean%20bandit%202011"
        },
        {
            "id": "16",
            "entry": "[16] Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538\u20131556, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yue%2C%20Y.%20Broder%2C%20J.%20Kleinberg%2C%20R.%20Joachims%2C%20T.%20The%20k-armed%20dueling%20bandits%20problem%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yue%2C%20Y.%20Broder%2C%20J.%20Kleinberg%2C%20R.%20Joachims%2C%20T.%20The%20k-armed%20dueling%20bandits%20problem%202012"
        },
        {
            "id": "17",
            "entry": "[17] M. Zoghi, S. Whiteson, R. Munos, and M. Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In International Conference on Machine Learning, pages 10\u201318, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoghi%2C%20M.%20Whiteson%2C%20S.%20Munos%2C%20R.%20Rijke%2C%20M.%20Relative%20upper%20confidence%20bound%20for%20the%20k-armed%20dueling%20bandit%20problem%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoghi%2C%20M.%20Whiteson%2C%20S.%20Munos%2C%20R.%20Rijke%2C%20M.%20Relative%20upper%20confidence%20bound%20for%20the%20k-armed%20dueling%20bandit%20problem%202014"
        }
    ]
}
