{
    "filename": "7853-predict-responsibly-improving-fairness-and-accuracy-by-learning-to-defer.pdf",
    "metadata": {
        "title": "Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer",
        "author": "David Madras, Toni Pitassi, Richard Zemel",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7853-predict-responsibly-improving-fairness-and-accuracy-by-learning-to-defer.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing learning to defer, which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "decision maker",
            "url": "https://en.wikipedia.org/wiki/decision_maker"
        },
        {
            "term": "decision making",
            "url": "https://en.wikipedia.org/wiki/decision_making"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "complex domain",
            "url": "https://en.wikipedia.org/wiki/complex_domain"
        },
        {
            "term": "disparate impact",
            "url": "https://en.wikipedia.org/wiki/disparate_impact"
        }
    ],
    "highlights": [
        "Can humans and machines make decisions jointly? A growing use of automated decision-making in complex domains such as loan approvals [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], medical diagnosis [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], and criminal justice [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], has raised questions about the role of machine learning in high-stakes decision-making, and the role of humans in overseeing and applying machine predictions",
        "We argue that since these models are often used as part of larger systems e.g. in tandem with another decision maker, they should learn to predict responsibly: the model should predict only if its predictions are reliably aligned with the system\u2019s objectives, which often include accuracy and fairness",
        "We provide theoretical and experimental evidence that learning to defer improves upon the standard rejection learning paradigm, if models are intended to work as part of a larger system",
        "Learning to defer, which generalizes rejection learning under this framework",
        "We demonstrate that deferring models can optimize the performance of decision-making pipelines as a whole, beyond the improvement provided by rejection learning"
    ],
    "key_statements": [
        "Can humans and machines make decisions jointly? A growing use of automated decision-making in complex domains such as loan approvals [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], medical diagnosis [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], and criminal justice [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], has raised questions about the role of machine learning in high-stakes decision-making, and the role of humans in overseeing and applying machine predictions",
        "We argue that since these models are often used as part of larger systems e.g. in tandem with another decision maker, they should learn to predict responsibly: the model should predict only if its predictions are reliably aligned with the system\u2019s objectives, which often include accuracy and fairness",
        "Our main contribution is the formulation of adaptive rejection learning, which we call learning to defer, where the model works adaptively with the decision-maker",
        "We provide theoretical and experimental evidence that learning to defer improves upon the standard rejection learning paradigm, if models are intended to work as part of a larger system",
        "We show experimentally that the equivalence between learning to defer with an oracle-decision maker and rejection learning holds in the fairness case.\n4 Related Work",
        "We find that the advantage of a deferring model holds in this case, as it adapts to the decision maker\u2019s extreme bias",
        "We find that the deferring model\u2019s adaptivity brings two advantages: it can adaptively defer at different rates for the two sensitive groups to counteract the decision maker\u2019s bias; and it is able to modulate the overall amount that it defers when the decision maker is biased",
        "We find that the deferring models, continue to achieve higher accuracy even when requiring that models attain a certain minimum subgroup accuracy",
        "Learning to defer, which generalizes rejection learning under this framework",
        "We demonstrate that deferring models can optimize the performance of decision-making pipelines as a whole, beyond the improvement provided by rejection learning"
    ],
    "summary": [
        "Can humans and machines make decisions jointly? A growing use of automated decision-making in complex domains such as loan approvals [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], medical diagnosis [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], and criminal justice [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], has raised questions about the role of machine learning in high-stakes decision-making, and the role of humans in overseeing and applying machine predictions.",
        "Our main contribution is the formulation of adaptive rejection learning, which we call learning to defer, where the model works adaptively with the decision-maker.",
        "Our experimental results show that in each scenario, learning to defer allows models to work with users to make fairer, more responsible decisions.",
        "We aim to learn a model which outputs predictive probabilities YM and binary deferral decisions s, in order to optimize the output of the system as a whole.",
        "This is usually learned by minimizing a classification objective Lreject with a penalty \u03b3reject for each rejection [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], where Y is ground truth, YM is the model output, and s is the reject variable (s = 1 means PASS); all binary: Lreject(Y, YM , s) = \u2212 [(1 \u2212 si) (Yi, YM,i) + si\u03b3reject]",
        "The model leverages information about the DM to make PASS decisions adaptively.",
        "In our decision-making pipeline, we aim to formulate a fair model which can be used for learning to defer (Eq 3)).",
        "Bower et al [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] describe fair sequential decision making but do not have a PASS concept or provide a learning procedure.",
        "Varshney and Alemzadeh [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] propose \u201csafety reserves\u201d and \u201csafe fail\u201d options which combine learning with rejection and fairness/safety, but do not analyze learning procedures or larger decision-making frameworks.",
        "3. Inconsistent DM, ignores fairness (DM\u2019s accuracy varies by subgroup, with total accuracy lower than model): Human DMs can be less accurate, despite having extra information [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "We consider the scenario where a DM has higher accuracy than the model we train, due to the DM having access to extra information/resources for security, efficiency, or contextual reasons.",
        "In Fig. 3a, we show that learning-to-defer achieves a better accuracy-fairness tradeoff than rejection learning.",
        "The fair rejection learning model outperforms binary baselines, by integrating the extra DM accuracy on some",
        "In Fig. 3c, we compare deferring and rejecting models, examining their classification accuracy at different deferral rates.",
        "We observe that for each deferral rate, the model that learned to defer achieves a higher classification accuracy.",
        "0.62 0.64 0.66 0.68 0.70 0.72 Minimum Subgroup Accuracy ring models deferred more on the DM\u2019s reliable (a) COMPAS dataset subgroup, and less on the unreliable subgroup, (b) Health dataset particularly as compared to rejection models.",
        "We demonstrate that deferring models can optimize the performance of decision-making pipelines as a whole, beyond the improvement provided by rejection learning.",
        "We show how models can learn to predict responsibly within their surrounding systems, an essential step towards fairer, more responsible machine learning"
    ],
    "headline": "We explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Josh Attenberg, Panagiotis G Ipeirotis, and Foster J Provost. Beat the machine: Challenging workers to find the unknown unknowns. Human Computation, 11(11), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Attenberg%2C%20Josh%20Ipeirotis%2C%20Panagiotis%20G.%20Provost%2C%20Foster%20J.%20Beat%20the%20machine%3A%20Challenging%20workers%20to%20find%20the%20unknown%20unknowns%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Attenberg%2C%20Josh%20Ipeirotis%2C%20Panagiotis%20G.%20Provost%2C%20Foster%20J.%20Beat%20the%20machine%3A%20Challenging%20workers%20to%20find%20the%20unknown%20unknowns%202011"
        },
        {
            "id": "2",
            "entry": "[2] Yahav Bechavod and Katrina Ligett. Learning Fair Classifiers: A Regularization-Inspired Approach. arXiv:1707.00044 [cs, stat], June 2017. URL http://arxiv.org/abs/1707.00044.arXiv:1707.00044.",
            "url": "http://arxiv.org/abs/1707.00044.arXiv:1707.00044",
            "arxiv_url": "https://arxiv.org/pdf/1707.00044"
        },
        {
            "id": "3",
            "entry": "[3] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1613\u20131622, Lille, France, 07\u201309 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/blundell15.html.",
            "url": "http://proceedings.mlr.press/v37/blundell15.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blundell%2C%20Charles%20Cornebise%2C%20Julien%20Kavukcuoglu%2C%20Koray%20Wierstra%2C%20Daan%20Weight%20uncertainty%20in%20neural%20network%202015-07"
        },
        {
            "id": "4",
            "entry": "[4] Amanda Bower, Sarah N. Kitchen, Laura Niss, Martin J. Strauss, Alexander Vargas, and Suresh Venkatasubramanian. Fair Pipelines. arXiv:1707.00391 [cs, stat], July 2017. URL http://arxiv.org/abs/1707.00391.arXiv:1707.00391.",
            "url": "http://arxiv.org/abs/1707.00391.arXiv:1707.00391",
            "arxiv_url": "https://arxiv.org/pdf/1707.00391"
        },
        {
            "id": "5",
            "entry": "[5] Jenna Burrell. How the machine \u2018thinks\u2019: Understanding opacity in machine learning algorithms. Big Data & Society, 3(1):2053951715622512, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burrell%2C%20Jenna%20How%20the%20machine%20%E2%80%98thinks%E2%80%99%3A%20Understanding%20opacity%20in%20machine%20learning%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burrell%2C%20Jenna%20How%20the%20machine%20%E2%80%98thinks%E2%80%99%3A%20Understanding%20opacity%20in%20machine%20learning%20algorithms%202016"
        },
        {
            "id": "6",
            "entry": "[6] Lowell W Busenitz and Jay B Barney. Differences between entrepreneurs and managers in large organizations: Biases and heuristics in strategic decision-making. Journal of business venturing, 12(1):9\u201330, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Busenitz%2C%20Lowell%20W.%20Barney%2C%20Jay%20B.%20Differences%20between%20entrepreneurs%20and%20managers%20in%20large%20organizations%3A%20Biases%20and%20heuristics%20in%20strategic%20decision-making%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Busenitz%2C%20Lowell%20W.%20Barney%2C%20Jay%20B.%20Differences%20between%20entrepreneurs%20and%20managers%20in%20large%20organizations%3A%20Biases%20and%20heuristics%20in%20strategic%20decision-making%201997"
        },
        {
            "id": "7",
            "entry": "[7] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2):153\u2013163, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017"
        },
        {
            "id": "8",
            "entry": "[8] C. Chow. An optimum character recognition system using decision function. IEEE T. C., 1957.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20C.%20An%20optimum%20character%20recognition%20system%20using%20decision%20function%201957",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20C.%20An%20optimum%20character%20recognition%20system%20using%20decision%20function%201957"
        },
        {
            "id": "9",
            "entry": "[9] C. Chow. On optimum recognition error and reject trade-off. IEEE T. C., 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20C.%20On%20optimum%20recognition%20error%20and%20reject%20trade-off%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20C.%20On%20optimum%20recognition%20error%20and%20reject%20trade-off%201970"
        },
        {
            "id": "10",
            "entry": "[10] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In International Conference on Algorithmic Learning Theory, pages 67\u201382.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20DeSalvo%2C%20Giulia%20Mohri%2C%20Mehryar%20Learning%20with%20rejection",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20DeSalvo%2C%20Giulia%20Mohri%2C%20Mehryar%20Learning%20with%20rejection"
        },
        {
            "id": "11",
            "entry": "[11] Shai Danziger, Jonathan Levav, and Liora Avnaim-Pesso. Extraneous factors in judicial decisions. Proceedings of the National Academy of Sciences, 108(17):6889\u20136892, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Danziger%2C%20Shai%20Levav%2C%20Jonathan%20Avnaim-Pesso%2C%20Liora%20Extraneous%20factors%20in%20judicial%20decisions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Danziger%2C%20Shai%20Levav%2C%20Jonathan%20Avnaim-Pesso%2C%20Liora%20Extraneous%20factors%20in%20judicial%20decisions%202011"
        },
        {
            "id": "12",
            "entry": "[12] Robyn M Dawes, David Faust, and Paul E Meehl. Clinical versus actuarial judgment. Science, 243(4899):1668\u20131674, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dawes%2C%20Robyn%20M.%20Faust%2C%20David%20Meehl%2C%20Paul%20E.%20Clinical%20versus%20actuarial%20judgment%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dawes%2C%20Robyn%20M.%20Faust%2C%20David%20Meehl%2C%20Paul%20E.%20Clinical%20versus%20actuarial%20judgment%201989"
        },
        {
            "id": "13",
            "entry": "[13] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages 214\u2013226. ACM, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "14",
            "entry": "[14] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115\u2013118, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esteva%2C%20Andre%20Kuprel%2C%20Brett%20Novoa%2C%20Roberto%20A.%20Ko%2C%20Justin%20Dermatologist-level%20classification%20of%20skin%20cancer%20with%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esteva%2C%20Andre%20Kuprel%2C%20Brett%20Novoa%2C%20Roberto%20A.%20Ko%2C%20Justin%20Dermatologist-level%20classification%20of%20skin%20cancer%20with%20deep%20neural%20networks%202017"
        },
        {
            "id": "15",
            "entry": "[15] Lydia Fischer and Thomas Villmann. A probabilistic classifier model with adaptive rejection option. Technical Report 1865-3960, January 2016. URL https://www.techfak.uni-bielefeld.de/~fschleif/mlr/mlr_01_2016.pdf.",
            "url": "https://www.techfak.uni-bielefeld.de/~fschleif/mlr/mlr_01_2016.pdf"
        },
        {
            "id": "16",
            "entry": "[16] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P. Gummadi, and Adrian Weller. On Fairness, Diversity and Randomness in Algorithmic Decision Making. arXiv:1706.10208 [cs, stat], June 2017. URL http://arxiv.org/abs/1706.10208.arXiv:1706.10208.",
            "url": "http://arxiv.org/abs/1706.10208.arXiv:1706.10208",
            "arxiv_url": "https://arxiv.org/pdf/1706.10208"
        },
        {
            "id": "17",
            "entry": "[17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321\u20131330, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/guo17a.html.",
            "url": "http://proceedings.mlr.press/v70/guo17a.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guo%2C%20Chuan%20Pleiss%2C%20Geoff%20Sun%2C%20Yu%20Weinberger%2C%20Kilian%20Q.%20On%20calibration%20of%20modern%20neural%20networks%202017-08"
        },
        {
            "id": "18",
            "entry": "[18] Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. In Advances in neural information processing systems, pages 3909\u2013 3917, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hadfield-Menell%2C%20Dylan%20Russell%2C%20Stuart%20J.%20Abbeel%2C%20Pieter%20Dragan%2C%20Anca%20Cooperative%20inverse%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hadfield-Menell%2C%20Dylan%20Russell%2C%20Stuart%20J.%20Abbeel%2C%20Pieter%20Dragan%2C%20Anca%20Cooperative%20inverse%20reinforcement%20learning%202016"
        },
        {
            "id": "19",
            "entry": "[19] Kelly Hannah-Moffat. Actuarial sentencing: An \u201cunsettled\u201d proposition. Justice Quarterly, 30 (2):270\u2013296, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hannah-Moffat%2C%20Kelly%20Actuarial%20sentencing%3A%20An%20%E2%80%9Cunsettled%E2%80%9D%20proposition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hannah-Moffat%2C%20Kelly%20Actuarial%20sentencing%3A%20An%20%E2%80%9Cunsettled%E2%80%9D%20proposition%202013"
        },
        {
            "id": "20",
            "entry": "[20] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In Advances in neural information processing systems, pages 3315\u20133323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016"
        },
        {
            "id": "21",
            "entry": "[21] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacobs%2C%20Robert%20A.%20Jordan%2C%20Michael%20I.%20Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Adaptive%20mixtures%20of%20local%20experts%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacobs%2C%20Robert%20A.%20Jordan%2C%20Michael%20I.%20Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Adaptive%20mixtures%20of%20local%20experts%201991"
        },
        {
            "id": "22",
            "entry": "[22] Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning: Classic and contextual bandits. In Advances in Neural Information Processing Systems, pages 325\u2013333, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joseph%2C%20Matthew%20Kearns%2C%20Michael%20Morgenstern%2C%20Jamie%20H.%20Roth%2C%20Aaron%20Fairness%20in%20learning%3A%20Classic%20and%20contextual%20bandits%202016"
        },
        {
            "id": "23",
            "entry": "[23] F. Kamiran and T. Calders. Classifying without discriminating. In 2nd International Conference on Computer, Control and Communication, 2009. IC4 2009, pages 1\u20136, February 2009. doi: 10.1109/IC4.2009.4909197.",
            "crossref": "https://dx.doi.org/10.1109/IC4.2009.4909197",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/IC4.2009.4909197"
        },
        {
            "id": "24",
            "entry": "[24] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prejudice remover regularizer. Machine Learning and Knowledge Discovery in Databases, pages 35\u201350, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamishima%2C%20Toshihiro%20Akaho%2C%20Shotaro%20Asoh%2C%20Hideki%20Sakuma%2C%20Jun%20Fairness-aware%20classifier%20with%20prejudice%20remover%20regularizer%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamishima%2C%20Toshihiro%20Akaho%2C%20Shotaro%20Asoh%2C%20Hideki%20Sakuma%2C%20Jun%20Fairness-aware%20classifier%20with%20prejudice%20remover%20regularizer%202012"
        },
        {
            "id": "25",
            "entry": "[25] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "26",
            "entry": "[26] Lauren Kirchner and Jeff Larson. How we analyzed the compas recidivism algorithm. In https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm, 2016.",
            "url": "https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirchner%2C%20Lauren%20Larson%2C%20Jeff%20How%20we%20analyzed%20the%20compas%20recidivism%20algorithm%202016"
        },
        {
            "id": "27",
            "entry": "[27] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent Trade-Offs in the Fair Determination of Risk Scores. arXiv:1609.05807 [cs, stat], September 2016. URL http://arxiv.org/abs/1609.05807.arXiv:1609.05807.",
            "url": "http://arxiv.org/abs/1609.05807.arXiv:1609.05807",
            "arxiv_url": "https://arxiv.org/pdf/1609.05807"
        },
        {
            "id": "28",
            "entry": "[28] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00712"
        },
        {
            "id": "29",
            "entry": "[29] Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classification. In Conference on Fairness, Accountability and Transparency, pages 107\u2013118, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20Aditya%20Krishna%20Williamson%2C%20Robert%20C.%20The%20cost%20of%20fairness%20in%20binary%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20Aditya%20Krishna%20Williamson%2C%20Robert%20C.%20The%20cost%20of%20fairness%20in%20binary%20classification%202018"
        },
        {
            "id": "30",
            "entry": "[30] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and calibration. In Advances in Neural Information Processing Systems, pages 5680\u20135689, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pleiss%2C%20Geoff%20Raghavan%2C%20Manish%20Wu%2C%20Felix%20Kleinberg%2C%20Jon%20On%20fairness%20and%20calibration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pleiss%2C%20Geoff%20Raghavan%2C%20Manish%20Wu%2C%20Felix%20Kleinberg%2C%20Jon%20On%20fairness%20and%20calibration%202017"
        },
        {
            "id": "31",
            "entry": "[31] Kush R Varshney and Homa Alemzadeh. On the safety of machine learning: Cyber-physical systems, decision sciences, and data products. Big data, 5(3):246\u2013255, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Varshney%2C%20Kush%20R.%20Alemzadeh%2C%20Homa%20On%20the%20safety%20of%20machine%20learning%3A%20Cyber-physical%20systems%2C%20decision%20sciences%2C%20and%20data%20products%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Varshney%2C%20Kush%20R.%20Alemzadeh%2C%20Homa%20On%20the%20safety%20of%20machine%20learning%3A%20Cyber-physical%20systems%2C%20decision%20sciences%2C%20and%20data%20products%202017"
        },
        {
            "id": "32",
            "entry": "[32] Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov, and Joseph E. Gonzalez. IDK Cascades: Fast Deep Learning by Learning not to Overthink. arXiv:1706.00885 [cs], June 2017. URL http://arxiv.org/abs/1706.00885.arXiv:1706.00885.",
            "url": "http://arxiv.org/abs/1706.00885.arXiv:1706.00885",
            "arxiv_url": "https://arxiv.org/pdf/1706.00885"
        },
        {
            "id": "33",
            "entry": "[33] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pages 1171\u20131180. International World Wide Web Conferences Steering Committee, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20beyond%20disparate%20treatment%20%26%20disparate%20impact%3A%20Learning%20classification%20without%20disparate%20mistreatment%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20beyond%20disparate%20treatment%20%26%20disparate%20impact%3A%20Learning%20classification%20without%20disparate%20mistreatment%202017"
        },
        {
            "id": "34",
            "entry": "[34] Richard Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning Fair Representations. In PMLR, pages 325\u2013333, February 2013. URL http://proceedings.mlr.press/v28/zemel13.html. ",
            "url": "http://proceedings.mlr.press/v28/zemel13.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20Richard%20Wu%2C%20Yu%20Swersky%2C%20Kevin%20Pitassi%2C%20Toni%20Learning%20Fair%20Representations%202013-02"
        }
    ]
}
