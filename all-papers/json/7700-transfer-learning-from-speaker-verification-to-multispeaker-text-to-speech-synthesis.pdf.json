{
    "filename": "7700-transfer-learning-from-speaker-verification-to-multispeaker-text-to-speech-synthesis.pdf",
    "metadata": {
        "title": "Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis",
        "author": "Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7700-transfer-learning-from-speaker-verification-to-multispeaker-text-to-speech-synthesis.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a fixed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker TTS task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation."
    },
    "keywords": [
        {
            "term": "natural speech",
            "url": "https://en.wikipedia.org/wiki/natural_speech"
        },
        {
            "term": "Mean Opinion Score",
            "url": "https://en.wikipedia.org/wiki/Mean_Opinion_Score"
        },
        {
            "term": "CSTR",
            "url": "https://en.wikipedia.org/wiki/CSTR"
        },
        {
            "term": "speaker verification",
            "url": "https://en.wikipedia.org/wiki/speaker_verification"
        },
        {
            "term": "high quality",
            "url": "https://en.wikipedia.org/wiki/high_quality"
        },
        {
            "term": "speech synthesis",
            "url": "https://en.wikipedia.org/wiki/speech_synthesis"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner",
        "Our work is most similar to the speaker encoding models in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], except that we utilize a network independently-trained for a speaker verification task on a large dataset of untranscribed audio from tens of thousands of speakers, using a state-of-the-art generalized end-to-end loss [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]. [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] incorporated a similar speaker-discriminative representation into their model, all components were trained jointly",
        "We present a neural network-based system for multispeaker TTS synthesis",
        "The system combines an independently trained speaker encoder network with a sequence-to-sequence TTS synthesis network and neural vocoder based on Tacotron 2",
        "By leveraging on the knowledge learned by the discriminative speaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen during training, but for speakers never seen before",
        "Through evaluations based on a speaker verification system as well as subjective listening tests, we demonstrated that the synthesized speech is reasonably similar to real speech from the target speakers, even on such unseen speakers"
    ],
    "key_statements": [
        "The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner",
        "Our work is most similar to the speaker encoding models in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], except that we utilize a network independently-trained for a speaker verification task on a large dataset of untranscribed audio from tens of thousands of speakers, using a state-of-the-art generalized end-to-end loss [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]. [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] incorporated a similar speaker-discriminative representation into their model, all components were trained jointly",
        "The mel spectrogram predicted by the synthesizer network captures all of the relevant detail needed for high quality synthesis of a variety of voices, allowing a multispeaker vocoder to be constructed by training on data from many speakers.\n2.4",
        "Results are shown in Table 1, comparing the proposed model to baseline multispeaker models that utilize a lookup table of speaker embeddings similar to [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], but otherwise have identical architectures to the proposed synthesizer network",
        "Mean Opinion Score in all datasets, with the VCTK model obtaining a Mean Opinion Score about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers",
        "This is the consequence of two drawbacks of the LibriSpeech dataset: (i) the lack of punctuation in transcripts, which makes it difficult for the model to learn to pause naturally, and the higher level of background noise compared to VCTK, some of which the synthesizer has learned to reproduce, despite denoising the training targets as described above",
        "We present a neural network-based system for multispeaker TTS synthesis",
        "The system combines an independently trained speaker encoder network with a sequence-to-sequence TTS synthesis network and neural vocoder based on Tacotron 2",
        "By leveraging on the knowledge learned by the discriminative speaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen during training, but for speakers never seen before",
        "Through evaluations based on a speaker verification system as well as subjective listening tests, we demonstrated that the synthesized speech is reasonably similar to real speech from the target speakers, even on such unseen speakers"
    ],
    "summary": [
        "The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner.",
        "A similar spectrogram encoder network, trained without a triplet loss, was shown to work for transferring target prosody to synthesized speech [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "Our work is most similar to the speaker encoding models in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], except that we utilize a network independently-trained for a speaker verification task on a large dataset of untranscribed audio from tens of thousands of speakers, using a state-of-the-art generalized end-to-end loss [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>].",
        "The mel spectrogram predicted by the synthesizer network captures all of the relevant detail needed for high quality synthesis of a variety of voices, allowing a multispeaker vocoder to be constructed by training on data from many speakers.",
        "This is the consequence of two drawbacks of the LibriSpeech dataset: (i) the lack of punctuation in transcripts, which makes it difficult for the model to learn to pause naturally, and the higher level of background noise compared to VCTK, some of which the synthesizer has learned to reproduce, despite denoising the training targets as described above.",
        "The proposed model obtains lower similarity between ground truth and synthesized speech.",
        "Train set of the synthesizer and vocoder networks; both models used an identical speaker encoder.",
        "As an objective metric of the degree of speaker similarity between synthesized and ground truth audio for unseen speakers, we evaluated the ability of a limited speaker verification system to distinguish synthetic from real speech.",
        "As shown in Table 4, as long as the synthesizer was trained on a sufficiently large set of speakers, i.e. on LibriSpeech, the synthesized speech is typically most similar to the ground truth voices.",
        "We first evaluate the speaker encoder trained on LibriSpeech Clean and Other sets, each of which contain a similar number of speakers.",
        "Bypassing the speaker encoder network and conditioning the synthesizer on random points in the speaker embedding space results in speech from fictitious speakers which are not present in the train or test sets of either the synthesizer or the speaker encoder.",
        "By leveraging on the knowledge learned by the discriminative speaker encoder, the synthesizer is able to generate high quality speech not only for speakers seen during training, but for speakers never seen before.",
        "It requires neither speaker identity labels for the synthesizer training data, nor high quality clean speech or transcripts for the speaker encoder training data.",
        "Through evaluations based on a speaker verification system as well as subjective listening tests, we demonstrated that the synthesized speech is reasonably similar to real speech from the target speakers, even on such unseen speakers"
    ],
    "headline": "We describe a neural network-based system for text-to-speech  synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Artificial Intelligence at Google \u2013 Our Principles. https://ai.google/principles/, 2018.",
            "url": "https://ai.google/principles/"
        },
        {
            "id": "2",
            "entry": "[2] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning with a few samples. arXiv preprint arXiv:1802.06006, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06006"
        },
        {
            "id": "3",
            "entry": "[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "4",
            "entry": "[4] Steven Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transactions on Acoustics, Speech, and Signal Processing, 27(2):113\u2013120, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boll%2C%20Steven%20Suppression%20of%20acoustic%20noise%20in%20speech%20using%20spectral%20subtraction%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boll%2C%20Steven%20Suppression%20of%20acoustic%20noise%20in%20speech%20using%20spectral%20subtraction%201979"
        },
        {
            "id": "5",
            "entry": "[5] Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan Wang, Luis C Cobo, Andrew Trask, Ben Laurie, et al. Sample efficient adaptive text-to-speech. arXiv preprint arXiv:1809.10460, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.10460"
        },
        {
            "id": "6",
            "entry": "[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. VoxCeleb2: Deep speaker recognition. In Interspeech, pages 1086\u20131090, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chung%2C%20Joon%20Son%20Nagrani%2C%20Arsha%20Zisserman%2C%20Andrew%20VoxCeleb2%3A%20Deep%20speaker%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chung%2C%20Joon%20Son%20Nagrani%2C%20Arsha%20Zisserman%2C%20Andrew%20VoxCeleb2%3A%20Deep%20speaker%20recognition%202018"
        },
        {
            "id": "7",
            "entry": "[7] Rama Doddipatla, Norbert Braunschweiler, and Ranniery Maia. Speaker adaptation in dnnbased speech synthesis using d-vectors. In Proc. Interspeech, pages 3404\u20133408, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doddipatla%2C%20Rama%20Braunschweiler%2C%20Norbert%20Maia%2C%20Ranniery%20Speaker%20adaptation%20in%20dnnbased%20speech%20synthesis%20using%20d-vectors%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doddipatla%2C%20Rama%20Braunschweiler%2C%20Norbert%20Maia%2C%20Ranniery%20Speaker%20adaptation%20in%20dnnbased%20speech%20synthesis%20using%20d-vectors%202017"
        },
        {
            "id": "8",
            "entry": "[8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2962\u20132970. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gibiansky%2C%20Andrew%20Arik%2C%20Sercan%20Diamos%2C%20Gregory%20Miller%2C%20John%20Deep%20Voice%202%3A%20Multi-speaker%20neural%20text-to-speech%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gibiansky%2C%20Andrew%20Arik%2C%20Sercan%20Diamos%2C%20Gregory%20Miller%2C%20John%20Deep%20Voice%202%3A%20Multi-speaker%20neural%20text-to-speech%202017"
        },
        {
            "id": "9",
            "entry": "[9] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent speaker verification. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 5115\u20135119. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heigold%2C%20Georg%20Moreno%2C%20Ignacio%20Bengio%2C%20Samy%20Shazeer%2C%20Noam%20End-to-end%20text-dependent%20speaker%20verification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heigold%2C%20Georg%20Moreno%2C%20Ignacio%20Bengio%2C%20Samy%20Shazeer%2C%20Noam%20End-to-end%20text-dependent%20speaker%20verification%202016"
        },
        {
            "id": "10",
            "entry": "[10] Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on a short untranscribed sample. arXiv preprint arXiv:1802.06984, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06984"
        },
        {
            "id": "11",
            "entry": "[11] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08612"
        },
        {
            "id": "12",
            "entry": "[12] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an ASR corpus based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5206\u20135210. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panayotov%2C%20Vassil%20Chen%2C%20Guoguo%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20LibriSpeech%3A%20an%20ASR%20corpus%20based%20on%20public%20domain%20audio%20books%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panayotov%2C%20Vassil%20Chen%2C%20Guoguo%20Povey%2C%20Daniel%20Khudanpur%2C%20Sanjeev%20LibriSpeech%3A%20an%20ASR%20corpus%20based%20on%20public%20domain%20audio%20books%202015"
        },
        {
            "id": "13",
            "entry": "[13] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. In Proc. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20O%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20Voice%203%202000speaker%20neural%20texttospeech%20In%20Proc%20International%20Conference%20on%20Learning%20Representations%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%20Ping%20Kainan%20Peng%20Andrew%20Gibiansky%20Sercan%20O%20Arik%20Ajay%20Kannan%20Sharan%20Narang%20Jonathan%20Raiman%20and%20John%20Miller%20Deep%20Voice%203%202000speaker%20neural%20texttospeech%20In%20Proc%20International%20Conference%20on%20Learning%20Representations%20ICLR%202018"
        },
        {
            "id": "14",
            "entry": "[14] ITUT Rec. P. 800: Methods for subjective determination of transmission quality. International Telecommunication Union, Geneva, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=P%2C%20I.T.U.T.Rec%20800%3A%20Methods%20for%20subjective%20determination%20of%20transmission%20quality%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=P%2C%20I.T.U.T.Rec%20800%3A%20Methods%20for%20subjective%20determination%20of%20transmission%20quality%201996"
        },
        {
            "id": "15",
            "entry": "[15] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui. Wu. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Jonathan%20Pang%2C%20Ruoming%20Weiss%2C%20Ron%20J.%20Schuster%2C%20Mike%20Natural%20TTS%20synthesis%20by%20conditioning%20WaveNet%20on%20mel%20spectrogram%20predictions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Jonathan%20Pang%2C%20Ruoming%20Weiss%2C%20Ron%20J.%20Schuster%2C%20Mike%20Natural%20TTS%20synthesis%20by%20conditioning%20WaveNet%20on%20mel%20spectrogram%20predictions%202018"
        },
        {
            "id": "16",
            "entry": "[16] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J. Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive speech synthesis with Tacotron. arXiv preprint arXiv:1803.09047, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09047"
        },
        {
            "id": "17",
            "entry": "[17] Jose Sotelo, Soroush Mehri, Kundan Kumar, Jo\u00e3o Felipe Santos, Kyle Kastner, Aaron Courville, and Yoshua Bengio. Char2Wav: End-to-end speech synthesis. In Proc. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sotelo%2C%20Jose%20Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Santos%2C%20Jo%C3%A3o%20Felipe%20Char2Wav%3A%20End-to-end%20speech%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sotelo%2C%20Jose%20Mehri%2C%20Soroush%20Kumar%2C%20Kundan%20Santos%2C%20Jo%C3%A3o%20Felipe%20Char2Wav%3A%20End-to-end%20speech%20synthesis%202017"
        },
        {
            "id": "18",
            "entry": "[18] Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nachmani. VoiceLoop: Voice fitting and synthesis via a phonological loop. In Proc. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taigman%2C%20Yaniv%20Wolf%2C%20Lior%20Polyak%2C%20Adam%20Nachmani%2C%20Eliya%20VoiceLoop%3A%20Voice%20fitting%20and%20synthesis%20via%20a%20phonological%20loop%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taigman%2C%20Yaniv%20Wolf%2C%20Lior%20Polyak%2C%20Adam%20Nachmani%2C%20Eliya%20VoiceLoop%3A%20Voice%20fitting%20and%20synthesis%20via%20a%20phonological%20loop%202018"
        },
        {
            "id": "19",
            "entry": "[19] A\u00e4ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. CoRR abs/1609.03499, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "20",
            "entry": "[20] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier GonzalezDominguez. Deep neural networks for small footprint text-dependent speaker verification. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 4052\u20134056. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Variani%2C%20Ehsan%20Lei%2C%20Xin%20McDermott%2C%20Erik%20Moreno%2C%20Ignacio%20Lopez%20Deep%20neural%20networks%20for%20small%20footprint%20text-dependent%20speaker%20verification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Variani%2C%20Ehsan%20Lei%2C%20Xin%20McDermott%2C%20Erik%20Moreno%2C%20Ignacio%20Lopez%20Deep%20neural%20networks%20for%20small%20footprint%20text-dependent%20speaker%20verification%202014"
        },
        {
            "id": "21",
            "entry": "[21] Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veaux%2C%20Christophe%20Yamagishi%2C%20Junichi%20MacDonald%2C%20Kirsten%20CSTR%20VCTK%20Corpus%3A%20English%20multi-speaker%20corpus%20for%20CSTR%20voice%20cloning%20toolkit%202017"
        },
        {
            "id": "22",
            "entry": "[22] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. Generalized end-to-end loss for speaker verification. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wan%2C%20Li%20Wang%2C%20Quan%20Papir%2C%20Alan%20Moreno%2C%20Ignacio%20Lopez%20Generalized%20end-to-end%20loss%20for%20speaker%20verification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wan%2C%20Li%20Wang%2C%20Quan%20Papir%2C%20Alan%20Moreno%2C%20Ignacio%20Lopez%20Generalized%20end-to-end%20loss%20for%20speaker%20verification%202018"
        },
        {
            "id": "23",
            "entry": "[23] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, and Rif A. Saurous. Tacotron: Towards end-to-end speech synthesis. In Proc. Interspeech, pages 4006\u20134010, August 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuxuan%20Wang%2C%20R.J.Skerry-Ryan%20Stanton%2C%20Daisy%20Wu%2C%20Yonghui%20Weiss%2C%20Ron%20J.%20Tacotron%3A%20Towards%20end-to-end%20speech%20synthesis%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuxuan%20Wang%2C%20R.J.Skerry-Ryan%20Stanton%2C%20Daisy%20Wu%2C%20Yonghui%20Weiss%2C%20Ron%20J.%20Tacotron%3A%20Towards%20end-to-end%20speech%20synthesis%202017-08"
        },
        {
            "id": "24",
            "entry": "[24] Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Fei Ren, Ye Jia, and Rif A Saurous. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. arXiv preprint arXiv:1803.09017, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1803.09017"
        }
    ]
}
