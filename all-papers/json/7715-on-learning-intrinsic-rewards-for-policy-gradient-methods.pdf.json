{
    "filename": "7715-on-learning-intrinsic-rewards-for-policy-gradient-methods.pdf",
    "metadata": {
        "title": "On Learning Intrinsic Rewards for Policy Gradient Methods",
        "author": "Zeyu Zheng, Junhyuk Oh, Satinder Singh",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7715-on-learning-intrinsic-rewards-for-policy-gradient-methods.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et al. [2010] that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. We also compare our method with using a constant \u201clive bonus\u201d and with using a count-based exploration bonus (i.e., pixel-SimHash). Our results show improved performance on most but not all of the domains."
    },
    "keywords": [
        {
            "term": "Reinforcement Learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_Learning"
        },
        {
            "term": "multi-layer perceptron",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptron"
        },
        {
            "term": "extrinsic reward",
            "url": "https://en.wikipedia.org/wiki/extrinsic_reward"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "highlights": [
        "In our setting we take on the challenge of showing that learning and using intrinsic rewards can help the Reinforcement Learning agent perform better while it is learning on a single task",
        "We evaluate our method on several Atari games with a state of the art A2C (Advantage Actor-Critic) [<a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a>] agent as well as on a few Mujoco domains with a state of the art policy optimization agent and show that learning intrinsic rewards can outperform using just extrinsic reward as well as using a combination of extrinsic reward and a constant \u201clive bonus\u201d [<a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016a</a>]",
        "The optimal intrinsic reward for a specific combination of Reinforcement Learning agent and environment is defined as the reward that when used by the agent for its learning in its environment maximizes the extrinsic reward",
        "In contrast to state-independent meta-parameters in [<a class=\"ref-link\" id=\"cXu_et+al_2018_a\" href=\"#rXu_et+al_2018_a\">Xu et al, 2018</a>], we propose a richer form of state-dependent meta-learner that directly changes the reward function of the agent, which can be potentially extended to hierarchical Reinforcement Learning.\n3 Gradient-Based Learning of Intrinsic Rewards: A Derivation",
        "We explored the following values for the intrinsic reward weighting coefficient \u03bb, {0.003, 0.005, 0.01, 0.02, 0.03, 0.05} and the following values for the term \u03be, {0.001, 0.01, 0.1, 1}, that weights the loss from the value function estimates with the loss from the intrinsic reward function\n1Our implementation is available at: https://github.com/Hwhitetooth/lirpg",
        "Our empirical results show promise in using intrinsic reward function learning as a kind of meta-learning to improve the performance of modern policy gradient architectures like A2C and policy optimization"
    ],
    "key_statements": [
        "In our setting we take on the challenge of showing that learning and using intrinsic rewards can help the Reinforcement Learning agent perform better while it is learning on a single task",
        "We evaluate our method on several Atari games with a state of the art A2C (Advantage Actor-Critic) [<a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a>] agent as well as on a few Mujoco domains with a state of the art policy optimization agent and show that learning intrinsic rewards can outperform using just extrinsic reward as well as using a combination of extrinsic reward and a constant \u201clive bonus\u201d [<a class=\"ref-link\" id=\"cDuan_et+al_2016_a\" href=\"#rDuan_et+al_2016_a\">Duan et al, 2016a</a>]",
        "The optimal intrinsic reward for a specific combination of Reinforcement Learning agent and environment is defined as the reward that when used by the agent for its learning in its environment maximizes the extrinsic reward",
        "In contrast to state-independent meta-parameters in [<a class=\"ref-link\" id=\"cXu_et+al_2018_a\" href=\"#rXu_et+al_2018_a\">Xu et al, 2018</a>], we propose a richer form of state-dependent meta-learner that directly changes the reward function of the agent, which can be potentially extended to hierarchical Reinforcement Learning.\n3 Gradient-Based Learning of Intrinsic Rewards: A Derivation",
        "Measure of performance we care about improving is the value of the extrinsic rewards achieved by the agent; the intrinsic rewards serve only to influence the change in policy parameters",
        "The LIRPG algorithm has two additional parameters relative to the baseline algorithm, the parameter \u03bb that controls how the intrinsic reward is scaled before adding it to the extrinsic reward and the step-size \u03b2; we describe how we choose these parameters below in our results",
        "We explored the following values for the intrinsic reward weighting coefficient \u03bb, {0.003, 0.005, 0.01, 0.02, 0.03, 0.05} and the following values for the term \u03be, {0.001, 0.01, 0.1, 1}, that weights the loss from the value function estimates with the loss from the intrinsic reward function\n1Our implementation is available at: https://github.com/Hwhitetooth/lirpg",
        "Our empirical results show promise in using intrinsic reward function learning as a kind of meta-learning to improve the performance of modern policy gradient architectures like A2C and policy optimization"
    ],
    "summary": [
        "In our setting we take on the challenge of showing that learning and using intrinsic rewards can help the RL agent perform better while it is learning on a single task.",
        "Measure of performance we care about improving is the value of the extrinsic rewards achieved by the agent; the intrinsic rewards serve only to influence the change in policy parameters.",
        "Figure 1 shows an abstract representation of our intrinsic reward augmented policy gradient based learning agent.",
        "Algorithm 1 LIRPG: Learning Intrinsic Reward for Policy Gradient",
        "Note that to compute the gradient of the extrinsic value Jex with respect to the intrinsic reward parameters \u03b7, we needed a new episode with the updated policy parameters \u03b8, requiring two episodes per iteration.",
        "Our overall objective in the following first set of experiments is to evaluate whether augmenting a policy gradient based RL agent with intrinsic rewards learned using our LIRPG algorithm improves performance relative to the baseline policy gradient based RL agent that uses just the extrinsic reward.",
        "We first perform this evaluation on multiple Atari games from the Arcade Learning Environment (ALE) platform [<a class=\"ref-link\" id=\"cBellemare_et+al_2013_a\" href=\"#rBellemare_et+al_2013_a\">Bellemare et al, 2013</a>] using the same open-source implementation with exactly the same hyper-parameters of the A2C algorithm [<a class=\"ref-link\" id=\"cMnih_et+al_2016_a\" href=\"#rMnih_et+al_2016_a\">Mnih et al, 2016</a>] from OpenAI [<a class=\"ref-link\" id=\"cDhariwal_et+al_2017_a\" href=\"#rDhariwal_et+al_2017_a\">Dhariwal et al, 2017</a>] for both our augmented agent as well as the baseline agent.",
        "We keep the default values of all hyper-parameters in the original OpenAI implementation of the A2C-based policy module unchanged for both the augmented and baseline agents.",
        "As for the Atari game results above, we kept all hyper-parameters unchanged to default values for the policy module of both baseline and augmented agents.",
        "The blue curves are PPO-live-bonus baseline agents, where we explored the value of live bonus over the set {0.001, 0.01, 0.1, 1} and plotted the curves for the domain-specific best performing choice.",
        "Our result shows that the agent can learn complex behaviors solely from the learned intrinsic reward on MuJoCo environment, and the intrinsic reward captures far more than a live bonus does; this is because the intrinsic reward module takes into account the extrinsic reward structure through its training.",
        "We derived a novel practical algorithm, LIRPG, for learning intrinsic reward functions in problems with high-dimensional observations for use with policy gradient based RL agents.",
        "Our empirical results show promise in using intrinsic reward function learning as a kind of meta-learning to improve the performance of modern policy gradient architectures like A2C and PPO."
    ],
    "headline": "Our results show improved performance on most but not all of the domains",
    "reference_links": [
        {
            "id": "Singh_et+al_2010_a",
            "entry": "Satinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Satinder%20Lewis%2C%20Richard%20L.%20Barto%2C%20Andrew%20G.%20Sorg%2C%20Jonathan%20Intrinsically%20motivated%20reinforcement%20learning%3A%20An%20evolutionary%20perspective%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Satinder%20Lewis%2C%20Richard%20L.%20Barto%2C%20Andrew%20G.%20Sorg%2C%20Jonathan%20Intrinsically%20motivated%20reinforcement%20learning%3A%20An%20evolutionary%20perspective%202010"
        },
        {
            "id": "Ng_et+al_1999_a",
            "entry": "Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 278\u2013287. Morgan Kaufmann Publishers Inc., 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Y.%20Harada%2C%20Daishi%20Russell%2C%20Stuart%20J.%20Policy%20invariance%20under%20reward%20transformations%3A%20Theory%20and%20application%20to%20reward%20shaping%201999"
        },
        {
            "id": "Rajeswaran_et+al_2017_a",
            "entry": "Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pages 6553\u20136564, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajeswaran%2C%20Aravind%20Lowrey%2C%20Kendall%20Todorov%2C%20Emanuel%20V.%20Kakade%2C%20Sham%20M.%20Towards%20generalization%20and%20simplicity%20in%20continuous%20control%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajeswaran%2C%20Aravind%20Lowrey%2C%20Kendall%20Todorov%2C%20Emanuel%20V.%20Kakade%2C%20Sham%20M.%20Towards%20generalization%20and%20simplicity%20in%20continuous%20control%202017"
        },
        {
            "id": "Bellemare_et+al_2016_a",
            "entry": "Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471\u20131479, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20Srinivasan%2C%20Sriram%20Ostrovski%2C%20Georg%20Schaul%2C%20Tom%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016"
        },
        {
            "id": "Georg_2017_a",
            "entry": "Georg Ostrovski, Marc G Bellemare, A\u00e4ron Oord, and R\u00e9mi Munos. Count-based exploration with neural density models. In International Conference on Machine Learning, pages 2721\u20132730, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Georg%20Ostrovski%2C%20Marc%20G%20Bellemare%2C%20A%C3%A4ron%20Oord%20Munos%2C%20R%C3%A9mi%20Count-based%20exploration%20with%20neural%20density%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Georg%20Ostrovski%2C%20Marc%20G%20Bellemare%2C%20A%C3%A4ron%20Oord%20Munos%2C%20R%C3%A9mi%20Count-based%20exploration%20with%20neural%20density%20models%202017"
        },
        {
            "id": "Tang_et+al_2017_a",
            "entry": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 2750\u20132759, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Haoran%20Houthooft%2C%20Rein%20Foote%2C%20Davis%20Stooke%2C%20Adam%20%23%20exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "Amodei_et+al_2016_a",
            "entry": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "Mnih_et+al_2016_a",
            "entry": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "Duan_et+al_2016_a",
            "entry": "Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u20131338, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Chen%2C%20Xi%20Houthooft%2C%20Rein%20Schulman%2C%20John%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "Sorg_et+al_2010_a",
            "entry": "Jonathan Sorg, Richard L Lewis, and Satinder Singh. Reward design via online gradient ascent. In Advances in Neural Information Processing Systems, pages 2190\u20132198, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sorg%2C%20Jonathan%20Lewis%2C%20Richard%20L.%20Singh%2C%20Satinder%20Reward%20design%20via%20online%20gradient%20ascent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sorg%2C%20Jonathan%20Lewis%2C%20Richard%20L.%20Singh%2C%20Satinder%20Reward%20design%20via%20online%20gradient%20ascent%202010"
        },
        {
            "id": "Guo_et+al_2016_a",
            "entry": "Xiaoxiao Guo, Satinder Singh, Richard Lewis, and Honglak Lee. Deep learning for reward design to improve monte carlo tree search in atari games. arXiv preprint arXiv:1604.07095, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.07095"
        },
        {
            "id": "Jaderberg_et+al_2016_a",
            "entry": "Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05397"
        },
        {
            "id": "Pathak_et+al_0000_a",
            "entry": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning (ICML), volume 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20Deepak%20Agrawal%2C%20Pulkit%20Efros%2C%20Alexei%20A.%20Darrell%2C%20Trevor%20Curiosity-driven%20exploration%20by%20self-supervised%20prediction"
        },
        {
            "id": "Schmidhuber_2010_a",
            "entry": "J\u00fcrgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010). IEEE Transactions on Autonomous Mental Development, 2(3):230\u2013247, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%C3%BCrgen%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%2023230247%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%C3%BCrgen%20Schmidhuber%20Formal%20theory%20of%20creativity%20fun%20and%20intrinsic%20motivation%2019902010%20IEEE%20Transactions%20on%20Autonomous%20Mental%20Development%2023230247%202010"
        },
        {
            "id": "Stadie_et+al_2015_a",
            "entry": "Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00814"
        },
        {
            "id": "Oudeyer_2009_a",
            "entry": "Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in Neurorobotics, 1:6, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20a%20typology%20of%20computational%20approaches%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oudeyer%2C%20Pierre-Yves%20Kaplan%2C%20Frederic%20What%20is%20intrinsic%20motivation%3F%20a%20typology%20of%20computational%20approaches%202009"
        },
        {
            "id": "Vezhnevets_et+al_2017_a",
            "entry": "Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In International Conference on Machine Learning, pages 3540\u20133549, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vezhnevets%2C%20Alexander%20Sasha%20Osindero%2C%20Simon%20Schaul%2C%20Tom%20Heess%2C%20Nicolas%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vezhnevets%2C%20Alexander%20Sasha%20Osindero%2C%20Simon%20Schaul%2C%20Tom%20Heess%2C%20Nicolas%20Feudal%20networks%20for%20hierarchical%20reinforcement%20learning%202017"
        },
        {
            "id": "Andrychowicz_et+al_2016_a",
            "entry": "Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pages 3981\u20133989, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20Gomez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "Santoro_et+al_2016_a",
            "entry": "Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In International conference on machine learning, pages 1842\u20131850, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santoro%2C%20Adam%20Bartunov%2C%20Sergey%20Botvinick%2C%20Matthew%20Wierstra%2C%20Daan%20Metalearning%20with%20memory-augmented%20neural%20networks%202016"
        },
        {
            "id": "Nichol_2018_a",
            "entry": "Alex Nichol and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02999"
        },
        {
            "id": "Finn_et+al_2017_a",
            "entry": "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017"
        },
        {
            "id": "Duan_et+al_2017_a",
            "entry": "Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in neural information processing systems, pages 1087\u20131098, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20OpenAI%20Jonathan%20One-shot%20imitation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Yan%20Andrychowicz%2C%20Marcin%20Stadie%2C%20Bradly%20Ho%2C%20OpenAI%20Jonathan%20One-shot%20imitation%20learning%202017"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05763"
        },
        {
            "id": "Duan_et+al_0000_a",
            "entry": "Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl\u03982: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016b.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02779"
        },
        {
            "id": "Xu_et+al_2018_a",
            "entry": "Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. arXiv preprint arXiv:1805.09801, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.09801"
        },
        {
            "id": "Sutton_et+al_2000_a",
            "entry": "Richard S Sutton, David A McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "Bellemare_et+al_2013_a",
            "entry": "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253\u2013279, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20Marc%20G.%20Naddaf%2C%20Yavar%20Veness%2C%20Joel%20Bowling%2C%20Michael%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013"
        },
        {
            "id": "Dhariwal_et+al_2017_a",
            "entry": "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/openai/baselines, 2017.",
            "url": "https://github.com/openai/baselines"
        },
        {
            "id": "Schulman_et+al_2017_a",
            "entry": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        }
    ]
}
