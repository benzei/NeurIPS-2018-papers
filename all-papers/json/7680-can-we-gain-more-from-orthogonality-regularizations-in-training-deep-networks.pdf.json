{
    "filename": "7680-can-we-gain-more-from-orthogonality-regularizations-in-training-deep-networks.pdf",
    "metadata": {
        "title": "Can We Gain More from Orthogonality Regularizations in Training Deep Networks?",
        "author": "Nitin Bansal, Xiaohan Chen, Zhangyang Wang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7680-can-we-gain-more-from-orthogonality-regularizations-in-training-deep-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available.1."
    },
    "keywords": [
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "spectral norm",
            "url": "https://en.wikipedia.org/wiki/spectral_norm"
        },
        {
            "term": "singular value decomposition",
            "url": "https://en.wikipedia.org/wiki/singular_value_decomposition"
        },
        {
            "term": "deep convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/deep_convolutional_neural_network"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "filter bank",
            "url": "https://en.wikipedia.org/wiki/filter_bank"
        }
    ],
    "highlights": [
        "Despite the tremendous success of deep convolutional neural networks (CNNs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], their training remains to be notoriously difficult both theoretically and practically, especially for state-of-the-art ultra-deep convolutional neural networks",
        "This paper focuses on one type of structural regularizations: orthogonality, to be imposed on linear transformations between hidden layers of convolutional neural networks",
        "We carefully inspect the training curves of different methods on CIFAR-10 and CIFAR-100, with ResNet-110 curves shown in Fig. 1 for example",
        "We showed that in all cases, we can achieve better accuracy, more stable training curve and smoother convergence",
        "The novel Spectral Restricted Isometry Property regularizer outperforms all else consistently and remarkably. Those regularizations demonstrate outstanding generality and easiness to use, suggesting that orthogonality regularizations should be considered as standard tools for training deeper convolutional neural networks",
        "We are interested to extend the evaluation of Spectral Restricted Isometry Property to training Recurrent Neural Networks and GANs"
    ],
    "key_statements": [
        "Despite the tremendous success of deep convolutional neural networks (CNNs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], their training remains to be notoriously difficult both theoretically and practically, especially for state-of-the-art ultra-deep convolutional neural networks",
        "Potential reasons accounting for such difficulty lie in multiple folds, ranging from vanishing/exploding gradients [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], to feature statistic shifts [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], to the proliferation of saddle points [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], and so on",
        "This paper focuses on one type of structural regularizations: orthogonality, to be imposed on linear transformations between hidden layers of convolutional neural networks",
        "The orthogonality implies energy preservation, which is extensively explored for filter banks in signal processing and guarantees that energy of activations will not be amplified [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "This paper investigates and pushes forward various ways to enforce orthogonality regularizations on training deep convolutional neural networks",
        "We extensively evaluate the proposed orthogonality regularizations on three state-of-the-art convolutional neural networks: ResNet [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], ResNeXt [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], and WideResNet [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "One may notice that enforcing matrix to beorthogonal during training will lead to its spectral norm being always equal to one, which links between regularizing orthogonality and spectrum",
        "Previous works [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] proposed to require the Gram matrix of the weight matrix to be close to identity, which we term as Soft Orthogonality (SO) regularization: (SO) \u03bb||W T W \u2212 I||2F , (1)",
        "We do not explicitly normalize the column norm of W to be one, we find experimentally that minimizing (4) often tends to implicitly encourage close-to-unit-column-norm W too, making the objective of (4) a viable approximation of mutual coherence (3)2",
        "For Mutual Coherence/Spectral Restricted Isometry Property regularizers, we find them insensitive to the choice of \u03bb2, potentially due to their stronger effects in enforcing W T W close to I; we stick to the initial \u03bb2 throughout training for them",
        "We carefully inspect the training curves of different methods on CIFAR-10 and CIFAR-100, with ResNet-110 curves shown in Fig. 1 for example",
        "The paper used a variant of Wide ResNet [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] with 22 layers of width 5, whose original top-1 error rate was 6.66% on on CIFAR-10, and and reported a reduced error rate of 5.68% with their proposed regularizer",
        "We presented an efficient mechanism for regularizing different flavors of orthogonality, on several state-of-art convolutional deep convolutional neural networks [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "We showed that in all cases, we can achieve better accuracy, more stable training curve and smoother convergence",
        "The novel Spectral Restricted Isometry Property regularizer outperforms all else consistently and remarkably. Those regularizations demonstrate outstanding generality and easiness to use, suggesting that orthogonality regularizations should be considered as standard tools for training deeper convolutional neural networks",
        "We are interested to extend the evaluation of Spectral Restricted Isometry Property to training Recurrent Neural Networks and GANs"
    ],
    "summary": [
        "Despite the tremendous success of deep convolutional neural networks (CNNs) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], their training remains to be notoriously difficult both theoretically and practically, especially for state-of-the-art ultra-deep CNNs.",
        "A few works [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] look at enforcing orthogonality regularizations or constraints throughout training, as part of their specialized models for applications such as classification [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] or person re-identification [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "A recent work [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] explored orthogonal regularization, by enforcing the Gram matrix of each weight matrix to be close to identity under Frobenius norm.",
        "One may notice that enforcing matrix to beorthogonal during training will lead to its spectral norm being always equal to one, which links between regularizing orthogonality and spectrum.",
        "Previous works [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] proposed to require the Gram matrix of the weight matrix to be close to identity, which we term as Soft Orthogonality (SO) regularization: (SO) \u03bb||W T W \u2212 I||2F , (1)",
        "We will train each of the three models with each of the proposed regularizers, and compare their performance with the original versions, in terms of both final accuracy and convergence.",
        "We observe that fully replacing 2 weight decay with orthogonal regularizers will accelerate and stabilize training at the beginning of training, but will negatively affect the final accuracies achievable.",
        "Comparison with Optimization over Multiple Dependent Stiefel Manifolds OMDSM We compare SRIP with OMDSM developed in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], which makes a fair comparison with ours, on soft regularization forms versus hard constraint forms of enforcing orthogonality.",
        "This work trained Wide ResNet 28-10 on CIFAR-10 and CIFAR-100 and got error rates 3.73% and 18.76% respectively, both being inferior to SRIP (3.60% for CIFAR-10 and 18.19% for CIFAR-100).",
        "The paper used a variant of Wide ResNet [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] with 22 layers of width 5, whose original top-1 error rate was 6.66% on on CIFAR-10, and and reported a reduced error rate of 5.68% with their proposed regularizer.",
        "We trained this same model using SRIP over the same augmented full training set, achieving 4.28% top-1 error, that shows a large gain over the Jacobian norm-based regularizer.",
        "Experiments on SVHN On the SVHN dataset, we train the original Wide ResNet 16-8 model, following its original implementation in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] with initial learning 0.01 which decays at epoch 60,120 and 160 all by a factor of 5.",
        "We tried to train Wide ResNet 28-10 with SRIP from three different random initializations, and find that the final accuracies very stable, with best accuracy 3.60%.",
        "A befitting quote would be: Enforce orthogonality in training your CNN and by no means will you regret!"
    ],
    "headline": "This paper seeks to answer the question: as the  orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep convolutional neural networks, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "2",
            "entry": "[2] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "3",
            "entry": "[3] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "4",
            "entry": "[4] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. In Advances in neural information processing systems, pages 2933\u20132941, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Yann%20N.%20Pascanu%2C%20Razvan%20Gulcehre%2C%20Caglar%20Cho%2C%20Kyunghyun%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20nonconvex%20optimization%202014"
        },
        {
            "id": "5",
            "entry": "[5] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6120"
        },
        {
            "id": "6",
            "entry": "[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "7",
            "entry": "[7] Jianping Zhou, Minh N Do, and Jelena Kovacevic. Special paraunitary matrices, cayley transform, and multidimensional orthogonal filter banks. IEEE Transactions on Image Processing, 15(2):511\u2013519, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Jianping%20Do%2C%20Minh%20N.%20Kovacevic%2C%20Jelena%20Special%20paraunitary%20matrices%2C%20cayley%20transform%2C%20and%20multidimensional%20orthogonal%20filter%20banks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Jianping%20Do%2C%20Minh%20N.%20Kovacevic%2C%20Jelena%20Special%20paraunitary%20matrices%2C%20cayley%20transform%2C%20and%20multidimensional%20orthogonal%20filter%20banks%202006"
        },
        {
            "id": "8",
            "entry": "[8] Pau Rodr\u00edguez, Jordi Gonzalez, Guillem Cucurull, Josep M Gonfaus, and Xavier Roca. Regularizing cnns with locally constrained decorrelations. arXiv preprint arXiv:1611.01967, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01967"
        },
        {
            "id": "9",
            "entry": "[9] Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et al. Natural neural networks. In Advances in Neural Information Processing Systems, pages 2071\u20132079, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guillaume%20Desjardins%20Karen%20Simonyan%20Razvan%20Pascanu%20et%20al%20Natural%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2020712079%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guillaume%20Desjardins%20Karen%20Simonyan%20Razvan%20Pascanu%20et%20al%20Natural%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2020712079%202015"
        },
        {
            "id": "10",
            "entry": "[10] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06422"
        },
        {
            "id": "11",
            "entry": "[11] Kui Jia, Dacheng Tao, Shenghua Gao, and Xiangmin Xu. Improving training of deep neural networks via singular value bounding. CoRR, abs/1611.06013, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.06013"
        },
        {
            "id": "12",
            "entry": "[12] Mehrtash Harandi and Basura Fernando. Generalized backpropagation,\\\u2019{E} tude de cas: Orthogonality. arXiv preprint arXiv:1611.05927, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.05927"
        },
        {
            "id": "13",
            "entry": "[13] Mete Ozay and Takayuki Okatani. Optimization on submanifolds of convolution kernels in cnns. arXiv preprint arXiv:1610.07008, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.07008"
        },
        {
            "id": "14",
            "entry": "[14] Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. arXiv preprint arXiv:1703.01827, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01827"
        },
        {
            "id": "15",
            "entry": "[15] Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, and Bo Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. arXiv preprint arXiv:1709.06079, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06079"
        },
        {
            "id": "16",
            "entry": "[16] Yifan Sun, Liang Zheng, Weijian Deng, and Shengjin Wang. Svdnet for pedestrian retrieval. arXiv preprint, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Yifan%20Zheng%2C%20Liang%20Deng%2C%20Weijian%20Wang%2C%20Shengjin%20Svdnet%20for%20pedestrian%20retrieval.%20arXiv%20p%202017"
        },
        {
            "id": "17",
            "entry": "[17] Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on information theory, 51(12):4203\u20134215, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Decoding%20by%20linear%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Decoding%20by%20linear%20programming%202005"
        },
        {
            "id": "18",
            "entry": "[18] David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289\u2013 1306, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Compressed%20sensing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Compressed%20sensing%202006"
        },
        {
            "id": "19",
            "entry": "[19] Zhaowen Wang, Jianchao Yang, Haichao Zhang, Zhangyang Wang, Yingzhen Yang, Ding Liu, and Thomas S Huang. Sparse Coding and its Applications in Computer Vision. World Scientific, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhaowen%20Wang%20Jianchao%20Yang%20Haichao%20Zhang%20Zhangyang%20Wang%20Yingzhen%20Yang%20Ding%20Liu%20and%20Thomas%20S%20Huang%20Sparse%20Coding%20and%20its%20Applications%20in%20Computer%20Vision%20World%20Scientific%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhaowen%20Wang%20Jianchao%20Yang%20Haichao%20Zhang%20Zhangyang%20Wang%20Yingzhen%20Yang%20Ding%20Liu%20and%20Thomas%20S%20Huang%20Sparse%20Coding%20and%20its%20Applications%20in%20Computer%20Vision%20World%20Scientific%202016"
        },
        {
            "id": "20",
            "entry": "[20] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 5987\u20135995. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "22",
            "entry": "[22] Tong Zhang. Sparse recovery with orthogonal matching pursuit under rip. IEEE Transactions on Information Theory, 57(9):6215\u20136221, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Sparse%20recovery%20with%20orthogonal%20matching%20pursuit%20under%20rip%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Sparse%20recovery%20with%20orthogonal%20matching%20pursuit%20under%20rip%202011"
        },
        {
            "id": "23",
            "entry": "[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "24",
            "entry": "[24] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901\u2013909, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "25",
            "entry": "[25] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, pages 1310\u20131318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20Razvan%20Mikolov%2C%20Tomas%20Bengio%2C%20Yoshua%20On%20the%20difficulty%20of%20training%20recurrent%20neural%20networks%202013"
        },
        {
            "id": "26",
            "entry": "[26] Victor Dorobantu, Per Andre Stromhaug, and Jess Renteria. Dizzyrnn: Reparameterizing recurrent neural networks for norm-preserving backpropagation. arXiv preprint arXiv:1612.04035, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04035"
        },
        {
            "id": "27",
            "entry": "[27] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pages 1120\u20131128, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "28",
            "entry": "[28] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. arXiv preprint arXiv:1612.00188, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00188"
        },
        {
            "id": "29",
            "entry": "[29] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. arXiv preprint arXiv:1702.00071, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.00071"
        },
        {
            "id": "30",
            "entry": "[30] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. In Advances in Neural Information Processing Systems, pages 4880\u20134888, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wisdom%2C%20Scott%20Powers%2C%20Thomas%20Hershey%2C%20John%20Jonathan%20Le%20Roux%2C%20and%20Les%20Atlas.%20Full-capacity%20unitary%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "31",
            "entry": "[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "32",
            "entry": "[32] Randall Balestriero and Richard Baraniuk. A spline theory of deep networks. Proceedings of the 35th International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balestriero%2C%20Randall%20Baraniuk%2C%20Richard%20A%20spline%20theory%20of%20deep%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balestriero%2C%20Randall%20Baraniuk%2C%20Richard%20A%20spline%20theory%20of%20deep%20networks%202018"
        },
        {
            "id": "33",
            "entry": "[33] Randall Balestriero and Richard Baraniuk. Mad max: Affine spline insights into deep learning. arXiv preprint arXiv:1805.06576, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.06576"
        },
        {
            "id": "34",
            "entry": "[34] Shengjie Wang, Abdel-rahman Mohamed, Rich Caruana, Jeff Bilmes, Matthai Plilipose, Matthew Richardson, Krzysztof Geras, Gregor Urban, and Ozlem Aslan. Analysis of deep neural networks with extended data jacobian matrix. In International Conference on Machine Learning, pages 718\u2013726, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Shengjie%20Mohamed%2C%20Abdel-rahman%20Caruana%2C%20Rich%20Bilmes%2C%20Jeff%20Analysis%20of%20deep%20neural%20networks%20with%20extended%20data%20jacobian%20matrix%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Shengjie%20Mohamed%2C%20Abdel-rahman%20Caruana%2C%20Rich%20Bilmes%2C%20Jeff%20Analysis%20of%20deep%20neural%20networks%20with%20extended%20data%20jacobian%20matrix%202016"
        },
        {
            "id": "35",
            "entry": "[35] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04836"
        },
        {
            "id": "36",
            "entry": "[36] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10941"
        },
        {
            "id": "37",
            "entry": "[37] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "38",
            "entry": "[38] Zhangyang Wang, Hongyu Xu, Haichuan Yang, Ding Liu, and Ji Liu. Learning simple thresholded features with sparse support recovery. arXiv preprint arXiv:1804.05515, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.05515"
        },
        {
            "id": "39",
            "entry": "[39] Zhouchen Lin, Canyi Lu, and Huan Li. Optimized projections for compressed sensing via direct mutual coherence minimization. arXiv preprint arXiv:1508.03117, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.03117"
        },
        {
            "id": "40",
            "entry": "[40] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630\u2013645.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks"
        },
        {
            "id": "41",
            "entry": "[41] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep neural networks. IEEE Transactions on Signal Processing, 65(16):4265\u20134280, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sokolic%2C%20Jure%20Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Rodrigues%2C%20Miguel%20R.D.%20Robust%20large%20margin%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sokolic%2C%20Jure%20Giryes%2C%20Raja%20Sapiro%2C%20Guillermo%20Rodrigues%2C%20Miguel%20R.D.%20Robust%20large%20margin%20deep%20neural%20networks%202017"
        }
    ]
}
