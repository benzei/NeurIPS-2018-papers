{
    "filename": "7570-faithful-inversion-of-generative-models-for-effective-amortized-inference.pdf",
    "metadata": {
        "title": "Faithful Inversion of Generative Models for Effective Amortized Inference",
        "author": "Stefan Webb\u2217 University of Oxford",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7570-faithful-inversion-of-generative-models-for-effective-amortized-inference.pdf"
        },
        "abstract": "Inference amortization methods share information across multiple posteriorinference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties: a) they do not encode any independence assertions that are absent from the model and b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches."
    },
    "keywords": [
        {
            "term": "Bayesian networks",
            "url": "https://en.wikipedia.org/wiki/Bayesian_networks"
        },
        {
            "term": "inference network",
            "url": "https://en.wikipedia.org/wiki/inference_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        }
    ],
    "highlights": [
        "Evidence from human cognition suggests that the brain reuses the results of past inferences to speed up subsequent related queries (<a class=\"ref-link\" id=\"cGershman_2014_a\" href=\"#rGershman_2014_a\"><a class=\"ref-link\" id=\"cGershman_2014_a\" href=\"#rGershman_2014_a\">Gershman & Goodman, 2014</a></a>)",
        "We note that the mean-field inference network has the same structure as the heuristic one that reverses the edges from the generative model",
        "We have presented Natural Minimal I-map generator, a tractable framework that, given the Bayesian networks structure for a generative model, produces a natural factorization for its inverse that is a minimal I-map for the model",
        "We have argued that this should be used to guide the design of the coarse-grain structure of the inference network in amortized inference",
        "Having empirically analyzed the implications of using Natural Minimal I-map generator, we find that it learns better inference networks than previous heuristic approaches",
        "In the context of variational auto-encoders, improved inference networks have a knock-on effect on the generative network, improving the generative networks as well"
    ],
    "key_statements": [
        "Evidence from human cognition suggests that the brain reuses the results of past inferences to speed up subsequent related queries (<a class=\"ref-link\" id=\"cGershman_2014_a\" href=\"#rGershman_2014_a\"><a class=\"ref-link\" id=\"cGershman_2014_a\" href=\"#rGershman_2014_a\">Gershman & Goodman, 2014</a></a>)",
        "We note that the mean-field inference network has the same structure as the heuristic one that reverses the edges from the generative model",
        "We observe that it is necessary to model at least the edges in an I-map for the inference network to be able to recover the posterior, and convergence is faster with fewer edges in the inference network",
        "Despite the more compact reverse-Natural Minimal I-map generator inverse converging faster than the forward-Natural Minimal I-map generator one, the latter seems to converges to a better final solution. This may be because the MADE approach could not be used for the reverse-Natural Minimal I-map generator inverse, but this is a subject for future investigation",
        "Figure 6c shows the average negative log-likelihood of 200 samples from the inference networks evaluated on the analytical posterior, conditioning on five fixed datasets sampled from the generative model not seen during learning",
        "We have presented Natural Minimal I-map generator, a tractable framework that, given the Bayesian networks structure for a generative model, produces a natural factorization for its inverse that is a minimal I-map for the model",
        "We have argued that this should be used to guide the design of the coarse-grain structure of the inference network in amortized inference",
        "Having empirically analyzed the implications of using Natural Minimal I-map generator, we find that it learns better inference networks than previous heuristic approaches",
        "In the context of variational auto-encoders, improved inference networks have a knock-on effect on the generative network, improving the generative networks as well"
    ],
    "summary": [
        "Evidence from human cognition suggests that the brain reuses the results of past inferences to speed up subsequent related queries (<a class=\"ref-link\" id=\"cGershman_2014_a\" href=\"#rGershman_2014_a\"><a class=\"ref-link\" id=\"cGershman_2014_a\" href=\"#rGershman_2014_a\">Gershman & Goodman, 2014</a></a>).",
        "The key idea in the operation of NaMI is to simulate variable elimination steps as a tool for successively determining a minimal, faithful, and natural inverse structure, which can be used to parametrize an inference network.",
        "The \u201ctopological mode,\u201d which we previously implicitly considered, simulates variable elimination in a topological ordering, producing an inverse that reverses the order of the random choices from the generative model.",
        "NaMI\u2019s graph inversion can be run in \u201creverse topological mode,\u201d which simulates variable elimination in a reverse topological ordering, producing an inverse that preserves the order of random choices in the generative model.",
        "Once we have obtained the faithful inverse structure H, the step is to use it to learn an inference network, q\u03c8(z | x).",
        "Using a minimally faithful inverse structure typically improves the best inference network attainable and the finite time training performance for both these settings, compared with previous naive approaches.",
        "When we invert this type of collapsed graph, we must do so with the understanding that the distribution over a vector-valued node in the inverse must express dependencies between all its elements in order for the inference network to be faithful.",
        "We note that the mean-field inference network has the same structure as the heuristic one that reverses the edges from the generative model.",
        "The results shown in Figure 4 indicate that using a faithful inverse on this model produces a significant improvement in learning over the mean-field inverse.",
        "Figure 6c shows the average negative log-likelihood of 200 samples from the inference networks evaluated on the analytical posterior, conditioning on five fixed datasets sampled from the generative model not seen during learning.",
        "The inference network terms with distributions over vectors were parametrized by MADE, and we compare the results for the fully-connected and reverse-NaMI inverses.",
        "We see that learning is faster for the minimally faithful reverse-NaMI method, relative to the fully-connected inverse, and converges to a better solution, in agreement with the other experiments.",
        "To further examine the hypothesis that a nonminimal faithful inverse has slower learning and converges to a worse solution relative to a minimal one, we performed the setup of Experiment 3.2 with depth d = 4, comparing the forward-NaMI network to two additional net-",
        "We have presented NaMI, a tractable framework that, given the BN structure for a generative model, produces a natural factorization for its inverse that is a minimal I-map for the model."
    ],
    "headline": "We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model",
    "reference_links": [
        {
            "id": "Burda_et+al_2016_a",
            "entry": "Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan. Importance weighted autoencoders. International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burda%2C%20Yuri%20Grosse%2C%20Roger%20Salakhutdinov%2C%20Ruslan%20Importance%20weighted%20autoencoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burda%2C%20Yuri%20Grosse%2C%20Roger%20Salakhutdinov%2C%20Ruslan%20Importance%20weighted%20autoencoders%202016"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "Chen, Tian Qi, Rubanova, Yulia, Bettencourt, Jesse, and Duvenaud, David. Neural ordinary differential equations. arXiv preprint arXiv:1806.07366, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.07366"
        },
        {
            "id": "Cremer_et+al_2017_a",
            "entry": "Cremer, Chris, Morris, Quaid, and Duvenaud, David. Reinterpreting importance-weighted autoencoders. International Conference on Learning Representations Workshop Track, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cremer%2C%20Chris%20Morris%2C%20Quaid%20Duvenaud%2C%20David%20Reinterpreting%20importance-weighted%20autoencoders%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cremer%2C%20Chris%20Morris%2C%20Quaid%20Duvenaud%2C%20David%20Reinterpreting%20importance-weighted%20autoencoders%202017"
        },
        {
            "id": "Cremer_et+al_2018_a",
            "entry": "Cremer, Chris, Li, Xuechen, and Duvenaud, David. Inference suboptimality in variational autoencoders. Proceedings of the International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cremer%2C%20Chris%20Li%2C%20Xuechen%20Duvenaud%2C%20David%20Inference%20suboptimality%20in%20variational%20autoencoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cremer%2C%20Chris%20Li%2C%20Xuechen%20Duvenaud%2C%20David%20Inference%20suboptimality%20in%20variational%20autoencoders%202018"
        },
        {
            "id": "Fishelson_2004_a",
            "entry": "Fishelson, Ma\u00e1yan and Geiger, Dan. Optimizing exact genetic linkage computations. Journal of Computational Biology, 11(2-3):263\u2013275, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fishelson%2C%20Ma%C3%A1yan%20Geiger%2C%20Dan%20Optimizing%20exact%20genetic%20linkage%20computations%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fishelson%2C%20Ma%C3%A1yan%20Geiger%2C%20Dan%20Optimizing%20exact%20genetic%20linkage%20computations%202004"
        },
        {
            "id": "Gan_et+al_2015_a",
            "entry": "Gan, Zhe, Li, Chunyuan, Henao, Ricardo, Carlson, David E, and Carin, Lawrence. Deep temporal sigmoid belief networks for sequence modeling. Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Zhe%20Li%2C%20Chunyuan%20Henao%2C%20Ricardo%20Carlson%2C%20David%20E.%20Deep%20temporal%20sigmoid%20belief%20networks%20for%20sequence%20modeling%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20Zhe%20Li%2C%20Chunyuan%20Henao%2C%20Ricardo%20Carlson%2C%20David%20E.%20Deep%20temporal%20sigmoid%20belief%20networks%20for%20sequence%20modeling%202015"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Germain, Mathieu, Gregor, Karol, Murray, Iain, and Larochelle, Hugo. MADE: masked autoencoder for distribution estimation. Proceedings of the International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20MADE%3A%20masked%20autoencoder%20for%20distribution%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Mathieu%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Larochelle%2C%20Hugo%20MADE%3A%20masked%20autoencoder%20for%20distribution%20estimation%202015"
        },
        {
            "id": "Gershman_2014_a",
            "entry": "Gershman, Samuel J and Goodman, Noah D. Amortized inference in probabilistic reasoning. In Proceedings of the Annual Conference of the Cognitive Science Society, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gershman%2C%20Samuel%20J.%20Goodman%2C%20Noah%20D.%20Amortized%20inference%20in%20probabilistic%20reasoning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gershman%2C%20Samuel%20J.%20Goodman%2C%20Noah%20D.%20Amortized%20inference%20in%20probabilistic%20reasoning%202014"
        },
        {
            "id": "Huang_et+al_2018_a",
            "entry": "Huang, Chin-Wei, Krueger, David, Lacoste, Alexandre, and Courville, Aaron. Neural Autoregressive Flows. Proceedings of the International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Chin-Wei%20Krueger%2C%20David%20Lacoste%2C%20Alexandre%20Courville%2C%20Aaron%20Neural%20Autoregressive%20Flows%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Chin-Wei%20Krueger%2C%20David%20Lacoste%2C%20Alexandre%20Courville%2C%20Aaron%20Neural%20Autoregressive%20Flows%202018"
        },
        {
            "id": "Jang_et+al_2017_a",
            "entry": "Jang, Eric, Gu, Shixiang, and Poole, Ben. Categorical reparameterization with Gumbel-softmax. International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jang%2C%20Eric%20Gu%2C%20Shixiang%20Poole%2C%20Ben%20Categorical%20reparameterization%20with%20Gumbel-softmax%202017"
        },
        {
            "id": "Johnson_et+al_2016_a",
            "entry": "Johnson, Matthew J, Duvenaud, David, Wiltschko, Alexander B, Datta, Sandeep R, and Adams, Ryan P. Composing graphical models with neural networks for structured representations and fast inference. arXiv preprint arXiv:1603.06277v2 [stat.ML], 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.06277v2"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "Kingma, Diederik P, Salimans, Tim, and Welling, Max. Improving variational inference with Inverse Autoregressive Flow. Advances in Neural Information Processing Systems, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Improving%20variational%20inference%20with%20Inverse%20Autoregressive%20Flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Welling%2C%20Max%20Improving%20variational%20inference%20with%20Inverse%20Autoregressive%20Flow%202016"
        },
        {
            "id": "Koller_2009_a",
            "entry": "Koller, Daphne and Friedman, Nir. Probabilistic Graphical Models. MIT Press, 2009. ISBN 9780262013192.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koller%2C%20Daphne%20Friedman%2C%20Nir%20Probabilistic%20Graphical%20Models%202009"
        },
        {
            "id": "Krishnan_et+al_2017_a",
            "entry": "Krishnan, Rahul G, Shalit, Uri, and Sontag, David. Structured inference networks for nonlinear state space models. Proceedings of the national conference on Artificial intelligence (AAAI), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnan%2C%20Rahul%20G.%20Shalit%2C%20Uri%20Sontag%2C%20David%20Structured%20inference%20networks%20for%20nonlinear%20state%20space%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishnan%2C%20Rahul%20G.%20Shalit%2C%20Uri%20Sontag%2C%20David%20Structured%20inference%20networks%20for%20nonlinear%20state%20space%20models%202017"
        },
        {
            "id": "Le_et+al_2017_a",
            "entry": "Le, Tuan Anh, Baydin, Atilim Gunes, and Wood, Frank. Inference compilation and universal probabilistic programming. In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Tuan%20Anh%20Baydin%2C%20Atilim%20Gunes%20Wood%2C%20Frank%20Inference%20compilation%20and%20universal%20probabilistic%20programming%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Tuan%20Anh%20Baydin%2C%20Atilim%20Gunes%20Wood%2C%20Frank%20Inference%20compilation%20and%20universal%20probabilistic%20programming%202017"
        },
        {
            "id": "Le_et+al_2018_a",
            "entry": "Le, Tuan Anh, Igl, Maximilian, Jin, Tom, Rainforth, Tom, and Wood, Frank. Auto-encoding Sequential Monte Carlo. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Tuan%20Anh%20Igl%2C%20Maximilian%20Jin%2C%20Tom%20Rainforth%2C%20Tom%20Auto-encoding%20Sequential%20Monte%20Carlo%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Tuan%20Anh%20Igl%2C%20Maximilian%20Jin%2C%20Tom%20Rainforth%2C%20Tom%20Auto-encoding%20Sequential%20Monte%20Carlo%202018"
        },
        {
            "id": "Maal_et+al_2016_a",
            "entry": "Maal\u00f8e, Lars, S\u00f8nderby, Casper Kaae, S\u00f8nderby, S\u00f8ren Kaae, and Winther, Ole. Auxiliary deep generative models. In Proceedings of the International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maal%C3%B8e%2C%20Lars%20S%C3%B8nderby%2C%20Casper%20Kaae%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Winther%2C%20Ole%20Auxiliary%20deep%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maal%C3%B8e%2C%20Lars%20S%C3%B8nderby%2C%20Casper%20Kaae%20S%C3%B8nderby%2C%20S%C3%B8ren%20Kaae%20Winther%2C%20Ole%20Auxiliary%20deep%20generative%20models%202016"
        },
        {
            "id": "Maddison_et+al_2017_a",
            "entry": "Maddison, Chris J, Lawson, John, Tucker, George, Heess, Nicolas, Norouzi, Mohammad, Mnih, Andriy, Doucet, Arnaud, and Teh, Yee. Filtering variational objectives. In Advances in Neural Information Processing Systems, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maddison%2C%20Chris%20J.%20Lawson%2C%20John%20Tucker%2C%20George%20Heess%2C%20Nicolas%20Filtering%20variational%20objectives%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maddison%2C%20Chris%20J.%20Lawson%2C%20John%20Tucker%2C%20George%20Heess%2C%20Nicolas%20Filtering%20variational%20objectives%202017"
        },
        {
            "id": "Maddison_et+al_2017_b",
            "entry": "Maddison, Chris J, Mnih, Andriy, and Teh, Yee Whye. The Concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20Teh%2C%20Yee%20Whye%20The%20Concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maddison%2C%20Chris%20J.%20Mnih%2C%20Andriy%20Teh%2C%20Yee%20Whye%20The%20Concrete%20distribution%3A%20A%20continuous%20relaxation%20of%20discrete%20random%20variables%202017"
        },
        {
            "id": "Naesseth_et+al_2018_a",
            "entry": "Naesseth, Christian A, Linderman, Scott W, Ranganath, Rajesh, and Blei, David M. Variational Sequential Monte Carlo. In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naesseth%2C%20Christian%20A.%20Linderman%2C%20Scott%20W.%20Ranganath%2C%20Rajesh%20Blei%2C%20David%20M.%20Variational%20Sequential%20Monte%20Carlo%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Naesseth%2C%20Christian%20A.%20Linderman%2C%20Scott%20W.%20Ranganath%2C%20Rajesh%20Blei%2C%20David%20M.%20Variational%20Sequential%20Monte%20Carlo%202018"
        },
        {
            "id": "Neal_1990_a",
            "entry": "Neal, Radford M. Learning stochastic feedforward networks. Department of Computer Science, University of Toronto, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Learning%20stochastic%20feedforward%201990"
        },
        {
            "id": "Neal_1998_a",
            "entry": "Neal, Radford M. Annealed Importance Sampling (technical report 9805 (revised)). Department of Statistics, University of Toronto, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Annealed%20Importance%20Sampling%20%28technical%20report%209805%201998"
        },
        {
            "id": "Paige_2016_a",
            "entry": "Paige, Brooks and Wood, Frank. Inference networks for Sequential Monte Carlo in graphical models. In Proceedings of the International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paige%2C%20Brooks%20Wood%2C%20Frank%20Inference%20networks%20for%20Sequential%20Monte%20Carlo%20in%20graphical%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paige%2C%20Brooks%20Wood%2C%20Frank%20Inference%20networks%20for%20Sequential%20Monte%20Carlo%20in%20graphical%20models%202016"
        },
        {
            "id": "Papamakarios_2015_a",
            "entry": "Papamakarios, George and Murray, Iain. Distilling intractable generative models. In Probabilistic Integration Workshop at Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papamakarios%2C%20George%20Murray%2C%20Iain%20Distilling%20intractable%20generative%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papamakarios%2C%20George%20Murray%2C%20Iain%20Distilling%20intractable%20generative%20models%202015"
        },
        {
            "id": "Rainforth_et+al_2018_a",
            "entry": "Rainforth, Tom, Kosiorek, Adam R, Le, Tuan Anh, Maddison, Chris J, Igl, Maximilian, Wood, Frank, and Teh, Yee Whye. Tighter variational bounds are not necessarily better. Proceedings of the International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rainforth%2C%20Tom%20Kosiorek%2C%20Adam%20R.%20Le%2C%20Tuan%20Anh%20Maddison%2C%20Chris%20J.%20Tighter%20variational%20bounds%20are%20not%20necessarily%20better%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rainforth%2C%20Tom%20Kosiorek%2C%20Adam%20R.%20Le%2C%20Tuan%20Anh%20Maddison%2C%20Chris%20J.%20Tighter%20variational%20bounds%20are%20not%20necessarily%20better%202018"
        },
        {
            "id": "Ranganath_et+al_2015_a",
            "entry": "Ranganath, Rajesh, Tang, Linpeng, Charlin, Laurent, and Blei, David M. Deep exponential families. In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20Rajesh%20Tang%2C%20Linpeng%20Charlin%2C%20Laurent%20Blei%2C%20David%20M.%20Deep%20exponential%20families%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20Rajesh%20Tang%2C%20Linpeng%20Charlin%2C%20Laurent%20Blei%2C%20David%20M.%20Deep%20exponential%20families%202015"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Rezende, Danilo and Mohamed, Shakir. Variational inference with normalizing flows. In Proceedings of the International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Mohamed%2C%20Shakir%20Variational%20inference%20with%20normalizing%20flows%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Mohamed%2C%20Shakir%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "Rezende_et+al_2014_a",
            "entry": "Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the International Conference on Machine Learning, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20Danilo%20Jimenez%20Mohamed%2C%20Shakir%20Wierstra%2C%20Daan%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "Ritchie_et+al_2016_a",
            "entry": "Ritchie, Daniel, Horsfall, Paul, and Goodman, Noah D. Deep amortized inference for probabilistic programs. arXiv preprint arXiv:1610.05735, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05735"
        },
        {
            "id": "Stuhlmueller_et+al_2013_a",
            "entry": "Stuhlm\u00fcller, Andreas, Taylor, Jacob, and Goodman, Noah. Learning stochastic inverses. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stuhlm%C3%BCller%2C%20Andreas%20Taylor%2C%20Jacob%20Goodman%2C%20Noah%20Learning%20stochastic%20inverses%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stuhlm%C3%BCller%2C%20Andreas%20Taylor%2C%20Jacob%20Goodman%2C%20Noah%20Learning%20stochastic%20inverses%202013"
        },
        {
            "id": "Uria_et+al_2016_a",
            "entry": "Uria, Benigno, C\u00f4t\u00e9, Marc-Alexandre, Gregor, Karol, Murray, Iain, and Larochelle, Hugo. Neural autoregressive distribution estimation. Journal of Machine Learning Research, 17(205):1\u201337, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uria%2C%20Benigno%20C%C3%B4t%C3%A9%2C%20Marc-Alexandre%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Neural%20autoregressive%20distribution%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uria%2C%20Benigno%20C%C3%B4t%C3%A9%2C%20Marc-Alexandre%20Gregor%2C%20Karol%20Murray%2C%20Iain%20Neural%20autoregressive%20distribution%20estimation%202016"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "van den Oord, Aaron, Kalchbrenner, Nal, Espeholt, Lasse, Vinyals, Oriol, Graves, Alex, et al. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%20Aaron%2C%20Kalchbrenner%20Nal%2C%20Espeholt%20Lasse%2C%20Vinyals%20Conditional%20image%20generation%20with%20PixelCNN%20decoders%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%20Aaron%2C%20Kalchbrenner%20Nal%2C%20Espeholt%20Lasse%2C%20Vinyals%20Conditional%20image%20generation%20with%20PixelCNN%20decoders%202016"
        },
        {
            "id": "Van_et+al_2016_b",
            "entry": "van den Oord, Aaron, Kalchbrenner, Nal, and Kavukcuoglu, Koray. Pixel recurrent neural networks. In Proceedings of the International Conference on Machine Learning, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20den%20Oord%20Aaron%2C%20Kalchbrenner%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20den%20Oord%20Aaron%2C%20Kalchbrenner%20Nal%20Kavukcuoglu%2C%20Koray%20Pixel%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Wu_et+al_2017_a",
            "entry": "Wu, Yuhuai, Burda, Yuri, Salakhutdinov, Ruslan, and Grosse, Roger. On the quantitative analysis of decoder-based generative models. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017"
        }
    ]
}
