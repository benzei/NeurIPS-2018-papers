{
    "filename": "7734-adaptive-negative-curvature-descent-with-applications-in-non-convex-optimization.pdf",
    "metadata": {
        "title": "Adaptive Negative Curvature Descent with Applications in Non-convex Optimization",
        "author": "Mingrui Liu, Zhe Li, Xiaoyu Wang, Jinfeng Yi, Tianbao Yang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7734-adaptive-negative-curvature-descent-with-applications-in-non-convex-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima. In existing studies, NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (e.g., 2 1) in order to achieve a sufficiently accurate second-order stationary solution (i.e., \u03bbmin(\u22072f (x)) \u2265 \u2212 2). One issue with this approach is that the target precision 2 is usually set to be very small in order to find a high quality solution, which increases the complexity for computing a negative curvature. To address this issue, we propose an adaptive NCD to allow an adaptive error dependent on the current gradient\u2019s magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization, and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "optimal solution",
            "url": "https://en.wikipedia.org/wiki/optimal_solution"
        }
    ],
    "highlights": [
        "We consider the following optimization problem: min f (x), (1)<br/><br/>x\u2208Rd where f (x) is a non-convex smooth function with Lipschitz continuous Hessian, which could has some special structure",
        "We propose an adaptive Negative curvature descent step based on full or sub-sampled Hessian that uses a noisy negative curvature to update the solution with an error of approximating the smallest eigen-value adaptive to the magnitude of the gradient at the time of invocation",
        "We present a stochastic algorithm based on the AdaNCDmb in Algorithm 4, which is referred to as S-AdaNCG",
        "We have developed several variants of adaptive negative curvature descent step that employ a noisy negative curvature direction for non-convex optimization",
        "The novelty of the proposed algorithms lie at that the noise level in approximating the negative curvature is adaptive to the magnitude of the current gradient instead of a prescribed small noise level, which could dramatically reduce the number of Hessian-vector products",
        "Building on the adaptive negative curvature descent step, we have developed several deterministic and stochastic algorithms and established their complexities"
    ],
    "key_statements": [
        "We consider the following optimization problem: min f (x), (1)<br/><br/>x\u2208Rd where f (x) is a non-convex smooth function with Lipschitz continuous Hessian, which could has some special structure",
        "We propose an adaptive Negative curvature descent step based on full or sub-sampled Hessian that uses a noisy negative curvature to update the solution with an error of approximating the smallest eigen-value adaptive to the magnitude of the gradient at the time of invocation",
        "We demonstrate the applications of the proposed adaptive Negative curvature descent steps in existing deterministic and stochastic optimization algorithms to match the state-of-the-art worst-case complexity for finding a second-order stationary point",
        "We demonstrate the applications of the proposed adaptive Negative curvature descent for stochastic non-convex optimization, and develop several stochastic algorithms that not only match the state-of-the-art worst-case time complexity but enjoy adaptively smaller time complexity for computing the negative curvature",
        "We present simple deterministic and stochastic algorithms by employing AdaNCD presented in the last section",
        "We present a stochastic algorithm based on the AdaNCDmb in Algorithm 4, which is referred to as S-AdaNCG",
        "The difference from [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] is that they studied how to use a first-order method without resorting to Hessianvector products to extract the negative curvature direction, while we focus on reducing the time complexity of negative curvature search using the proposed adaptive Negative curvature descent",
        "We have developed several variants of adaptive negative curvature descent step that employ a noisy negative curvature direction for non-convex optimization",
        "The novelty of the proposed algorithms lie at that the noise level in approximating the negative curvature is adaptive to the magnitude of the current gradient instead of a prescribed small noise level, which could dramatically reduce the number of Hessian-vector products",
        "Building on the adaptive negative curvature descent step, we have developed several deterministic and stochastic algorithms and established their complexities"
    ],
    "summary": [
        "We consider the following optimization problem: min f (x), (1)<br/><br/>x\u2208Rd where f (x) is a non-convex smooth function with Lipschitz continuous Hessian, which could has some special structure.",
        "It has been leveraged to design deterministic and stochastic non-convex optimization with state-of-the-art time complexities for finding a second-order stationary point [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>].",
        "Building on the proposed adaptive NCD step, we design two simple algorithms to enjoy a second-order convergence for deterministic and stochastic non-convex optimization.",
        "We demonstrate the applications of the proposed adaptive NCD steps in existing deterministic and stochastic optimization algorithms to match the state-of-the-art worst-case complexity for finding a second-order stationary point.",
        "Several stochastic algorithms use the negative curvature information to derive the stateof-the-art time complexities for finding a second-order stationary point for non-convex optimization [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], which combine existing stochastic first-order algorithms and a NCD method with differences lying at how to compute the negative curvature.",
        "We demonstrate the applications of the proposed adaptive NCD for stochastic non-convex optimization, and develop several stochastic algorithms that not only match the state-of-the-art worst-case time complexity but enjoy adaptively smaller time complexity for computing the negative curvature.",
        "There exist algorithms to implement negative curvature search (NCS) for two different cases: deterministic objective and stochastic objective with theoretical guarantee, which we provide in the supplement.",
        "Remark: We can analyze the worst-case time complexity of S-AdaNCG by using randomized algorithms as in Lemma 1 to compute the negative curvature with Tn(f, \u03b5, \u03b4, d) = O",
        "We note that the subroutine AdaNCG provides the same guarantee as the NCD in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>], i.e., returning a solution xj satisfying \u03bbmin(\u22072f) \u2265 \u2212 2 with high probability.",
        "We present a stochastic algorithm for tackling a stochastic objective f (x) = E[f (x; \u03be)] in order to achieve a state-of-the-art worse-case complexity for finding a second-order stationary point.",
        "The difference from [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] is that they studied how to use a first-order method without resorting to Hessianvector products to extract the negative curvature direction, while we focus on reducing the time complexity of NCS using the proposed adaptive NCD.",
        "Our result below shows AdaNCD-SCSG h\u221aas a worst-case time complexity that matches the state-of-the-art time complexity for finding an ( 1, 1) second-order stationary point.",
        "The novelty of the proposed algorithms lie at that the noise level in approximating the negative curvature is adaptive to the magnitude of the current gradient instead of a prescribed small noise level, which could dramatically reduce the number of Hessian-vector products.",
        "Building on the adaptive negative curvature descent step, we have developed several deterministic and stochastic algorithms and established their complexities.",
        "The effectiveness of adaptive negative curvature descent is demonstrated by empirical studies"
    ],
    "headline": "We propose an adaptive Negative curvature descent to allow an adaptive error dependent on the current gradient\u2019s magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between a noisy Negative curvature descent step and gradient descent step",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1195\u20131199, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Naman%20Zhu%2C%20Zeyuan%20Allen%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Naman%20Zhu%2C%20Zeyuan%20Allen%20Bullins%2C%20Brian%20Hazan%2C%20Elad%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. CoRR, /abs/1708.08694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. CoRR, abs/1711.06673, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06673"
        },
        {
            "id": "4",
            "entry": "[4] Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization. CoRR, abs/1611.00756, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "5",
            "entry": "[5] Coralia Cartis, Nicholas I. M. Gould, and Philippe L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part i: motivation, convergence and numerical results. Mathematical Programming, 127(2):245\u2013295, Apr 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20I.M.%20Toint%2C%20Philippe%20L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20i%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20I.M.%20Toint%2C%20Philippe%20L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20i%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011-04"
        },
        {
            "id": "6",
            "entry": "[6] Coralia Cartis, Nicholas I. M. Gould, and Philippe L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part ii: worst-case functionand derivative-evaluation complexity. Mathematical Programming, 130(2):295\u2013319, Dec 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20I.M.%20Toint%2C%20Philippe%20L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20ii%3A%20worst-case%20functionand%20derivative-evaluation%20complexity%202011-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20I.M.%20Toint%2C%20Philippe%20L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20ii%3A%20worst-case%20functionand%20derivative-evaluation%20complexity%202011-12"
        },
        {
            "id": "7",
            "entry": "[7] Frank E. Curtis and Daniel P. Robinson. Exploiting negative curvature in deterministic and stochastic optimization. CoRR, abs/1703.00412, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00412"
        },
        {
            "id": "8",
            "entry": "[8] Rong-En Fan and Chih-Jen Lin. Libsvm data: Classification, regression and multi-label. URL: http://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets, 2011.",
            "url": "http://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets"
        },
        {
            "id": "9",
            "entry": "[9] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Peter Gr\u00fcnwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th Conference on Learning Theory (COLT), volume 40, pages 797\u2013842. PMLR, 03\u201306 Jul 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015-07"
        },
        {
            "id": "10",
            "entry": "[10] Christopher J. Hillar and Lek-Heng Lim. Most tensor problems are np-hard. J. ACM, 60(6):45:1\u201345:39, November 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christopher%2C%20J.%20Hillar%20and%20Lek-Heng%20Lim.%20Most%20tensor%20problems%20are%20np-hard%202013-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christopher%2C%20J.%20Hillar%20and%20Lek-Heng%20Lim.%20Most%20tensor%20problems%20are%20np-hard%202013-11"
        },
        {
            "id": "11",
            "entry": "[11] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "12",
            "entry": "[12] Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via scsg methods. In Advances in Neural Information Processing Systems, pages 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20Lihua%20Ju%2C%20Cheng%20Chen%2C%20Jianbo%20Jordan%2C%20Michael%20I.%20Non-convex%20finite-sum%20optimization%20via%20scsg%20methods%202017"
        },
        {
            "id": "13",
            "entry": "[13] A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. A Wiley-Interscience publication. Wiley, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovsky%2C%20A.S.%20Yudin%2C%20D.B.%20Problem%20Complexity%20and%20Method%20Efficiency%20in%20Optimization.%20A%20Wiley-Interscience%20publication%201983"
        },
        {
            "id": "14",
            "entry": "[14] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "15",
            "entry": "[15] Sashank J Reddi, Suvrit Sra, Barnab\u00e1s P\u00f3czos, and Alex Smola. Fast incremental method for smooth nonconvex optimization. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pages 1971\u2013 1977. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Smola%2C%20Alex%20Fast%20incremental%20method%20for%20smooth%20nonconvex%20optimization%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20Sashank%20J.%20Sra%2C%20Suvrit%20P%C3%B3czos%2C%20Barnab%C3%A1s%20Smola%2C%20Alex%20Fast%20incremental%20method%20for%20smooth%20nonconvex%20optimization%201971"
        },
        {
            "id": "16",
            "entry": "[16] Sashank J. Reddi, Manzil Zaheer, Suvrit Sra, Barnab\u00e1s P\u00f3czos, Francis R. Bach, Ruslan Salakhutdinov, and Alexander J. Smola. A generic approach for escaping saddle points. CoRR, abs/1709.01434, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01434"
        },
        {
            "id": "17",
            "entry": "[17] Cl\u00e9ment W. Royer and Stephen J. Wright. Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization. CoRR, abs/1706.03131, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03131"
        },
        {
            "id": "18",
            "entry": "[18] Peng Xu, Farbod Roosta-Khorasani, and Michael W. Mahoney. Newton-type methods for non-convex optimization under inexact hessian information. CoRR, abs/1708.07164, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07164"
        },
        {
            "id": "19",
            "entry": "[19] Yi Xu, Rong Jin, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. CoRR, abs/1711.01944, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01944"
        },
        {
            "id": "20",
            "entry": "[20] Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient langevin dynamics. In Proceedings of the 30th Conference on Learning Theory (COLT), pages 1980\u20132022, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Liang%2C%20Percy%20Charikar%2C%20Moses%20A%20hitting%20time%20analysis%20of%20stochastic%20gradient%20langevin%20dynamics%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Liang%2C%20Percy%20Charikar%2C%20Moses%20A%20hitting%20time%20analysis%20of%20stochastic%20gradient%20langevin%20dynamics%201980"
        }
    ]
}
