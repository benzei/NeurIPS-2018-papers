{
    "filename": "7703-explaining-deep-learning-models-a-bayesian-non-parametric-approach.pdf",
    "metadata": {
        "title": "Explaining Deep Learning Models -- A Bayesian Non-parametric Approach",
        "author": "Wenbo Guo, Sui Huang, Yunzhe Tao, Xinyu Xing, Lin Lin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7703-explaining-deep-learning-models-a-bayesian-non-parametric-approach.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "elastic net",
            "url": "https://en.wikipedia.org/wiki/elastic_net"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "regression model",
            "url": "https://en.wikipedia.org/wiki/regression_model"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        }
    ],
    "highlights": [
        "We propose a new technical approach that not only explains an individual decision but, more importantly, extracts generalizable insights from the target model",
        "Given a learning model g : Rp \u2192 R, we can approximate g(\u00b7) with a mixture model using {X, y}, a set of data samples as well as their corresponding predictions obtained from model g, i.e., yi = g",
        "Figure 1 illustrates the generalizable insights that our proposed solution distilled from the target MLP and CNNs models, respectively",
        "They can extract generalizable insights learned by a target model and use it to scrutinize model strengths and weaknesses",
        "While our proposed approach exhibits outstanding performance in explaining individual decisions, and provides a user with an ability to discover model weaknesses, its performance may not be good enough when applied to interpreting temporal learning models"
    ],
    "key_statements": [
        "We propose a new technical approach that not only explains an individual decision but, more importantly, extracts generalizable insights from the target model",
        "Given a learning model g : Rp \u2192 R, we can approximate g(\u00b7) with a mixture model using {X, y}, a set of data samples as well as their corresponding predictions obtained from model g, i.e., yi = g",
        "We address this issue by establishing a dirichlet process mixture model with multiple elastic nets (DMM-MEN)",
        "Figure 1 illustrates the generalizable insights that our proposed solution distilled from the target MLP and CNNs models, respectively",
        "To further validate the fidelity of the insights illustrated in Figure 1, we construct new testing cases based on top 50/75/100/125/150 pixels deemed important by our proposed solution respectively and measure the proportion of these testing samples that are classified as positive cases by the target models",
        "They can extract generalizable insights learned by a target model and use it to scrutinize model strengths and weaknesses",
        "While our proposed approach exhibits outstanding performance in explaining individual decisions, and provides a user with an ability to discover model weaknesses, its performance may not be good enough when applied to interpreting temporal learning models"
    ],
    "summary": [
        "We propose a new technical approach that not only explains an individual decision but, more importantly, extracts generalizable insights from the target model.",
        "Similar to prior research [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>], we can take this linear regression model to pinpoint the important features and take them as an explanation for the corresponding individual decision.",
        "As is illustrated in both figures, the classification rates of the target models on these perturbed samples are impacted dramatically once we start manipulating top 50/75 important features identified by our proposed solution in these images.",
        "To further validate the fidelity of the insights illustrated in Figure 1, we construct new testing cases based on top 50/75/100/125/150 pixels deemed important by our proposed solution respectively and measure the proportion of these testing samples that are classified as positive cases by the target models.",
        "The intuition behind this exercise is that, similar to the experiments described earlier, we would like to see significantly higher positive classification rates leveraging the insights from our proposed solution than creating cases around randomly selected pixels.",
        "Our proposed solution does not only extract generalizable insights from the target models but demonstrate superior performance in explaining individual decisions.",
        "Due to the ultra high dimensionality concern, which we will discuss we adopt the methodology in [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] to generate data to explain individual decisions.",
        "We apply our solution as well as LIME and SHAP to each of the images shown in the figure and select and highlight the top 20 segments that each approach deems important to the decision made by deep neural network classifiers.",
        "One possible explanation is that both LIME and SHAP assume the local decision boundary of the target model to be linear while the proposed approach conducts the variable selection by applying a non-linear approximation.",
        "Our evaluation described in Section 4 indicates that the proposed solution (DMM-MEN) could extract generalizable insights even from high dimensional data (e.g. Fashion MNIST).",
        "As is shown in Section 4, our solution significantly outperforms the state-of-the-art solutions in explaining individual decisions made on ultra-high dimensional data samples.",
        "While our proposed approach exhibits outstanding performance in explaining individual decisions, and provides a user with an ability to discover model weaknesses, its performance may not be good enough when applied to interpreting temporal learning models.",
        "As part of the future work, we will equip our approach with the ability of dissecting temporal learning models"
    ],
    "headline": "We propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M\u00fcller, and W. Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=On%20pixel-wise%20explanations%20for%20non-linear%20classifier%20decisions%20by%20layer-wise%20relevance%20propagation%202015"
        },
        {
            "id": "2",
            "entry": "[2] F. Chollet et al. Keras, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=F%20Chollet%20et%20al%20Keras%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=F%20Chollet%20et%20al%20Keras%202015"
        },
        {
            "id": "3",
            "entry": "[3] A. J. Cron and M. West. Efficient classification-based relabeling in mixture models. The American Statistician, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cron%2C%20A.J.%20West%2C%20M.%20Efficient%20classification-based%20relabeling%20in%20mixture%20models%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cron%2C%20A.J.%20West%2C%20M.%20Efficient%20classification-based%20relabeling%20in%20mixture%20models%202011"
        },
        {
            "id": "4",
            "entry": "[4] P. Dabkowski and Y. Gal. Real time image saliency for black box classifiers. In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dabkowski%2C%20P.%20Gal%2C%20Y.%20Real%20time%20image%20saliency%20for%20black%20box%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dabkowski%2C%20P.%20Gal%2C%20Y.%20Real%20time%20image%20saliency%20for%20black%20box%20classifiers%202017"
        },
        {
            "id": "5",
            "entry": "[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the 22nd Conference on Computer Vision and Pattern Recognition. (CVPR), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "6",
            "entry": "[6] R. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the 16th International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fong%2C%20R.%20Vedaldi%2C%20A.%20Interpretable%20explanations%20of%20black%20boxes%20by%20meaningful%20perturbation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fong%2C%20R.%20Vedaldi%2C%20A.%20Interpretable%20explanations%20of%20black%20boxes%20by%20meaningful%20perturbation%202017"
        },
        {
            "id": "7",
            "entry": "[7] N. Frosst and G. Hinton. Distilling a neural network into a soft decision tree. arXiv preprint arXiv:1711.09784, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09784"
        },
        {
            "id": "8",
            "entry": "[8] C. Gan, N. Wang, Y. Yang, D.-Y. Yeung, and A. G. Hauptmann. Devnet: A deep event network for multimedia event detection and evidence recounting. In Proceedings of the 28th Conference on Computer Vision and Pattern Recognition. (CVPR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20C.%20Wang%2C%20N.%20Yang%2C%20Y.%20Yeung%2C%20D.-Y.%20Devnet%3A%20A%20deep%20event%20network%20for%20multimedia%20event%20detection%20and%20evidence%20recounting%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20C.%20Wang%2C%20N.%20Yang%2C%20Y.%20Yeung%2C%20D.-Y.%20Devnet%3A%20A%20deep%20event%20network%20for%20multimedia%20event%20detection%20and%20evidence%20recounting%202015"
        },
        {
            "id": "9",
            "entry": "[9] C. Hans. Elastic net regression modeling with the orthant normal prior. Journal of the American Statistical Association, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hans%2C%20C.%20Elastic%20net%20regression%20modeling%20with%20the%20orthant%20normal%20prior%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hans%2C%20C.%20Elastic%20net%20regression%20modeling%20with%20the%20orthant%20normal%20prior%202011"
        },
        {
            "id": "10",
            "entry": "[10] C. Hennig. Methods for merging gaussian mixture components. Advances in data analysis and classification, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hennig%2C%20C.%20Methods%20for%20merging%20gaussian%20mixture%20components.%20Advances%20in%20data%20analysis%20and%20classification%202010"
        },
        {
            "id": "11",
            "entry": "[11] H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ishwaran%2C%20H.%20James%2C%20L.F.%20Gibbs%20sampling%20methods%20for%20stick-breaking%20priors%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ishwaran%2C%20H.%20James%2C%20L.F.%20Gibbs%20sampling%20methods%20for%20stick-breaking%20priors%202001"
        },
        {
            "id": "12",
            "entry": "[12] B. Kim, R. Khanna, and O. O. Koyejo. Examples are not enough, learn to criticize! criticism for interpretability. In Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20B.%20Khanna%2C%20R.%20Koyejo%2C%20O.O.%20Examples%20are%20not%20enough%2C%20learn%20to%20criticize%21%20criticism%20for%20interpretability%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20B.%20Khanna%2C%20R.%20Koyejo%2C%20O.O.%20Examples%20are%20not%20enough%2C%20learn%20to%20criticize%21%20criticism%20for%20interpretability%202016"
        },
        {
            "id": "13",
            "entry": "[13] W. Knight. The dark secret at the heart of ai. https://www.technologyreview.com/s/604087/, 2017.",
            "url": "https://www.technologyreview.com/s/604087/"
        },
        {
            "id": "14",
            "entry": "[14] W. Knight. The financial world wants to open ai\u2019s black boxes. https://www.technologyreview.com/s/604122/, 2017.",
            "url": "https://www.technologyreview.com/s/604122/"
        },
        {
            "id": "15",
            "entry": "[15] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017"
        },
        {
            "id": "16",
            "entry": "[16] Y. LeCun, C. Cortes, and C. J. Burges. The mnist database of handwritten digits, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Cortes%2C%20C.%20Burges%2C%20C.J.%20The%20mnist%20database%20of%20handwritten%20digits%201998"
        },
        {
            "id": "17",
            "entry": "[17] J. Li, W. Monroe, and D. Jurafsky. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08220"
        },
        {
            "id": "18",
            "entry": "[18] Q. Li, N. Lin, et al. The bayesian elastic net. Bayesian Analysis, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Q.%20Lin%2C%20N.%20The%20bayesian%20elastic%20net%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Q.%20Lin%2C%20N.%20The%20bayesian%20elastic%20net%202010"
        },
        {
            "id": "19",
            "entry": "[19] F. Liang, J. Kim, and Q. Song. A bootstrap metropolis\u2013hastings algorithm for bayesian analysis of big data. Technometrics, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20F.%20Kim%2C%20J.%20Song%2C%20Q.%20A%20bootstrap%20metropolis%E2%80%93hastings%20algorithm%20for%20bayesian%20analysis%20of%20big%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20F.%20Kim%2C%20J.%20Song%2C%20Q.%20A%20bootstrap%20metropolis%E2%80%93hastings%20algorithm%20for%20bayesian%20analysis%20of%20big%20data%202016"
        },
        {
            "id": "20",
            "entry": "[20] Z. C. Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03490"
        },
        {
            "id": "21",
            "entry": "[21] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lundberg%2C%20S.M.%20Lee%2C%20S.-I.%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lundberg%2C%20S.M.%20Lee%2C%20S.-I.%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017"
        },
        {
            "id": "22",
            "entry": "[22] J.-M. Marin, K. Mengersen, and C. P. Robert. Bayesian modelling and inference on mixtures of distributions. Handbook of statistics, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marin%2C%20J.-M.%20Mengersen%2C%20K.%20Robert%2C%20C.P.%20Bayesian%20modelling%20and%20inference%20on%20mixtures%20of%20distributions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marin%2C%20J.-M.%20Mengersen%2C%20K.%20Robert%2C%20C.P.%20Bayesian%20modelling%20and%20inference%20on%20mixtures%20of%20distributions%202005"
        },
        {
            "id": "23",
            "entry": "[23] M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining (KDD), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20M.T.%20Singh%2C%20S.%20Guestrin%2C%20C.%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ribeiro%2C%20M.T.%20Singh%2C%20S.%20Guestrin%2C%20C.%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016"
        },
        {
            "id": "24",
            "entry": "[24] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. arxiv. org/abs/1610.02391 v3, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Selvaraju%2C%20R.R.%20Cogswell%2C%20M.%20Das%2C%20A.%20Vedantam%2C%20R.%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Selvaraju%2C%20R.R.%20Cogswell%2C%20M.%20Das%2C%20A.%20Vedantam%2C%20R.%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202016"
        },
        {
            "id": "25",
            "entry": "[25] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shrikumar%2C%20A.%20Greenside%2C%20P.%20Kundaje%2C%20A.%20Learning%20important%20features%20through%20propagating%20activation%20differences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shrikumar%2C%20A.%20Greenside%2C%20P.%20Kundaje%2C%20A.%20Learning%20important%20features%20through%20propagating%20activation%20differences%202017"
        },
        {
            "id": "26",
            "entry": "[26] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "27",
            "entry": "[27] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "28",
            "entry": "[28] A. Smith and G. Roberts. Bayesian computation via the gibbs sampler and related markov chain monte carlo methods. Journal of the Royal Statistical Society. Series B, pages 3\u201323, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20A.%20Roberts%2C%20G.%20Bayesian%20computation%20via%20the%20gibbs%20sampler%20and%20related%20markov%20chain%20monte%20carlo%20methods%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20A.%20Roberts%2C%20G.%20Bayesian%20computation%20via%20the%20gibbs%20sampler%20and%20related%20markov%20chain%20monte%20carlo%20methods%201993"
        },
        {
            "id": "29",
            "entry": "[29] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional net. In Proceedings of the 3rd International Conference on Learning Representations Workshop (ICLR Workshop), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20J.T.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Riedmiller%2C%20M.%20Striving%20for%20simplicity%3A%20The%20all%20convolutional%20net%202015"
        },
        {
            "id": "30",
            "entry": "[30] S. Srivastava, V. Cevher, Q. Dinh, and D. Dunson. Wasp: Scalable bayes via barycenters of subset posteriors. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20S.%20Cevher%2C%20V.%20Dinh%2C%20Q.%20Dunson%2C%20D.%20Wasp%3A%20Scalable%20bayes%20via%20barycenters%20of%20subset%20posteriors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20S.%20Cevher%2C%20V.%20Dinh%2C%20Q.%20Dunson%2C%20D.%20Wasp%3A%20Scalable%20bayes%20via%20barycenters%20of%20subset%20posteriors%202015"
        },
        {
            "id": "31",
            "entry": "[31] M. Suchard, Q. Wang, C. Chan, F. J., A. Cron, and M. West. Understanding gpu programming for statistical computation: Studies in massively parallel massive mixtures. Journal of computational and graphical statistics, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suchard%2C%20M.%20Wang%2C%20Q.%20Chan%2C%20C.%20J.%2C%20F.%20Understanding%20gpu%20programming%20for%20statistical%20computation%3A%20Studies%20in%20massively%20parallel%20massive%20mixtures%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suchard%2C%20M.%20Wang%2C%20Q.%20Chan%2C%20C.%20J.%2C%20F.%20Understanding%20gpu%20programming%20for%20statistical%20computation%3A%20Studies%20in%20massively%20parallel%20massive%20mixtures%202010"
        },
        {
            "id": "32",
            "entry": "[32] M. Sundararajan, A. Taly, and Q. Yan. Gradients of counterfactuals. arXiv preprint arXiv:1611.02639, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02639"
        },
        {
            "id": "33",
            "entry": "[33] M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth, and F. Doshi-Velez. Beyond sparsity: Tree regularization of deep models for interpretability. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20M.%20Hughes%2C%20M.C.%20Parbhoo%2C%20S.%20Zazzi%2C%20M.%20Beyond%20sparsity%3A%20Tree%20regularization%20of%20deep%20models%20for%20interpretability%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20M.%20Hughes%2C%20M.C.%20Parbhoo%2C%20S.%20Zazzi%2C%20M.%20Beyond%20sparsity%3A%20Tree%20regularization%20of%20deep%20models%20for%20interpretability%202018"
        },
        {
            "id": "34",
            "entry": "[34] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "35",
            "entry": "[35] H. Yang, D. Dunson, and D. Banks. The multiple bayesian elastic net. submitted for publication, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20H.%20Dunson%2C%20D.%20Banks%2C%20D.%20The%20multiple%20bayesian%20elastic%20net%202011"
        },
        {
            "id": "36",
            "entry": "[36] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Proceedings of the 13rd European Conference on Computer Vision (ECCV), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zeiler%2C%20M.D.%20Fergus%2C%20R.%20Visualizing%20and%20understanding%20convolutional%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zeiler%2C%20M.D.%20Fergus%2C%20R.%20Visualizing%20and%20understanding%20convolutional%20networks%202014"
        },
        {
            "id": "37",
            "entry": "[37] L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling. Visualizing deep neural network decisions: Prediction difference analysis. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zintgraf%2C%20L.M.%20Cohen%2C%20T.S.%20Adel%2C%20T.%20Welling%2C%20M.%20Visualizing%20deep%20neural%20network%20decisions%3A%20Prediction%20difference%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zintgraf%2C%20L.M.%20Cohen%2C%20T.S.%20Adel%2C%20T.%20Welling%2C%20M.%20Visualizing%20deep%20neural%20network%20decisions%3A%20Prediction%20difference%20analysis%202017"
        },
        {
            "id": "38",
            "entry": "[38] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2005. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20H.%20Hastie%2C%20T.%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20H.%20Hastie%2C%20T.%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005"
        }
    ]
}
