{
    "filename": "7521-tree-to-tree-neural-networks-for-program-translation.pdf",
    "metadata": {
        "title": "Tree-to-tree Neural Networks for Program Translation",
        "author": "Xinyun Chen, Chang Liu, Dawn Song",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7521-tree-to-tree-neural-networks-for-program-translation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects."
    },
    "keywords": [
        {
            "term": "source tree",
            "url": "https://en.wikipedia.org/wiki/source_tree"
        },
        {
            "term": "statistical machine translation",
            "url": "https://en.wikipedia.org/wiki/statistical_machine_translation"
        },
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "programming language",
            "url": "https://en.wikipedia.org/wiki/programming_language"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Programs are the main tool for building computer applications, the IT industry, and the digital world",
        "The variety of different programming languages introduces a burden when programmers want to combine programs written in different languages together",
        "We study neural machine translation methods to handle the program translation problem",
        "We are the first to consider neural network approaches for the program translation problem, and are the first to demonstrate a successful design of tree-to-tree neural network combining both a tree-RNN encoder and a tree-RNN decoder for translation tasks",
        "Extensive evaluation demonstrates that our tree-to-tree neural network outperforms several state-of-the-art models",
        "We believe that our proposed tree-to-tree neural network has the potential to generalize to other tree-to-tree tasks, and we consider it as future work"
    ],
    "key_statements": [
        "Programs are the main tool for building computer applications, the IT industry, and the digital world",
        "The variety of different programming languages introduces a burden when programmers want to combine programs written in different languages together",
        "We study neural machine translation methods to handle the program translation problem",
        "Another work [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] proposes to use a tree-structured encoder-decoder architecture for natural language translation, where both the encoder and the decoder are variants of the RNNG model [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]; the performance of their model is slightly worse than the sequence-to-sequence model with attention, which is mainly due to the fact that their attention mechanism can not condition the future attention weights on previously computed ones",
        "We are the first to consider neural network approaches for the program translation problem, and are the first to demonstrate a successful design of tree-to-tree neural network combining both a tree-RNN encoder and a tree-RNN decoder for translation tasks",
        "Extensive evaluation demonstrates that our tree-to-tree neural network outperforms several state-of-the-art models",
        "We believe that our proposed tree-to-tree neural network has the potential to generalize to other tree-to-tree tasks, and we consider it as future work"
    ],
    "summary": [
        "Programs are the main tool for building computer applications, the IT industry, and the digital world.",
        "We observe that in the program translation problem, both source and target programs have their parse trees.",
        "Another work [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] proposes a tree-based attentional encoder-decoder architecture for natural language translation, but their model performs even worse than the attentional sequence-to-sequence baseline model.",
        "Experimental results demonstrate that our tree-to-tree model outperforms other state-of-the-art neural networks on the program translation tasks, and yields a margin of up to 5% on the token accuracy and up to 15% on the program accuracy.",
        "Rather than modeling the input program as a sequence of tokens, we can consider the problem as translating a source tree into a target tree.",
        "To capture the intuition of the modular translation process, the decoder employs an attention mechanism to locate the corresponding source sub-tree when expanding the non-terminal.",
        "We evaluate our tree-to-tree neural network with several baseline approaches on the program translation task.",
        "We first describe three benchmark datasets in Section 4.1 for evaluating different aspects; we evaluate our tree-to-tree model against several baseline approaches, including the state-of-the-art neural network approaches and program translation approaches.",
        "These results demonstrate that tree2tree model is more capable of learning the correspondence between the source and the target programs; in particular, it is significantly better than other baselines at handling longer inputs.",
        "Several works propose to adapt phrase-based statistical machine translation models and leverage grammatical structures of programming languages for code migration [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>].",
        "Another work [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] proposes to use a tree-structured encoder-decoder architecture for natural language translation, where both the encoder and the decoder are variants of the RNNG model [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]; the performance of their model is slightly worse than the sequence-to-sequence model with attention, which is mainly due to the fact that their attention mechanism can not condition the future attention weights on previously computed ones.",
        "We are the first to consider neural network approaches for the program translation problem, and are the first to demonstrate a successful design of tree-to-tree neural network combining both a tree-RNN encoder and a tree-RNN decoder for translation tasks.",
        "The models are hard to generalize to programs longer than the training ones; it is unclear how to handle an infinite vocabulary set that may be employed in real-world applications; further, the training requires a dataset of aligned input-output pairs, which may be lacking in practice.",
        "We believe that our proposed tree-to-tree neural network has the potential to generalize to other tree-to-tree tasks, and we consider it as future work"
    ],
    "headline": "We develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. Aharoni and Y. Goldberg. Towards string-to-tree neural machine translation. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aharoni%2C%20R.%20Goldberg%2C%20Y.%20Towards%20string-to-tree%20neural%20machine%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aharoni%2C%20R.%20Goldberg%2C%20Y.%20Towards%20string-to-tree%20neural%20machine%20translation%202017"
        },
        {
            "id": "2",
            "entry": "[2] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton. A survey of machine learning for big code and naturalness. arXiv preprint arXiv:1709.06182, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06182"
        },
        {
            "id": "3",
            "entry": "[3] D. Alvarez-Melis and T. S. Jaakkola. Tree-structured decoding with doubly-recurrent neural networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alvarez-Melis%2C%20D.%20Jaakkola%2C%20T.S.%20Tree-structured%20decoding%20with%20doubly-recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alvarez-Melis%2C%20D.%20Jaakkola%2C%20T.S.%20Tree-structured%20decoding%20with%20doubly-recurrent%20neural%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "5",
            "entry": "[5] M. Balog, A. L. Gaunt, M. Brockschmidt, S. Nowozin, and D. Tarlow. Deepcoder: Learning to write programs. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balog%2C%20M.%20Gaunt%2C%20A.L.%20Brockschmidt%2C%20M.%20Nowozin%2C%20S.%20Deepcoder%3A%20Learning%20to%20write%20programs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balog%2C%20M.%20Gaunt%2C%20A.L.%20Brockschmidt%2C%20M.%20Nowozin%2C%20S.%20Deepcoder%3A%20Learning%20to%20write%20programs%202017"
        },
        {
            "id": "6",
            "entry": "[6] J. Bradbury and R. Socher. Towards neural machine translation with latent tree attention. arXiv preprint arXiv:1709.01915, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01915"
        },
        {
            "id": "7",
            "entry": "[7] X. Chen, C. Liu, E. C. Shin, D. Song, and M. Chen. Latent attention for if-then program synthesis. In Advances in Neural Information Processing Systems, pages 4574\u20134582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Liu%2C%20C.%20Shin%2C%20E.C.%20Song%2C%20D.%20Latent%20attention%20for%20if-then%20program%20synthesis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Liu%2C%20C.%20Shin%2C%20E.C.%20Song%2C%20D.%20Latent%20attention%20for%20if-then%20program%20synthesis%202016"
        },
        {
            "id": "8",
            "entry": "[8] X. Chen, C. Liu, and D. Song. Towards synthesizing complex programs from input-output examples. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Liu%2C%20C.%20Song%2C%20D.%20Towards%20synthesizing%20complex%20programs%20from%20input-output%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Liu%2C%20C.%20Song%2C%20D.%20Towards%20synthesizing%20complex%20programs%20from%20input-output%20examples%202018"
        },
        {
            "id": "9",
            "entry": "[9] S. J. K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural machine translation. In ACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20S.J.K.%20Memisevic%2C%20R.%20Bengio%2C%20Y.%20On%20using%20very%20large%20target%20vocabulary%20for%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20S.J.K.%20Memisevic%2C%20R.%20Bengio%2C%20Y.%20On%20using%20very%20large%20target%20vocabulary%20for%20neural%20machine%20translation%202015"
        },
        {
            "id": "10",
            "entry": "[10] J. Devlin, J. Uesato, S. Bhupatiraju, R. Singh, A. Mohamed, and P. Kohli. Robustfill: Neural program learning under noisy I/O. arXiv preprint arXiv:1703.07469, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07469"
        },
        {
            "id": "11",
            "entry": "[11] L. Dong and M. Lapata. Language to logical form with neural attention. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dong%2C%20L.%20Lapata%2C%20M.%20Language%20to%20logical%20form%20with%20neural%20attention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dong%2C%20L.%20Lapata%2C%20M.%20Language%20to%20logical%20form%20with%20neural%20attention%202016"
        },
        {
            "id": "12",
            "entry": "[12] C. Dyer, A. Kuncoro, M. Ballesteros, and N. A. Smith. Recurrent neural network grammars. In NAACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dyer%2C%20C.%20Kuncoro%2C%20A.%20Ballesteros%2C%20M.%20Smith%2C%20N.A.%20Recurrent%20neural%20network%20grammars%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dyer%2C%20C.%20Kuncoro%2C%20A.%20Ballesteros%2C%20M.%20Smith%2C%20N.A.%20Recurrent%20neural%20network%20grammars%202016"
        },
        {
            "id": "13",
            "entry": "[13] A. Eriguchi, K. Hashimoto, and Y. Tsuruoka. Tree-to-sequence attentional neural machine translation. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eriguchi%2C%20A.%20Hashimoto%2C%20K.%20Tsuruoka%2C%20Y.%20Tree-to-sequence%20attentional%20neural%20machine%20translation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eriguchi%2C%20A.%20Hashimoto%2C%20K.%20Tsuruoka%2C%20Y.%20Tree-to-sequence%20attentional%20neural%20machine%20translation%202016"
        },
        {
            "id": "14",
            "entry": "[14] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pages 820\u2013828, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20D.%20Xia%2C%20Y.%20Qin%2C%20T.%20Wang%2C%20L.%20Dual%20learning%20for%20machine%20translation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20D.%20Xia%2C%20Y.%20Qin%2C%20T.%20Wang%2C%20L.%20Dual%20learning%20for%20machine%20translation%202016"
        },
        {
            "id": "15",
            "entry": "[15] Java2CSharp. Java2csharp. http://sourceforge.net/projects/j2cstranslator/, 2018.",
            "url": "http://sourceforge.net/projects/j2cstranslator/"
        },
        {
            "id": "16",
            "entry": "[16] S. Karaivanov, V. Raychev, and M. Vechev. Phrase-based statistical translation of programming languages. In Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software, pages 173\u2013184. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karaivanov%2C%20S.%20Raychev%2C%20V.%20Vechev%2C%20M.%20Phrase-based%20statistical%20translation%20of%20programming%20languages%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karaivanov%2C%20S.%20Raychev%2C%20V.%20Vechev%2C%20M.%20Phrase-based%20statistical%20translation%20of%20programming%20languages%202014"
        },
        {
            "id": "17",
            "entry": "[17] A. Karpathy, J. Johnson, and L. Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02078"
        },
        {
            "id": "18",
            "entry": "[18] M. J. Kusner, B. Paige, and J. M. Hernandez-Lobato. Grammar variational autoencoder. arXiv preprint arXiv:1703.01925, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01925"
        },
        {
            "id": "19",
            "entry": "[19] W. Ling, E. Grefenstette, K. M. Hermann, T. Kocisky, A. Senior, F. Wang, and P. Blunsom. Latent predictor networks for code generation. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ling%2C%20W.%20Grefenstette%2C%20E.%20Hermann%2C%20K.M.%20Kocisky%2C%20T.%20Blunsom.%20Latent%20predictor%20networks%20for%20code%20generation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ling%2C%20W.%20Grefenstette%2C%20E.%20Hermann%2C%20K.M.%20Kocisky%2C%20T.%20Blunsom.%20Latent%20predictor%20networks%20for%20code%20generation%202016"
        },
        {
            "id": "20",
            "entry": "[20] T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20T.%20Pham%2C%20H.%20Manning%2C%20C.D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20T.%20Pham%2C%20H.%20Manning%2C%20C.D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015"
        },
        {
            "id": "21",
            "entry": "[21] A. T. Nguyen, T. T. Nguyen, and T. N. Nguyen. Lexical statistical machine translation for language migration. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering, pages 651\u2013654. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20A.T.%20Nguyen%2C%20T.T.%20Nguyen%2C%20T.N.%20Lexical%20statistical%20machine%20translation%20for%20language%20migration%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20A.T.%20Nguyen%2C%20T.T.%20Nguyen%2C%20T.N.%20Lexical%20statistical%20machine%20translation%20for%20language%20migration%202013"
        },
        {
            "id": "22",
            "entry": "[22] A. T. Nguyen, T. T. Nguyen, and T. N. Nguyen. Divide-and-conquer approach for multi-phase statistical migration for source code (t). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on, pages 585\u2013596. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20A.T.%20Nguyen%2C%20T.T.%20Nguyen%2C%20T.N.%20Divide-and-conquer%20approach%20for%20multi-phase%20statistical%20migration%20for%20source%20code%20%28t%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20A.T.%20Nguyen%2C%20T.T.%20Nguyen%2C%20T.N.%20Divide-and-conquer%20approach%20for%20multi-phase%20statistical%20migration%20for%20source%20code%20%28t%29%202015"
        },
        {
            "id": "23",
            "entry": "[23] T. D. Nguyen, A. T. Nguyen, and T. N. Nguyen. Mapping api elements for code migration with vector representations. In Software Engineering Companion (ICSE-C), IEEE/ACM International Conference on, pages 756\u2013758. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20T.D.%20Nguyen%2C%20A.T.%20Nguyen%2C%20T.N.%20Mapping%20api%20elements%20for%20code%20migration%20with%20vector%20representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20T.D.%20Nguyen%2C%20A.T.%20Nguyen%2C%20T.N.%20Mapping%20api%20elements%20for%20code%20migration%20with%20vector%20representations%202016"
        },
        {
            "id": "24",
            "entry": "[24] Y. Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and S. Nakamura. Learning to generate pseudo-code from source code using statistical machine translation (t). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on, pages 574\u2013584. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oda%2C%20Y.%20Fudaba%2C%20H.%20Neubig%2C%20G.%20Hata%2C%20H.%20Learning%20to%20generate%20pseudo-code%20from%20source%20code%20using%20statistical%20machine%20translation%20%28t%29%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oda%2C%20Y.%20Fudaba%2C%20H.%20Neubig%2C%20G.%20Hata%2C%20H.%20Learning%20to%20generate%20pseudo-code%20from%20source%20code%20using%20statistical%20machine%20translation%20%28t%29%202015"
        },
        {
            "id": "25",
            "entry": "[25] E. Parisotto, A.-r. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. Neuro-symbolic program synthesis. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20E.%20Mohamed%2C%20A.-r%20Singh%2C%20R.%20Li%2C%20L.%20Neuro-symbolic%20program%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20E.%20Mohamed%2C%20A.-r%20Singh%2C%20R.%20Li%2C%20L.%20Neuro-symbolic%20program%20synthesis%202017"
        },
        {
            "id": "26",
            "entry": "[26] M. Rabinovich, M. Stern, and D. Klein. Abstract syntax networks for code generation and semantic parsing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1139\u20131149, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rabinovich%2C%20M.%20Stern%2C%20M.%20Klein%2C%20D.%20Abstract%20syntax%20networks%20for%20code%20generation%20and%20semantic%20parsing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rabinovich%2C%20M.%20Stern%2C%20M.%20Klein%2C%20D.%20Abstract%20syntax%20networks%20for%20code%20generation%20and%20semantic%20parsing%202017"
        },
        {
            "id": "27",
            "entry": "[27] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 129\u2013136, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20R.%20Lin%2C%20C.C.%20Manning%2C%20C.%20Ng%2C%20A.Y.%20Parsing%20natural%20scenes%20and%20natural%20language%20with%20recursive%20neural%20networks%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20R.%20Lin%2C%20C.C.%20Manning%2C%20C.%20Ng%2C%20A.Y.%20Parsing%20natural%20scenes%20and%20natural%20language%20with%20recursive%20neural%20networks%202011"
        },
        {
            "id": "28",
            "entry": "[28] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the conference on empirical methods in natural language processing, pages 151\u2013161. Association for Computational Linguistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20R.%20Pennington%2C%20J.%20Huang%2C%20E.H.%20Ng%2C%20A.Y.%20Semi-supervised%20recursive%20autoencoders%20for%20predicting%20sentiment%20distributions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20R.%20Pennington%2C%20J.%20Huang%2C%20E.H.%20Ng%2C%20A.Y.%20Semi-supervised%20recursive%20autoencoders%20for%20predicting%20sentiment%20distributions%202011"
        },
        {
            "id": "29",
            "entry": "[29] K. S. Tai, R. Socher, and C. D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tai%2C%20K.S.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Improved%20semantic%20representations%20from%20tree-structured%20long%20short-term%20memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tai%2C%20K.S.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Improved%20semantic%20representations%20from%20tree-structured%20long%20short-term%20memory%20networks%202015"
        },
        {
            "id": "30",
            "entry": "[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20Vaswani%20N%20Shazeer%20N%20Parmar%20J%20Uszkoreit%20L%20Jones%20A%20N%20Gomez%20%C5%81%20Kaiser%20and%20I%20Polosukhin%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2060006010%202017"
        },
        {
            "id": "31",
            "entry": "[31] O. Vinyals, \u0141. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Kaiser%2C%20%C5%81.%20Koo%2C%20T.%20Petrov%2C%20S.%20Grammar%20as%20a%20foreign%20language%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Kaiser%2C%20%C5%81.%20Koo%2C%20T.%20Petrov%2C%20S.%20Grammar%20as%20a%20foreign%20language%202015"
        },
        {
            "id": "32",
            "entry": "[32] P. Yin and G. Neubig. A syntactic neural model for general-purpose code generation. In ACL, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20P.%20Neubig%2C%20G.%20A%20syntactic%20neural%20model%20for%20general-purpose%20code%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20P.%20Neubig%2C%20G.%20A%20syntactic%20neural%20model%20for%20general-purpose%20code%20generation%202017"
        },
        {
            "id": "33",
            "entry": "[33] X. Zhang, L. Lu, and M. Lapata. Top-down tree long short-term memory networks. In Proceedings of NAACL-HLT, pages 310\u2013320, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20X.%20Lu%2C%20L.%20Lapata%2C%20M.%20Top-down%20tree%20long%20short-term%20memory%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20X.%20Lu%2C%20L.%20Lapata%2C%20M.%20Top-down%20tree%20long%20short-term%20memory%20networks%202016"
        },
        {
            "id": "34",
            "entry": "[34] X. Zhu, P. Sobihani, and H. Guo. Long short-term memory over recursive structures. In International Conference on Machine Learning, pages 1604\u20131612, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20X.%20Sobihani%2C%20P.%20Guo%2C%20H.%20Long%20short-term%20memory%20over%20recursive%20structures%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20X.%20Sobihani%2C%20P.%20Guo%2C%20H.%20Long%20short-term%20memory%20over%20recursive%20structures%202015"
        }
    ]
}
