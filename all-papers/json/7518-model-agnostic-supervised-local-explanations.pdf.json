{
    "filename": "7518-model-agnostic-supervised-local-explanations.pdf",
    "metadata": {
        "title": "Model Agnostic Supervised Local Explanations",
        "author": "Gregory Plumb, Denali Molitor, Ameet S. Talwalkar",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7518-model-agnostic-supervised-local-explanations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Specifically, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations."
    },
    "keywords": [
        {
            "term": "predictive model",
            "url": "https://en.wikipedia.org/wiki/predictive_model"
        },
        {
            "term": "influence function",
            "url": "https://en.wikipedia.org/wiki/influence_function"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "gradient boosted regression trees",
            "url": "https://en.wikipedia.org/wiki/Gradient_Boosted_Regression_Trees"
        },
        {
            "term": "feature selection",
            "url": "https://en.wikipedia.org/wiki/feature_selection"
        },
        {
            "term": "decision tree",
            "url": "https://en.wikipedia.org/wiki/decision_tree"
        },
        {
            "term": "linear model",
            "url": "https://en.wikipedia.org/wiki/linear_model"
        },
        {
            "term": "MAPLE",
            "url": "https://en.wikipedia.org/wiki/MAPLE"
        },
        {
            "term": "random forests",
            "url": "https://en.wikipedia.org/wiki/random_forests"
        },
        {
            "term": "black box",
            "url": "https://en.wikipedia.org/wiki/black_box"
        },
        {
            "term": "Root mean squared errors",
            "url": "https://en.wikipedia.org/wiki/Root_Mean_Squared_Error"
        },
        {
            "term": "test point",
            "url": "https://en.wikipedia.org/wiki/test_point"
        }
    ],
    "highlights": [
        "Based on the ideas in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], we define a local explanation at x, denoted expx(), as an interpretable function that approximates pred() well for x in a neighborhood of x where pred() is the predictive model being explained",
        "We present experiments demonstrating that: 1) MAPLE is generally at least as accurate as random forests, gradient boosted regression trees, and SILO 2) MAPLE provides faithful self-explanations, i.e., its local linear model at x is a good local explanation of the prediction at x 3) MAPLE is more accurate in predicting a black-box predictive model\u2019s response than a comparable and popular explanation system, LIME [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] 4) The local training distribution can be used to detect the presence of global patterns in the predictive model.\n5.1",
        "We demonstrate that the local linear model that MAPLE uses to make its prediction doubles as an effective local explanation",
        "We have shown that MAPLE is effective both as a predictive model and an explanation system",
        "We have demonstrated how to use its local training distribution to address two key weaknesses of local explanations: 1) Detecting and modeling global patterns, and 2) Determining whether an exemplar explanation can be applied to a new test point",
        "Some interesting avenues of future work include: 1) Exploiting the fact that MAPLE is a locally linear model to tap into the wide range of approaches that use influence functions to improve model accuracy or identify interesting data points via measures such as leverage and Cook\u2019s distance [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]; 2) Exploring the use of local feature selection approaches with MAPLE, e.g., by considering impurity reductions along the paths through all trees for a given test point; and 3) Exploring methods other than tree ensembles for defining the similarity weights from Eq 2"
    ],
    "key_statements": [
        "Based on the ideas in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], we define a local explanation at x, denoted expx(), as an interpretable function that approximates pred() well for x in a neighborhood of x where pred() is the predictive model being explained",
        "The first outlines interpretability and the second reviews the random forest literature that is most relevant to MAPLE.\n1The source code for our model and experiments is at https://github.com/GDPlumb/MAPLE.\n2.1",
        "While MAPLE does not directly offer global explanations, we show how to use it to detect global patterns in Sec. 4.1, which are the patterns global explanations represent best",
        "We present experiments demonstrating that: 1) MAPLE is generally at least as accurate as random forests, gradient boosted regression trees, and SILO 2) MAPLE provides faithful self-explanations, i.e., its local linear model at x is a good local explanation of the prediction at x 3) MAPLE is more accurate in predicting a black-box predictive model\u2019s response than a comparable and popular explanation system, LIME [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] 4) The local training distribution can be used to detect the presence of global patterns in the predictive model.\n5.1",
        "We demonstrate that the local linear model that MAPLE uses to make its prediction doubles as an effective local explanation",
        "We have shown that MAPLE is effective both as a predictive model and an explanation system",
        "We have demonstrated how to use its local training distribution to address two key weaknesses of local explanations: 1) Detecting and modeling global patterns, and 2) Determining whether an exemplar explanation can be applied to a new test point",
        "Some interesting avenues of future work include: 1) Exploiting the fact that MAPLE is a locally linear model to tap into the wide range of approaches that use influence functions to improve model accuracy or identify interesting data points via measures such as leverage and Cook\u2019s distance [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]; 2) Exploring the use of local feature selection approaches with MAPLE, e.g., by considering impurity reductions along the paths through all trees for a given test point; and 3) Exploring methods other than tree ensembles for defining the similarity weights from Eq 2"
    ],
    "summary": [
        "Based on the ideas in [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], we define a local explanation at x, denoted expx(), as an interpretable function that approximates pred() well for x in a neighborhood of x where pred() is the predictive model being explained.",
        "The most closely related work to ours is LIME [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], which provides a local explanation by fitting a sparse linear model to the predictive model\u2019s response via sampling randomly around the point being explained.",
        "MAPLE combines these procedures by using SILO\u2019s local training distribution and the best d features from DStump to solve the weighted linear regression problem {[xi]Ad , w, yi}.",
        "When MAPLE makes a prediction/local explanation, it uses a local linear model, where the coefficients determine the estimated local effect of each feature.",
        "If the local training distributions are roughly centered around the test point during the grid search and change smoothly during it, as seen in Fig. 2a, the effect of that feature likely does not have significant global patterns.",
        "We present experiments demonstrating that: 1) MAPLE is generally at least as accurate as random forests, GBRT, and SILO 2) MAPLE provides faithful self-explanations, i.e., its local linear model at x is a good local explanation of the prediction at x 3) MAPLE is more accurate in predicting a black-box predictive model\u2019s response than a comparable and popular explanation system, LIME [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] 4) The local training distribution can be used to detect the presence of global patterns in the predictive model.",
        "To demonstrate how the local training distribution can be used to make inferences about the global patterns of the model, we work with the datasets introduced in Fig. 1 of Sec. 1.",
        "We have demonstrated how to use its local training distribution to address two key weaknesses of local explanations: 1) Detecting and modeling global patterns, and 2) Determining whether an exemplar explanation can be applied to a new test point.",
        "Some interesting avenues of future work include: 1) Exploiting the fact that MAPLE is a locally linear model to tap into the wide range of approaches that use influence functions to improve model accuracy or identify interesting data points via measures such as leverage and Cook\u2019s distance [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]; 2) Exploring the use of local feature selection approaches with MAPLE, e.g., by considering impurity reductions along the paths through all trees for a given test point; and 3) Exploring methods other than tree ensembles for defining the similarity weights from Eq 2."
    ],
    "headline": "We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert M\u00c3\u017eller. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun):1803\u20131831, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20and%20Klaus-Robert%20M%C3%83%C5%BEller.%20How%20to%20explain%20individual%20classification%20decisions%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baehrens%2C%20David%20Schroeter%2C%20Timon%20Harmeling%2C%20Stefan%20Kawanabe%2C%20Motoaki%20and%20Klaus-Robert%20M%C3%83%C5%BEller.%20How%20to%20explain%20individual%20classification%20decisions%202010"
        },
        {
            "id": "2",
            "entry": "[2] Jacob Bien and Robert Tibshirani. Prototype selection for interpretable classification. The Annals of Applied Statistics, pages 2403\u20132424, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bien%2C%20Jacob%20Tibshirani%2C%20Robert%20Prototype%20selection%20for%20interpretable%20classification%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bien%2C%20Jacob%20Tibshirani%2C%20Robert%20Prototype%20selection%20for%20interpretable%20classification%202011"
        },
        {
            "id": "3",
            "entry": "[3] Adam Bloniarz, Ameet Talwalkar, Bin Yu, and Christopher Wu. Supervised neighborhoods for distributed nonparametric regression. In Artificial Intelligence and Statistics, pages 1450\u20131459, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bloniarz%2C%20Adam%20Talwalkar%2C%20Ameet%20Yu%2C%20Bin%20Wu%2C%20Christopher%20Supervised%20neighborhoods%20for%20distributed%20nonparametric%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bloniarz%2C%20Adam%20Talwalkar%2C%20Ameet%20Yu%2C%20Bin%20Wu%2C%20Christopher%20Supervised%20neighborhoods%20for%20distributed%20nonparametric%20regression%202016"
        },
        {
            "id": "4",
            "entry": "[4] Leo Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Random%20forests%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Random%20forests%202001"
        },
        {
            "id": "5",
            "entry": "[5] Samprit Chatterjee and Ali S Hadi. Influential observations, high leverage points, and outliers in linear regression. Statistical Science, pages 379\u2013393, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chatterjee%2C%20Samprit%20Hadi%2C%20Ali%20S.%20Influential%20observations%2C%20high%20leverage%20points%2C%20and%20outliers%20in%20linear%20regression%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chatterjee%2C%20Samprit%20Hadi%2C%20Ali%20S.%20Influential%20observations%2C%20high%20leverage%20points%2C%20and%20outliers%20in%20linear%20regression%201986"
        },
        {
            "id": "6",
            "entry": "[6] R Dennis Cook and Sanford Weisberg. Characterizations of an empirical influence function for detecting influential cases in regression. Technometrics, 22(4):495\u2013508, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cook%2C%20R.Dennis%20Weisberg%2C%20Sanford%20Characterizations%20of%20an%20empirical%20influence%20function%20for%20detecting%20influential%20cases%20in%20regression%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cook%2C%20R.Dennis%20Weisberg%2C%20Sanford%20Characterizations%20of%20an%20empirical%20influence%20function%20for%20detecting%20influential%20cases%20in%20regression%201980"
        },
        {
            "id": "7",
            "entry": "[7] Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dheeru%2C%20Dua%20Taniskidou%2C%20Efi%20Karra%20UCI%20machine%20learning%20repository%202017"
        },
        {
            "id": "8",
            "entry": "[8] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doshi-Velez%2C%20Finale%20Kim%2C%20Been%20Towards%20a%20rigorous%20science%20of%20interpretable%20machine%20learning"
        },
        {
            "id": "9",
            "entry": "[9] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189\u20131232, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Friedman%2C%20Jerome%20H.%20Greedy%20function%20approximation%3A%20a%20gradient%20boosting%20machine%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Friedman%2C%20Jerome%20H.%20Greedy%20function%20approximation%3A%20a%20gradient%20boosting%20machine%202001"
        },
        {
            "id": "10",
            "entry": "[10] Shachar Kaufman, Saharon Rosset, Claudia Perlich, and Ori Stitelman. Leakage in data mining: Formulation, detection, and avoidance. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(4):15, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaufman%2C%20Shachar%20Rosset%2C%20Saharon%20Perlich%2C%20Claudia%20Stitelman%2C%20Ori%20Leakage%20in%20data%20mining%3A%20Formulation%2C%20detection%2C%20and%20avoidance%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaufman%2C%20Shachar%20Rosset%2C%20Saharon%20Perlich%2C%20Claudia%20Stitelman%2C%20Ori%20Leakage%20in%20data%20mining%3A%20Formulation%2C%20detection%2C%20and%20avoidance%202012"
        },
        {
            "id": "11",
            "entry": "[11] Jalil Kazemitabar, Arash Amini, Adam Bloniarz, and Ameet S Talwalkar. Variable importance using decision trees. In Advances in Neural Information Processing Systems, pages 425\u2013434, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kazemitabar%2C%20Jalil%20Amini%2C%20Arash%20Bloniarz%2C%20Adam%20Talwalkar%2C%20Ameet%20S.%20Variable%20importance%20using%20decision%20trees%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kazemitabar%2C%20Jalil%20Amini%2C%20Arash%20Bloniarz%2C%20Adam%20Talwalkar%2C%20Ameet%20S.%20Variable%20importance%20using%20decision%20trees%202017"
        },
        {
            "id": "12",
            "entry": "[12] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04730"
        },
        {
            "id": "13",
            "entry": "[13] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1675\u20131684. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakkaraju%2C%20Himabindu%20Bach%2C%20Stephen%20H.%20Leskovec%2C%20Jure%20Interpretable%20decision%20sets%3A%20A%20joint%20framework%20for%20description%20and%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakkaraju%2C%20Himabindu%20Bach%2C%20Stephen%20H.%20Leskovec%2C%20Jure%20Interpretable%20decision%20sets%3A%20A%20joint%20framework%20for%20description%20and%20prediction%202016"
        },
        {
            "id": "14",
            "entry": "[14] Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. Journal of the American Statistical Association, 101(474):578\u2013590, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Yi%20Jeon%2C%20Yongho%20Random%20forests%20and%20adaptive%20nearest%20neighbors%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Yi%20Jeon%2C%20Yongho%20Random%20forests%20and%20adaptive%20nearest%20neighbors%202006"
        },
        {
            "id": "15",
            "entry": "[15] Zachary C Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03490"
        },
        {
            "id": "16",
            "entry": "[16] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pages 4768\u20134777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lundberg%2C%20Scott%20M.%20Lee%2C%20Su-In%20A%20unified%20approach%20to%20interpreting%20model%20predictions%202017"
        },
        {
            "id": "17",
            "entry": "[17] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135\u20131144. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Why%20should%20i%20trust%20you%3F%3A%20Explaining%20the%20predictions%20of%20any%20classifier%202016"
        },
        {
            "id": "18",
            "entry": "[18] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision modelagnostic explanations. AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Anchors%3A%20High-precision%20modelagnostic%20explanations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ribeiro%2C%20Marco%20Tulio%20Singh%2C%20Sameer%20Guestrin%2C%20Carlos%20Anchors%3A%20High-precision%20modelagnostic%20explanations%202018"
        },
        {
            "id": "19",
            "entry": "[19] Erwan Scornet. Random forests and kernel methods. IEEE Transactions on Information Theory, 62(3):1485\u20131500, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scornet%2C%20Erwan%20Random%20forests%20and%20kernel%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scornet%2C%20Erwan%20Random%20forests%20and%20kernel%20methods%202016"
        }
    ]
}
