{
    "filename": "7304-geometrically-coupled-monte-carlo-sampling.pdf",
    "metadata": {
        "title": "Geometrically Coupled Monte Carlo Sampling",
        "author": "Mark Rowland, Krzysztof M. Choromanski, Fran\u00e7ois Chalus, Aldo Pacchiano, Tamas Sarlos, Richard E. Turner, Adrian Weller",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7304-geometrically-coupled-monte-carlo-sampling.pdf"
        },
        "abstract": "Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks. We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies. We compare our new strategies against prior methods for improving sample efficiency, including quasi-Monte Carlo, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling."
    },
    "keywords": [
        {
            "term": "radial basis function",
            "url": "https://en.wikipedia.org/wiki/radial_basis_function"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "euclidean space",
            "url": "https://en.wikipedia.org/wiki/euclidean_space"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "optimal transport",
            "url": "https://en.wikipedia.org/wiki/optimal_transport"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "Evolution Strategies",
            "url": "https://en.wikipedia.org/wiki/Evolution_Strategies"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "monte carlo",
            "url": "https://en.wikipedia.org/wiki/monte_carlo"
        }
    ],
    "highlights": [
        "Monte Carlo (MC) methods are popular in many areas of machine learning, including approximate Bayesian inference [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>], reinforcement learning (RL) [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], and random feature approximations for kernel methods [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>]",
        "To connect to QMC, we show that sets of geometrically coupled Monte Carlo samples give rise to low discrepancy sequences",
        "Having described our notions of optimal couplings in the previous section and obtained several sampling schemes, we provide an interesting connection between our geometrically coupled samples and low discrepancy sequences that are studied in the QMC literature",
        "We provide results on the concentration of zeroth order gradient estimators for reinforcement learning applications, helping to explain their efficacy. This area is one of the main applications of the Geometrically Coupled Monte Carlo methods introduced in Section 2, and we present experiments for these applications in Section 5.1",
        "We have introduced Monte Carlo coupling strategies in Euclidean spaces for improving algorithms that typically operate in a high-dimensional, low-sample regime, demonstrating fundamental connections to multi-marginal transport",
        "We highlight more general solution of the K-optimality criterion, and incorporation of a sampling cost penalty into the corresponding objective as interesting problems left open by this paper"
    ],
    "key_statements": [
        "Monte Carlo (MC) methods are popular in many areas of machine learning, including approximate Bayesian inference [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>], reinforcement learning (RL) [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], and random feature approximations for kernel methods [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>]",
        "There may be an imbalance between the computational cost in drawing Monte Carlo samples from the distribution of interest, and the subsequent cost incurred due to downstream computation with the samples",
        "When a sample represents the configuration of weights in a policy network for an reinforcement learning problem, the cost of computing forward passes, backpropagating gradients through the network, and interacting with the environment, is much greater than drawing the sample itself",
        "We show several settings where the multi-marginal transport problem can be solved analytically",
        "To connect to QMC, we show that sets of geometrically coupled Monte Carlo samples give rise to low discrepancy sequences",
        "We present the first explanation of the success of structured orthogonal matrices for scalable radial basis function kernel approximation via discrepancy theory",
        "We provide exponentially small upper bounds on failure probabilities for estimators of gradients of Gaussian smoothings of blackbox functions based on the gradient sensing mechanism, both for unstructured and orthogonal settings [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "First we develop the theoretical properties of K-optimal couplings, starting with the intimate connection between K-optimal couplings and multi-marginal transport theory [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "Because all samples are regarded as random variables which are constrained to be marginally distributed according to \u03b7, a coupling maintains the usual unbiasedness guarantees of finite-sample Monte Carlo estimators",
        "The problem is intractable to solve analytically in general, so we present several solutions in settings with additional restrictions, either on the number of samples m in the problem, or on the types of couplings considered",
        "We emphasise that we focus on applications where these increases in sampling costs are insignificant relative to the downstream costs of computing with the samples",
        "Having described our notions of optimal couplings in the previous section and obtained several sampling schemes, we provide an interesting connection between our geometrically coupled samples and low discrepancy sequences that are studied in the QMC literature",
        "Our main interest is in the local discrepancy function disrS : Rd \u2192 R parametrised by a given set of samples S = {X1, ..., X|S|} and defined as follows: disrS (u)",
        "We provide results on the concentration of zeroth order gradient estimators for reinforcement learning applications, helping to explain their efficacy. This area is one of the main applications of the Geometrically Coupled Monte Carlo methods introduced in Section 2, and we present experiments for these applications in Section 5.1",
        "We provide the first result showing exponential concentration for the Evolution Strategies (ES) gradient estimator [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>] in this setting",
        "We provide exponential concentration bounds for orthogonal gradient estimators",
        "We provide concentration bounds for gradient estimators of the form:",
        "Figure 2 shows comparison of different Monte Carlo methods using antithetic variant for m = 8, 32, 48 samples given to the Monte Carlo estimator per iteration of the optimization routine",
        "We have introduced Monte Carlo coupling strategies in Euclidean spaces for improving algorithms that typically operate in a high-dimensional, low-sample regime, demonstrating fundamental connections to multi-marginal transport",
        "We highlight more general solution of the K-optimality criterion, and incorporation of a sampling cost penalty into the corresponding objective as interesting problems left open by this paper"
    ],
    "summary": [
        "Monte Carlo (MC) methods are popular in many areas of machine learning, including approximate Bayesian inference [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>], reinforcement learning (RL) [<a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], and random feature approximations for kernel methods [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>].",
        "To connect to QMC, we show that sets of geometrically coupled Monte Carlo samples give rise to low discrepancy sequences.",
        "Given a kernel K : Rd \u00d7 Rd \u2192 R, a K-optimal coupling is a solution to the optimisation problem",
        "The optimisation problem defining K-optimality in Equation (2) is equivalent to the following multi-marginal transport problem: arg min EX1:m\u223c\u03bc",
        "Because all samples are regarded as random variables which are constrained to be marginally distributed according to \u03b7, a coupling maintains the usual unbiasedness guarantees of finite-sample Monte Carlo estimators.",
        "Given a probability distribution \u03b7 \u2208 P(Rd) and a kernel K : Rd \u00d7 Rd \u2192 R, a coupling \u03bc \u2208 \u039bm(\u03b7) is K-optimal iff it is solves the optimisation problem in Expression (4).",
        "We study the objective defining K-optimal couplings, as given in Definition 2.2.",
        "Having described our notions of optimal couplings in the previous section and obtained several sampling schemes, we provide an interesting connection between our geometrically coupled samples and low discrepancy sequences that are studied in the QMC literature.",
        "Our main interest is in the local discrepancy function disrS : Rd \u2192 R parametrised by a given set of samples S = {X1, ..., X|S|} and defined as follows: disrS (u)",
        "Our main result of this section shows that local discrepancy disrF\u03bb(S)(u) for a fixed u \u2208 [0, 1]d is better concentrated around 0 for regular distributions \u03bb if orthogonal sets of samples S are used instead of independent samples.",
        "We conclude that orthogonal samples lead to strictly better guarantees regarding the approximation error of If for functions f with bounded variation and regular distributions \u03bb than standard MC mechanisms.",
        "For the case of pairs of antithetic coupled gradient estimators, one can obtain a similar bound with comparable performance using this technique.",
        "We train neural network policies with d \u2265 96 parameters and optimize the blackbox function F that takes as input parameters of the neural network and outputs the total reward, by applying MC estimators of gradients of Gaussian smoothings of F , as described in Expression (6).",
        "Figure 2 shows comparison of different MC methods using antithetic variant for m = 8, 32, 48 samples given to the MC estimator per iteration of the optimization routine.",
        "We have introduced Monte Carlo coupling strategies in Euclidean spaces for improving algorithms that typically operate in a high-dimensional, low-sample regime, demonstrating fundamental connections to multi-marginal transport.",
        "We highlight more general solution of the K-optimality criterion, and incorporation of a sampling cost penalty into the corresponding objective as interesting problems left open by this paper"
    ],
    "headline": "We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Aistleitner, C. and Dick, J. (2015). Functions of bounded variation, signed measures, and a general Koksma-Hlawka inequality. Acta Arithmetica, 167(2):143\u2013171.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aistleitner%2C%20C.%20Dick%2C%20J.%20Functions%20of%20bounded%20variation%2C%20signed%20measures%2C%20and%20a%20general%20Koksma-Hlawka%20inequality%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aistleitner%2C%20C.%20Dick%2C%20J.%20Functions%20of%20bounded%20variation%2C%20signed%20measures%2C%20and%20a%20general%20Koksma-Hlawka%20inequality%202015"
        },
        {
            "id": "2",
            "entry": "[2] Aliprantis, C. D. and Border, K. C. (2006). Infinite Dimensional Analysis: a Hitchhiker\u2019s Guide. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aliprantis%2C%20C.D.%20Border%2C%20K.C.%20Infinite%20Dimensional%20Analysis%3A%20a%20Hitchhiker%E2%80%99s%20Guide%202006"
        },
        {
            "id": "3",
            "entry": "[3] Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein generative adversarial networks. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] Avron, H., Sindhwani, V., Yang, J., and Mahoney, M. W. (2016). Quasi-Monte Carlo feature maps for shift-invariant kernels. Journal of Machine Learning Research, 17:120:1\u2013120:38.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Avron%2C%20H.%20Sindhwani%2C%20V.%20Yang%2C%20J.%20Mahoney%2C%20M.W.%20Quasi-Monte%20Carlo%20feature%20maps%20for%20shift-invariant%20kernels%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Avron%2C%20H.%20Sindhwani%2C%20V.%20Yang%2C%20J.%20Mahoney%2C%20M.W.%20Quasi-Monte%20Carlo%20feature%20maps%20for%20shift-invariant%20kernels%202016"
        },
        {
            "id": "5",
            "entry": "[5] Bellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., and Munos, R. (2017). The Cramer distance as a solution to biased Wasserstein gradients. arXiv, abs/1705.10743.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10743"
        },
        {
            "id": "6",
            "entry": "[6] Boucheron, S., Lugosi, G., and Massart, P. (2013). Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boucheron%2C%20S.%20Lugosi%2C%20G.%20Massart%2C%20P.%20Concentration%20Inequalities%3A%20A%20Nonasymptotic%20Theory%20of%20Independence%202013"
        },
        {
            "id": "7",
            "entry": "[7] Brauchart, J. S. and Dick, J. (2012). Quasi-monte carlo rules for numerical integration over the unit sphere S2. Numerische Mathematik, 121(3):473\u2013502.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brauchart%2C%20J.S.%20Dick%2C%20J.%20Quasi-monte%20carlo%20rules%20for%20numerical%20integration%20over%20the%20unit%20sphere%20S2%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brauchart%2C%20J.S.%20Dick%2C%20J.%20Quasi-monte%20carlo%20rules%20for%20numerical%20integration%20over%20the%20unit%20sphere%20S2%202012"
        },
        {
            "id": "8",
            "entry": "[8] Briol, F.-X., Oates, C. J., Cockayne, J., Chen, W. Y., and Girolami, M. (2017). On the sampling problem for kernel quadrature. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Briol%2C%20F.-X.%20Oates%2C%20C.J.%20Cockayne%2C%20J.%20Chen%2C%20W.Y.%20On%20the%20sampling%20problem%20for%20kernel%20quadrature%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Briol%2C%20F.-X.%20Oates%2C%20C.J.%20Cockayne%2C%20J.%20Chen%2C%20W.Y.%20On%20the%20sampling%20problem%20for%20kernel%20quadrature%202017"
        },
        {
            "id": "9",
            "entry": "[9] Buchholz, A., Wenzel, F., and Mandt, S. (2018). Quasi-Monte Carlo variational inference. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buchholz%2C%20A.%20Wenzel%2C%20F.%20Mandt%2C%20S.%20Quasi-Monte%20Carlo%20variational%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buchholz%2C%20A.%20Wenzel%2C%20F.%20Mandt%2C%20S.%20Quasi-Monte%20Carlo%20variational%20inference%202018"
        },
        {
            "id": "10",
            "entry": "[10] Chen, Y., Welling, M., and Smola, A. J. (2010). Super-samples from kernel herding. In Uncertainty in Artificial Intelligence (UAI).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Welling%2C%20M.%20Smola%2C%20A.J.%20Super-samples%20from%20kernel%20herding%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Y.%20Welling%2C%20M.%20Smola%2C%20A.J.%20Super-samples%20from%20kernel%20herding%202010"
        },
        {
            "id": "11",
            "entry": "[11] Choromanski, K., Downey, C., and Boots, B. (2018a). Initialization matters: Orthogonal predictive state recurrent neural networks. In International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanski%2C%20K.%20Downey%2C%20C.%20Boots%2C%20B.%20Initialization%20matters%3A%20Orthogonal%20predictive%20state%20recurrent%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanski%2C%20K.%20Downey%2C%20C.%20Boots%2C%20B.%20Initialization%20matters%3A%20Orthogonal%20predictive%20state%20recurrent%20neural%20networks%202018"
        },
        {
            "id": "12",
            "entry": "[12] Choromanski, K., Rowland, M., Sarlos, T., Sindhwani, V., Turner, R., and Weller, A. (2018b). The geometry of random features. In Artificial Intelligence and Statistics (AISTATS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanski%2C%20K.%20Rowland%2C%20M.%20Sarlos%2C%20T.%20Sindhwani%2C%20V.%20The%20geometry%20of%20random%20features%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanski%2C%20K.%20Rowland%2C%20M.%20Sarlos%2C%20T.%20Sindhwani%2C%20V.%20The%20geometry%20of%20random%20features%202018"
        },
        {
            "id": "13",
            "entry": "[13] Choromanski, K., Rowland, M., Sindhwani, V., Turner, R., and Weller, A. (2018c). Structured evolution with compact architectures for scalable policy optimization. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanski%2C%20K.%20Rowland%2C%20M.%20Sindhwani%2C%20V.%20Turner%2C%20R.%20Structured%20evolution%20with%20compact%20architectures%20for%20scalable%20policy%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanski%2C%20K.%20Rowland%2C%20M.%20Sindhwani%2C%20V.%20Turner%2C%20R.%20Structured%20evolution%20with%20compact%20architectures%20for%20scalable%20policy%20optimization%202018"
        },
        {
            "id": "14",
            "entry": "[14] Choromanski, K. M., Rowland, M., and Weller, A. (2017). The unreasonable effectiveness of structured random orthogonal embeddings. In Neural Information Processing Systems (NIPS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanski%2C%20K.M.%20Rowland%2C%20M.%20Weller%2C%20A.%20The%20unreasonable%20effectiveness%20of%20structured%20random%20orthogonal%20embeddings%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanski%2C%20K.M.%20Rowland%2C%20M.%20Weller%2C%20A.%20The%20unreasonable%20effectiveness%20of%20structured%20random%20orthogonal%20embeddings%202017"
        },
        {
            "id": "15",
            "entry": "[15] Coumans, E. and Bai, Y. (2016\u20132018). Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org.",
            "url": "http://pybullet.org"
        },
        {
            "id": "16",
            "entry": "[16] Dick, J., Hinrichs, A., and Pillichshammer, F. (2015). Proof techniques in quasi-Monte Carlo theory. J. Complexity, 31(3):327\u2013371.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dick%2C%20J.%20Hinrichs%2C%20A.%20Pillichshammer%2C%20F.%20Proof%20techniques%20in%20quasi-Monte%20Carlo%20theory%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dick%2C%20J.%20Hinrichs%2C%20A.%20Pillichshammer%2C%20F.%20Proof%20techniques%20in%20quasi-Monte%20Carlo%20theory%202015"
        },
        {
            "id": "17",
            "entry": "[17] Gin, E. and Nickl, R. (2015). Mathematical Foundations of Infinite-Dimensional Statistical Models. Cambridge University Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gin%2C%20E.%20Nickl%2C%20R.%20Mathematical%20Foundations%20of%20Infinite-Dimensional%20Statistical%20Models%202015"
        },
        {
            "id": "18",
            "entry": "[18] Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., and Smola, A. (2012). A kernel two-sample test. J. Mach. Learn. Res., 13:723\u2013773.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Sch%C3%B6lkopf%2C%20B.%20A%20kernel%20two-sample%20test%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20A.%20Borgwardt%2C%20K.M.%20Rasch%2C%20M.J.%20Sch%C3%B6lkopf%2C%20B.%20A%20kernel%20two-sample%20test%202012"
        },
        {
            "id": "19",
            "entry": "[19] Halmos, P. R. (2013). Measure Theory, volume 18. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Halmos%2C%20P.R.%20Measure%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Halmos%2C%20P.R.%20Measure%202013"
        },
        {
            "id": "20",
            "entry": "[20] Halton, J. (1960). On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals. Numerische Mathematik, 2:84\u201390.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Halton%2C%20J.%20On%20the%20efficiency%20of%20certain%20quasi-random%20sequences%20of%20points%20in%20evaluating%20multi-dimensional%20integrals%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Halton%2C%20J.%20On%20the%20efficiency%20of%20certain%20quasi-random%20sequences%20of%20points%20in%20evaluating%20multi-dimensional%20integrals%201960"
        },
        {
            "id": "21",
            "entry": "[21] Hammersley, J. M. and Morton, K. W. (1956). A new Monte Carlo technique: antithetic variates. Mathematical Proceedings of the Cambridge Philosophical Society, 52:449\u2013475.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hammersley%2C%20J.M.%20Morton%2C%20K.W.%20A%20new%20Monte%20Carlo%20technique%3A%20antithetic%20variates%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hammersley%2C%20J.M.%20Morton%2C%20K.W.%20A%20new%20Monte%20Carlo%20technique%3A%20antithetic%20variates%201956"
        },
        {
            "id": "22",
            "entry": "[22] Huszar, F. and Duvenaud, D. K. (2012). Optimally-weighted herding is Bayesian quadrature. In Uncertainty in Artificial Intelligence (UAI).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huszar%2C%20F.%20Duvenaud%2C%20D.K.%20Optimally-weighted%20herding%20is%20Bayesian%20quadrature%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huszar%2C%20F.%20Duvenaud%2C%20D.K.%20Optimally-weighted%20herding%20is%20Bayesian%20quadrature%202012"
        },
        {
            "id": "23",
            "entry": "[23] Kanagawa, M., Hennig, P., Sejdinovic, D., and Sriperumbudur, B. K. (2018). Gaussian processes and kernel methods: A review on connections and equivalences. In Arxiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kanagawa%2C%20M.%20Hennig%2C%20P.%20Sejdinovic%2C%20D.%20Sriperumbudur%2C%20B.K.%20Gaussian%20processes%20and%20kernel%20methods%3A%20A%20review%20on%20connections%20and%20equivalences.%20In%20Arxiv%202018"
        },
        {
            "id": "24",
            "entry": "[24] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20bayes%202014"
        },
        {
            "id": "25",
            "entry": "[25] Lomel\u00ed, M., Rowland, M., Gretton, A., and Ghahramani, Z. (2018). Antithetic and Monte Carlo kernel estimators for partial rankings. In Arxiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lomel%C3%AD%2C%20M.%20Rowland%2C%20M.%20Gretton%2C%20A.%20Ghahramani%2C%20Z.%20Antithetic%20and%20Monte%20Carlo%20kernel%20estimators%20for%20partial%20rankings.%20In%20Arxiv%202018"
        },
        {
            "id": "26",
            "entry": "[26] Mania, H., Guy, A., and Recht, B. (2018). Simple random search provides a competitive approach to reinforcement learning. In Arxiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mania%2C%20H.%20Guy%2C%20A.%20Recht%2C%20B.%20Simple%20random%20search%20provides%20a%20competitive%20approach%20to%20reinforcement%20learning.%20In%20Arxiv%202018"
        },
        {
            "id": "27",
            "entry": "[27] Marcus, M. B. and Shepp, L. A. (1972). Sample behavior of Gaussian processes. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, pages 423\u2013441.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcus%2C%20M.B.%20Shepp%2C%20L.A.%20Sample%20behavior%20of%20Gaussian%20processes%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcus%2C%20M.B.%20Shepp%2C%20L.A.%20Sample%20behavior%20of%20Gaussian%20processes%201972"
        },
        {
            "id": "28",
            "entry": "[28] Niederreiter, H. (1992). Random number generation and quasi-Monte Carlo methods. In Society for Industrial and Applied Mathematics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niederreiter%2C%20H.%20Random%20number%20generation%20and%20quasi-Monte%20Carlo%20methods%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niederreiter%2C%20H.%20Random%20number%20generation%20and%20quasi-Monte%20Carlo%20methods%201992"
        },
        {
            "id": "29",
            "entry": "[29] Paskov, S. (1993). Average case complexity of multivariate integration for smooth functions. Journal of Complexity, 9(2):291 \u2013 312.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paskov%2C%20S.%20Average%20case%20complexity%20of%20multivariate%20integration%20for%20smooth%20functions%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paskov%2C%20S.%20Average%20case%20complexity%20of%20multivariate%20integration%20for%20smooth%20functions%201993"
        },
        {
            "id": "30",
            "entry": "[30] Pass, B. (2014). Multi-marginal optimal transport: Theory and applications. ESAIM: Mathematical Modelling and Numerical Analysis, 49.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pass%2C%20B.%20Multi-marginal%20optimal%20transport%3A%20Theory%20and%20applications%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pass%2C%20B.%20Multi-marginal%20optimal%20transport%3A%20Theory%20and%20applications%202014"
        },
        {
            "id": "31",
            "entry": "[31] Rahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines. In Neural Information Processing Systems (NIPS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "32",
            "entry": "[32] Rasmussen, C. and Ghahramani, Z. (2003). Bayesian Monte Carlo. In Neural Information Processing Systems (NIPS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%20C%20and%20Ghahramani%20Z%202003%20Bayesian%20Monte%20Carlo%20In%20Neural%20Information%20Processing%20Systems%20NIPS",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%20C%20and%20Ghahramani%20Z%202003%20Bayesian%20Monte%20Carlo%20In%20Neural%20Information%20Processing%20Systems%20NIPS"
        },
        {
            "id": "33",
            "entry": "[33] Rasmussen, C. E. and Williams, C. K. I. (2005). Gaussian Processes for Machine Learning. The MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202005"
        },
        {
            "id": "34",
            "entry": "[34] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning (ICML).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.J.%20Mohamed%2C%20S.%20Wierstra%2C%20D.%20Stochastic%20backpropagation%20and%20approximate%20inference%20in%20deep%20generative%20models%202014"
        },
        {
            "id": "35",
            "entry": "[35] Robert, C. P. and Casella, G. (2005). Monte Carlo Statistical Methods. Springer-Verlag.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robert%2C%20C.P.%20Casella%2C%20G.%20Monte%20Carlo%20Statistical%20Methods%202005"
        },
        {
            "id": "36",
            "entry": "[36] Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution strategies as a scalable alternative to reinforcement learning. In arXiv.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Ho%2C%20J.%20Chen%2C%20X.%20Sidor%2C%20S.%20Evolution%20strategies%20as%20a%20scalable%20alternative%20to%20reinforcement%20learning.%20In%20arXiv%202017"
        },
        {
            "id": "37",
            "entry": "[37] Sloan, I. H. and Wozniakowski, H. (1998). When are quasi-Monte Carlo algorithms efficient for high dimensional integrals? J. Complexity, 14(1):1\u201333.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sloan%2C%20I.H.%20Wozniakowski%2C%20H.%20When%20are%20quasi-Monte%20Carlo%20algorithms%20efficient%20for%20high%20dimensional%20integrals%3F%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sloan%2C%20I.H.%20Wozniakowski%2C%20H.%20When%20are%20quasi-Monte%20Carlo%20algorithms%20efficient%20for%20high%20dimensional%20integrals%3F%201998"
        },
        {
            "id": "38",
            "entry": "[38] Stewart, G. W. (1980). The efficient generation of random orthogonal matrices with an application to condition estimators. SIAM Journal on Numerical Analysis, 17(3):403\u2013409.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stewart%2C%20G.W.%20The%20efficient%20generation%20of%20random%20orthogonal%20matrices%20with%20an%20application%20to%20condition%20estimators%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stewart%2C%20G.W.%20The%20efficient%20generation%20of%20random%20orthogonal%20matrices%20with%20an%20application%20to%20condition%20estimators%201980"
        },
        {
            "id": "39",
            "entry": "[39] Talagrand, M. (1987). Regularity of Gaussian processes. Acta Math., 159:99\u2013149.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20M.%20Regularity%20of%20Gaussian%20processes%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Talagrand%2C%20M.%20Regularity%20of%20Gaussian%20processes%201987"
        },
        {
            "id": "40",
            "entry": "[40] Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Welling%2C%20M.%20Teh%2C%20Y.W.%20Bayesian%20learning%20via%20stochastic%20gradient%20Langevin%20dynamics%202011"
        },
        {
            "id": "41",
            "entry": "[41] Yu, F., Suresh, A., Choromanski, K., Holtmann-Rice, D., and Kumar, S. (2016). Orthogonal random features. In Neural Information Processing Systems (NIPS). ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20F.%20Suresh%2C%20A.%20Choromanski%2C%20K.%20Holtmann-Rice%2C%20D.%20Orthogonal%20random%20features%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20F.%20Suresh%2C%20A.%20Choromanski%2C%20K.%20Holtmann-Rice%2C%20D.%20Orthogonal%20random%20features%202016"
        }
    ]
}
