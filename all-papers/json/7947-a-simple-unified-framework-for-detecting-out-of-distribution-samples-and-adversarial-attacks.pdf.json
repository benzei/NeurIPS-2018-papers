{
    "filename": "7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf",
    "metadata": {
        "title": "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks",
        "author": "Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (lowand upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models."
    },
    "keywords": [
        {
            "term": "small number",
            "url": "https://en.wikipedia.org/wiki/small_number"
        },
        {
            "term": "kernel density",
            "url": "https://en.wikipedia.org/wiki/kernel_density"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "effective method",
            "url": "https://en.wikipedia.org/wiki/effective_method"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "true positive rate",
            "url": "https://en.wikipedia.org/wiki/true_positive_rate"
        },
        {
            "term": "receiver operating characteristic",
            "url": "https://en.wikipedia.org/wiki/receiver_operating_characteristic"
        },
        {
            "term": "new class",
            "url": "https://en.wikipedia.org/wiki/new_class"
        },
        {
            "term": "real world",
            "url": "https://en.wikipedia.org/wiki/real_world"
        },
        {
            "term": "true negative rate",
            "url": "https://en.wikipedia.org/wiki/true_negative_rate"
        },
        {
            "term": "posterior distribution",
            "url": "https://en.wikipedia.org/wiki/posterior_distribution"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "Gaussian discriminant analysis",
            "url": "https://en.wikipedia.org/wiki/Gaussian_discriminant_analysis"
        },
        {
            "term": "area under curve",
            "url": "https://en.wikipedia.org/wiki/area_under_curve"
        }
    ],
    "highlights": [
        "Deep neural networks (DNNs) have achieved high accuracy on many classification tasks, e.g., speech recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], object detection [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] and image classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "We propose a simple yet effective method, which is applicable to any pre-trained softmax neural classifier for detecting abnormal test samples including OOD and adversarial ones",
        "We found that our proposed method is more robust in the choice of its hyperparameters as well as against extreme scenarios, e.g., when the training dataset has some noisy, random labels or a small number of data samples",
        "Given deep neural networks (DNNs) with the softmax classifier, we propose a simple yet effective method for detecting abnormal samples such as out-of-distribution (OOD) and adversarial ones",
        "We show that the Mahalanobis distance-based confidence score can be utilized in class-incremental learning tasks [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]: a classifier pre-trained on base classes is progressively updated whenever a new class with corresponding samples occurs",
        "We propose a simple yet effective method for detecting abnormal test samples including both out-of-distribution and adversarial ones"
    ],
    "key_statements": [
        "Deep neural networks (DNNs) have achieved high accuracy on many classification tasks, e.g., speech recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], object detection [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] and image classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "The predictive uncertainty of deep neural networks is closely related to the problem of detecting abnormal samples that are drawn far away from in-distribution statistically or adversarially",
        "Confidence scores were proposed based on density estimators to characterize them in feature spaces of deep neural networks [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "We propose a simple yet effective method, which is applicable to any pre-trained softmax neural classifier for detecting abnormal test samples including OOD and adversarial ones",
        "We demonstrate the effectiveness of the proposed method using deep convolutional neural networks, such as DenseNet [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] and ResNet [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] trained for image classification tasks on various datasets including CIFAR [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], SVHN [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>], ImageNet [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] and LSUN [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>]",
        "We found that our proposed method is more robust in the choice of its hyperparameters as well as against extreme scenarios, e.g., when the training dataset has some noisy, random labels or a small number of data samples",
        "We present a simple method which accommodates a new class at any time by computing the class mean of the new class and updating the tied covariance of all classes",
        "We show that the proposed method outperforms other baseline methods, such as Euclidean distance-based classifier and re-trained softmax classifier",
        "This evidences that our approach have a potential to apply to many other related machine learning tasks, such as active learning [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], ensemble learning [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and few-shot learning [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>].\n2 Mahalanobis distance-based score from generative classifier",
        "Given deep neural networks (DNNs) with the softmax classifier, we propose a simple yet effective method for detecting abnormal samples such as out-of-distribution (OOD) and adversarial ones",
        "We show that the Mahalanobis distance-based confidence score can be utilized in class-incremental learning tasks [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]: a classifier pre-trained on base classes is progressively updated whenever a new class with corresponding samples occurs",
        "We present a Mahalanobis distance-based classifier based on (3), which tries to accommodate a new class by computing and updating the class mean and covariance, as described in Algorithm 2",
        "In order to evaluate the robustness of our method, we measure the detection performance when all hyperparameters are tuned only using in-distribution and adversarial samples generated by FGSM [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "We propose a simple yet effective method for detecting abnormal test samples including both out-of-distribution and adversarial ones",
        "Our main idea is inducing a generative classifier under LDA assumption, and defining new confidence score based on it",
        "We believe that our approach have a potential to apply to many other related machine learning tasks, e.g., active learning [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], ensemble learning [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and few-shot learning [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]"
    ],
    "summary": [
        "Deep neural networks (DNNs) have achieved high accuracy on many classification tasks, e.g., speech recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], object detection [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] and image classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "Confidence scores were proposed based on density estimators to characterize them in feature spaces of DNNs [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "We propose a simple yet effective method, which is applicable to any pre-trained softmax neural classifier for detecting abnormal test samples including OOD and adversarial ones.",
        "We define the confidence score using the Mahalanobis distance with respect to the closest classconditional distribution, where its parameters are chosen as empirical class means and tied empirical covariance of training samples.",
        "Given deep neural networks (DNNs) with the softmax classifier, we propose a simple yet effective method for detecting abnormal samples such as out-of-distribution (OOD) and adversarial ones.",
        "To further improve the performance, we consider measuring and combining the confidence scores from not only the final features but the other low-level features in DNNs. Formally, given training data, we extract the-th hidden features of DNNs, denoted by f(x), and compute their empirical class means and tied covariances, i.e., \u03bcb,c and \u2303b.",
        "We measure the area under ROC (AUROC) curves of the threshold-based detector using the confidence score in (2) computed at different basic blocks of DenseNet [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] trained on CIFAR-10 dataset, where the overall trends on ResNet are similar.",
        "We show that the Mahalanobis distance-based confidence score can be utilized in class-incremental learning tasks [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>]: a classifier pre-trained on base classes is progressively updated whenever a new class with corresponding samples occurs.",
        "For the problem of detecting out-of-distribution (OOD) samples, we train DenseNet with 100 layers and ResNet with 34 layers for classifying CIFAR-10, CIFAR-100 and SVHN datasets.",
        "In order to evaluate the robustness of our method, we measure the detection performance when all hyperparameters are tuned only using in-distribution and adversarial samples generated by FGSM [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>].",
        "For the problem of detecting adversarial samples, we train DenseNet and ResNet for classifying CIFAR-10, CIFAR-100 and SVHN datasets, and the corresponding test dataset is used as the",
        "Our proposed Mahalanobis distance-based classifier outperforms the other methods by a significant margin, as the number of new classes increases, there is a crossing in the right figure of Figure 4(b) in small regimes.",
        "We propose a simple yet effective method for detecting abnormal test samples including both out-of-distribution and adversarial ones.",
        "With calibration techniques such as input pre-processing and feature ensemble, our method performs very strongly across multiple tasks: detecting out-of-distribution samples, detecting adversarial attacks and classincremental learning.",
        "We believe that our approach have a potential to apply to many other related machine learning tasks, e.g., active learning [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], ensemble learning [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and few-shot learning [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]"
    ],
    "headline": "We propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Amodei, Dario, Ananthanarayanan, Sundaram, Anubhai, Rishita, Bai, Jingliang, Battenberg, Eric, Case, Carl, Casper, Jared, Catanzaro, Bryan, Cheng, Qiang, Chen, Guoliang, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amodei%2C%20Dario%20Ananthanarayanan%2C%20Sundaram%20Anubhai%2C%20Rishita%20Bai%2C%20Jingliang%20Deep%20speech%202%3A%20End-to-end%20speech%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amodei%2C%20Dario%20Ananthanarayanan%2C%20Sundaram%20Anubhai%2C%20Rishita%20Bai%2C%20Jingliang%20Deep%20speech%202%3A%20End-to-end%20speech%20recognition%202016"
        },
        {
            "id": "2",
            "entry": "[2] Amodei, Dario, Olah, Chris, Steinhardt, Jacob, Christiano, Paul, Schulman, John, and Mane, Dan. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "3",
            "entry": "[3] Carlini, Nicholas and Wagner, David. Adversarial examples are not easily detected: Bypassing ten detection methods. In ACM workshop on AISec, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017"
        },
        {
            "id": "4",
            "entry": "[4] Chrabaszcz, Patryk, Loshchilov, Ilya, and Hutter, Frank. A downsampled variant of imagenet as an alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08819"
        },
        {
            "id": "5",
            "entry": "[5] Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "6",
            "entry": "[6] Evtimov, Ivan, Eykholt, Kevin, Fernandes, Earlence, Kohno, Tadayoshi, Li, Bo, Prakash, Atul, Rahmati, Amir, and Song, Dawn. Robust physical-world attacks on machine learning models. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evtimov%2C%20Ivan%20Eykholt%2C%20Kevin%20Fernandes%2C%20Earlence%20Kohno%2C%20Tadayoshi%20Robust%20physical-world%20attacks%20on%20machine%20learning%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evtimov%2C%20Ivan%20Eykholt%2C%20Kevin%20Fernandes%2C%20Earlence%20Kohno%2C%20Tadayoshi%20Robust%20physical-world%20attacks%20on%20machine%20learning%20models%202018"
        },
        {
            "id": "7",
            "entry": "[7] Feinman, Reuben, Curtin, Ryan R, Shintre, Saurabh, and Gardner, Andrew B. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00410"
        },
        {
            "id": "8",
            "entry": "[8] Gal, Yarin, Islam, Riashat, and Ghahramani, Zoubin. Deep bayesian active learning with image data. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Islam%2C%20Riashat%20Ghahramani%2C%20Zoubin%20Deep%20bayesian%20active%20learning%20with%20image%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Islam%2C%20Riashat%20Ghahramani%2C%20Zoubin%20Deep%20bayesian%20active%20learning%20with%20image%20data%202017"
        },
        {
            "id": "9",
            "entry": "[9] Girshick, Ross. Fast r-cnn. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girshick%20Ross%20Fast%20rcnn%20In%20ICCV%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girshick%20Ross%20Fast%20rcnn%20In%20ICCV%202015"
        },
        {
            "id": "10",
            "entry": "[10] Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial examples. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "11",
            "entry": "[11] Guo, Chuan, Rana, Mayank, Cisse, Moustapha, and van der Maaten, Laurens. Countering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00117"
        },
        {
            "id": "12",
            "entry": "[12] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "13",
            "entry": "[13] Hendrycks, Dan and Gimpel, Kevin. A baseline for detecting misclassified and out-ofdistribution examples in neural networks. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hendrycks%2C%20Dan%20Gimpel%2C%20Kevin%20A%20baseline%20for%20detecting%20misclassified%20and%20out-ofdistribution%20examples%20in%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hendrycks%2C%20Dan%20Gimpel%2C%20Kevin%20A%20baseline%20for%20detecting%20misclassified%20and%20out-ofdistribution%20examples%20in%20neural%20networks%202017"
        },
        {
            "id": "14",
            "entry": "[14] Huang, Gao and Liu, Zhuang. Densely connected convolutional networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "15",
            "entry": "[15] Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images"
        },
        {
            "id": "16",
            "entry": "[16] Kurakin, Alexey, Goodfellow, Ian, and Bengio, Samy. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "17",
            "entry": "[17] Lasserre, Julia A, Bishop, Christopher M, and Minka, Thomas P. Principled hybrids of generative and discriminative models. In CVPR, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lasserre%2C%20Julia%20A.%20Bishop%2C%20Christopher%20M.%20Minka%2C%20Thomas%20P.%20Principled%20hybrids%20of%20generative%20and%20discriminative%20models%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lasserre%2C%20Julia%20A.%20Bishop%2C%20Christopher%20M.%20Minka%2C%20Thomas%20P.%20Principled%20hybrids%20of%20generative%20and%20discriminative%20models%202006"
        },
        {
            "id": "18",
            "entry": "[18] Lee, Kibok, Lee, Kimin, Min, Kyle, Zhang, Yuting, Shin, Jinwoo, and Lee, Honglak. Hierarchical novelty detection for visual object recognition. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Kibok%20Lee%2C%20Kimin%20Min%2C%20Kyle%20Zhang%2C%20Yuting%20Hierarchical%20novelty%20detection%20for%20visual%20object%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Kibok%20Lee%2C%20Kimin%20Min%2C%20Kyle%20Zhang%2C%20Yuting%20Hierarchical%20novelty%20detection%20for%20visual%20object%20recognition%202018"
        },
        {
            "id": "19",
            "entry": "[19] Lee, Kimin, Hwang, Changho, Park, KyoungSoo, and Shin, Jinwoo. Confident multiple choice learning. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Kimin%20Hwang%2C%20Changho%20Park%2C%20KyoungSoo%20Shin%2C%20Jinwoo%20Confident%20multiple%20choice%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Kimin%20Hwang%2C%20Changho%20Park%2C%20KyoungSoo%20Shin%2C%20Jinwoo%20Confident%20multiple%20choice%20learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] Lee, Kimin, Lee, Honglak, Lee, Kibok, and Shin, Jinwoo. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Kimin%20Lee%2C%20Honglak%20Lee%2C%20Kibok%20Shin%2C%20Jinwoo%20Training%20confidence-calibrated%20classifiers%20for%20detecting%20out-of-distribution%20samples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Kimin%20Lee%2C%20Honglak%20Lee%2C%20Kibok%20Shin%2C%20Jinwoo%20Training%20confidence-calibrated%20classifiers%20for%20detecting%20out-of-distribution%20samples%202018"
        },
        {
            "id": "21",
            "entry": "[21] Liang, Shiyu, Li, Yixuan, and Srikant, R. Principled detection of out-of-distribution examples in neural networks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Shiyu%20Li%2C%20Yixuan%20Srikant%2C%20R.%20Principled%20detection%20of%20out-of-distribution%20examples%20in%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Shiyu%20Li%2C%20Yixuan%20Srikant%2C%20R.%20Principled%20detection%20of%20out-of-distribution%20examples%20in%20neural%20networks%202018"
        },
        {
            "id": "22",
            "entry": "[22] Ma, Xingjun, Li, Bo, Wang, Yisen, Erfani, Sarah M, Wijewickrema, Sudanthi, Houle, Michael E, Schoenebeck, Grant, Song, Dawn, and Bailey, James. Characterizing adversarial subspaces using local intrinsic dimensionality. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Xingjun%20Li%2C%20Bo%20Wang%2C%20Yisen%20Erfani%2C%20Sarah%20M.%20Characterizing%20adversarial%20subspaces%20using%20local%20intrinsic%20dimensionality%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Xingjun%20Li%2C%20Bo%20Wang%2C%20Yisen%20Erfani%2C%20Sarah%20M.%20Characterizing%20adversarial%20subspaces%20using%20local%20intrinsic%20dimensionality%202018"
        },
        {
            "id": "23",
            "entry": "[23] Maaten, Laurens van der and Hinton, Geoffrey. Visualizing data using t-sne. Journal of machine learning research, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maaten%2C%20Laurens%20van%20der%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maaten%2C%20Laurens%20van%20der%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "24",
            "entry": "[24] McCloskey, Michael and Cohen, Neal J. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation. Elsevier, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCloskey%2C%20Michael%20Cohen%2C%20Neal%20J.%20Catastrophic%20interference%20in%20connectionist%20networks%3A%20The%20sequential%20learning%20problem.%20In%20Psychology%20of%20learning%20and%20motivation%201989"
        },
        {
            "id": "25",
            "entry": "[25] Mensink, Thomas, Verbeek, Jakob, Perronnin, Florent, and Csurka, Gabriela. Distance-based image classification: Generalizing to new classes at near-zero cost. IEEE transactions on pattern analysis and machine intelligence, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mensink%2C%20Thomas%20Verbeek%2C%20Jakob%20Perronnin%2C%20Florent%20Csurka%2C%20Gabriela%20Distance-based%20image%20classification%3A%20Generalizing%20to%20new%20classes%20at%20near-zero%20cost.%20IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence%202013"
        },
        {
            "id": "26",
            "entry": "[26] Moosavi Dezfooli, Seyed Mohsen, Fawzi, Alhussein, and Frossard, Pascal. Deepfool: a simple and accurate method to fool deep neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dezfooli%2C%20Moosavi%20Mohsen%2C%20Seyed%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dezfooli%2C%20Moosavi%20Mohsen%2C%20Seyed%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20Deepfool%3A%20a%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "27",
            "entry": "[27] Murphy, Kevin P. Machine learning: a probabilistic perspective. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murphy%2C%20Kevin%20P.%20Machine%20learning%3A%20a%20probabilistic%20perspective%202012"
        },
        {
            "id": "28",
            "entry": "[28] Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Reading digits in natural images with unsupervised feature learning. In NIPS workshop, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "29",
            "entry": "[29] Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander. icarl: Incremental classifier and representation learning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rebuffi%2C%20Sylvestre-Alvise%20Kolesnikov%2C%20Alexander%20icarl%3A%20Incremental%20classifier%20and%20representation%20learning%202017"
        },
        {
            "id": "30",
            "entry": "[30] Sharif, Mahmood, Bhagavatula, Sruti, Bauer, Lujo, and Reiter, Michael K. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In ACM SIGSAC, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "31",
            "entry": "[31] Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et al. Matching networks for one shot learning. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20Oriol%20Blundell%2C%20Charles%20Lillicrap%2C%20Tim%20Wierstra%2C%20Daan%20Matching%20networks%20for%20one%20shot%20learning%202016"
        },
        {
            "id": "32",
            "entry": "[32] Yu, Fisher, Seff, Ari, Zhang, Yinda, Song, Shuran, Funkhouser, Thomas, and Xiao, Jianxiong. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        }
    ]
}
