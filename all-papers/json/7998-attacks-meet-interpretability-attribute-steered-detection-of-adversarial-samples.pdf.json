{
    "filename": "7998-attacks-meet-interpretability-attribute-steered-detection-of-adversarial-samples.pdf",
    "metadata": {
        "title": "Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples",
        "author": "Guanhong Tao, Shiqing Ma, Yingqi Liu, Xiangyu Zhang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7998-attacks-meet-interpretability-attribute-steered-detection-of-adversarial-samples.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible features/attributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94% detection accuracy for 7 different kinds of attacks with 9.91% false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55% accuracy with 23.3% false positives."
    },
    "keywords": [
        {
            "term": "Deep neural networks",
            "url": "https://en.wikipedia.org/wiki/Deep_neural_networks"
        },
        {
            "term": "face recognition",
            "url": "https://en.wikipedia.org/wiki/face_recognition"
        },
        {
            "term": "false positive",
            "url": "https://en.wikipedia.org/wiki/false_positive"
        },
        {
            "term": "receptive field",
            "url": "https://en.wikipedia.org/wiki/receptive_field"
        }
    ],
    "highlights": [
        "Work<br/><br/> Model Interpretation: Interpreting Deep neural networks is a long-standing challenge",
        "We propose an adversarial sample detection technique called AmI (Attacks meet Interpretability) for face recognition systems (FRSes) based on interpretability",
        "Our technique consists of two key steps: (1) identifying neurons that correspond to human perceptible attributes, called attribute witnesses, by analyzing the FRS\u2019s behavior on a small set of training inputs; (2) given a test image, observe the activation values of attribute witnesses and their comparison with the activation values of other neurons, to predict if the image is adversarial",
        "The base image and each substituted image are executed by the FRS and the internal neurons that have non-trivial activation value differences are extracted as a superset of the witness of the replaced attribute",
        "We propose an adversarial sample detection technique AmI in the context of face recognition, by leveraging interpretability of Deep neural networks",
        "Our technique features a number of critical design choices: novel bi-directional correspondence inference between face attributes and internal neurons; using attribute level mutation instead of pixel level mutation; and neuron strengthening and weakening"
    ],
    "key_statements": [
        "Work<br/><br/> Model Interpretation: Interpreting Deep neural networks is a long-standing challenge",
        "We propose an adversarial sample detection technique called AmI (Attacks meet Interpretability) for face recognition systems (FRSes) based on interpretability",
        "AmI first extracts a set of neurons that are critical to individual human face attributes leveraging a small set of training images",
        "We propose a novel technique AmI that utilizes interpretability of face recognition systems to detect adversarial samples",
        "Existing work investigates interpretability from multiple perspectives and feature visualization is one of them.While explaining Deep neural networks largely lies in understanding the role of individual neurons and layers, feature visualization tackles the problem by generating neurons\u2019 receptive fields, which are input regions of interest maximizing the activation value of a neuron [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>\u2013<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "If the receptive field of a set of neurons overlaps with some human-interpretable object, the set of neurons is labeled with the corresponding object and regarded as the detector of the object",
        "The reasoning in feature visualization and in [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] is one-way, that is, pixel perturbations lead to neuron value changes",
        "Our technique consists of two key steps: (1) identifying neurons that correspond to human perceptible attributes, called attribute witnesses, by analyzing the FRS\u2019s behavior on a small set of training inputs; (2) given a test image, observe the activation values of attribute witnesses and their comparison with the activation values of other neurons, to predict if the image is adversarial",
        "The base image and each substituted image are executed by the FRS and the internal neurons that have non-trivial activation value differences are extracted as a superset of the witness of the replaced attribute",
        "To support our design choice of using bi-directional reasoning in extracting attribute witnesses and using neuron strengthening and weakening, we conducted a few additional experiments: (1) we use only attribute substitution to extract witnesses and build the attribute-steered model for detection with results shown in the AS row of Table 3; (2) we use only attribute preservation (AP); (3) we only weaken non-witnesses (WKN); and (4) we only strengthen witnesses (STN)",
        "Our results show that AS or attribute preservation alone can detect adversarial samples with good accuracy, their false positive rates are very high (i.e., 20.41% and 30.61%)",
        "The reason is that they extract too many neurons as the witnesses, which are strengthened in the new model, leading to wrong classification results for benign inputs",
        "We propose an adversarial sample detection technique AmI in the context of face recognition, by leveraging interpretability of Deep neural networks",
        "Our technique features a number of critical design choices: novel bi-directional correspondence inference between face attributes and internal neurons; using attribute level mutation instead of pixel level mutation; and neuron strengthening and weakening",
        "Results demonstrate that AmI is highly effective, superseding the state-of-the-art in the targeted context"
    ],
    "summary": [
        "Work<br/><br/> Model Interpretation: Interpreting DNNs is a long-standing challenge.",
        "We propose an adversarial sample detection technique called AmI (Attacks meet Interpretability) for face recognition systems (FRSes) based on interpretability.",
        "AmI first extracts a set of neurons that are critical to individual human face attributes leveraging a small set of training images.",
        "Our technique consists of two key steps: (1) identifying neurons that correspond to human perceptible attributes, called attribute witnesses, by analyzing the FRS\u2019s behavior on a small set of training inputs; (2) given a test image, observe the activation values of attribute witnesses and their comparison with the activation values of other neurons, to predict if the image is adversarial.",
        "For each test input, we use both the original model and the attribute-steered model to predict its identity.",
        "The base image and each substituted image are executed by the FRS and the internal neurons that have non-trivial activation value differences are extracted as a superset of the witness of the replaced attribute.",
        "We call the resulted images the attribute-preserved images, which are used to extract the neurons whose activation values have no or small differences compared to those of the base image in step D .",
        "For a benign input that the FRS can reason about based on attributes, the new model would be able to produce consistent prediction result as the original model.",
        "To support our design choice of using bi-directional reasoning in extracting attribute witnesses and using neuron strengthening and weakening, we conducted a few additional experiments: (1) we use only attribute substitution to extract witnesses and build the attribute-steered model for detection with results shown in the AS row of Table 3; (2) we use only attribute preservation (AP); (3) we only weaken non-witnesses (WKN); and (4) we only strengthen witnesses (STN).",
        "The reason is that they extract too many neurons as the witnesses, which are strengthened in the new model, leading to wrong classification results for benign inputs.",
        "We further investigate the robustness of AmI employing different sets of witnesses by excluding those of some attribute during adversary detection.",
        "We propose an adversarial sample detection technique AmI in the context of face recognition, by leveraging interpretability of DNNs. Our technique features a number of critical design choices: novel bi-directional correspondence inference between face attributes and internal neurons; using attribute level mutation instead of pixel level mutation; and neuron strengthening and weakening.",
        "Results demonstrate that AmI is highly effective, superseding the state-of-the-art in the targeted context"
    ],
    "headline": "We argue that adversarial sample attacks are deeply entangled with interpretability of Deep neural networks models: while classification results on benign inputs can be reasoned based on the human perceptible features/attributes, results on adversarial samples can hardly be explained",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A Convolutional Neural Network for Modelling Sentences. arXiv preprint arXiv:1404.2188, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1404.2188"
        },
        {
            "id": "2",
            "entry": "[2] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning Deep Features for Scene Recognition Using Places Database. In Advances in Neural Information Processing Systems (NIPS), pages 487\u2013495, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Bolei%20Lapedriza%2C%20Agata%20Xiao%2C%20Jianxiong%20Torralba%2C%20Antonio%20Learning%20Deep%20Features%20for%20Scene%20Recognition%20Using%20Places%20Database%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Bolei%20Lapedriza%2C%20Agata%20Xiao%2C%20Jianxiong%20Torralba%2C%20Antonio%20Learning%20Deep%20Features%20for%20Scene%20Recognition%20Using%20Places%20Database%202014"
        },
        {
            "id": "3",
            "entry": "[3] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards RealTime Object Detection with Region Proposal Networks. In Advances in Neural Information Processing Systems (NIPS), pages 91\u201399, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20RealTime%20Object%20Detection%20with%20Region%20Proposal%20Networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ren%2C%20Shaoqing%20He%2C%20Kaiming%20Girshick%2C%20Ross%20Sun%2C%20Jian%20Faster%20R-CNN%3A%20Towards%20RealTime%20Object%20Detection%20with%20Region%20Proposal%20Networks%202015"
        },
        {
            "id": "4",
            "entry": "[4] Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS), pages 1285\u20131298, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Min%20Li%2C%20Feifei%20Zheng%2C%20Guineng%20Srikumar%2C%20Vivek%20DeepLog%3A%20Anomaly%20Detection%20and%20Diagnosis%20from%20System%20Logs%20through%20Deep%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Min%20Li%2C%20Feifei%20Zheng%2C%20Guineng%20Srikumar%2C%20Vivek%20DeepLog%3A%20Anomaly%20Detection%20and%20Diagnosis%20from%20System%20Logs%20through%20Deep%20Learning%202017"
        },
        {
            "id": "5",
            "entry": "[5] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing Properties of Neural Networks. In International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Christian%20Szegedy%20Wojciech%20Zaremba%20Ilya%20Sutskever%20Joan%20Bruna%20Dumitru%20Erhan%20Ian%20Goodfellow%20and%20Rob%20Fergus%20Intriguing%20Properties%20of%20Neural%20Networks%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Christian%20Szegedy%20Wojciech%20Zaremba%20Ilya%20Sutskever%20Joan%20Bruna%20Dumitru%20Erhan%20Ian%20Goodfellow%20and%20Rob%20Fergus%20Intriguing%20Properties%20of%20Neural%20Networks%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202014"
        },
        {
            "id": "6",
            "entry": "[6] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "7",
            "entry": "[7] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial Examples in the Physical World. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "8",
            "entry": "[8] Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks. In IEEE Symposium on Security and Privacy (S&P), pages 39\u201357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20Evaluating%20the%20Robustness%20of%20Neural%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Towards%20Evaluating%20the%20Robustness%20of%20Neural%20Networks%202017"
        },
        {
            "id": "9",
            "entry": "[9] Nicholas Carlini and David Wagner. Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (AISec), pages 3\u201314, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20Examples%20Are%20Not%20Easily%20Detected%3A%20Bypassing%20Ten%20Detection%20Methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20Adversarial%20Examples%20Are%20Not%20Easily%20Detected%3A%20Bypassing%20Ten%20Detection%20Methods%202017"
        },
        {
            "id": "10",
            "entry": "[10] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning Attack on Neural Networks. In Proceedings of the 25nd Annual Network and Distributed System Security Symposium (NDSS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yingqi%20Liu%20Shiqing%20Ma%20Yousra%20Aafer%20WenChuan%20Lee%20Juan%20Zhai%20Weihang%20Wang%20and%20Xiangyu%20Zhang%20Trojaning%20Attack%20on%20Neural%20Networks%20In%20Proceedings%20of%20the%2025nd%20Annual%20Network%20and%20Distributed%20System%20Security%20Symposium%20NDSS%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yingqi%20Liu%20Shiqing%20Ma%20Yousra%20Aafer%20WenChuan%20Lee%20Juan%20Zhai%20Weihang%20Wang%20and%20Xiangyu%20Zhang%20Trojaning%20Attack%20on%20Neural%20Networks%20In%20Proceedings%20of%20the%2025nd%20Annual%20Network%20and%20Distributed%20System%20Security%20Symposium%20NDSS%202018"
        },
        {
            "id": "11",
            "entry": "[11] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. DeepXplore: Automated Whitebox Testing of Deep Learning Systems. In Proceedings of the 26th Symposium on Operating Systems Principles (SOSP), pages 1\u201318, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pei%2C%20Kexin%20Cao%2C%20Yinzhi%20Yang%2C%20Junfeng%20Jana%2C%20Suman%20DeepXplore%3A%20Automated%20Whitebox%20Testing%20of%20Deep%20Learning%20Systems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pei%2C%20Kexin%20Cao%2C%20Yinzhi%20Yang%2C%20Junfeng%20Jana%2C%20Suman%20DeepXplore%3A%20Automated%20Whitebox%20Testing%20of%20Deep%20Learning%20Systems%202017"
        },
        {
            "id": "12",
            "entry": "[12] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "13",
            "entry": "[13] Shixiang Gu and Luca Rigazio. Towards Deep Neural Network Architectures Robust to Adversarial Examples. arXiv preprint arXiv:1412.5068, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.5068"
        },
        {
            "id": "14",
            "entry": "[14] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. In IEEE Symposium on Security and Privacy (S&P), pages 582\u2013597, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20Defense%20to%20Adversarial%20Perturbations%20against%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20Defense%20to%20Adversarial%20Perturbations%20against%202016"
        },
        {
            "id": "15",
            "entry": "[15] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting Adversarial Samples from Artifacts. arXiv preprint arXiv:1703.00410, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00410"
        },
        {
            "id": "16",
            "entry": "[16] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the (Statistical) Detection of Adversarial Examples. arXiv preprint arXiv:1702.06280, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.06280"
        },
        {
            "id": "17",
            "entry": "[17] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On Detecting Adversarial Perturbations. arXiv preprint arXiv:1702.04267, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04267"
        },
        {
            "id": "18",
            "entry": "[18] Weilin Xu, David Evans, and Yanjun Qi. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. In Proceedings of the 25nd Annual Network and Distributed System Security Symposium (NDSS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Weilin%20Evans%2C%20David%20Qi%2C%20Yanjun%20Feature%20Squeezing%3A%20Detecting%20Adversarial%20Examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Weilin%20Evans%2C%20David%20Qi%2C%20Yanjun%20Feature%20Squeezing%3A%20Detecting%20Adversarial%20Examples%202018"
        },
        {
            "id": "19",
            "entry": "[19] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. Deep Face Recognition. In Proceedings of the British Machine Vision Conference (BMVC), pages 1\u201312, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omkar%20M%20Parkhi%20Andrea%20Vedaldi%20Andrew%20Zisserman%20et%20al%20Deep%20Face%20Recognition%20In%20Proceedings%20of%20the%20British%20Machine%20Vision%20Conference%20BMVC%20pages%20112%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Omkar%20M%20Parkhi%20Andrea%20Vedaldi%20Andrew%20Zisserman%20et%20al%20Deep%20Face%20Recognition%20In%20Proceedings%20of%20the%20British%20Machine%20Vision%20Conference%20BMVC%20pages%20112%202015"
        },
        {
            "id": "20",
            "entry": "[20] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing Higher-Layer Features of A Deep Network. University of Montreal, 1341(3):1, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erhan%2C%20Dumitru%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Visualizing%20Higher-Layer%20Features%20of%20A%20Deep%20Network%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erhan%2C%20Dumitru%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Visualizing%20Higher-Layer%20Features%20of%20A%20Deep%20Network%202009"
        },
        {
            "id": "21",
            "entry": "[21] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. arXiv preprint arXiv:1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "22",
            "entry": "[22] Aravindh Mahendran and Andrea Vedaldi. Visualizing Deep Convolutional Neural Networks Using Natural Pre-images. International Journal of Computer Vision, 120(3):233\u2013255, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Visualizing%20Deep%20Convolutional%20Neural%20Networks%20Using%20Natural%20Pre-images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahendran%2C%20Aravindh%20Vedaldi%2C%20Andrea%20Visualizing%20Deep%20Convolutional%20Neural%20Networks%20Using%20Natural%20Pre-images%202016"
        },
        {
            "id": "23",
            "entry": "[23] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding Neural Networks Through Deep Visualization. arXiv preprint arXiv:1506.06579, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.06579"
        },
        {
            "id": "24",
            "entry": "[24] Anh Nguyen, Jason Yosinski, and Jeff Clune. Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks. arXiv preprint arXiv:1602.03616, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.03616"
        },
        {
            "id": "25",
            "entry": "[25] AmIAttribute. AmIAttribute/AmI. https://github.com/AmIAttribute/AmI, 2018. (Accessed on 05/18/2018).",
            "url": "https://github.com/AmIAttribute/AmI"
        },
        {
            "id": "26",
            "entry": "[26] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network Dissection: Quantifying Interpretability of Deep Visual Representations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3319\u20133327, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20Dissection%3A%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bau%2C%20David%20Zhou%2C%20Bolei%20Khosla%2C%20Aditya%20Oliva%2C%20Aude%20Network%20Dissection%3A%20Quantifying%20Interpretability%20of%20Deep%20Visual%20Representations%202017"
        },
        {
            "id": "27",
            "entry": "[27] TB Brown, D Man\u00e9, A Roy, M Abadi, and J Gilmer. Adversarial Patch. arXiv preprint cs.CV/1712.09665, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20T.B.%20Man%C3%A9%2C%20D.%20Roy%2C%20A.%20Abadi%2C%20M.%20Adversarial%20Patch%202017"
        },
        {
            "id": "28",
            "entry": "[28] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS), pages 1528\u20131540, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20Crime%3A%20Real%20and%20Stealthy%20Attacks%20on%20State-of-the-Art%20Face%20Recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20Crime%3A%20Real%20and%20Stealthy%20Attacks%20on%20State-of-the-Art%20Face%20Recognition%202016"
        },
        {
            "id": "29",
            "entry": "[29] Davis E King. Dlib-ml: A Machine Learning Toolkit. Journal of Machine Learning Research (JMLR), 10(Jul):1755\u20131758, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=King%2C%20Davis%20E.%20Dlib-ml%3A%20A%20Machine%20Learning%20Toolkit%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=King%2C%20Davis%20E.%20Dlib-ml%3A%20A%20Machine%20Learning%20Toolkit%202009"
        },
        {
            "id": "30",
            "entry": "[30] Hiroyuki Takeda, Sina Farsiu, and Peyman Milanfar. Kernel Regression for Image Processing and Reconstruction. IEEE Transactions on Image Processing, 16(2):349\u2013366, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takeda%2C%20Hiroyuki%20Farsiu%2C%20Sina%20Milanfar%2C%20Peyman%20Kernel%20Regression%20for%20Image%20Processing%20and%20Reconstruction%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takeda%2C%20Hiroyuki%20Farsiu%2C%20Sina%20Milanfar%2C%20Peyman%20Kernel%20Regression%20for%20Image%20Processing%20and%20Reconstruction%202007"
        },
        {
            "id": "31",
            "entry": "[31] Antoni Buades, Bartomeu Coll, and J-M Morel. A Non-local Algorithm for Image Denoising. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, pages 60\u201365, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buades%2C%20Antoni%20Coll%2C%20Bartomeu%20Morel%2C%20J.-M.%20A%20Non-local%20Algorithm%20for%20Image%20Denoising%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buades%2C%20Antoni%20Coll%2C%20Bartomeu%20Morel%2C%20J.-M.%20A%20Non-local%20Algorithm%20for%20Image%20Denoising%202005"
        },
        {
            "id": "32",
            "entry": "[32] Nasser M Nasrabadi. Pattern Recognition and Machine Learning. Journal of Electronic Imaging, 16(4):049901, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nasrabadi%2C%20Nasser%20M.%20Pattern%20Recognition%20and%20Machine%20Learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nasrabadi%2C%20Nasser%20M.%20Pattern%20Recognition%20and%20Machine%20Learning%202007"
        },
        {
            "id": "33",
            "entry": "[33] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Technical report, Technical Report 07-49, University of Massachusetts, Amherst, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gary%20B.%20Ramesh%2C%20Manu%20Berg%2C%20Tamara%20Learned-Miller%2C%20Erik%20Labeled%20Faces%20in%20the%20Wild%3A%20A%20Database%20for%20Studying%20Face%20Recognition%20in%20Unconstrained%20Environments%202007"
        },
        {
            "id": "34",
            "entry": "[34] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In IEEE International Conference on Computer Vision (ICCV), pages 3730\u20133738, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20Learning%20Face%20Attributes%20in%20the%20Wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20Learning%20Face%20Attributes%20in%20the%20Wild%202015"
        },
        {
            "id": "35",
            "entry": "[35] Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and Simile Classifiers for Face Verification. In IEEE International Conference on Computer Vision (ICCV), pages 365\u2013372, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20Neeraj%20Berg%2C%20Alexander%20C.%20Belhumeur%2C%20Peter%20N.%20Nayar%2C%20Shree%20K.%20Attribute%20and%20Simile%20Classifiers%20for%20Face%20Verification%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20Neeraj%20Berg%2C%20Alexander%20C.%20Belhumeur%2C%20Peter%20N.%20Nayar%2C%20Shree%20K.%20Attribute%20and%20Simile%20Classifiers%20for%20Face%20Verification%202009"
        },
        {
            "id": "36",
            "entry": "[36] Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri, Alexander Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan Sheatsley, et al. CleverHans v2.0.0: An Adversarial Machine Learning Library. arXiv preprint arXiv:1610.00768, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.00768"
        },
        {
            "id": "37",
            "entry": "[37] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-Based%20Learning%20Applied%20to%20Document%20Recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-Based%20Learning%20Applied%20to%20Document%20Recognition%201998"
        },
        {
            "id": "38",
            "entry": "[38] Alex Krizhevsky and Geoffrey Hinton. Learning Multiple Layers of Features from Tiny Images. 2009. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images%202009"
        }
    ]
}
