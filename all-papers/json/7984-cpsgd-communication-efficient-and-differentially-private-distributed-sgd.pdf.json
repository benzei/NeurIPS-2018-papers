{
    "filename": "7984-cpsgd-communication-efficient-and-differentially-private-distributed-sgd.pdf",
    "metadata": {
        "title": "cpSGD: Communication-efficient and differentially-private distributed SGD",
        "author": "Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X. Yu, Sanjiv Kumar, Brendan McMahan",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7984-cpsgd-communication-efficient-and-differentially-private-distributed-sgd.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For d variables and n \u21e1 d clients, the proposed method uses O(log log(nd)) bits of communication per client per coordinate and ensures constant privacy. We also improve previous analysis of the Binomial mechanism showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest."
    },
    "keywords": [
        {
            "term": "mean square error",
            "url": "https://en.wikipedia.org/wiki/mean_square_error"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "differential privacy",
            "url": "https://en.wikipedia.org/wiki/differential_privacy"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Background<br/><br/>Distributed stochastic gradient descent (SGD) is a basic building block of modern machine learning [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We focus on the problem of distributed mean estimation (DME), where the goal is to estimate the mean of a set of vectors.\n1.3",
        "We study privacy from the lens of differential privacy (DP)",
        "We extend the analysis of 1-dimensional Binomial mechanism to d dimensions",
        "In Section 2, we review the notion of differential privacy and state our results for the Binomial mechanism",
        "Motivated by the fact that the convergence of stochastic gradient descent can be reduced to the error in gradient estimate computation per-round, we formally describe the problem of distributed mean estimation in Section 3 and state our results in Section 4"
    ],
    "key_statements": [
        "Background<br/><br/>Distributed stochastic gradient descent (SGD) is a basic building block of modern machine learning [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We focus on the problem of distributed mean estimation (DME), where the goal is to estimate the mean of a set of vectors.\n1.3",
        "We study privacy from the lens of differential privacy (DP)",
        "We extend the analysis of 1-dimensional Binomial mechanism to d dimensions",
        "In Section 2, we review the notion of differential privacy and state our results for the Binomial mechanism",
        "Motivated by the fact that the convergence of stochastic gradient descent can be reduced to the error in gradient estimate computation per-round, we formally describe the problem of distributed mean estimation in Section 3 and state our results in Section 4",
        "In Section 4.2, we provide and analyze the implementation of the binomial mechanism in conjunction with quantization in the context of distributed mean estimation",
        "We describe our algorithms, the associated mean square error, and the privacy guarantees in the context of distributed mean estimation"
    ],
    "summary": [
        "Background<br/><br/>Distributed stochastic gradient descent (SGD) is a basic building block of modern machine learning [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "Previous analysis of the Binomial mechanism) was due to [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], who analyzed the 1-dimensional case for p = 1/2 and showed that to achieve (\", ) differential privacy, N needs to be 64 log(2/ )/\"2.",
        "Differentially-private distributed mean estimation (DME): A direct application of Gaussian mechanism requires n \u00b7 d reals and n \u00b7 d \u00b7 log bits of communication.",
        "We first propose a direct application of quantization [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] and Binomial mechanism and characterize its privacy/error guarantees along with its communication costs.",
        "For \" = O(1), we provide an algorithm achieving the same privacy and error tradeoff as that of the Gaussian mechanism with communication",
        "In Section 2, we review the notion of differential privacy and state our results for the Binomial mechanism.",
        "Motivated by the fact that the convergence of SGD can be reduced to the error in gradient estimate computation per-round, we formally describe the problem of DME in Section 3 and state our results in Section 4.",
        "The main idea is for each client to add noise drawn from an appropriately parameterized Binomial distribution to each quantized value before sending to the server.",
        "In any independent communication protocol, each client transmits a function of Xi), and a central server estimates the mean by some function of q(X1), q(X2), .",
        "To state the privacy results for DME, we define the notion of data sets and neighbors as follows.",
        "Before stating the formal guarantees we will require the definitions of the following quantities representing the sensitivity of the quantization protocol in the appropriate norm.",
        "There exists an implementation of \u21e1sk(Bin(m, p)), which achieves the same privacy and error as the full precision Gaussian mechanism with a total communication complexity of",
        "As seen in Corollary 2, for \u21e1sk(Bin(m, p)) to have error and privacy same as that of the Gaussian mechanism, the best bound on the communication cost guaranteed is \u2326) bits per coordinate irrespective of how large n is.",
        "If mnp(1 p) max (23 log(10d/ ), 2 1(D, Xmax)) , X (Rot(\u21e1sk(Bin(m, p)))) is (\", 3 ) differentially private where \" is q q",
        "There exists an implementation of Rot(\u21e1sk(Bin(m, p)), HA), that achieves the same error and privacy of the full precision Gaussian mechanism with a total communication complexity:",
        "Rot(\u21e1sk(Bin(m, p)), HA) has the same privacy and utilities as the Gaussian mechanism, but with just O) communication cost.",
        "The key fact utilized in this analysis is that Binomial distribution is locally rotationinvariant around the mean"
    ],
    "headline": "We study algorithms that achieve both communication efficiency and differential privacy",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "2",
            "entry": "[2] Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 308\u2013318. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Chu%2C%20Andy%20Ian%20Goodfellow%2C%20H.Brendan%20McMahan%20Mironov%2C%20Ilya%20Deep%20learning%20with%20differential%20privacy%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Chu%2C%20Andy%20Ian%20Goodfellow%2C%20H.Brendan%20McMahan%20Mironov%2C%20Ilya%20Deep%20learning%20with%20differential%20privacy%202016"
        },
        {
            "id": "3",
            "entry": "[3] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast JohnsonLindenstrauss transform. In STOC, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ailon%2C%20Nir%20Chazelle%2C%20Bernard%20Approximate%20nearest%20neighbors%20and%20the%20fast%20JohnsonLindenstrauss%20transform%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ailon%2C%20Nir%20Chazelle%2C%20Bernard%20Approximate%20nearest%20neighbors%20and%20the%20fast%20JohnsonLindenstrauss%20transform%202006"
        },
        {
            "id": "4",
            "entry": "[4] Dan Alistarh, Demjan Grubic, Jerry Liu, Ryota Tomioka, and Milan Vojnovic. Communicationefficient stochastic gradient descent, with applications to neural networks. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alistarh%2C%20Dan%20Grubic%2C%20Demjan%20Liu%2C%20Jerry%20Tomioka%2C%20Ryota%20Communicationefficient%20stochastic%20gradient%20descent%2C%20with%20applications%20to%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Dan Alistarh, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Randomized quantization for communication-optimal stochastic gradient descent. arXiv:1610.02132, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.02132"
        },
        {
            "id": "6",
            "entry": "[6] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 464\u2013473. IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bassily%2C%20Raef%20Smith%2C%20Adam%20Thakurta%2C%20Abhradeep%20Private%20empirical%20risk%20minimization%3A%20Efficient%20algorithms%20and%20tight%20error%20bounds%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bassily%2C%20Raef%20Smith%2C%20Adam%20Thakurta%2C%20Abhradeep%20Private%20empirical%20risk%20minimization%3A%20Efficient%20algorithms%20and%20tight%20error%20bounds%202014"
        },
        {
            "id": "7",
            "entry": "[7] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. pages 1175\u20131191, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonawitz%2C%20Keith%20Ivanov%2C%20Vladimir%20Kreuter%2C%20Ben%20Antonio%20Marcedone%2C%20H.Brendan%20McMahan%20Practical%20secure%20aggregation%20for%20privacy-preserving%20machine%20learning%202017"
        },
        {
            "id": "8",
            "entry": "[8] Leon Bottou. The infinite mnist dataset.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20Leon%20The%20infinite%20mnist%20dataset"
        },
        {
            "id": "9",
            "entry": "[9] Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew. Deep learning with cots hpc systems. In International Conference on Machine Learning, pages 1337\u20131345, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Huval%2C%20Brody%20Wang%2C%20Tao%20Wu%2C%20David%20Deep%20learning%20with%20cots%20hpc%20systems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Huval%2C%20Brody%20Wang%2C%20Tao%20Wu%2C%20David%20Deep%20learning%20with%20cots%20hpc%20systems%202013"
        },
        {
            "id": "10",
            "entry": "[10] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures & Algorithms, 22(1):60\u201365, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20Sanjoy%20Gupta%2C%20Anupam%20An%20elementary%20proof%20of%20a%20theorem%20of%20johnson%20and%20lindenstrauss%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20Sanjoy%20Gupta%2C%20Anupam%20An%20elementary%20proof%20of%20a%20theorem%20of%20johnson%20and%20lindenstrauss%202003"
        },
        {
            "id": "11",
            "entry": "[11] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in neural information processing systems, pages 1223\u20131231, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20Jeffrey%20Corrado%2C%20Greg%20Monga%2C%20Rajat%20Chen%2C%20Kai%20Large%20scale%20distributed%20deep%20networks%202012"
        },
        {
            "id": "12",
            "entry": "[12] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Eurocrypt, volume 4004, pages 486\u2013503.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Kenthapadi%2C%20Krishnaram%20McSherry%2C%20Frank%20Mironov%2C%20Ilya%20Our%20data%2C%20ourselves%3A%20Privacy%20via%20distributed%20noise%20generation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Kenthapadi%2C%20Krishnaram%20McSherry%2C%20Frank%20Mironov%2C%20Ilya%20Our%20data%2C%20ourselves%3A%20Privacy%20via%20distributed%20noise%20generation"
        },
        {
            "id": "13",
            "entry": "[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In TCC, volume 3876, pages 265\u2013284.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20McSherry%2C%20Frank%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20Calibrating%20noise%20to%20sensitivity%20in%20private%20data%20analysis",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20McSherry%2C%20Frank%20Nissim%2C%20Kobbi%20Smith%2C%20Adam%20Calibrating%20noise%20to%20sensitivity%20in%20private%20data%20analysis"
        },
        {
            "id": "14",
            "entry": "[14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3&#8211;4):211\u2013407, August 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy.%20Found%202014-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Roth%2C%20Aaron%20The%20algorithmic%20foundations%20of%20differential%20privacy.%20Found%202014-08"
        },
        {
            "id": "15",
            "entry": "[15] Peter Elias. Universal codeword sets and representations of the integers. IEEE transactions on information theory, 21(2):194\u2013203, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elias%2C%20Peter%20Universal%20codeword%20sets%20and%20representations%20of%20the%20integers%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elias%2C%20Peter%20Universal%20codeword%20sets%20and%20representations%20of%20the%20integers%201975"
        },
        {
            "id": "16",
            "entry": "[16] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20Saeed%20Lan%2C%20Guanghui%20Stochastic%20first-and%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "17",
            "entry": "[17] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1737\u20131746, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20Suyog%20Agrawal%2C%20Ankur%20Gopalakrishnan%2C%20Kailash%20Narayanan%2C%20Pritish%20Deep%20learning%20with%20limited%20numerical%20precision%202015"
        },
        {
            "id": "18",
            "entry": "[18] Kathy J Horadam. Hadamard matrices and their applications. Princeton university press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Horadam%2C%20Kathy%20J.%20Hadamard%20matrices%20and%20their%20applications%202012"
        },
        {
            "id": "19",
            "entry": "[19] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. IEEE Transactions on Information Theory, 63(6):4037\u20134049, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kairouz%2C%20Peter%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20The%20composition%20theorem%20for%20differential%20privacy%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kairouz%2C%20Peter%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20The%20composition%20theorem%20for%20differential%20privacy%202017"
        },
        {
            "id": "20",
            "entry": "[20] Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.05492"
        },
        {
            "id": "21",
            "entry": "[21] Jakub Konecnyand Peter Richt\u00e1rik. Randomized distributed mean estimation: Accuracy vs communication. arXiv preprint arXiv:1611.07555, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.07555"
        },
        {
            "id": "22",
            "entry": "[22] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In OSDI, volume 1, page 3, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "23",
            "entry": "[23] Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. Communication efficient distributed machine learning with the parameter server. In Advances in Neural Information Processing Systems, pages 19\u201327, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Smola%2C%20Alexander%20J.%20Yu%2C%20Kai%20Communication%20efficient%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Smola%2C%20Alexander%20J.%20Yu%2C%20Kai%20Communication%20efficient%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "24",
            "entry": "[24] Stuart Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129\u2013137, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lloyd%2C%20Stuart%20Least%20squares%20quantization%20in%20PCM%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lloyd%2C%20Stuart%20Least%20squares%20quantization%20in%20PCM%201982"
        },
        {
            "id": "25",
            "entry": "[25] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In HLT, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDonald%2C%20Ryan%20Hall%2C%20Keith%20Mann%2C%20Gideon%20Distributed%20training%20strategies%20for%20the%20structured%20perceptron%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDonald%2C%20Ryan%20Hall%2C%20Keith%20Mann%2C%20Gideon%20Distributed%20training%20strategies%20for%20the%20structured%20perceptron%202010"
        },
        {
            "id": "26",
            "entry": "[26] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20H.Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20H.Brendan%20Moore%2C%20Eider%20Ramage%2C%20Daniel%20Hampson%2C%20Seth%20and%20Blaise%20Aguera%20y%20Arcas.%20Communication-efficient%20learning%20of%20deep%20networks%20from%20decentralized%20data%202016"
        },
        {
            "id": "27",
            "entry": "[27] H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Federated learning of deep networks using model averaging. arXiv:1602.05629, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.05629"
        },
        {
            "id": "28",
            "entry": "[28] Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of deep neural networks with natural gradient and parameter averaging. arXiv preprint, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Povey%2C%20Daniel%20Zhang%2C%20Xiaohui%20Khudanpur%2C%20Sanjeev%20Parallel%20training%20of%20deep%20neural%20networks%20with%20natural%20gradient%20and%20parameter%20averaging.%20arXiv%20p%202014"
        },
        {
            "id": "29",
            "entry": "[29] Alexander Rakhlin, Ohad Shamir, Karthik Sridharan, et al. Making gradient descent optimal for strongly convex stochastic optimization. In ICML. Citeseer, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakhlin%2C%20Alexander%20Shamir%2C%20Ohad%20Sridharan%2C%20Karthik%20Making%20gradient%20descent%20optimal%20for%20strongly%20convex%20stochastic%20optimization%202012"
        },
        {
            "id": "30",
            "entry": "[30] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693\u2013701, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "31",
            "entry": "[31] Anand D Sarwate and Kamalika Chaudhuri. Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data. IEEE signal processing magazine, 30(5):86\u201394, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sarwate%2C%20Anand%20D.%20Chaudhuri%2C%20Kamalika%20Signal%20processing%20and%20machine%20learning%20with%20differential%20privacy%3A%20Algorithms%20and%20challenges%20for%20continuous%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sarwate%2C%20Anand%20D.%20Chaudhuri%2C%20Kamalika%20Signal%20processing%20and%20machine%20learning%20with%20differential%20privacy%3A%20Algorithms%20and%20challenges%20for%20continuous%20data%202013"
        },
        {
            "id": "32",
            "entry": "[32] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seide%2C%20Frank%20Fu%2C%20Hao%20Droppo%2C%20Jasha%20Li%2C%20Gang%201-bit%20stochastic%20gradient%20descent%20and%20its%20application%20to%20data-parallel%20distributed%20training%20of%20speech%20dnns%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seide%2C%20Frank%20Fu%2C%20Hao%20Droppo%2C%20Jasha%20Li%2C%20Gang%201-bit%20stochastic%20gradient%20descent%20and%20its%20application%20to%20data-parallel%20distributed%20training%20of%20speech%20dnns%202014"
        },
        {
            "id": "33",
            "entry": "[33] Ananda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean estimation with limited communication. In International Conference on Machine Learning, pages 3329\u20133337, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ananda%20Theertha%20Suresh%2C%20X.Yu%20Felix%20Kumar%2C%20Sanjiv%20McMahan%2C%20H.Brendan%20Distributed%20mean%20estimation%20with%20limited%20communication%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ananda%20Theertha%20Suresh%2C%20X.Yu%20Felix%20Kumar%2C%20Sanjiv%20McMahan%2C%20H.Brendan%20Distributed%20mean%20estimation%20with%20limited%20communication%202017"
        },
        {
            "id": "34",
            "entry": "[34] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. arXiv preprint arXiv:1705.07878, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07878"
        },
        {
            "id": "35",
            "entry": "[35] Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolton differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the 2017 ACM International Conference on Management of Data, pages 1307\u20131322. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Xi%20Li%2C%20Fengan%20Kumar%2C%20Arun%20Chaudhuri%2C%20Kamalika%20Bolton%20differential%20privacy%20for%20scalable%20stochastic%20gradient%20descent-based%20analytics%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Xi%20Li%2C%20Fengan%20Kumar%2C%20Arun%20Chaudhuri%2C%20Kamalika%20Bolton%20differential%20privacy%20for%20scalable%20stochastic%20gradient%20descent-based%20analytics%202017"
        },
        {
            "id": "36",
            "entry": "[36] Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835\u20131854, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Kun%20Ling%2C%20Qing%20Yin%2C%20Wotao%20On%20the%20convergence%20of%20decentralized%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Kun%20Ling%2C%20Qing%20Yin%2C%20Wotao%20On%20the%20convergence%20of%20decentralized%20gradient%20descent%202016"
        },
        {
            "id": "37",
            "entry": "[37] Huizi Mao Yu Wang Bill Dally Yujun Lin, Song Han. Deep gradient compression: Reducing the communication bandwidth for distributed training. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Huizi%20Mao%20Yu%20Wang%20Bill%20Dally%20Yujun%20Han%2C%20Song%20Deep%20gradient%20compression%3A%20Reducing%20the%20communication%20bandwidth%20for%20distributed%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Huizi%20Mao%20Yu%20Wang%20Bill%20Dally%20Yujun%20Han%2C%20Song%20Deep%20gradient%20compression%3A%20Reducing%20the%20communication%20bandwidth%20for%20distributed%20training%202018"
        },
        {
            "id": "38",
            "entry": "[38] Takuya Akiba Yusuke Tsuzuku, Hiroto Imachi. Variance-based gradient compression for efficient distributed deep learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsuzuku%2C%20Takuya%20Akiba%20Yusuke%20Imachi%2C%20Hiroto%20Variance-based%20gradient%20compression%20for%20efficient%20distributed%20deep%20learning%202018"
        },
        {
            "id": "39",
            "entry": "[39] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014. ",
            "arxiv_url": "https://arxiv.org/pdf/1409.2329"
        }
    ]
}
