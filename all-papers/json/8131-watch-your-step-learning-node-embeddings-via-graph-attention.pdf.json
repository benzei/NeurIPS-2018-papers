{
    "filename": "8131-watch-your-step-learning-node-embeddings-via-graph-attention.pdf",
    "metadata": {
        "title": "Watch Your Step: Learning Node Embeddings via Graph Attention",
        "author": "Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, Alexander A. Alemi",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8131-watch-your-step-learning-node-embeddings-via-graph-attention.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Graph embedding methods represent nodes in a continuous vector space, preserving different types of relational information from the graph. There are many hyperparameters to these methods (e.g. the length of a random walk) which have to be manually tuned for every graph. In this paper, we replace previously fixed hyperparameters with trainable ones that we automatically learn via backpropagation. In particular, we propose a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective. Unlike previous approaches to attention models, the method that we propose utilizes attention parameters exclusively on the data itself (e.g. on the random walk), and are not used by the model for inference. We experiment on link prediction tasks, as we aim to produce embeddings that best-preserve the graph structure, generalizing to unseen information. We improve state-of-the-art results on a comprehensive suite of real-world graph datasets including social, collaboration, and biological networks, where we observe that our graph attention model can reduce the error by up to 20%-40%. We show that our automatically-learned attention parameters can vary significantly per graph, and correspond to the optimal choice of hyperparameter if we manually tune existing methods."
    },
    "keywords": [
        {
            "term": "Natural Language Processing",
            "url": "https://en.wikipedia.org/wiki/Natural_Language_Processing"
        },
        {
            "term": "power series",
            "url": "https://en.wikipedia.org/wiki/power_series"
        },
        {
            "term": "random walk",
            "url": "https://en.wikipedia.org/wiki/random_walk"
        },
        {
            "term": "Singular Value Decomposition",
            "url": "https://en.wikipedia.org/wiki/Singular_Value_Decomposition"
        },
        {
            "term": "transition matrix",
            "url": "https://en.wikipedia.org/wiki/transition_matrix"
        },
        {
            "term": "graph embedding",
            "url": "https://en.wikipedia.org/wiki/graph_embedding"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "Unsupervised graph embedding methods seek to learn representations that encode the graph structure",
        "We show the mathematical equivalence between the context distribution and the co-efficients of power series of the transition matrix",
        "We evaluate on a number of challenging link prediction tasks comprised of real world datasets, including social, collaboration, and biological networks",
        "We propose to train softmax attention model on the infinite power series of the transition matrix",
        "We evaluate the quality of embeddings produced when random walks are augmented with attention, through experiments on link prediction [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "We propose an attention mechanism for learning the context distribution used in graph embedding methods"
    ],
    "key_statements": [
        "Unsupervised graph embedding methods seek to learn representations that encode the graph structure",
        "We show the mathematical equivalence between the context distribution and the co-efficients of power series of the transition matrix",
        "We evaluate on a number of challenging link prediction tasks comprised of real world datasets, including social, collaboration, and biological networks",
        "We propose to train softmax attention model on the infinite power series of the transition matrix",
        "We evaluate the quality of embeddings produced when random walks are augmented with attention, through experiments on link prediction [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]",
        "The hyper-parameter C determines the highest power of the transition matrix, and the maximum context size available to the attention model",
        "We suggest using large values for C, since the attention weights can effectively use a subset of the transition matrix powers",
        "We propose an attention mechanism for learning the context distribution used in graph embedding methods",
        "We show significant improvements on link prediction and node classification over state-of-theart baselines, reducing error on link prediction and classification, respectively by up to 40% and 10%"
    ],
    "summary": [
        "Unsupervised graph embedding methods seek to learn representations that encode the graph structure.",
        "We propose an extendible family of graph attention models that can learn arbitrary context distributions.",
        "We extend the the Negative Log Graph Likelihood (NLGL) loss (Eq 4) to include attention parameters on the random walk sampling.",
        "To learn Q automatically, we propose an attention model which guides the random surfer on \u201cwhere to attend to\u201d as a function of distance from the source node.",
        "We propose to train softmax attention model on the infinite power series of the transition matrix.",
        "We evaluate the quality of embeddings produced when random walks are augmented with attention, through experiments on link prediction [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>].",
        "It simulates random walks and uses word2vec to learn node embeddings.",
        "The hyper-parameter C determines the highest power of the transition matrix, and the maximum context size available to the attention model.",
        "We have introduced other hyper-parameters \u2013 specifically walk length and a regularization term \u03b2 for the softmax attention model.",
        "Figures 2a and 2b both show that the softmax attention weights drop to almost zero if the graph can be preserved using shorter walks, which is not possible with fixed-form distributions (e.g. U).",
        "We learn embeddings from only the graph structure, without observing node features nor labels during training.",
        "We do not specify a fixed context distribution apriori, whereas we push gradients through the random walk to those parameters, which we jointly train while learning the embeddings.",
        "We propose an attention mechanism for learning the context distribution used in graph embedding methods.",
        "We derive the closed-form expectation of the DeepWalk [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] co-occurrence statistics, showing an equivalence between the context distribution hyper-parameters, and the coefficients of the power series of the graph transition matrix.",
        "We propose to replace the context hyper-parameters with trainable models, that we learn jointly with the embeddings on an objective that preserves the graph structure.",
        "We propose Graph Attention Models, using a softmax to learn a free-form contexts distribution with a parameter for each type of context similarity.",
        "In addition to improved performance, our method can obviate the manual grid search over hyper-parameters: walk length and form of context distribution, which can drastically fluctuate the quality of the learned embeddings and are different for every graph.",
        "We believe that our contribution in replacing these sampling hyperparameters with a learnable context distribution is general and can be applied to many domains and modeling techniques in graph representation learning."
    ],
    "headline": "We propose a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Abu-El-Haija. Proportionate gradient updates with percentdelta. In arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abu-El-Haija%2C%20S.%20Proportionate%20gradient%20updates%20with%20percentdelta%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abu-El-Haija%2C%20S.%20Proportionate%20gradient%20updates%20with%20percentdelta%202017"
        },
        {
            "id": "2",
            "entry": "[2] S. Abu-El-Haija, B. Perozzi, and R. Al-Rfou. Learning edge representations via low-rank asymmetric projections. In ACM International Conference on Information and Knowledge Management (CIKM), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abu-El-Haija%2C%20S.%20Perozzi%2C%20B.%20Al-Rfou%2C%20R.%20Learning%20edge%20representations%20via%20low-rank%20asymmetric%20projections%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abu-El-Haija%2C%20S.%20Perozzi%2C%20B.%20Al-Rfou%2C%20R.%20Learning%20edge%20representations%20via%20low-rank%20asymmetric%20projections%202017"
        },
        {
            "id": "3",
            "entry": "[3] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Atwood%2C%20J.%20Towsley%2C%20D.%20Diffusion-convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Atwood%2C%20J.%20Towsley%2C%20D.%20Diffusion-convolutional%20neural%20networks%202016"
        },
        {
            "id": "4",
            "entry": "[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20D.%20Cho%2C%20K.%20Bengio%2C%20Y.%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "5",
            "entry": "[5] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. In Neural Computation, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Belkin%2C%20M.%20Niyogi%2C%20P.%20Laplacian%20eigenmaps%20for%20dimensionality%20reduction%20and%20data%20representation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Belkin%2C%20M.%20Niyogi%2C%20P.%20Laplacian%20eigenmaps%20for%20dimensionality%20reduction%20and%20data%20representation%202003"
        },
        {
            "id": "6",
            "entry": "[6] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. In IEEE Signal Processing Magazine, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bronstein%2C%20M.M.%20Bruna%2C%20J.%20LeCun%2C%20Y.%20Szlam%2C%20A.%20Geometric%20deep%20learning%3A%20going%20beyond%20euclidean%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bronstein%2C%20M.M.%20Bruna%2C%20J.%20LeCun%2C%20Y.%20Szlam%2C%20A.%20Geometric%20deep%20learning%3A%20going%20beyond%20euclidean%20data%202017"
        },
        {
            "id": "7",
            "entry": "[7] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and deep locally connected networks on graphs. In International Conference on Learning Representations, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruna%2C%20J.%20Zaremba%2C%20W.%20Szlam%2C%20A.%20LeCun%2C%20Y.%20Spectral%20networks%20and%20deep%20locally%20connected%20networks%20on%20graphs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruna%2C%20J.%20Zaremba%2C%20W.%20Szlam%2C%20A.%20LeCun%2C%20Y.%20Spectral%20networks%20and%20deep%20locally%20connected%20networks%20on%20graphs%202013"
        },
        {
            "id": "8",
            "entry": "[8] S. Cao, W. Lu, and Q. Xu. Deep neural networks for learning graph representations. In Association for the Advancement of Artificial Intelligence, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20S.%20Lu%2C%20W.%20Xu%2C%20Q.%20Deep%20neural%20networks%20for%20learning%20graph%20representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20S.%20Lu%2C%20W.%20Xu%2C%20Q.%20Deep%20neural%20networks%20for%20learning%20graph%20representations%202016"
        },
        {
            "id": "9",
            "entry": "[9] H. Chen, B. Perozzi, R. Al-Rfou, and S. Skiena. A tutorial on network embeddings. arXiv preprint arXiv:1808.02590, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1808.02590"
        },
        {
            "id": "10",
            "entry": "[10] H. Chen, B. Perozzi, Y. Hu, and S. Skiena. Harp: hierarchical representation learning for networks. The 32nd AAAI Conference on Artificial Intelligence, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20H.%20Perozzi%2C%20B.%20Hu%2C%20Y.%20Skiena%2C%20S.%20Harp%3A%20hierarchical%20representation%20learning%20for%20networks.%20The%2032nd%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20H.%20Perozzi%2C%20B.%20Hu%2C%20Y.%20Skiena%2C%20S.%20Harp%3A%20hierarchical%20representation%20learning%20for%20networks.%20The%2032nd%202018"
        },
        {
            "id": "11",
            "entry": "[11] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20H.%20Dai%2C%20B.%20Song%2C%20L.%20Discriminative%20embeddings%20of%20latent%20variable%20models%20for%20structured%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20H.%20Dai%2C%20B.%20Song%2C%20L.%20Discriminative%20embeddings%20of%20latent%20variable%20models%20for%20structured%20data%202016"
        },
        {
            "id": "12",
            "entry": "[12] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defferrard%2C%20M.%20Bresson%2C%20X.%20Vandergheynst%2C%20P.%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defferrard%2C%20M.%20Bresson%2C%20X.%20Vandergheynst%2C%20P.%20Convolutional%20neural%20networks%20on%20graphs%20with%20fast%20localized%20spectral%20filtering%202016"
        },
        {
            "id": "13",
            "entry": "[13] D. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duvenaud%2C%20D.%20Maclaurin%2C%20D.%20Iparraguirre%2C%20J.%20Bombarell%2C%20R.%20Convolutional%20networks%20on%20graphs%20for%20learning%20molecular%20fingerprints%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duvenaud%2C%20D.%20Maclaurin%2C%20D.%20Iparraguirre%2C%20J.%20Bombarell%2C%20R.%20Convolutional%20networks%20on%20graphs%20for%20learning%20molecular%20fingerprints%202015"
        },
        {
            "id": "14",
            "entry": "[14] P. Goyal and E. Ferrara. Graph embedding techniques, applications, and performance: A survey. In Knowledge-Based Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goyal%2C%20P.%20Ferrara%2C%20E.%20Graph%20embedding%20techniques%2C%20applications%2C%20and%20performance%3A%20A%20survey.%20In%20Knowledge-Based%20Systems%202018"
        },
        {
            "id": "15",
            "entry": "[15] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In International Conference on Knowledge Discovery and Data Mining, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grover%2C%20A.%20Leskovec%2C%20J.%20node2vec%3A%20Scalable%20feature%20learning%20for%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grover%2C%20A.%20Leskovec%2C%20J.%20node2vec%3A%20Scalable%20feature%20learning%20for%20networks%202016"
        },
        {
            "id": "16",
            "entry": "[16] N. Halko, P. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. In SIAM Review, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Halko%2C%20N.%20Martinsson%2C%20P.%20Tropp%2C%20J.A.%20Finding%20structure%20with%20randomness%3A%20Probabilistic%20algorithms%20for%20constructing%20approximate%20matrix%20decompositions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Halko%2C%20N.%20Martinsson%2C%20P.%20Tropp%2C%20J.A.%20Finding%20structure%20with%20randomness%3A%20Probabilistic%20algorithms%20for%20constructing%20approximate%20matrix%20decompositions%202011"
        },
        {
            "id": "17",
            "entry": "[17] W. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20W.%20Ying%2C%20R.%20Leskovec%2C%20J.%20Inductive%20representation%20learning%20on%20large%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20W.%20Ying%2C%20R.%20Leskovec%2C%20J.%20Inductive%20representation%20learning%20on%20large%20graphs%202017"
        },
        {
            "id": "18",
            "entry": "[18] W. L. Hamilton, R. Ying, and J. Leskovec. Representation learning on graphs: Methods and applications. In IEEE Data Engineering Bulletin, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamilton%2C%20W.L.%20Ying%2C%20R.%20Leskovec%2C%20J.%20Representation%20learning%20on%20graphs%3A%20Methods%20and%20applications%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamilton%2C%20W.L.%20Ying%2C%20R.%20Leskovec%2C%20J.%20Representation%20learning%20on%20graphs%3A%20Methods%20and%20applications%202017"
        },
        {
            "id": "19",
            "entry": "[19] G. J, S. Ganguly, M. Gupta, V. Varma, and V. Pudi. Author2vec: Learning author representations by combining content and link information. In International Conference Companion on World Wide Web (WWW), WWW \u201916 Companion, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%2C%20G.%20Ganguly%2C%20S.%20Gupta%2C%20M.%20Varma%2C%20V.%20Author2vec%3A%20Learning%20author%20representations%20by%20combining%20content%20and%20link%20information%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%2C%20G.%20Ganguly%2C%20S.%20Gupta%2C%20M.%20Varma%2C%20V.%20Author2vec%3A%20Learning%20author%20representations%20by%20combining%20content%20and%20link%20information%202016"
        },
        {
            "id": "20",
            "entry": "[20] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20T.N.%20Welling%2C%20M.%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20T.N.%20Welling%2C%20M.%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] O. Levy, Y. Goldberg, and I. Dagan. Improving distributional similarity with lessons learned from word embeddings. In TACL, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20O.%20Goldberg%2C%20Y.%20Dagan%2C%20I.%20Improving%20distributional%20similarity%20with%20lessons%20learned%20from%20word%20embeddings%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20O.%20Goldberg%2C%20Y.%20Dagan%2C%20I.%20Improving%20distributional%20similarity%20with%20lessons%20learned%20from%20word%20embeddings%202015"
        },
        {
            "id": "22",
            "entry": "[22] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Tarlow%2C%20D.%20Brockschmidt%2C%20M.%20Zemel%2C%20R.%20Gated%20graph%20sequence%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Tarlow%2C%20D.%20Brockschmidt%2C%20M.%20Zemel%2C%20R.%20Gated%20graph%20sequence%20neural%20networks%202016"
        },
        {
            "id": "23",
            "entry": "[23] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. In Journal of American Society for Information Science and Technology, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liben-Nowell%2C%20D.%20Kleinberg%2C%20J.%20The%20link-prediction%20problem%20for%20social%20networks%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liben-Nowell%2C%20D.%20Kleinberg%2C%20J.%20The%20link-prediction%20problem%20for%20social%20networks%202007"
        },
        {
            "id": "24",
            "entry": "[24] Y. Luo, Q. Wang, B. Wang, and L. Guo. Context-dependent knowledge graph embedding. In Conference on Emperical Methods in Natural Language Processing (EMNLP), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Y.%20Wang%2C%20Q.%20Wang%2C%20B.%20Guo%2C%20L.%20Context-dependent%20knowledge%20graph%20embedding%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Y.%20Wang%2C%20Q.%20Wang%2C%20B.%20Guo%2C%20L.%20Context-dependent%20knowledge%20graph%20embedding%202015"
        },
        {
            "id": "25",
            "entry": "[25] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems NIPS. 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Sutskever%2C%20I.%20Chen%2C%20K.%20Corrado%2C%20G.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20T.%20Sutskever%2C%20I.%20Chen%2C%20K.%20Corrado%2C%20G.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "26",
            "entry": "[26] V. Mnih, N. Heess, A. Graves, and k. kavukcuoglu. Recurrent models of visual attention. In Advances in Neural Information Processing Systems (NIPS). 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Heess%2C%20N.%20Graves%2C%20A.%20k.%20kavukcuoglu%20Recurrent%20models%20of%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Heess%2C%20N.%20Graves%2C%20A.%20k.%20kavukcuoglu%20Recurrent%20models%20of%20visual%20attention%202014"
        },
        {
            "id": "27",
            "entry": "[27] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niepert%2C%20M.%20Ahmed%2C%20M.%20Kutzkov%2C%20K.%20Learning%20convolutional%20neural%20networks%20for%20graphs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niepert%2C%20M.%20Ahmed%2C%20M.%20Kutzkov%2C%20K.%20Learning%20convolutional%20neural%20networks%20for%20graphs%202016"
        },
        {
            "id": "28",
            "entry": "[28] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing, EMNLP, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20J.%20Socher%2C%20R.%20Manning%2C%20C.D.%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "29",
            "entry": "[29] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Knowledge Discovery and Data Mining (KDD), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perozzi%2C%20B.%20Al-Rfou%2C%20R.%20Skiena%2C%20S.%20Deepwalk%3A%20Online%20learning%20of%20social%20representations.%20In%20Knowledge%20Discovery%20and%20Data%20Mining%20%28KDD%29%202014"
        },
        {
            "id": "30",
            "entry": "[30] B. Perozzi, V. Kulkarni, H. Chen, and S. Skiena. Don\u2019t walk, skip!: Online learning of multi-scale network embeddings. In Advances in Social Networks Analysis and Mining (ASONAM), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perozzi%2C%20B.%20Kulkarni%2C%20V.%20Chen%2C%20H.%20Skiena%2C%20S.%20Don%E2%80%99t%20walk%2C%20skip%21%3A%20Online%20learning%20of%20multi-scale%20network%20embeddings%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perozzi%2C%20B.%20Kulkarni%2C%20V.%20Chen%2C%20H.%20Skiena%2C%20S.%20Don%E2%80%99t%20walk%2C%20skip%21%3A%20Online%20learning%20of%20multi-scale%20network%20embeddings%202017"
        },
        {
            "id": "31",
            "entry": "[31] V. Ramanathan, J. Huang, S. Abu-El-Haija, A. Gorban, K. Murphy, and L. Fei-Fei. Detecting events and key actors in multi-person videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramanathan%2C%20V.%20Huang%2C%20J.%20Abu-El-Haija%2C%20S.%20Gorban%2C%20A.%20Detecting%20events%20and%20key%20actors%20in%20multi-person%20videos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramanathan%2C%20V.%20Huang%2C%20J.%20Abu-El-Haija%2C%20S.%20Gorban%2C%20A.%20Detecting%20events%20and%20key%20actors%20in%20multi-person%20videos%202016"
        },
        {
            "id": "32",
            "entry": "[32] F. Scarselli, M. Gori, A. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. In IEEE Trans. on Neural Networks, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scarselli%2C%20F.%20Gori%2C%20M.%20Tsoi%2C%20A.%20Hagenbuchner%2C%20M.%20The%20graph%20neural%20network%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scarselli%2C%20F.%20Gori%2C%20M.%20Tsoi%2C%20A.%20Hagenbuchner%2C%20M.%20The%20graph%20neural%20network%20model%202009"
        },
        {
            "id": "33",
            "entry": "[33] C. Stark, B. Breitkreutz, T. Reguly, L. Boucher, A. Breitkreutz, and M. Tyers. Biogrid: A general repository for interaction datasets. In Nucleic Acids Research, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stark%2C%20C.%20Breitkreutz%2C%20B.%20Reguly%2C%20T.%20Boucher%2C%20L.%20Biogrid%3A%20A%20general%20repository%20for%20interaction%20datasets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stark%2C%20C.%20Breitkreutz%2C%20B.%20Reguly%2C%20T.%20Boucher%2C%20L.%20Biogrid%3A%20A%20general%20repository%20for%20interaction%20datasets%202006"
        },
        {
            "id": "34",
            "entry": "[34] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random walk with restart and its applications. In International Conference on Data Mining (ICDM). IEEE, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tong%2C%20H.%20Faloutsos%2C%20C.%20Pan%2C%20J.-Y.%20Fast%20random%20walk%20with%20restart%20and%20its%20applications%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tong%2C%20H.%20Faloutsos%2C%20C.%20Pan%2C%20J.-Y.%20Fast%20random%20walk%20with%20restart%20and%20its%20applications%202006"
        },
        {
            "id": "35",
            "entry": "[35] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and Y. Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Velickovic%2C%20P.%20Cucurull%2C%20G.%20Casanova%2C%20A.%20Romero%2C%20A.%20Graph%20attention%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Velickovic%2C%20P.%20Cucurull%2C%20G.%20Casanova%2C%20A.%20Romero%2C%20A.%20Graph%20attention%20networks%202018"
        },
        {
            "id": "36",
            "entry": "[36] D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In International Conference on Knowledge Discovery and Data Mining, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20D.%20Cui%2C%20P.%20Zhu%2C%20W.%20Structural%20deep%20network%20embedding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20D.%20Cui%2C%20P.%20Zhu%2C%20W.%20Structural%20deep%20network%20embedding%202016"
        },
        {
            "id": "37",
            "entry": "[37] Z. Yang, W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Cohen%2C%20W.%20Salakhutdinov%2C%20R.%20Revisiting%20semi-supervised%20learning%20with%20graph%20embeddings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Cohen%2C%20W.%20Salakhutdinov%2C%20R.%20Revisiting%20semi-supervised%20learning%20with%20graph%20embeddings%202016"
        },
        {
            "id": "38",
            "entry": "[38] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy. Hierarchical attention networks for document classification. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Yang%2C%20D.%20Dyer%2C%20C.%20He%2C%20X.%20Hierarchical%20attention%20networks%20for%20document%20classification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Yang%2C%20D.%20Dyer%2C%20C.%20He%2C%20X.%20Hierarchical%20attention%20networks%20for%20document%20classification%202016"
        }
    ]
}
