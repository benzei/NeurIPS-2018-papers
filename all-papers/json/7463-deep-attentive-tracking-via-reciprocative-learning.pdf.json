{
    "filename": "7463-deep-attentive-tracking-via-reciprocative-learning.pdf",
    "metadata": {
        "title": "Deep Attentive Tracking via Reciprocative Learning",
        "author": "Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, Ming-Hsuan Yang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7463-deep-attentive-tracking-via-reciprocative-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches."
    },
    "keywords": [
        {
            "term": "visual tracking",
            "url": "https://en.wikipedia.org/wiki/visual_tracking"
        },
        {
            "term": "object tracking",
            "url": "https://en.wikipedia.org/wiki/object_tracking"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "The recent years have witnessed growing interest in developing visual tracking methods for various vision applications",
        "The state-of-the-art trackers based on discriminative correlation filters (DCFs) [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] regress input features into a Gaussian response map for target localization",
        "We propose a reciprocative learning scheme to exploit visual attention within the tracking-by-detection framework",
        "We use attention maps as a regularization term coupled with the original classification loss function for training discriminative classifiers",
        "Our classifier learns to attend to the robust features over a long temporal span",
        "Extensive evaluations on the benchmarks demonstrate that our method performs favorably against state-of-the-art approaches"
    ],
    "key_statements": [
        "The recent years have witnessed growing interest in developing visual tracking methods for various vision applications",
        "Visual attention plays an important role in facilitating tracking target objects in videos",
        "The state-of-the-art trackers based on discriminative correlation filters (DCFs) [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] regress input features into a Gaussian response map for target localization",
        "They often apply empirical spatial weights to input features to suppress the boundary effect caused by the Fourier transform",
        "We propose a reciprocative learning algorithm to exploit visual attention which advances the tracking-by-detection framework",
        "We propose a reciprocative learning algorithm to exploit visual attention within the trackingby-detection framework",
        "We use the attention maps as regularization terms coupled with the classification loss to train deep classifiers, which in themselves learn to attend to temporal robust features",
        "We propose a reciprocative learning process to learn an attentive classifier in the tracking-by-detection framework",
        "We exploit attention maps as regularization terms coupled with the original classification loss for training classifiers",
        "We propose a reciprocative learning scheme to activate the attentive ability of the classifier in the tracking-by-detection framework",
        "We illustrate how the attention map gradually regularizes the classifier through reciprocative learning.\n3.1",
        "With the use of the attention regularization, the classifier iteratively learns to attend to every region that can differentiate the target from the background",
        "We report the distance precision (DP) and overlap success (OS) rates under the one-pass evaluation (OPE)",
        "We propose a reciprocative learning scheme to exploit visual attention within the tracking-by-detection framework",
        "We use attention maps as a regularization term coupled with the original classification loss function for training discriminative classifiers",
        "Our classifier learns to attend to the robust features over a long temporal span",
        "Extensive evaluations on the benchmarks demonstrate that our method performs favorably against state-of-the-art approaches"
    ],
    "summary": [
        "The recent years have witnessed growing interest in developing visual tracking methods for various vision applications.",
        "We propose a reciprocative learning algorithm to exploit visual attention which advances the tracking-by-detection framework.",
        "We feed an input sample into a deep tracking-by-detection network and compute the corresponding classification score.",
        "The learned classifiers directly predict the classification score of each input sample for target localization.",
        "We use the attention maps as regularization terms coupled with the classification loss to train deep classifiers, which in themselves learn to attend to temporal robust features.",
        "We propose a reciprocative learning process to learn an attentive classifier in the tracking-by-detection framework.",
        "We exploit attention maps as regularization terms coupled with the original classification loss for training classifiers.",
        "We propose a reciprocative learning scheme to activate the attentive ability of the classifier in the tracking-by-detection framework.",
        "With the use of the attention regularization, the classifier iteratively learns to attend to every region that can differentiate the target from the background.",
        "Along the tracking process, the reciprocative learning scheme helps the attention map cover the whole target region and strengthens the instance awareness.",
        "The baseline tracker with reciprocative learning achieves much larger performance gains than that with the attentive feature scheme: 3.2% vs.",
        "Figure 4 shows that our method generally performs well against state-of-the-art approaches on the OTB-2013 dataset with the distance precision and overlap success metrics.",
        "The favorable results against state-of-the-art approaches demonstrate the effectiveness of our method in significantly reducing the average center distance error and increasing the tracking success rates.",
        "We evaluate our method on the VOT-2016 dataset with the comparison to state-of-the-art trackers including Staple [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], MDNet [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>], CCOT [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], EBT [<a class=\"ref-link\" id=\"c58\" href=\"#r58\">58</a>], DeepSRDCF [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], and SiamFC [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "We propose a reciprocative learning scheme to exploit visual attention within the tracking-by-detection framework.",
        "We first compute the classification loss in a forward propagation, and take the partial derivatives with respect to this sample in a backward propagation as attention maps.",
        "We use attention maps as a regularization term coupled with the original classification loss function for training discriminative classifiers.",
        "Compared with existing attention models proposing additional modules to generate feature weights, the proposed reciprocative learning algorithm uses attention maps to regularize the classifier learning.",
        "Extensive evaluations on the benchmarks demonstrate that our method performs favorably against state-of-the-art approaches"
    ],
    "headline": "We propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] B. Babenko, M.-H. Yang, and S. Belongie. Robust object tracking with online multiple instance learning. IEEE PAMI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Babenko%2C%20B.%20Yang%2C%20M.-H.%20Belongie%2C%20S.%20Robust%20object%20tracking%20with%20online%20multiple%20instance%20learning%202011"
        },
        {
            "id": "2",
            "entry": "[2] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H. Torr. Staple: Complementary learners for real-time tracking. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertinetto%2C%20L.%20Valmadre%2C%20J.%20Golodetz%2C%20S.%20Miksik%2C%20O.%20Staple%3A%20Complementary%20learners%20for%20real-time%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertinetto%2C%20L.%20Valmadre%2C%20J.%20Golodetz%2C%20S.%20Miksik%2C%20O.%20Staple%3A%20Complementary%20learners%20for%20real-time%20tracking%202016"
        },
        {
            "id": "3",
            "entry": "[3] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr. Fully-convolutional siamese networks for object tracking. In ECCVW, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertinetto%2C%20L.%20Valmadre%2C%20J.%20Henriques%2C%20J.F.%20Vedaldi%2C%20A.%20Fully-convolutional%20siamese%20networks%20for%20object%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertinetto%2C%20L.%20Valmadre%2C%20J.%20Henriques%2C%20J.F.%20Vedaldi%2C%20A.%20Fully-convolutional%20siamese%20networks%20for%20object%20tracking%202016"
        },
        {
            "id": "4",
            "entry": "[4] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui. Visual object tracking using adaptive correlation filters. In CVPR, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bolme%2C%20D.S.%20Beveridge%2C%20J.R.%20Draper%2C%20B.A.%20Lui%2C%20Y.M.%20Visual%20object%20tracking%20using%20adaptive%20correlation%20filters.%20In%20CVPR%202010"
        },
        {
            "id": "5",
            "entry": "[5] B. Cai, X. Xu, X. Xing, K. Jia, J. Miao, and D. Tao. Bit: Biologically inspired tracker. IEEE TIP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20B.%20Xu%2C%20X.%20Xing%2C%20X.%20Jia%2C%20K.%20Bit%3A%20Biologically%20inspired%20tracker%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20B.%20Xu%2C%20X.%20Xing%2C%20X.%20Jia%2C%20K.%20Bit%3A%20Biologically%20inspired%20tracker%202016"
        },
        {
            "id": "6",
            "entry": "[6] J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, J. Y. Choi, et al. Attentional correlation filter network for adaptive visual tracking. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20J.%20Chang%2C%20H.J.%20Yun%2C%20S.%20Fischer%2C%20T.%20Attentional%20correlation%20filter%20network%20for%20adaptive%20visual%20tracking%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20J.%20Chang%2C%20H.J.%20Yun%2C%20S.%20Fischer%2C%20T.%20Attentional%20correlation%20filter%20network%20for%20adaptive%20visual%20tracking%202017"
        },
        {
            "id": "7",
            "entry": "[7] J. Choi, H. Jin Chang, J. Jeong, Y. Demiris, and J. Young Choi. Visual tracking using attentionmodulated disintegration and integration. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20J.%20Chang%2C%20H.Jin%20Jeong%2C%20J.%20Demiris%2C%20Y.%20Visual%20tracking%20using%20attentionmodulated%20disintegration%20and%20integration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20J.%20Chang%2C%20H.Jin%20Jeong%2C%20J.%20Demiris%2C%20Y.%20Visual%20tracking%20using%20attentionmodulated%20disintegration%20and%20integration%202016"
        },
        {
            "id": "8",
            "entry": "[8] Q. Chu, W. Ouyang, H. Li, X. Wang, B. Liu, and N. Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chu%2C%20Q.%20Ouyang%2C%20W.%20Li%2C%20H.%20Wang%2C%20X.%20Online%20multi-object%20tracking%20using%20cnn-based%20single%20object%20tracker%20with%20spatial-temporal%20attention%20mechanism%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chu%2C%20Q.%20Ouyang%2C%20W.%20Li%2C%20H.%20Wang%2C%20X.%20Online%20multi-object%20tracking%20using%20cnn-based%20single%20object%20tracker%20with%20spatial-temporal%20attention%20mechanism%202017"
        },
        {
            "id": "9",
            "entry": "[9] M. Danelljan, G. H\u00e4ger, F. Khan, and M. Felsberg. Accurate scale estimation for robust visual tracking. In BMVC, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Danelljan%2C%20M.%20H%C3%A4ger%2C%20G.%20Khan%2C%20F.%20Felsberg%2C%20M.%20Accurate%20scale%20estimation%20for%20robust%20visual%20tracking%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Danelljan%2C%20M.%20H%C3%A4ger%2C%20G.%20Khan%2C%20F.%20Felsberg%2C%20M.%20Accurate%20scale%20estimation%20for%20robust%20visual%20tracking%202014"
        },
        {
            "id": "10",
            "entry": "[10] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg. Convolutional features for correlation filter based visual tracking. In ICCVW, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Danelljan%2C%20M.%20Hager%2C%20G.%20Khan%2C%20F.Shahbaz%20Felsberg%2C%20M.%20Convolutional%20features%20for%20correlation%20filter%20based%20visual%20tracking%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Danelljan%2C%20M.%20Hager%2C%20G.%20Khan%2C%20F.Shahbaz%20Felsberg%2C%20M.%20Convolutional%20features%20for%20correlation%20filter%20based%20visual%20tracking%202015"
        },
        {
            "id": "11",
            "entry": "[11] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg. Learning spatially regularized correlation filters for visual tracking. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Danelljan%2C%20M.%20Hager%2C%20G.%20Khan%2C%20F.Shahbaz%20Felsberg%2C%20M.%20Learning%20spatially%20regularized%20correlation%20filters%20for%20visual%20tracking%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Danelljan%2C%20M.%20Hager%2C%20G.%20Khan%2C%20F.Shahbaz%20Felsberg%2C%20M.%20Learning%20spatially%20regularized%20correlation%20filters%20for%20visual%20tracking%202015"
        },
        {
            "id": "12",
            "entry": "[12] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg. Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Danelljan%2C%20M.%20Hager%2C%20G.%20Khan%2C%20F.Shahbaz%20Felsberg%2C%20M.%20Adaptive%20decontamination%20of%20the%20training%20set%3A%20A%20unified%20formulation%20for%20discriminative%20visual%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Danelljan%2C%20M.%20Hager%2C%20G.%20Khan%2C%20F.Shahbaz%20Felsberg%2C%20M.%20Adaptive%20decontamination%20of%20the%20training%20set%3A%20A%20unified%20formulation%20for%20discriminative%20visual%20tracking%202016"
        },
        {
            "id": "13",
            "entry": "[13] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg. Beyond correlation filters: Learning continuous convolution operators for visual tracking. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Danelljan%2C%20M.%20Robinson%2C%20A.%20Khan%2C%20F.S.%20Felsberg%2C%20M.%20Beyond%20correlation%20filters%3A%20Learning%20continuous%20convolution%20operators%20for%20visual%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Danelljan%2C%20M.%20Robinson%2C%20A.%20Khan%2C%20F.S.%20Felsberg%2C%20M.%20Beyond%20correlation%20filters%3A%20Learning%20continuous%20convolution%20operators%20for%20visual%20tracking%202016"
        },
        {
            "id": "14",
            "entry": "[14] W. Du, Y. Wang, and Y. Qiao. Rpan: An end-to-end recurrent pose-attention network for action recognition in videos. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20W.%20Wang%2C%20Y.%20Qiao%2C%20Y.%20Rpan%3A%20An%20end-to-end%20recurrent%20pose-attention%20network%20for%20action%20recognition%20in%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20W.%20Wang%2C%20Y.%20Qiao%2C%20Y.%20Rpan%3A%20An%20end-to-end%20recurrent%20pose-attention%20network%20for%20action%20recognition%20in%20videos%202017"
        },
        {
            "id": "15",
            "entry": "[15] J. Gao, H. Ling, W. Hu, and J. Xing. Transfer learning based visual tracking with gaussian processes regression. In ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20J.%20Ling%2C%20H.%20Hu%2C%20W.%20Xing%2C%20J.%20Transfer%20learning%20based%20visual%20tracking%20with%20gaussian%20processes%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20J.%20Ling%2C%20H.%20Hu%2C%20W.%20Xing%2C%20J.%20Transfer%20learning%20based%20visual%20tracking%20with%20gaussian%20processes%20regression%202014"
        },
        {
            "id": "16",
            "entry": "[16] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girshick%2C%20R.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Malik%2C%20J.%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girshick%2C%20R.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Malik%2C%20J.%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation%202014"
        },
        {
            "id": "17",
            "entry": "[17] H. Grabner, M. Grabner, and H. Bischof. Real-time tracking via on-line boosting. In BMVC, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grabner%2C%20H.%20Grabner%2C%20M.%20Bischof%2C%20H.%20Real-time%20tracking%20via%20on-line%20boosting%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grabner%2C%20H.%20Grabner%2C%20M.%20Bischof%2C%20H.%20Real-time%20tracking%20via%20on-line%20boosting%202006"
        },
        {
            "id": "18",
            "entry": "[18] S. Hare, S. Golodetz, A. Saffari, V. Vineet, M.-M. Cheng, S. L. Hicks, and P. H. Torr. Struck: Structured output tracking with kernels. IEEE PAMI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hare%2C%20S.%20Golodetz%2C%20S.%20Saffari%2C%20A.%20Vineet%2C%20V.%20Struck%3A%20Structured%20output%20tracking%20with%20kernels%202016"
        },
        {
            "id": "19",
            "entry": "[19] D. Held, S. Thrun, and S. Savarese. Learning to track at 100 fps with deep regression networks. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Held%2C%20D.%20Thrun%2C%20S.%20Savarese%2C%20S.%20Learning%20to%20track%20at%20100%20fps%20with%20deep%20regression%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Held%2C%20D.%20Thrun%2C%20S.%20Savarese%2C%20S.%20Learning%20to%20track%20at%20100%20fps%20with%20deep%20regression%20networks%202016"
        },
        {
            "id": "20",
            "entry": "[20] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Exploiting the circulant structure of tracking-by-detection with kernels. In ECCV, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henriques%2C%20J.F.%20Caseiro%2C%20R.%20Martins%2C%20P.%20Batista%2C%20J.%20Exploiting%20the%20circulant%20structure%20of%20tracking-by-detection%20with%20kernels%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henriques%2C%20J.F.%20Caseiro%2C%20R.%20Martins%2C%20P.%20Batista%2C%20J.%20Exploiting%20the%20circulant%20structure%20of%20tracking-by-detection%20with%20kernels%202012"
        },
        {
            "id": "21",
            "entry": "[21] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-speed tracking with kernelized correlation filters. IEEE PAMI, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henriques%2C%20J.F.%20Caseiro%2C%20R.%20Martins%2C%20P.%20Batista%2C%20J.%20High-speed%20tracking%20with%20kernelized%20correlation%20filters%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henriques%2C%20J.F.%20Caseiro%2C%20R.%20Martins%2C%20P.%20Batista%2C%20J.%20High-speed%20tracking%20with%20kernelized%20correlation%20filters%202015"
        },
        {
            "id": "22",
            "entry": "[22] S. Hong, T. You, S. Kwak, and B. Han. Online tracking by learning discriminative saliency map with convolutional neural network. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20S.%20You%2C%20T.%20Kwak%2C%20S.%20Han%2C%20B.%20Online%20tracking%20by%20learning%20discriminative%20saliency%20map%20with%20convolutional%20neural%20network%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20S.%20You%2C%20T.%20Kwak%2C%20S.%20Han%2C%20B.%20Online%20tracking%20by%20learning%20discriminative%20saliency%20map%20with%20convolutional%20neural%20network%202015"
        },
        {
            "id": "23",
            "entry": "[23] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. arXiv preprint: 1709.01507, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01507"
        },
        {
            "id": "24",
            "entry": "[24] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Spatial%20transformer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20M.%20Simonyan%2C%20K.%20Zisserman%2C%20A.%20Spatial%20transformer%20networks%202015"
        },
        {
            "id": "25",
            "entry": "[25] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-detection. IEEE PAMI, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Z%20Kalal%20K%20Mikolajczyk%20and%20J%20Matas%20Trackinglearningdetection%20IEEE%20PAMI%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Z%20Kalal%20K%20Mikolajczyk%20and%20J%20Matas%20Trackinglearningdetection%20IEEE%20PAMI%202012"
        },
        {
            "id": "26",
            "entry": "[26] H. Kiani Galoogahi, A. Fagg, and S. Lucey. Learning background-aware correlation filters for visual tracking. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Galoogahi%2C%20H.Kiani%20Fagg%2C%20A.%20Lucey%2C%20S.%20Learning%20background-aware%20correlation%20filters%20for%20visual%20tracking%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Galoogahi%2C%20H.Kiani%20Fagg%2C%20A.%20Lucey%2C%20S.%20Learning%20background-aware%20correlation%20filters%20for%20visual%20tracking%202017"
        },
        {
            "id": "27",
            "entry": "[27] A. Kosiorek, A. Bewley, and I. Posner. Hierarchical attentive recurrent tracking. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kosiorek%2C%20A.%20Bewley%2C%20A.%20Posner%2C%20I.%20Hierarchical%20attentive%20recurrent%20tracking%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kosiorek%2C%20A.%20Bewley%2C%20A.%20Posner%2C%20I.%20Hierarchical%20attentive%20recurrent%20tracking%202017"
        },
        {
            "id": "28",
            "entry": "[28] M. Kristan and et al. The visual object tracking vot2016 challenge results. In ECCVW, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%2C%20M.%20Kristan%20The%20visual%20object%20tracking%20vot2016%20challenge%20results%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=and%2C%20M.%20Kristan%20The%20visual%20object%20tracking%20vot2016%20challenge%20results%202016"
        },
        {
            "id": "29",
            "entry": "[29] Y. Li and J. Zhu. A scale adaptive kernel correlation filter tracker with feature integration. In ECCVW, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Zhu%2C%20J.%20A%20scale%20adaptive%20kernel%20correlation%20filter%20tracker%20with%20feature%20integration%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Zhu%2C%20J.%20A%20scale%20adaptive%20kernel%20correlation%20filter%20tracker%20with%20feature%20integration%202014"
        },
        {
            "id": "30",
            "entry": "[30] Y. Li, J. Zhu, and S. C. Hoi. Reliable patch trackers: Robust visual tracking by exploiting reliable patches. In CVPR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Zhu%2C%20J.%20Hoi%2C%20S.C.%20Reliable%20patch%20trackers%3A%20Robust%20visual%20tracking%20by%20exploiting%20reliable%20patches%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Zhu%2C%20J.%20Hoi%2C%20S.C.%20Reliable%20patch%20trackers%3A%20Robust%20visual%20tracking%20by%20exploiting%20reliable%20patches%202015"
        },
        {
            "id": "31",
            "entry": "[31] A. Lukezic, T. Voj\u00edr, L. C. Zajc, J. Matas, and M. Kristan. Discriminative correlation filter with channel and spatial reliability. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lukezic%2C%20A.%20Voj%C3%ADr%2C%20T.%20Zajc%2C%20L.C.%20Matas%2C%20J.%20Discriminative%20correlation%20filter%20with%20channel%20and%20spatial%20reliability%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lukezic%2C%20A.%20Voj%C3%ADr%2C%20T.%20Zajc%2C%20L.C.%20Matas%2C%20J.%20Discriminative%20correlation%20filter%20with%20channel%20and%20spatial%20reliability%202017"
        },
        {
            "id": "32",
            "entry": "[32] W. Luo, P. Sun, F. Zhong, W. Liu, T. Zhang, and Y. Wang. End-to-end active object tracking via reinforcement learning. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20W.%20Sun%2C%20P.%20Zhong%2C%20F.%20Liu%2C%20W.%20End-to-end%20active%20object%20tracking%20via%20reinforcement%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20W.%20Sun%2C%20P.%20Zhong%2C%20F.%20Liu%2C%20W.%20End-to-end%20active%20object%20tracking%20via%20reinforcement%20learning%202018"
        },
        {
            "id": "33",
            "entry": "[33] W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, X. Zhao, and T.-K. Kim. Multiple object tracking: A literature review. arXiv:1409.7618, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.7618"
        },
        {
            "id": "34",
            "entry": "[34] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical convolutional features for visual tracking. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20C.%20Huang%2C%20J.-B.%20Yang%2C%20X.%20Yang%2C%20M.-H.%20Hierarchical%20convolutional%20features%20for%20visual%20tracking%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20C.%20Huang%2C%20J.-B.%20Yang%2C%20X.%20Yang%2C%20M.-H.%20Hierarchical%20convolutional%20features%20for%20visual%20tracking%202015"
        },
        {
            "id": "35",
            "entry": "[35] H. Nam and B. Han. Learning multi-domain convolutional neural networks for visual tracking. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nam%2C%20H.%20Han%2C%20B.%20Learning%20multi-domain%20convolutional%20neural%20networks%20for%20visual%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nam%2C%20H.%20Han%2C%20B.%20Learning%20multi-domain%20convolutional%20neural%20networks%20for%20visual%20tracking%202016"
        },
        {
            "id": "36",
            "entry": "[36] J. Ning, J. Yang, S. Jiang, L. Zhang, and M.-H. Yang. Object tracking via dual linear structured svm and explicit feature map. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ning%2C%20J.%20Yang%2C%20J.%20Jiang%2C%20S.%20Zhang%2C%20L.%20Object%20tracking%20via%20dual%20linear%20structured%20svm%20and%20explicit%20feature%20map%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ning%2C%20J.%20Yang%2C%20J.%20Jiang%2C%20S.%20Zhang%2C%20L.%20Object%20tracking%20via%20dual%20linear%20structured%20svm%20and%20explicit%20feature%20map%202016"
        },
        {
            "id": "37",
            "entry": "[37] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20A.%20Gross%2C%20S.%20Chintala%2C%20S.%20Chanan%2C%20G.%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "38",
            "entry": "[38] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, J. Lim, and M.-H. Yang. Hedged deep tracking. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%20Qi%20S%20Zhang%20L%20Qin%20H%20Yao%20Q%20Huang%20J%20Lim%20and%20MH%20Yang%20Hedged%20deep%20tracking%20In%20CVPR%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%20Qi%20S%20Zhang%20L%20Qin%20H%20Yao%20Q%20Huang%20J%20Lim%20and%20MH%20Yang%20Hedged%20deep%20tracking%20In%20CVPR%202016"
        },
        {
            "id": "39",
            "entry": "[39] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Selvaraju%2C%20R.R.%20Cogswell%2C%20M.%20Das%2C%20A.%20Vedantam%2C%20R.%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Selvaraju%2C%20R.R.%20Cogswell%2C%20M.%20Das%2C%20A.%20Vedantam%2C%20R.%20Grad-cam%3A%20Visual%20explanations%20from%20deep%20networks%20via%20gradient-based%20localization%202017"
        },
        {
            "id": "40",
            "entry": "[40] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint: 1312.6034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6034"
        },
        {
            "id": "41",
            "entry": "[41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "42",
            "entry": "[42] A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, and M. Shah. Visual tracking: An experimental survey. IEEE PAMI, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smeulders%2C%20A.W.%20Chu%2C%20D.M.%20Cucchiara%2C%20R.%20Calderara%2C%20S.%20Visual%20tracking%3A%20An%20experimental%20survey%202014"
        },
        {
            "id": "43",
            "entry": "[43] Y. Song, C. Ma, L. Gong, J. Zhang, R. W. Lau, and M.-H. Yang. Crest: Convolutional residual learning for visual tracking. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Y.%20Ma%2C%20C.%20Gong%2C%20L.%20Zhang%2C%20J.%20Crest%3A%20Convolutional%20residual%20learning%20for%20visual%20tracking%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Y.%20Ma%2C%20C.%20Gong%2C%20L.%20Zhang%2C%20J.%20Crest%3A%20Convolutional%20residual%20learning%20for%20visual%20tracking%202017"
        },
        {
            "id": "44",
            "entry": "[44] Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, L. Rynson, and M.-H. Yang. Vital: Visual tracking via adversarial learning. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20Y.%20Ma%2C%20C.%20Wu%2C%20X.%20Gong%2C%20L.%20Vital%3A%20Visual%20tracking%20via%20adversarial%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20Y.%20Ma%2C%20C.%20Wu%2C%20X.%20Gong%2C%20L.%20Vital%3A%20Visual%20tracking%20via%20adversarial%20learning%202018"
        },
        {
            "id": "45",
            "entry": "[45] R. Tao, E. Gavves, and A. W. Smeulders. Siamese instance search for tracking. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tao%2C%20R.%20Gavves%2C%20E.%20Smeulders%2C%20A.W.%20Siamese%20instance%20search%20for%20tracking%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tao%2C%20R.%20Gavves%2C%20E.%20Smeulders%2C%20A.W.%20Siamese%20instance%20search%20for%20tracking%202016"
        },
        {
            "id": "46",
            "entry": "[46] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H. Torr. End-to-end representation learning for correlation filter based tracking. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Valmadre%2C%20J.%20Bertinetto%2C%20L.%20Henriques%2C%20J.%20Vedaldi%2C%20A.%20End-to-end%20representation%20learning%20for%20correlation%20filter%20based%20tracking%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Valmadre%2C%20J.%20Bertinetto%2C%20L.%20Henriques%2C%20J.%20Vedaldi%2C%20A.%20End-to-end%20representation%20learning%20for%20correlation%20filter%20based%20tracking%202017"
        },
        {
            "id": "47",
            "entry": "[47] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, and X. Tang. Residual attention network for image classification. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20F.%20Jiang%2C%20M.%20Qian%2C%20C.%20Yang%2C%20S.%20Residual%20attention%20network%20for%20image%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20F.%20Jiang%2C%20M.%20Qian%2C%20C.%20Yang%2C%20S.%20Residual%20attention%20network%20for%20image%20classification%202017"
        },
        {
            "id": "48",
            "entry": "[48] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual tracking with fully convolutional networks. In ICCV, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20L.%20Ouyang%2C%20W.%20Wang%2C%20X.%20Lu%2C%20H.%20Visual%20tracking%20with%20fully%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20L.%20Ouyang%2C%20W.%20Wang%2C%20X.%20Lu%2C%20H.%20Visual%20tracking%20with%20fully%20convolutional%20networks%202015"
        },
        {
            "id": "49",
            "entry": "[49] N. Wang, S. Li, A. Gupta, and D.-Y. Yeung. Transferring rich feature hierarchies for robust visual tracking. arXiv preprint: 1501.04587, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1501.04587"
        },
        {
            "id": "50",
            "entry": "[50] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A benchmark. In CVPR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Lim%2C%20J.%20Yang%2C%20M.-H.%20Online%20object%20tracking%3A%20A%20benchmark.%20In%20CVPR%202013"
        },
        {
            "id": "51",
            "entry": "[51] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark. IEEE PAMI, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Lim%2C%20J.%20Yang%2C%20M.-H.%20Object%20tracking%20benchmark%202015"
        },
        {
            "id": "52",
            "entry": "[52] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20K.%20Ba%2C%20J.%20Kiros%2C%20R.%20Cho%2C%20K.%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20K.%20Ba%2C%20J.%20Kiros%2C%20R.%20Cho%2C%20K.%20attend%20and%20tell%3A%20Neural%20image%20caption%20generation%20with%20visual%20attention%202015"
        },
        {
            "id": "53",
            "entry": "[53] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Y. Choi. Action-decision networks for visual tracking with deep reinforcement learning. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yun%2C%20S.%20Choi%2C%20J.%20Yoo%2C%20Y.%20Yun%2C%20K.%20Action-decision%20networks%20for%20visual%20tracking%20with%20deep%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yun%2C%20S.%20Choi%2C%20J.%20Yoo%2C%20Y.%20Yun%2C%20K.%20Action-decision%20networks%20for%20visual%20tracking%20with%20deep%20reinforcement%20learning%202017"
        },
        {
            "id": "54",
            "entry": "[54] J. Zhang, S. Ma, and S. Sclaroff. Meem: robust tracking via multiple experts using entropy minimization. In ECCV, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20J.%20Ma%2C%20S.%20Sclaroff%2C%20S.%20Meem%3A%20robust%20tracking%20via%20multiple%20experts%20using%20entropy%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20J.%20Ma%2C%20S.%20Sclaroff%2C%20S.%20Meem%3A%20robust%20tracking%20via%20multiple%20experts%20using%20entropy%20minimization%202014"
        },
        {
            "id": "55",
            "entry": "[55] K. Zhang, Q. Liu, Y. Wu, and M.-H. Yang. Robust visual tracking via convolutional networks without training. IEEE TIP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20K.%20Liu%2C%20Q.%20Wu%2C%20Y.%20Yang%2C%20M.-H.%20Robust%20visual%20tracking%20via%20convolutional%20networks%20without%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20K.%20Liu%2C%20Q.%20Wu%2C%20Y.%20Yang%2C%20M.-H.%20Robust%20visual%20tracking%20via%20convolutional%20networks%20without%20training%202016"
        },
        {
            "id": "56",
            "entry": "[56] L. Zhang, J. Varadarajan, P. N. Suganthan, N. Ahuja, and P. Moulin. Robust visual tracking using oblique random forests. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20L.%20Varadarajan%2C%20J.%20Suganthan%2C%20P.N.%20Ahuja%2C%20N.%20Robust%20visual%20tracking%20using%20oblique%20random%20forests%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20L.%20Varadarajan%2C%20J.%20Suganthan%2C%20P.N.%20Ahuja%2C%20N.%20Robust%20visual%20tracking%20using%20oblique%20random%20forests%202017"
        },
        {
            "id": "57",
            "entry": "[57] T. Zhang, C. Xu, and M.-H. Yang. Multi-task correlation particle filter for robust object tracking. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20T.%20Xu%2C%20C.%20Yang%2C%20M.-H.%20Multi-task%20correlation%20particle%20filter%20for%20robust%20object%20tracking%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20T.%20Xu%2C%20C.%20Yang%2C%20M.-H.%20Multi-task%20correlation%20particle%20filter%20for%20robust%20object%20tracking%202017"
        },
        {
            "id": "58",
            "entry": "[58] G. Zhu, F. Porikli, and H. Li. Beyond local search: Tracking objects everywhere with instancespecific proposals. In CVPR, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20G.%20Porikli%2C%20F.%20Li%2C%20H.%20Beyond%20local%20search%3A%20Tracking%20objects%20everywhere%20with%20instancespecific%20proposals%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20G.%20Porikli%2C%20F.%20Li%2C%20H.%20Beyond%20local%20search%3A%20Tracking%20objects%20everywhere%20with%20instancespecific%20proposals%202016"
        }
    ]
}
