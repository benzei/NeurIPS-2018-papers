{
    "filename": "7563-near-optimal-exploration-exploitation-in-non-communicating-markov-decision-processes.pdf",
    "metadata": {
        "title": "Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes",
        "author": "Ronan Fruit, Matteo Pirotta, Alessandro Lazaric",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7563-near-optimal-exploration-exploitation-in-non-communicating-markov-decision-processes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "While designing the state space of an MDP, it is common to include states that are transient or not reachable by any policy (e.g., in mountain car, the product space of speed and position contains configurations that are not physically reachable). This results in weakly-communicating or multi-chain MDPs. In this paper, we introduce TUCRL, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular, for any MDP with SC communicating states\u221a, A actions and \u0393C \u2264 SC possible communicating next states, we derive a O(DC \u0393CSCAT ) regret bound, where DC is the diameter (i.e., the length of the longest shortest path between any two states) of the communicating part of the MDP. This is in contrast with existing optimistic algorithms (e.g., UCRL, Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs, as well as posterior sampling or regularised algorithms (e.g., REGAL), which require prior knowledge on the bias span of the optimal policy to achieve sub-linear regret. We also prove that in weaklycommunicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally, we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art."
    },
    "keywords": [
        {
            "term": "Markov Decision Process",
            "url": "https://en.wikipedia.org/wiki/Markov_Decision_Process"
        },
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "REGAL",
            "url": "https://en.wikipedia.org/wiki/Regal"
        },
        {
            "term": "markov decision processes",
            "url": "https://en.wikipedia.org/wiki/markov_decision_processes"
        }
    ],
    "highlights": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] studies the problem of learning in sequential decision-making problems where the dynamics of the environment is unknown, but can be learnt by performing actions and observing their outcome in an online fashion",
        "We present Truncated Upper-Confidence for Reinforcement Learning, an algorithm designed to trade-off exploration and exploitation in weakly-communicating and multi-chain Markov Decision Process (e.g., Markov Decision Process with misspecified states) without any prior knowledge and under the only assumption that the agent starts from a state in a communicating subset of the Markov Decision Process (Sec. 3)",
        "We introduce Truncated Upper-Confidence for Reinforcement Learning (TUCRL), an optimistic online Reinforcement learning algorithm that efficiently balances exploration and exploitation to learn in non-communicating Markov Decision Process without prior knowledge (Fig. 2)",
        "We introduced Truncated Upper-Confidence for Reinforcement Learning, an algorithm that efficiently balances exploration and exploitation in weaklycommunicating and multi-chain Markov Decision Process, when the starting state s1 belongs to a communicating set (Asm. 1)",
        "We showed that Truncated Upper-Confidence for Reinforcement Learning achieves a square-root regret bound and that, in the general case, it is not possible to design algorithm with logarithmic regret and polynomial dependence on the Markov Decision Process parameters",
        "Several questions remain open: 1) relaxing Asm. 1 by considering a transient initial state (i.e., s1 \u2208 ST), 2) refining the lower bound of Jaksch et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to understand whether it is possible to scale with spS {h\u2217} instead of D without any prior knowledge"
    ],
    "key_statements": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] studies the problem of learning in sequential decision-making problems where the dynamics of the environment is unknown, but can be learnt by performing actions and observing their outcome in an online fashion",
        "We focus on the regret framework in infinite-horizon average-reward problems [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>], where the exploration-exploitation performance is evaluated by comparing the rewards accumulated by the learning agent and an optimal policy",
        "An alternative approach is posterior sampling (PS) [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], which maintains a posterior distribution over Markov Decision Process and, at each step, samples an Markov Decision Process and executes the corresponding optimal policy [e.g., 8, 9, 10, 11, 12]",
        "B we show on a very simple\n1We notice that the problem of weakly-communicating Markov Decision Process and misspecified states does not hold in the more restrictive setting of finite horizon [e.g., 8] since exploration is directly tailored to the states that are reachable within the known horizon, or under the assumption of the existence of a recurrent state [e.g., 16]",
        "We present Truncated Upper-Confidence for Reinforcement Learning, an algorithm designed to trade-off exploration and exploitation in weakly-communicating and multi-chain Markov Decision Process (e.g., Markov Decision Process with misspecified states) without any prior knowledge and under the only assumption that the agent starts from a state in a communicating subset of the Markov Decision Process (Sec. 3)",
        "When the true Markov Decision Process is weakly-communicating, we prove that Truncated Upper-Confidence for Reinforcement Learning achieves a O( T ) regret that with polynomial dependency on the Markov Decision Process parameters",
        "We introduce Truncated Upper-Confidence for Reinforcement Learning (TUCRL), an optimistic online Reinforcement learning algorithm that efficiently balances exploration and exploitation to learn in non-communicating Markov Decision Process without prior knowledge (Fig. 2)",
        "We prove that the regret of Truncated Upper-Confidence for Reinforcement Learning is bounded as follows",
        "E we show that a minor modification to the confidence intervals of Mk makes the shortest paths between any two states s, s \u2208 SkC equivalent in both sets of plausible Markov Decision Process, providing the bound spSkC {wk} \u2264 DC",
        "We further study Truncated Upper-Confidence for Reinforcement Learning regret in the simple three-state domain introduced in [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] (see App",
        "SCAL and Truncated Upper-Confidence for Reinforcement Learning seem to achieve different growth rates in re\u221agret: while SCAL appears to reach a logarithmic growth, the regret of Truncated Upper-Confidence for Reinforcement Learning seems to grow as T with periodic \u201cjumps\u201d that are increasingly distant from each other. This can be explained by the way the algorithm works: while most of the time Truncated Upper-Confidence for Reinforcement Learning is optimistic on the restricted state space SC (i.e., SkC = SC), it periodically allows transitions to the set ST (i.e., SkC = S), which is not reachable",
        "Since Truncated Upper-Confidence for Reinforcement Learning satisfies 1., it cannot satisfy 2. This matches the empirical results presented in Sec. 4 where we observed that when the diameter is infin\u221aite, the growth rates of the regret of SCAL and Truncated Upper-Confidence for Reinforcement Learning were respectively logarithmic and of order \u0398( T )",
        "We introduced Truncated Upper-Confidence for Reinforcement Learning, an algorithm that efficiently balances exploration and exploitation in weaklycommunicating and multi-chain Markov Decision Process, when the starting state s1 belongs to a communicating set (Asm. 1)",
        "We showed that Truncated Upper-Confidence for Reinforcement Learning achieves a square-root regret bound and that, in the general case, it is not possible to design algorithm with logarithmic regret and polynomial dependence on the Markov Decision Process parameters",
        "Several questions remain open: 1) relaxing Asm. 1 by considering a transient initial state (i.e., s1 \u2208 ST), 2) refining the lower bound of Jaksch et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to understand whether it is possible to scale with spS {h\u2217} instead of D without any prior knowledge"
    ],
    "summary": [
        "Reinforcement learning (RL) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] studies the problem of learning in sequential decision-making problems where the dynamics of the environment is unknown, but can be learnt by performing actions and observing their outcome in an online fashion.",
        "Whenever the state space is misspecified or the MDP is weakly communicating (i.e., D = +\u221e), OFU-based algorithms (e.g.,UCRL) optimistically attribute large reward and non-zero probability to reach states that have never been observed, and they tend to repeatedly attempt to explore unreachable states.",
        "The regret is still defined as before, where the learning performance is compared to the optimal gain g\u2217(s1) related to the communicating set of states SC s1.",
        "We introduce Truncated Upper-Confidence for Reinforcement Learning (TUCRL), an optimistic online RL algorithm that efficiently balances exploration and exploitation to learn in non-communicating MDPs without prior knowledge (Fig. 2).",
        "We start focusing on the poorly visited state-action pairs, i.e., (s, a) \u2208 Kk. In this case TUCRL may suffer the maximum per-step regret rmax but the number of times this event happen is cumulatively \u201csmall\u201d (App. D.4.1): 4Notice that M \u2217 \u2208 Mk is true w.h.p. since Mk is obtained using non-truncated confidence intervals.",
        "In App. E we show that a minor modification to the confidence intervals of Mk makes the shortest paths between any two states s, s \u2208 SkC equivalent in both sets of plausible MDPs, providing the bound spSkC {wk} \u2264 DC.",
        "This can be explained by the way the algorithm works: while most of the time TUCRL is optimistic on the restricted state space SC (i.e., SkC = SC), it periodically allows transitions to the set ST (i.e., SkC = S), which is not reachable.",
        "We further investigate the empirical difference between SCAL and TUCRL and prove an impossibility result characterising the exploration-exploitation dilemma when the diameter is allowed to be infinite and no prior knowledge on the optimal bias span is available.",
        "This matches the empirical results presented in Sec. 4 where we observed that when the diameter is infin\u221aite, the growth rates of the regret of SCAL and TUCRL were respectively logarithmic and of order \u0398( T ).",
        "We introduced TUCRL, an algorithm that efficiently balances exploration and exploitation in weaklycommunicating and multi-chain MDPs, when the starting state s1 belongs to a communicating set (Asm. 1).",
        "We showed that TUCRL achieves a square-root regret bound and that, in the general case, it is not possible to design algorithm with logarithmic regret and polynomial dependence on the MDP parameters.",
        "Several questions remain open: 1) relaxing Asm. 1 by considering a transient initial state (i.e., s1 \u2208 ST), 2) refining the lower bound of Jaksch et al [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to understand whether it is possible to scale with spS {h\u2217} instead of D without any prior knowledge"
    ],
    "headline": "We introduce Truncated Upper-Confidence for Reinforcement Learning, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process  without requiring any form of prior knowledge",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "2",
            "entry": "[2] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11:1563\u20131600, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaksch%2C%20Thomas%20Ortner%2C%20Ronald%20Auer%2C%20Peter%20Near-optimal%20regret%20bounds%20for%20reinforcement%20learning%202010"
        },
        {
            "id": "3",
            "entry": "[3] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In UAI, pages 35\u201342. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Tewari%2C%20Ambuj%20REGAL%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20MDPs%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Tewari%2C%20Ambuj%20REGAL%3A%20A%20regularization%20based%20algorithm%20for%20reinforcement%20learning%20in%20weakly%20communicating%20MDPs%202009"
        },
        {
            "id": "4",
            "entry": "[4] Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Emma Brunskill. Regret minimization in mdps with options without prior knowledge. In NIPS, pages 3169\u20133179, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fruit%2C%20Ronan%20Pirotta%2C%20Matteo%20Lazaric%2C%20Alessandro%20Brunskill%2C%20Emma%20Regret%20minimization%20in%20mdps%20with%20options%20without%20prior%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fruit%2C%20Ronan%20Pirotta%2C%20Matteo%20Lazaric%2C%20Alessandro%20Brunskill%2C%20Emma%20Regret%20minimization%20in%20mdps%20with%20options%20without%20prior%20knowledge%202017"
        },
        {
            "id": "5",
            "entry": "[5] Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for undiscounted reinforcement learning in mdps. In ALT, volume 83 of Proceedings of Machine Learning Research, pages 770\u2013805. PMLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talebi%2C%20Mohammad%20Sadegh%20Maillard%2C%20Odalric-Ambrym%20Variance-aware%20regret%20bounds%20for%20undiscounted%20reinforcement%20learning%20in%20mdps%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Talebi%2C%20Mohammad%20Sadegh%20Maillard%2C%20Odalric-Ambrym%20Variance-aware%20regret%20bounds%20for%20undiscounted%20reinforcement%20learning%20in%20mdps%202018"
        },
        {
            "id": "6",
            "entry": "[6] Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient bias-spanconstrained exploration-exploitation in reinforcement learning. CoRR, abs/1802.04020, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04020"
        },
        {
            "id": "7",
            "entry": "[7] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20William%20R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "8",
            "entry": "[8] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In NIPS, pages 3003\u20133011, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20efficient%20reinforcement%20learning%20via%20posterior%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Russo%2C%20Daniel%20Roy%2C%20Benjamin%20Van%20efficient%20reinforcement%20learning%20via%20posterior%20sampling%202013"
        },
        {
            "id": "9",
            "entry": "[9] Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Bayesian optimal control of smoothly parameterized systems. In UAI, pages 1\u201311. AUAI Press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Bayesian%20optimal%20control%20of%20smoothly%20parameterized%20systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Bayesian%20optimal%20control%20of%20smoothly%20parameterized%20systems%202015"
        },
        {
            "id": "10",
            "entry": "[10] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning? In ICML, volume 70 of Proceedings of Machine Learning Research, pages 2701\u20132710. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Why%20is%20posterior%20sampling%20better%20than%20optimism%20for%20reinforcement%20learning%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Roy%2C%20Benjamin%20Van%20Why%20is%20posterior%20sampling%20better%20than%20optimism%20for%20reinforcement%20learning%3F%202017"
        },
        {
            "id": "11",
            "entry": "[11] Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning unknown markov decision processes: A thompson sampling approach. In NIPS, pages 1333\u20131342, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ouyang%2C%20Yi%20Gagrani%2C%20Mukul%20Nayyar%2C%20Ashutosh%20Jain%2C%20Rahul%20Learning%20unknown%20markov%20decision%20processes%3A%20A%20thompson%20sampling%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ouyang%2C%20Yi%20Gagrani%2C%20Mukul%20Nayyar%2C%20Ashutosh%20Jain%2C%20Rahul%20Learning%20unknown%20markov%20decision%20processes%3A%20A%20thompson%20sampling%20approach%202017"
        },
        {
            "id": "12",
            "entry": "[12] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. In NIPS, pages 1184\u20131194, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Optimistic%20posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20Shipra%20Jia%2C%20Randy%20Optimistic%20posterior%20sampling%20for%20reinforcement%20learning%3A%20worst-case%20regret%20bounds%202017"
        },
        {
            "id": "13",
            "entry": "[13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "14",
            "entry": "[14] Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University of Cambridge, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moore%2C%20Andrew%20William%20Efficient%20memory-based%20learning%20for%20robot%20control%201990"
        },
        {
            "id": "15",
            "entry": "[15] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of IEEE 36th Annual Foundations of Computer Science, pages 322\u2013331, Oct 1995. doi: 10.1109/SFCS.1995.492488.",
            "crossref": "https://dx.doi.org/10.1109/SFCS.1995.492488",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1109/SFCS.1995.492488"
        },
        {
            "id": "16",
            "entry": "[16] Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized markov decision processes. In COLT, volume 40 of JMLR Workshop and Conference Proceedings, pages 861\u2013898. JMLR.org, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopalan%2C%20Aditya%20Mannor%2C%20Shie%20Thompson%20sampling%20for%20learning%20parameterized%20markov%20decision%20processes%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopalan%2C%20Aditya%20Mannor%2C%20Shie%20Thompson%20sampling%20for%20learning%20parameterized%20markov%20decision%20processes%202015"
        },
        {
            "id": "17",
            "entry": "[17] Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Bayesian optimal control of smoothly parameterized systems. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI\u201915, pages 2\u201311, Arlington, Virginia, United States, 2015. AUAI Press. ISBN 978-0-9966431-0-8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Bayesian%20optimal%20control%20of%20smoothly%20parameterized%20systems",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbasi-Yadkori%2C%20Yasin%20Szepesv%C3%A1ri%2C%20Csaba%20Bayesian%20optimal%20control%20of%20smoothly%20parameterized%20systems"
        },
        {
            "id": "18",
            "entry": "[18] Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning unknown markov decision processes: A thompson sampling approach. In Advances in Neural Information Processing Systems 30, pages 1333\u20131342. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ouyang%2C%20Yi%20Gagrani%2C%20Mukul%20Nayyar%2C%20Ashutosh%20Jain%2C%20Rahul%20Learning%20unknown%20markov%20decision%20processes%3A%20A%20thompson%20sampling%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ouyang%2C%20Yi%20Gagrani%2C%20Mukul%20Nayyar%2C%20Ashutosh%20Jain%2C%20Rahul%20Learning%20unknown%20markov%20decision%20processes%3A%20A%20thompson%20sampling%20approach%202017"
        },
        {
            "id": "19",
            "entry": "[19] Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, and Nikos Vlassis. Posterior sampling for large scale reinforcement learning. CoRR, abs/1711.07979, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.07979"
        },
        {
            "id": "20",
            "entry": "[20] Ian Osband and Benjamin Van Roy. Posterior sampling for reinforcement learning without episodes. CoRR, abs/1608.02731, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.02731"
        },
        {
            "id": "21",
            "entry": "[21] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY, USA, 1994. ISBN 0471619779.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Puterman%2C%20Martin%20L.%20Markov%20Decision%20Processes%3A%20Discrete%20Stochastic%20Dynamic%20Programming%201994"
        },
        {
            "id": "22",
            "entry": "[22] Jean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Tuning bandit algorithms in stochastic environments. In Algorithmic Learning Theory, pages 150\u2013165, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20Jean-Yves%20Munos%2C%20R%C3%A9mi%20Szepesv%C3%A1ri%2C%20Csaba%20Tuning%20bandit%20algorithms%20in%20stochastic%20environments%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20Jean-Yves%20Munos%2C%20R%C3%A9mi%20Szepesv%C3%A1ri%2C%20Csaba%20Tuning%20bandit%20algorithms%20in%20stochastic%20environments%202007"
        },
        {
            "id": "23",
            "entry": "[23] Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample-variance penalization. In COLT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20Andreas%20Pontil%2C%20Massimiliano%20Empirical%20bernstein%20bounds%20and%20sample-variance%20penalization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20Pontil%2C%20Massimiliano%20Empirical%20bernstein%20bounds%20and%20sample-variance%20penalization%202009"
        },
        {
            "id": "24",
            "entry": "[24] Thomas G. Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. J. Artif. Intell. Res., 13:227\u2013303, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dietterich%2C%20Thomas%20G.%20Hierarchical%20reinforcement%20learning%20with%20the%20MAXQ%20value%20function%20decomposition%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dietterich%2C%20Thomas%20G.%20Hierarchical%20reinforcement%20learning%20with%20the%20MAXQ%20value%20function%20decomposition%202000"
        },
        {
            "id": "25",
            "entry": "[25] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "26",
            "entry": "[26] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. Adaptive computation and machine learning. MIT Press, second edition, 2018. ISBN 9780262039246.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Barto%2C%20Andrew%20G.%20Reinforcement%20learning%3A%20An%20introduction.%20Adaptive%20computation%20and%20machine%20learning%202018"
        },
        {
            "id": "27",
            "entry": "[27] Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, and Daniil Ryabko. Optimal regret bounds for selecting the state representation in reinforcement learning. In Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 543\u2013551, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maillard%2C%20Odalric-Ambrym%20Nguyen%2C%20Phuong%20Ortner%2C%20Ronald%20Ryabko%2C%20Daniil%20Optimal%20regret%20bounds%20for%20selecting%20the%20state%20representation%20in%20reinforcement%20learning%202013-06-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maillard%2C%20Odalric-Ambrym%20Nguyen%2C%20Phuong%20Ortner%2C%20Ronald%20Ryabko%2C%20Daniil%20Optimal%20regret%20bounds%20for%20selecting%20the%20state%20representation%20in%20reinforcement%20learning%202013-06-17"
        }
    ]
}
