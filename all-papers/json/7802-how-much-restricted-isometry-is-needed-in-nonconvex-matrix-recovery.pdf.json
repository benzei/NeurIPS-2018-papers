{
    "filename": "7802-how-much-restricted-isometry-is-needed-in-nonconvex-matrix-recovery.pdf",
    "metadata": {
        "title": "How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?",
        "author": "Richard Zhang, Cedric Josz, Somayeh Sojoudi, Javad Lavaei",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7802-how-much-restricted-isometry-is-needed-in-nonconvex-matrix-recovery.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "When the linear measurements of an instance of low-rank matrix recovery satisfy a restricted isometry property (RIP)\u2014i.e. they are approximately normpreserving\u2014the problem is known to contain no spurious local minima, so exact recovery is guaranteed. In this paper, we show that moderate RIP is not enough to eliminate spurious local minima, so existing results can only hold for near-perfect RIP. In fact, counterexamples are ubiquitous: we prove that every x is the spurious local minimum of a rank-1 instance of matrix recovery that satisfies RIP. One specific counterexample has RIP constant \u03b4 = 1/2, but causes randomly initialized stochastic gradient descent (SGD) to fail 12% of the time. SGD is frequently able to avoid and escape spurious local minima, but this empirical result shows that it can occasionally be defeated by their existence. Hence, while exact recovery guarantees will likely require a proof of no spurious local minima, arguments based solely on norm preservation will only be applicable to a narrow set of nearly-isotropic instances."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "restricted isometry property",
            "url": "https://en.wikipedia.org/wiki/restricted_isometry_property"
        },
        {
            "term": "restricted isometry",
            "url": "https://en.wikipedia.org/wiki/restricted_isometry"
        },
        {
            "term": "linear matrix inequality",
            "url": "https://en.wikipedia.org/wiki/linear_matrix_inequality"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Several important nonconvex problems in machine learning have been shown to contain no spurious local minima [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "The usual firstand second-order necessary conditions for local optimality are sufficient for global optimality; satisfying them to -accuracy will yield a point within an -neighborhood of a globally optimal solution",
        "Our results show that spurious local minima can exist arbitrarily close to the solution",
        "Most of these existing results are based on a norm-preserving argument: relating A \u2248 xxT \u2212 Z F and arguing that a lack of spurious local minima in the latter implies a similar statement in the former",
        "Our key message in this paper is that moderate restricted isometry property is not enough to eliminate spurious local minima",
        "We formulate a convex optimization problem in Section 2 that generates counterexamples that satisfy restricted isometry property but contain spurious local minima. Solving this convex formulation in closed-form in Section 3 shows that counterexamples are ubiquitous: almost any rank-1 Z 0 and any x \u2208 Rn can respectively be the ground truth and spurious local minimum to an instance of matrix recovery satisfying restricted isometry property"
    ],
    "key_statements": [
        "Several important nonconvex problems in machine learning have been shown to contain no spurious local minima [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>]",
        "The usual firstand second-order necessary conditions for local optimality are sufficient for global optimality; satisfying them to -accuracy will yield a point within an -neighborhood of a globally optimal solution",
        "In Section 3, we prove that, in the rank-1 case, almost every choice of x, Z generates an instance of (1) with a strict spurious local minimum",
        "Our results show that spurious local minima can exist arbitrarily close to the solution",
        "Most of these existing results are based on a norm-preserving argument: relating A \u2248 xxT \u2212 Z F and arguing that a lack of spurious local minima in the latter implies a similar statement in the former",
        "Our key message in this paper is that moderate restricted isometry property is not enough to eliminate spurious local minima",
        "We formulate a convex optimization problem in Section 2 that generates counterexamples that satisfy restricted isometry property but contain spurious local minima. Solving this convex formulation in closed-form in Section 3 shows that counterexamples are ubiquitous: almost any rank-1 Z 0 and any x \u2208 Rn can respectively be the ground truth and spurious local minimum to an instance of matrix recovery satisfying restricted isometry property"
    ],
    "summary": [
        "Several important nonconvex problems in machine learning have been shown to contain no spurious local minima [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>].",
        "There exists an instance of (1) satisfying (n, \u03b4n)-RIP with \u03b4n < 1 that has Z = zzT as the ground truth and x as a strict spurious local minimum, i.e. with zero gradient and a positive definite Hessian.",
        "Given arbitrary x \u2208 Rn\u00d7r and rank-r positive semidefinite matrix Z \u2208 Sn, consider the problem of finding an instance of (1) with Z as the ground truth and x as a spurious local minimum.",
        "If the optimal value \u03b7 is strictly positive, the recovered A yields an RIP instance of (1) with zzT as the ground truth and x as a spurious local minimum, as desired.",
        "We write f (x) = A 2, and optimize over the spurious local minimum x \u2208 Rn\u00d7r, the rank-r ground truth Z 0, and the linear operator A : Rn\u00d7n \u2192 Rm. Note that \u03b4 gives a \u201cno spurious local minima\u201d guarantee, due to the inexistence of counterexamples.",
        "Figure 2: \u201cBad\u201d instance (n = 12, r = 2) with RIP constant \u03b4 = 0.973 and spurious local min at xloc satisfying xxT F / zzT F \u2248 4.",
        "Figure 3: \u201cGood\u201d instance (n = 12, r = 1) with RIP constant \u03b4 = 1/2 and spurious local min at xloc satisfying xxT F / zzT F = 1/2 and xT z = 0.",
        "This section empirically measures SGD on two instances of (1) with different values of \u03b4, both engineered to contain spurious local minima by numerically solving (10).",
        "Recent results have shown that if the linear measurements of the low-rank matrix satisfy a restricted isometry property (RIP), the problem contains no spurious local minima, so exact recovery is guaranteed.",
        "Most of these existing results are based on a norm-preserving argument: relating A \u2248 xxT \u2212 Z F and arguing that a lack of spurious local minima in the latter implies a similar statement in the former.",
        "We formulate a convex optimization problem in Section 2 that generates counterexamples that satisfy RIP but contain spurious local minima.",
        "Solving this convex formulation in closed-form in Section 3 shows that counterexamples are ubiquitous: almost any rank-1 Z 0 and any x \u2208 Rn can respectively be the ground truth and spurious local minimum to an instance of matrix recovery satisfying RIP.",
        "In Section 5, randomly initialized SGD solved one example with a 100% success rate over 1,000 trials, despite the presence of spurious local minima."
    ],
    "headline": "We show that moderate restricted isometry property is not enough to eliminate spurious local minima, so existing results can only hold for near-perfect restricted isometry property",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alekh Agarwal, Olivier Chapelle, Miroslav Dud\u00edk, and John Langford. A reliable effective terascale linear learning system. The Journal of Machine Learning Research, 15(1):1111\u2013 1133, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20Alekh%20Chapelle%2C%20Olivier%20Dud%C3%ADk%2C%20Miroslav%20Langford%2C%20John%20A%20reliable%20effective%20terascale%20linear%20learning%20system",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20Alekh%20Chapelle%2C%20Olivier%20Dud%C3%ADk%2C%20Miroslav%20Langford%2C%20John%20A%20reliable%20effective%20terascale%20linear%20learning%20system"
        },
        {
            "id": "2",
            "entry": "[2] Erling D Andersen and Knud D Andersen. The MOSEK interior point optimizer for linear programming: an implementation of the homogeneous algorithm. In High performance optimization, pages 197\u2013232.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andersen%2C%20Erling%20D.%20Andersen%2C%20Knud%20D.%20The%20MOSEK%20interior%20point%20optimizer%20for%20linear%20programming%3A%20an%20implementation%20of%20the%20homogeneous%20algorithm",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andersen%2C%20Erling%20D.%20Andersen%2C%20Knud%20D.%20The%20MOSEK%20interior%20point%20optimizer%20for%20linear%20programming%3A%20an%20implementation%20of%20the%20homogeneous%20algorithm"
        },
        {
            "id": "3",
            "entry": "[3] Francis R Bach. Consistency of trace norm minimization. Journal of Machine Learning Research, 9(Jun):1019\u20131048, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20R.%20Consistency%20of%20trace%20norm%20minimization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20R.%20Consistency%20of%20trace%20norm%20minimization%202008"
        },
        {
            "id": "4",
            "entry": "[4] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873\u20133881, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Srebro%2C%20Nati%20Global%20optimality%20of%20local%20search%20for%20low%20rank%20matrix%20recovery%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Srebro%2C%20Nati%20Global%20optimality%20of%20local%20search%20for%20low%20rank%20matrix%20recovery%202016"
        },
        {
            "id": "5",
            "entry": "[5] L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010, pages 177\u2013186.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent"
        },
        {
            "id": "6",
            "entry": "[6] L\u00e9on Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural information processing systems, pages 161\u2013168, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Bousquet%2C%20Olivier%20The%20tradeoffs%20of%20large%20scale%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Bousquet%2C%20Olivier%20The%20tradeoffs%20of%20large%20scale%20learning%202008"
        },
        {
            "id": "7",
            "entry": "[7] Nicolas Boumal, P-A Absil, and Coralia Cartis. Global rates of convergence for nonconvex optimization on manifolds. IMA Journal of Numerical Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nicolas%20Boumal%2C%20P.-A.Absil%20Cartis%2C%20Coralia%20Global%20rates%20of%20convergence%20for%20nonconvex%20optimization%20on%20manifolds%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nicolas%20Boumal%2C%20P.-A.Absil%20Cartis%2C%20Coralia%20Global%20rates%20of%20convergence%20for%20nonconvex%20optimization%20on%20manifolds%202018"
        },
        {
            "id": "8",
            "entry": "[8] Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro approach works on smooth semidefinite programs. In Advances in Neural Information Processing Systems, pages 2757\u20132765, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boumal%2C%20Nicolas%20Voroninski%2C%20Vlad%20Bandeira%2C%20Afonso%20The%20non-convex%20Burer-Monteiro%20approach%20works%20on%20smooth%20semidefinite%20programs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boumal%2C%20Nicolas%20Voroninski%2C%20Vlad%20Bandeira%2C%20Afonso%20The%20non-convex%20Burer-Monteiro%20approach%20works%20on%20smooth%20semidefinite%20programs%202016"
        },
        {
            "id": "9",
            "entry": "[9] Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. Linear matrix inequalities in system and control theory, volume 15. SIAM, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Ghaoui%2C%20Laurent%20El%20Feron%2C%20Eric%20Balakrishnan%2C%20Venkataramanan%20Linear%20matrix%20inequalities%20in%20system%20and%20control%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20Stephen%20Ghaoui%2C%20Laurent%20El%20Feron%2C%20Eric%20Balakrishnan%2C%20Venkataramanan%20Linear%20matrix%20inequalities%20in%20system%20and%20control%201994"
        },
        {
            "id": "10",
            "entry": "[10] T Tony Cai and Anru Zhang. Sharp RIP bound for sparse signal and low-rank matrix recovery. Applied and Computational Harmonic Analysis, 35(1):74\u201393, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20T.Tony%20Zhang%2C%20Anru%20Sharp%20RIP%20bound%20for%20sparse%20signal%20and%20low-rank%20matrix%20recovery%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20T.Tony%20Zhang%2C%20Anru%20Sharp%20RIP%20bound%20for%20sparse%20signal%20and%20low-rank%20matrix%20recovery%202013"
        },
        {
            "id": "11",
            "entry": "[11] Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57(4):2342\u20132359, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Plan%2C%20Yaniv%20Tight%20oracle%20inequalities%20for%20low-rank%20matrix%20recovery%20from%20a%20minimal%20number%20of%20noisy%20random%20measurements%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Plan%2C%20Yaniv%20Tight%20oracle%20inequalities%20for%20low-rank%20matrix%20recovery%20from%20a%20minimal%20number%20of%20noisy%20random%20measurements%202011"
        },
        {
            "id": "12",
            "entry": "[12] Emmanuel J Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009"
        },
        {
            "id": "13",
            "entry": "[13] Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on pure and applied mathematics, 59(8):1207\u20131223, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Romberg%2C%20Justin%20K.%20Tao%2C%20Terence%20Stable%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20measurements%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Romberg%2C%20Justin%20K.%20Tao%2C%20Terence%20Stable%20signal%20recovery%20from%20incomplete%20and%20inaccurate%20measurements%202006"
        },
        {
            "id": "14",
            "entry": "[14] Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on information theory, 51(12):4203\u20134215, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Decoding%20by%20linear%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20Decoding%20by%20linear%20programming%202005"
        },
        {
            "id": "15",
            "entry": "[15] Emmanuel J Cand\u00e8s and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053\u20132080, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20The%20power%20of%20convex%20relaxation%3A%20Near-optimal%20matrix%20completion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20The%20power%20of%20convex%20relaxation%3A%20Near-optimal%20matrix%20completion%202010"
        },
        {
            "id": "16",
            "entry": "[16] Coralia Cartis, Nicholas IM Gould, and Ph L Toint. Complexity bounds for second-order optimality in unconstrained optimization. Journal of Complexity, 28(1):93\u2013108, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20I.M.%20and%20Ph%20L%20Toint.%20Complexity%20bounds%20for%20second-order%20optimality%20in%20unconstrained%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20Coralia%20Gould%2C%20Nicholas%20I.M.%20and%20Ph%20L%20Toint.%20Complexity%20bounds%20for%20second-order%20optimality%20in%20unconstrained%20optimization%202012"
        },
        {
            "id": "17",
            "entry": "[17] Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1. SIAM, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrew%20R%20Conn%20Nicholas%20IM%20Gould%20and%20Ph%20L%20Toint%20Trust%20region%20methods%20volume%201%20SIAM%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrew%20R%20Conn%20Nicholas%20IM%20Gould%20and%20Ph%20L%20Toint%20Trust%20region%20methods%20volume%201%20SIAM%202000"
        },
        {
            "id": "18",
            "entry": "[18] Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pages 1067\u20131077, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20S.%20Jin%2C%20Chi%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%20Gradient%20descent%20can%20take%20exponential%20time%20to%20escape%20saddle%20points%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20S.%20Jin%2C%20Chi%20Lee%2C%20Jason%20D.%20Jordan%2C%20Michael%20I.%20Gradient%20descent%20can%20take%20exponential%20time%20to%20escape%20saddle%20points%202017"
        },
        {
            "id": "19",
            "entry": "[19] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2013online stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797\u2013842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%93online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20points%E2%80%93online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "20",
            "entry": "[20] Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In International Conference on Machine Learning, pages 1233\u2013 1242, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Jin%2C%20Chi%20Zheng%2C%20Yi%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Jin%2C%20Chi%20Zheng%2C%20Yi%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017"
        },
        {
            "id": "21",
            "entry": "[21] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pages 2973\u20132981, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Lee%2C%20Jason%20D.%20Ma%2C%20Tengyu%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Lee%2C%20Jason%20D.%20Ma%2C%20Tengyu%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016"
        },
        {
            "id": "22",
            "entry": "[22] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, pages 1225\u2013 1234, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Recht%2C%20Ben%20Singer%2C%20Yoram%20Train%20faster%2C%20generalize%20better%3A%20Stability%20of%20stochastic%20gradient%20descent%202016"
        },
        {
            "id": "23",
            "entry": "[23] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 665\u2013674. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Prateek%20Netrapalli%2C%20Praneeth%20Sanghavi%2C%20Sujay%20Low-rank%20matrix%20completion%20using%20alternating%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Prateek%20Netrapalli%2C%20Praneeth%20Sanghavi%2C%20Sujay%20Low-rank%20matrix%20completion%20using%20alternating%20minimization%202013"
        },
        {
            "id": "24",
            "entry": "[24] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International Conference on Machine Learning, pages 1724\u20131732, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017"
        },
        {
            "id": "25",
            "entry": "[25] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE Transactions on Information Theory, 56(6):2980\u20132998, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keshavan%2C%20Raghunandan%20H.%20Montanari%2C%20Andrea%20Oh%2C%20Sewoong%20Matrix%20completion%20from%20a%20few%20entries%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keshavan%2C%20Raghunandan%20H.%20Montanari%2C%20Andrea%20Oh%2C%20Sewoong%20Matrix%20completion%20from%20a%20few%20entries%202010"
        },
        {
            "id": "26",
            "entry": "[26] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11(Jul):2057\u20132078, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keshavan%2C%20Raghunandan%20H.%20Montanari%2C%20Andrea%20Oh%2C%20Sewoong%20Matrix%20completion%20from%20noisy%20entries%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keshavan%2C%20Raghunandan%20H.%20Montanari%2C%20Andrea%20Oh%2C%20Sewoong%20Matrix%20completion%20from%20noisy%20entries%202010"
        },
        {
            "id": "27",
            "entry": "[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "28",
            "entry": "[28] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246\u20131257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016"
        },
        {
            "id": "29",
            "entry": "[29] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Polyak%2C%20Boris%20T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "30",
            "entry": "[30] Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach. In Artificial Intelligence and Statistics, pages 65\u201374, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Park%2C%20Dohyung%20Kyrillidis%2C%20Anastasios%20Carmanis%2C%20Constantine%20Sanghavi%2C%20Sujay%20Non-square%20matrix%20sensing%20without%20spurious%20local%20minima%20via%20the%20Burer-Monteiro%20approach%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Park%2C%20Dohyung%20Kyrillidis%2C%20Anastasios%20Carmanis%2C%20Constantine%20Sanghavi%2C%20Sujay%20Non-square%20matrix%20sensing%20without%20spurious%20local%20minima%20via%20the%20Burer-Monteiro%20approach%202017"
        },
        {
            "id": "31",
            "entry": "[31] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471\u2013501, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Fazel%2C%20Maryam%20and%20Pablo%20A%20Parrilo.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Fazel%2C%20Maryam%20and%20Pablo%20A%20Parrilo.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010"
        },
        {
            "id": "32",
            "entry": "[32] Benjamin Recht and Christopher R\u00e9. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5(2):201\u2013226, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20R%C3%A9%2C%20Christopher%20Parallel%20stochastic%20gradient%20algorithms%20for%20large-scale%20matrix%20completion%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20R%C3%A9%2C%20Christopher%20Parallel%20stochastic%20gradient%20algorithms%20for%20large-scale%20matrix%20completion%202013"
        },
        {
            "id": "33",
            "entry": "[33] Jos F Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization methods and software, 11(1-4):625\u2013653, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sturm%2C%20Jos%20F.%20Using%20SeDuMi%201.02%2C%20a%20MATLAB%20toolbox%20for%20optimization%20over%20symmetric%20cones%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sturm%2C%20Jos%20F.%20Using%20SeDuMi%201.02%2C%20a%20MATLAB%20toolbox%20for%20optimization%20over%20symmetric%20cones%201999"
        },
        {
            "id": "34",
            "entry": "[34] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery using nonconvex optimization. In International Conference on Machine Learning, pages 2351\u20132360, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20Complete%20dictionary%20recovery%20using%20nonconvex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20Complete%20dictionary%20recovery%20using%20nonconvex%20optimization%202015"
        },
        {
            "id": "35",
            "entry": "[35] Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory (ISIT), 2016 IEEE International Symposium on, pages 2379\u20132383. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20A%20geometric%20analysis%20of%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ju%20Qu%2C%20Qing%20Wright%2C%20John%20A%20geometric%20analysis%20of%20phase%20retrieval%202016"
        },
        {
            "id": "36",
            "entry": "[36] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. IEEE Transactions on Information Theory, 62(11):6535\u20136579, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20Ruoyu%20Luo%2C%20Zhi-Quan%20Guaranteed%20matrix%20completion%20via%20non-convex%20factorization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20Ruoyu%20Luo%2C%20Zhi-Quan%20Guaranteed%20matrix%20completion%20via%20non-convex%20factorization%202016"
        },
        {
            "id": "37",
            "entry": "[37] Kim-Chuan Toh, Michael J Todd, and Reha H T\u00fct\u00fcnc\u00fc. Sdpt3\u2013a matlab software package for semidefinite programming, version 1.3. Optimization methods and software, 11(1-4):545\u2013581, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Toh%2C%20Kim-Chuan%20Todd%2C%20Michael%20J.%20T%C3%BCt%C3%BCnc%C3%BC%2C%20Reha%20H.%20Sdpt3%E2%80%93a%20matlab%20software%20package%20for%20semidefinite%20programming%2C%20version%201.3%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Toh%2C%20Kim-Chuan%20Todd%2C%20Michael%20J.%20T%C3%BCt%C3%BCnc%C3%BC%2C%20Reha%20H.%20Sdpt3%E2%80%93a%20matlab%20software%20package%20for%20semidefinite%20programming%2C%20version%201.3%201999"
        },
        {
            "id": "38",
            "entry": "[38] HuiMin Wang and Song Li. The bounds of restricted isometry constants for low rank matrices recovery. Science China Mathematics, 56(6):1117\u20131127, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20HuiMin%20Li%2C%20Song%20The%20bounds%20of%20restricted%20isometry%20constants%20for%20low%20rank%20matrices%20recovery%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20HuiMin%20Li%2C%20Song%20The%20bounds%20of%20restricted%20isometry%20constants%20for%20low%20rank%20matrices%20recovery%202013"
        },
        {
            "id": "39",
            "entry": "[39] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pages 4151\u20134161, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2041514161%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2041514161%202017"
        },
        {
            "id": "40",
            "entry": "[40] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "41",
            "entry": "[41] Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank matrix estimation. In Advances in Neural Information Processing Systems, pages 559\u2013567, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Tuo%20Wang%2C%20Zhaoran%20Liu%2C%20Han%20A%20nonconvex%20optimization%20framework%20for%20low%20rank%20matrix%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Tuo%20Wang%2C%20Zhaoran%20Liu%2C%20Han%20A%20nonconvex%20optimization%20framework%20for%20low%20rank%20matrix%20estimation%202015"
        },
        {
            "id": "42",
            "entry": "[42] Qinqing Zheng and John Lafferty. A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements. In Advances in Neural Information Processing Systems, pages 109\u2013117, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Qinqing%20Lafferty%2C%20John%20A%20convergent%20gradient%20descent%20algorithm%20for%20rank%20minimization%20and%20semidefinite%20programming%20from%20random%20linear%20measurements%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Qinqing%20Lafferty%2C%20John%20A%20convergent%20gradient%20descent%20algorithm%20for%20rank%20minimization%20and%20semidefinite%20programming%20from%20random%20linear%20measurements%202015"
        },
        {
            "id": "43",
            "entry": "[43] Zhihui Zhu, Qiuwei Li, Gongguo Tang, and Michael B Wakin. Global optimality in low-rank matrix optimization. IEEE Transactions on Signal Processing, 66(13):3614\u20133628, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Zhihui%20Li%2C%20Qiuwei%20Tang%2C%20Gongguo%20Wakin%2C%20Michael%20B.%20Global%20optimality%20in%20low-rank%20matrix%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Zhihui%20Li%2C%20Qiuwei%20Tang%2C%20Gongguo%20Wakin%2C%20Michael%20B.%20Global%20optimality%20in%20low-rank%20matrix%20optimization%202018"
        }
    ]
}
