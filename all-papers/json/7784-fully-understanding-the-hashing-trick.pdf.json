{
    "filename": "7784-fully-understanding-the-hashing-trick.pdf",
    "metadata": {
        "title": "Fully Understanding The Hashing Trick",
        "author": "Lior Kamma, Casper B. Freksen, Kasper Green Larsen",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7784-fully-understanding-the-hashing-trick.pdf"
        },
        "abstract": "Feature hashing, also known as the hashing trick, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix A : Rn \u2192 Rm (where m n) in order to reduce the dimension of the data from n to m while approximately preserving the Euclidean norm. Every column of A contains exactly one non-zero entry, equals to either \u22121 or 1."
    },
    "keywords": [
        {
            "term": "random projection",
            "url": "https://en.wikipedia.org/wiki/random_projection"
        },
        {
            "term": "feature hashing",
            "url": "https://en.wikipedia.org/wiki/feature_hashing"
        },
        {
            "term": "hashing trick",
            "url": "https://en.wikipedia.org/wiki/hashing_trick"
        },
        {
            "term": "manifold learning",
            "url": "https://en.wikipedia.org/wiki/manifold_learning"
        },
        {
            "term": "numerical linear algebra",
            "url": "https://en.wikipedia.org/wiki/numerical_linear_algebra"
        },
        {
            "term": "dimensionality reduction",
            "url": "https://en.wikipedia.org/wiki/dimensionality_reduction"
        }
    ],
    "highlights": [
        "Dimensionality reduction that approximately preserves Euclidean distances is a key tool used as a preprocessing step in many geometric, algebraic and classification algorithms, whose performance heavily depends on the dimension of the input",
        "Its applications range upon nearest neighbor search [AC09, HIM12, AIL+15], classification and regression [RR08, MM09, PBMID14], manifold learning [HWB08] sparse recovery [CT06] and numerical linear algebra [CW09, MM13, S\u00e1r06]",
        "Our empirical results show that in practice the constants inside the \u0398-notation are significantly tighter than the theoretical proof might suggest, and feature hashing performs well for a larger scope of vectors",
        "The results of these experiments can be seen in Figure 4 and Figure 5, from which we conclude that feature hashing performs even better on the real-world data we tested compared to the synthetic data, as the Theorem 2 constant is always above 1.1 and 0.8 with and without tf-idf, respectively"
    ],
    "key_statements": [
        "Dimensionality reduction that approximately preserves Euclidean distances is a key tool used as a preprocessing step in many geometric, algebraic and classification algorithms, whose performance heavily depends on the dimension of the input",
        "Its applications range upon nearest neighbor search [AC09, HIM12, AIL+15], classification and regression [RR08, MM09, PBMID14], manifold learning [HWB08] sparse recovery [CT06] and numerical linear algebra [CW09, MM13, S\u00e1r06]",
        "Our empirical results show that in practice the constants inside the \u0398-notation are significantly tighter than the theoretical proof might suggest, and feature hashing performs well for a larger scope of vectors",
        "The results of these experiments can be seen in Figure 4 and Figure 5, from which we conclude that feature hashing performs even better on the real-world data we tested compared to the synthetic data, as the Theorem 2 constant is always above 1.1 and 0.8 with and without tf-idf, respectively"
    ],
    "summary": [
        "Dimensionality reduction that approximately preserves Euclidean distances is a key tool used as a preprocessing step in many geometric, algebraic and classification algorithms, whose performance heavily depends on the dimension of the input.",
        "The best result to date in constructing a sparse Johnson-Lindenstrauss matrix is due to Kane and Nelson [KN14], who presented a distribution over matrices satisfying (1) in which every column has at most s = O(\u03b5\u22121 lg(1/\u03b4)) non-zero entries.",
        "The main result of this paper is a tight tradeoff between the target dimension m, the approximation ratio \u03b5, the error probability \u03b4 and x \u221e/ x 2.",
        "Our main result is the following theorem, which gives tight asymptotic bounds for the performance of feature hashing, closing the long-standing gap.",
        "Our empirical results show that in practice the constants inside the \u0398-notation are significantly tighter than the theoretical proof might suggest, and feature hashing performs well for a larger scope of vectors.",
        "Proof Technique As a fundamental step in the proof of Theorem 2 we prove tight asymptotic bounds for high-order norms of the approximation factor.2 More formally, for every x \u2208 Rn \\ {0} let",
        "While their proof methods give bounds for high-order moments similar to those in Lemmas 3 and 4, they rely heavily on the fact that s is relatively large.",
        "We complement our theoretical bounds with experimental results on both real and synthetic data.",
        "The goal of the experiments is to give bounds on some of the constants hidden in the main theorem.",
        "For the real-world data we tested feature hashing on, the constant is at least 1.1 or 0.8, based on the data set.",
        "To find a bound on the constant of the \u0398-notation in Theorem 2, we truncated data points that did not satisfy",
        "We ran experiments on real-world data, namely bag-of-words representations of 1500 NIPS papers with stopwords removed [DKT17b, New08].",
        "The results of these experiments can be seen in Figure 4 and Figure 5, from which we conclude that feature hashing performs even better on the real-world data we tested compared to the synthetic data, as the Theorem 2 constant is always above 1.1 and 0.8 with and without tf-idf, respectively.",
        "The random data was independent between experiments with diffent values of m, between synthetic and real world experiments, and between the random number generator used for vector generation and hashing.",
        "In order to efficiently do the 220 hashings per vector for the real world data, we utilised the high independence of double tabulation hashing."
    ],
    "headline": "For a synthetic set of generated bit-vectors, we show that whenever 1 \u03b4 \u03b52 2 \u03b52 \u03b4 the constant hidden by the",
    "reference_links": [
        {
            "id": "Ailon_2009_a",
            "entry": "N. Ailon and B. Chazelle. The fast Johnson-Lindenstrauss transform and approximate nearest neighbors. SIAM J. Comput., 39(1):302\u2013322, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ailon%2C%20N.%20Chazelle%2C%20B.%20The%20fast%20Johnson-Lindenstrauss%20transform%20and%20approximate%20nearest%20neighbors%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ailon%2C%20N.%20Chazelle%2C%20B.%20The%20fast%20Johnson-Lindenstrauss%20transform%20and%20approximate%20nearest%20neighbors%202009"
        },
        {
            "id": "Achlioptas_2003_a",
            "entry": "D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. J. Comput. Syst. Sci., 66(4):671\u2013687, June 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achlioptas%2C%20D.%20Database-friendly%20random%20projections%3A%20Johnson-Lindenstrauss%20with%20binary%20coins%202003-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achlioptas%2C%20D.%20Database-friendly%20random%20projections%3A%20Johnson-Lindenstrauss%20with%20binary%20coins%202003-06"
        },
        {
            "id": "Andoni_et+al_2015_a",
            "entry": "A. Andoni, P. Indyk, T. Laarhoven, I. Razenshteyn, and L. Schmidt. Practical and optimal lsh for angular distance. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201915, pages 1225\u20131233. MIT Press, 2015. Available from: http://dl.acm.org/citation.cfm?id=2969239.2969376.",
            "url": "http://dl.acm.org/citation.cfm?id=2969239.2969376",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andoni%2C%20A.%20Indyk%2C%20P.%20Laarhoven%2C%20T.%20Razenshteyn%2C%20I.%20Practical%20and%20optimal%20lsh%20for%20angular%20distance%202015"
        },
        {
            "id": "Charikar_et+al_2004_a",
            "entry": "[CCF04] M. Charikar, K. C. Chen, and M. Farach-Colton. Finding frequent items in data streams. Theor. Comput. Sci., 312(1):3\u201315, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Charikar%2C%20M.%20Chen%2C%20K.C.%20Farach-Colton%2C%20M.%20Finding%20frequent%20items%20in%20data%20streams%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Charikar%2C%20M.%20Chen%2C%20K.C.%20Farach-Colton%2C%20M.%20Finding%20frequent%20items%20in%20data%20streams%202004"
        },
        {
            "id": "Cohen_et+al_2018_a",
            "entry": "M. B. Cohen, T. S. Jayram, and J. Nelson. Simple analyses of the sparse JohnsonLindenstrauss transform. In 1st Symposium on Simplicity in Algorithms, SOSA 2018, January 7-10, 2018, New Orleans, LA, USA, pages 15:1\u201315:9, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20M.B.%20Jayram%2C%20T.S.%20Nelson%2C%20J.%20Simple%20analyses%20of%20the%20sparse%20JohnsonLindenstrauss%20transform.%20In%201st%20Symposium%20on%20Simplicity%20in%20Algorithms%2C%20SOSA%202018-01-07"
        },
        {
            "id": "Cand_2006_a",
            "entry": "E. J. Cand\u00e8s and T. Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406\u2013 5425, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20E.J.%20Tao%2C%20T.%20Near-optimal%20signal%20recovery%20from%20random%20projections%3A%20Universal%20encoding%20strategies%3F%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20E.J.%20Tao%2C%20T.%20Near-optimal%20signal%20recovery%20from%20random%20projections%3A%20Universal%20encoding%20strategies%3F%202006"
        },
        {
            "id": "Clarkson_2009_a",
            "entry": "K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the Forty-first Annual ACM Symposium on Theory of Computing, pages 205\u2013214. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Numerical%20linear%20algebra%20in%20the%20streaming%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20K.L.%20Woodruff%2C%20D.P.%20Numerical%20linear%20algebra%20in%20the%20streaming%20model%202009"
        },
        {
            "id": "Dalessandro_2013_a",
            "entry": "B. Dalessandro. Bring the noise: Embracing randomness is the key to scaling up machine learning algorithms. Big Data, 1(2):110\u2013112, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dalessandro%2C%20B.%20Bring%20the%20noise%3A%20Embracing%20randomness%20is%20the%20key%20to%20scaling%20up%20machine%20learning%20algorithms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dalessandro%2C%20B.%20Bring%20the%20noise%3A%20Embracing%20randomness%20is%20the%20key%20to%20scaling%20up%20machine%20learning%20algorithms%202013"
        },
        {
            "id": "Dasgupta_2003_a",
            "entry": "S. Dasgupta and A. Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Random Struct. Algorithms, 22(1):60\u201365, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20S.%20Gupta%2C%20A.%20An%20elementary%20proof%20of%20a%20theorem%20of%20Johnson%20and%20Lindenstrauss%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20S.%20Gupta%2C%20A.%20An%20elementary%20proof%20of%20a%20theorem%20of%20Johnson%20and%20Lindenstrauss%202003"
        },
        {
            "id": "Dahlgaard_et+al_2015_a",
            "entry": "S. Dahlgaard, M. B. T. Knudsen, E. Rotenberg, and M. Thorup. Hashing for statistics over k-partitions. In Proceedings of the 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), FOCS \u201915, pages 1292\u20131310. IEEE Computer Society, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dahlgaard%2C%20S.%20Knudsen%2C%20M.B.T.%20Rotenberg%2C%20E.%20Thorup%2C%20M.%20Hashing%20for%20statistics%20over%20k-partitions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dahlgaard%2C%20S.%20Knudsen%2C%20M.B.T.%20Rotenberg%2C%20E.%20Thorup%2C%20M.%20Hashing%20for%20statistics%20over%20k-partitions%202015"
        },
        {
            "id": "Dasgupta_et+al_2010_a",
            "entry": "A. Dasgupta, R. Kumar, and T. Sarl\u00f3s. A sparse Johnson-Lindenstrauss transform. In Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5-8 June 2010, STOC \u201910, pages 341\u2013350, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20A.%20Kumar%2C%20R.%20Sarl%C3%B3s%2C%20T.%20A%20sparse%20Johnson-Lindenstrauss%20transform%202010-06-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20A.%20Kumar%2C%20R.%20Sarl%C3%B3s%2C%20T.%20A%20sparse%20Johnson-Lindenstrauss%20transform%202010-06-05"
        },
        {
            "id": "Dahlgaard_et+al_2017_a",
            "entry": "[DKT17a] S. Dahlgaard, M. Knudsen, and M. Thorup. Practical hash functions for similarity estimation and dimensionality reduction. In Advances in Neural Information Processing Systems 30, pages 6615\u20136625. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dahlgaard%2C%20S.%20Knudsen%2C%20M.%20M.%20Thorup.%20Practical%20hash%20functions%20for%20similarity%20estimation%20and%20dimensionality%20reduction%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dahlgaard%2C%20S.%20Knudsen%2C%20M.%20M.%20Thorup.%20Practical%20hash%20functions%20for%20similarity%20estimation%20and%20dimensionality%20reduction%202017"
        },
        {
            "id": "Dheeru_2017_a",
            "entry": "[DKT17b] D. Dheeru and E. Karra Taniskidou. UCI machine learning repository, 2017. Available from: http://archive.ics.uci.edu/ml.",
            "url": "http://archive.ics.uci.edu/ml"
        },
        {
            "id": "Freksen_2017_a",
            "entry": "C. B. Freksen and K. G. Larsen. On using toeplitz and circulant matrices for JohnsonLindenstrauss transforms. In 28th International Symposium on Algorithms and Computation, ISAAC 2017, December 9-12, 2017, Phuket, Thailand, pages 32:1\u201332:12, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freksen%2C%20C.B.%20Larsen%2C%20K.G.%20On%20using%20toeplitz%20and%20circulant%20matrices%20for%20JohnsonLindenstrauss%20transforms%202017-12-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freksen%2C%20C.B.%20Larsen%2C%20K.G.%20On%20using%20toeplitz%20and%20circulant%20matrices%20for%20JohnsonLindenstrauss%20transforms%202017-12-09"
        },
        {
            "id": "Har-Peled_et+al_2012_a",
            "entry": "[HIM12] S. Har-Peled, P. Indyk, and R. Motwani. Approximate nearest neighbor: Towards removing the curse of dimensionality. Theory of Computing, 8(1):321\u2013350, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Har-Peled%2C%20S.%20Indyk%2C%20P.%20Motwani%2C%20R.%20Approximate%20nearest%20neighbor%3A%20Towards%20removing%20the%20curse%20of%20dimensionality%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Har-Peled%2C%20S.%20Indyk%2C%20P.%20Motwani%2C%20R.%20Approximate%20nearest%20neighbor%3A%20Towards%20removing%20the%20curse%20of%20dimensionality%202012"
        },
        {
            "id": "Hegde_et+al_2008_a",
            "entry": "[HWB08] C. Hegde, M. Wakin, and R. Baraniuk. Random projections for manifold learning. In Advances in Neural Information Processing Systems 20, pages 641\u2013648. Curran Associates, Inc., 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hegde%2C%20C.%20Wakin%2C%20M.%20Baraniuk%2C%20R.%20Random%20projections%20for%20manifold%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hegde%2C%20C.%20Wakin%2C%20M.%20Baraniuk%2C%20R.%20Random%20projections%20for%20manifold%20learning%202008"
        },
        {
            "id": "Johnson_1984_a",
            "entry": "W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. In Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26 of Contemporary Mathematics, pages 189\u2013206. American Mathematical Society, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20W.%20Lindenstrauss%2C%20J.%20Extensions%20of%20Lipschitz%20mappings%20into%20a%20Hilbert%20space%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20W.%20Lindenstrauss%2C%20J.%20Extensions%20of%20Lipschitz%20mappings%20into%20a%20Hilbert%20space%201984"
        },
        {
            "id": "Jayram_2013_a",
            "entry": "T. S. Jayram and D. P. Woodruff. Optimal bounds for Johnson-Lindenstrauss transforms and streaming problems with subconstant error. ACM Trans. Algorithms, 9(3):26:1\u2013 26:17, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jayram%2C%20T.S.%20Woodruff%2C%20D.P.%20Optimal%20bounds%20for%20Johnson-Lindenstrauss%20transforms%20and%20streaming%20problems%20with%20subconstant%20error%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jayram%2C%20T.S.%20Woodruff%2C%20D.P.%20Optimal%20bounds%20for%20Johnson-Lindenstrauss%20transforms%20and%20streaming%20problems%20with%20subconstant%20error%202013"
        },
        {
            "id": "Kane_2014_a",
            "entry": "D. M. Kane and J. Nelson. Sparser Johnson-Lindenstrauss transforms. J. ACM, 61(1):4:1\u20134:23, January 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kane%2C%20D.M.%20Nelson%2C%20J.%20Sparser%20Johnson-Lindenstrauss%20transforms%202014-01-23",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kane%2C%20D.M.%20Nelson%2C%20J.%20Sparser%20Johnson-Lindenstrauss%20transforms%202014-01-23"
        },
        {
            "id": "Larsen_2017_a",
            "entry": "K. G. Larsen and J. Nelson. Optimality of the Johnson-Lindenstrauss lemma. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages 633\u2013638, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larsen%2C%20K.G.%20Nelson%2C%20J.%20Optimality%20of%20the%20Johnson-Lindenstrauss%20lemma%202017-10-15",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larsen%2C%20K.G.%20Nelson%2C%20J.%20Optimality%20of%20the%20Johnson-Lindenstrauss%20lemma%202017-10-15"
        },
        {
            "id": "Matou_2008_a",
            "entry": "[Mat08] J. Matou\u0161ek. On variants of the Johnson-Lindenstrauss lemma. Random Struct. Algorithms, 33(2):142\u2013156, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matou%C5%A1ek%2C%20J.%20On%20variants%20of%20the%20Johnson-Lindenstrauss%20lemma%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matou%C5%A1ek%2C%20J.%20On%20variants%20of%20the%20Johnson-Lindenstrauss%20lemma%202008"
        },
        {
            "id": "Maillard_2009_a",
            "entry": "[MM09] O. Maillard and R. Munos. Compressed least-squares regression. In Advances in Neural Information Processing Systems 22, pages 1213\u20131221. Curran Associates, Inc., 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maillard%2C%20O.%20Munos%2C%20R.%20Compressed%20least-squares%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maillard%2C%20O.%20Munos%2C%20R.%20Compressed%20least-squares%20regression%202009"
        },
        {
            "id": "Meng_2013_a",
            "entry": "X. Meng and M. W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In Symposium on Theory of Computing Conference, STOC\u201913, Palo Alto, CA, USA, June 1-4, 2013, STOC \u201913, pages 91\u2013100, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20X.%20Mahoney%2C%20M.W.%20Low-distortion%20subspace%20embeddings%20in%20input-sparsity%20time%20and%20applications%20to%20robust%20linear%20regression%202013-06-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20X.%20Mahoney%2C%20M.W.%20Low-distortion%20subspace%20embeddings%20in%20input-sparsity%20time%20and%20applications%20to%20robust%20linear%20regression%202013-06-01"
        },
        {
            "id": "Newman_2008_a",
            "entry": "[New08] D. Newman. Bag of words data set, 2008. Available from: https://archive.ics.uci.edu/ml/datasets/Bag+of+Words.",
            "url": "https://archive.ics.uci.edu/ml/datasets/Bag+of+Words"
        },
        {
            "id": "Nelson_2013_a",
            "entry": "J. Nelson and H. L. Nguyen. Sparsity lower bounds for dimensionality reducing maps. In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing, STOC \u201913, pages 101\u2013110. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelson%2C%20J.%20Nguyen%2C%20H.L.%20Sparsity%20lower%20bounds%20for%20dimensionality%20reducing%20maps%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelson%2C%20J.%20Nguyen%2C%20H.L.%20Sparsity%20lower%20bounds%20for%20dimensionality%20reducing%20maps%202013"
        },
        {
            "id": "Paul_et+al_2014_a",
            "entry": "[PBMID14] S. Paul, C. Boutsidis, M. Magdon-Ismail, and P. Drineas. Random projections for linear support vector machines. ACM Trans. Knowl. Discov. Data, 8(4):22:1\u201322:25, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paul%2C%20S.%20Boutsidis%2C%20C.%20Magdon-Ismail%2C%20M.%20Drineas%2C%20P.%20Random%20projections%20for%20linear%20support%20vector%20machines%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paul%2C%20S.%20Boutsidis%2C%20C.%20Magdon-Ismail%2C%20M.%20Drineas%2C%20P.%20Random%20projections%20for%20linear%20support%20vector%20machines%202014"
        },
        {
            "id": "Rahimi_2008_a",
            "entry": "A. Rahimi and B. Recht. Random features for large-scale kernel machines. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1177\u20131184. Curran Associates, Inc., 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20A.%20Recht%2C%20B.%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "S_2006_a",
            "entry": "T. S\u00e1rlos. Improved approximation algorithms for large matrices via random projections. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, pages 143\u2013152. IEEE Computer Society, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%C3%A1rlos%2C%20T.%20Improved%20approximation%20algorithms%20for%20large%20matrices%20via%20random%20projections%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=S%C3%A1rlos%2C%20T.%20Improved%20approximation%20algorithms%20for%20large%20matrices%20via%20random%20projections%202006"
        },
        {
            "id": "Suthaharan_2015_a",
            "entry": "S. Suthaharan. Machine Learning Models and Algorithms for Big Data Classification: Thinking with Examples for Effective Learning. Springer Publishing Company, Incorporated, 1st edition, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suthaharan%2C%20S.%20Machine%20Learning%20Models%20and%20Algorithms%20for%20Big%20Data%20Classification%3A%20Thinking%20with%20Examples%20for%20Effective%20Learning%202015"
        },
        {
            "id": "M_2014_a",
            "entry": "M. Thorup. Simple tabulation, fast expanders, double tabulation, and high independence. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science(FOCS), pages 90\u201399. IEEE Computer Society, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M.%20Thorup.%20Simple%20tabulation%2C%20fast%20expanders%2C%20double%20tabulation%2C%20and%20high%20independence%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M.%20Thorup.%20Simple%20tabulation%2C%20fast%20expanders%2C%20double%20tabulation%2C%20and%20high%20independence%202014"
        },
        {
            "id": "Thorup_2012_a",
            "entry": "M. Thorup and Y. Zhang. Tabulation-based 5-independent hashing with applications to linear probing and second moment estimation. SIAM J. Comput., 41(2):293\u2013331, April 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thorup%2C%20M.%20Zhang%2C%20Y.%20Tabulation-based%205-independent%20hashing%20with%20applications%20to%20linear%20probing%20and%20second%20moment%20estimation%202012-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thorup%2C%20M.%20Zhang%2C%20Y.%20Tabulation-based%205-independent%20hashing%20with%20applications%20to%20linear%20probing%20and%20second%20moment%20estimation%202012-04"
        },
        {
            "id": "Vempala_2005_a",
            "entry": "[Vem05] S. S. Vempala. The random projection method. DIMACS : series in discrete mathematics and theoretical computer science. American Mathematical Society, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vempala%2C%20S.S.%20The%20random%20projection%20method.%20DIMACS%20%3A%20series%20in%20discrete%20mathematics%20and%20theoretical%20computer%20science%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vempala%2C%20S.S.%20The%20random%20projection%20method.%20DIMACS%20%3A%20series%20in%20discrete%20mathematics%20and%20theoretical%20computer%20science%202005"
        },
        {
            "id": "Weinberger_et+al_2009_a",
            "entry": "[WDL+09] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J. Attenberg. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1113\u20131120, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20K.%20Dasgupta%2C%20A.%20Langford%2C%20J.%20Smola%2C%20A.%20Feature%20hashing%20for%20large%20scale%20multitask%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20K.%20Dasgupta%2C%20A.%20Langford%2C%20J.%20Smola%2C%20A.%20Feature%20hashing%20for%20large%20scale%20multitask%20learning%202009"
        }
    ]
}
