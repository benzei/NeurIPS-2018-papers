{
    "filename": "7478-sega-variance-reduction-via-gradient-sketching.pdf",
    "metadata": {
        "title": "SEGA: Variance Reduction via Gradient Sketching",
        "author": "Filip Hanzely, Konstantin Mishchenko, Peter Richtarik",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching.pdf"
        },
        "abstract": "We propose a randomized first order optimization method\u2014SEGA (SkEtched GrAdient)\u2014which progressively throughout its iterations builds a variancereduced estimate of the gradient from random linear measurements (sketches) of the gradient. In each iteration, SEGA updates the current estimate of the gradient through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "linear convergence",
            "url": "https://en.wikipedia.org/wiki/linear_convergence"
        },
        {
            "term": "unbiased estimate",
            "url": "https://en.wikipedia.org/wiki/unbiased_estimate"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "coordinate descent",
            "url": "https://en.wikipedia.org/wiki/coordinate_descent"
        },
        {
            "term": "matrix inversion",
            "url": "https://en.wikipedia.org/wiki/matrix_inversion"
        },
        {
            "term": "linear system",
            "url": "https://en.wikipedia.org/wiki/linear_system"
        },
        {
            "term": "SEGA",
            "url": "https://en.wikipedia.org/wiki/SEGA"
        },
        {
            "term": "iterative method",
            "url": "https://en.wikipedia.org/wiki/iterative_method"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "We assume that an oracle provides a random linear transformation (i.e., a sketch) of the gradient, which is the information available to drive the iterative",
        "Refined results for coordinate sketches are presented in Section 4, where we describe and analyze an accelerated variant of SkEtched GrAdient Method",
        "In Section 4 we show that SkEtched GrAdient Method enjoys, up to a small constant factor, the same theoretical iteration complexity as coordinate descent",
        "We present one of the key theorems of the paper, stating a linear convergence of SkEtched GrAdient Method",
        "Specialized to a particular choice of the distribution D, which makes SkEtched GrAdient Method use the same random gradient information as that used in modern randomized coordinate descent methods, SkEtched GrAdient Method attains, up to a small constant factor, the same convergence rate as coordinate descent methods",
        "We provide Corollary 4.5, which shows that Algorithm 2 with single coordinate sampling enjoys, up to a constant factor, the same convergence rate as state-of-the-art accelerated coordinate descent method NUACDM [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]"
    ],
    "key_statements": [
        "We assume that an oracle provides a random linear transformation (i.e., a sketch) of the gradient, which is the information available to drive the iterative",
        "Refined results for coordinate sketches are presented in Section 4, where we describe and analyze an accelerated variant of SkEtched GrAdient Method",
        "In Section 4 we show that SkEtched GrAdient Method enjoys, up to a small constant factor, the same theoretical iteration complexity as coordinate descent",
        "We present one of the key theorems of the paper, stating a linear convergence of SkEtched GrAdient Method",
        "Specialized to a particular choice of the distribution D, which makes SkEtched GrAdient Method use the same random gradient information as that used in modern randomized coordinate descent methods, SkEtched GrAdient Method attains, up to a small constant factor, the same convergence rate as coordinate descent methods",
        "In Section 4.2 we develop SkEtched GrAdient Method with in a general setup known as arbitrary sampling [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] (Theorem 4.2)",
        "In order to draw a direct comparison with general variants of coordinate descent methods, we consider sketches in (3) that are column submatrices of the identity matrix: S = IS, where S is a random subset of [n] d=ef {1, 2, . . . , n}",
        "We provide Corollary 4.5, which shows that Algorithm 2 with single coordinate sampling enjoys, up to a constant factor, the same convergence rate as state-of-the-art accelerated coordinate descent method NUACDM [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]"
    ],
    "summary": [
        "We assume that an oracle provides a random linear transformation (i.e., a sketch) of the gradient, which is the information available to drive the iterative",
        "Randomized coordinate descent (CD) methods use oracle (2) with D corresponding to a distribution over standard basis vectors.",
        "Our method is able to work with an arbitrary regularizer due to its ability to build an unbiased variance-reduced estimate of the gradient of f throughout the iterative process from the random sketches provided by the oracle.",
        "Refined results for coordinate sketches are presented in Section 4, where we describe and analyze an accelerated variant of SEGA.",
        "We introduce a learning process for estimating the gradient from the sketched information provided by (2); this will be used as a subroutine of SEGA.",
        "Algorithm 1: SEGA: SkEtched GrAdient Method",
        "In the setup of Example 2.1, both SEGA and CD obtain new gradient information in the form of a random partial derivative of f .",
        "We state a linear convergence result for SEGA (Algorithm 1) for general sketch distributions D under smoothness and strong convexity assumptions.",
        "In Section 4.2 we develop SEGA with in a general setup known as arbitrary sampling [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] (Theorem 4.2).",
        "Corollary 4.3 and Corollary 4.5 provide us with importance sampling for both nonaccelerated and accelerated method, which matches up to a constant factor cutting-edge CD rates [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] under the same oracle and assumptions5.",
        "In order to draw a direct comparison with general variants of CD methods, we consider sketches in (3) that are column submatrices of the identity matrix: S = IS, where S is a random subset of [n] d=ef {1, 2, .",
        "We state the convergence rate of SEGA for coordinate sketches with arbitrary sampling of subsets of coordinates.",
        "The analogous accelerated CD method, in which a single coordinate is sampled in every iteration, was developed and analyzed in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "We provide Corollary 4.5, which shows that Algorithm 2 with single coordinate sampling enjoys, up to a constant factor, the same convergence rate as state-of-the-art accelerated coordinate descent method NUACDM [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "We consider the2 ball constrained problem (R is the indicator function of the unit ball) with the oracle providing the sketched gradient in the random Gaussian direction.",
        "We compare SEGA to the random direct search (RDS) method [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] under a zerothorder oracle and R = 0."
    ],
    "headline": "We propose a randomized first order optimization method\u2014SkEtched GrAdient Method \u2014which progressively throughout its iterations builds a variancereduced estimate of the gradient from random linear measurements  of the gradient",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200\u20131205. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "2",
            "entry": "[2] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. In Innovations in Theoretical Computer Science, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Orecchia%2C%20Lorenzo%20Linear%20coupling%3A%20An%20ultimate%20unification%20of%20gradient%20and%20mirror%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Orecchia%2C%20Lorenzo%20Linear%20coupling%3A%20An%20ultimate%20unification%20of%20gradient%20and%20mirror%20descent%202017"
        },
        {
            "id": "3",
            "entry": "[3] Zeyuan Allen-Zhu, Zheng Qu, Peter Richtarik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1110\u20131119, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Yuan%2C%20Yang%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Yuan%2C%20Yang%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016"
        },
        {
            "id": "4",
            "entry": "[4] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "5",
            "entry": "[5] El Houcine Bergou, Peter Richtarik, and Eduard Gorbunov. Random direct search method for minimizing nonconvex, convex and strongly convex functions. Manuscript, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergou%2C%20El%20Houcine%20Richtarik%2C%20Peter%20Gorbunov%2C%20Eduard%20Random%20direct%20search%20method%20for%20minimizing%20nonconvex%2C%20convex%20and%20strongly%20convex%20functions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergou%2C%20El%20Houcine%20Richtarik%2C%20Peter%20Gorbunov%2C%20Eduard%20Random%20direct%20search%20method%20for%20minimizing%20nonconvex%2C%20convex%20and%20strongly%20convex%20functions%202018"
        },
        {
            "id": "6",
            "entry": "[6] Antonin Chambolle, Matthias J Ehrhardt, Peter Richtarik, and Carola-Bibiane Schoenlieb. Stochastic primal-dual hybrid gradient algorithm with arbitrary sampling and imaging applications. SIAM Journal on Optimization, 28(4):27832808, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20Antonin%20Ehrhardt%2C%20Matthias%20J.%20Richtarik%2C%20Peter%20Schoenlieb%2C%20Carola-Bibiane%20Stochastic%20primal-dual%20hybrid%20gradient%20algorithm%20with%20arbitrary%20sampling%20and%20imaging%20applications%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20Antonin%20Ehrhardt%2C%20Matthias%20J.%20Richtarik%2C%20Peter%20Schoenlieb%2C%20Carola-Bibiane%20Stochastic%20primal-dual%20hybrid%20gradient%20algorithm%20with%20arbitrary%20sampling%20and%20imaging%20applications%202018"
        },
        {
            "id": "7",
            "entry": "[7] Chih-Chung Chang and Chih-Jen Lin. LibSVM: A library for support vector machines. ACM transactions on intelligent systems and technology (TIST), 2(3):27, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20LibSVM%3A%20A%20library%20for%20support%20vector%20machines%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Chih-Chung%20Chang%20and%20Chih-Jen%20Lin.%20LibSVM%3A%20A%20library%20for%20support%20vector%20machines%202011"
        },
        {
            "id": "8",
            "entry": "[8] Andrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to derivative-free optimization, volume 8.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrew%20R%20Conn%20Katya%20Scheinberg%20and%20Luis%20N%20Vicente%20Introduction%20to%20derivativefree%20optimization%20volume%208",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrew%20R%20Conn%20Katya%20Scheinberg%20and%20Luis%20N%20Vicente%20Introduction%20to%20derivativefree%20optimization%20volume%208"
        },
        {
            "id": "9",
            "entry": "[9] Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization, 19(3):1171\u20131183, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d%E2%80%99Aspremont%2C%20Alexandre%20Smooth%20optimization%20with%20approximate%20gradient%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=d%E2%80%99Aspremont%2C%20Alexandre%20Smooth%20optimization%20with%20approximate%20gradient%202008"
        },
        {
            "id": "10",
            "entry": "[10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "11",
            "entry": "[11] Olivier Devolder, Francois Glineur, and Yurii Nesterov. First-order methods of smooth convex optimization with inexact oracle. Mathematical Programming, 146(1-2):37\u201375, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devolder%2C%20Olivier%20Glineur%2C%20Francois%20Nesterov%2C%20Yurii%20First-order%20methods%20of%20smooth%20convex%20optimization%20with%20inexact%20oracle%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devolder%2C%20Olivier%20Glineur%2C%20Francois%20Nesterov%2C%20Yurii%20First-order%20methods%20of%20smooth%20convex%20optimization%20with%20inexact%20oracle%202014"
        },
        {
            "id": "12",
            "entry": "[12] Olivier Fercoq and Peter Richtarik. Accelerated, parallel and proximal coordinate descent. SIAM Journal on Optimization, (25):1997\u20132023, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fercoq%2C%20Olivier%20Richtarik%2C%20Peter%20Accelerated%2C%20parallel%20and%20proximal%20coordinate%20descent%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fercoq%2C%20Olivier%20Richtarik%2C%20Peter%20Accelerated%2C%20parallel%20and%20proximal%20coordinate%20descent%201997"
        },
        {
            "id": "13",
            "entry": "[13] Robert M Gower, Donald Goldfarb, and Peter Richtarik. Stochastic block BFGS: squeezing more curvature out of data. In 33rd International Conference on Machine Learning, pages 1869\u20131878, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gower%2C%20Robert%20M.%20Goldfarb%2C%20Donald%20Richtarik%2C%20Peter%20Stochastic%20block%20BFGS%3A%20squeezing%20more%20curvature%20out%20of%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gower%2C%20Robert%20M.%20Goldfarb%2C%20Donald%20Richtarik%2C%20Peter%20Stochastic%20block%20BFGS%3A%20squeezing%20more%20curvature%20out%20of%20data%202016"
        },
        {
            "id": "14",
            "entry": "[14] Robert M Gower, Filip Hanzely, Peter Richtarik, and Sebastian Stich. Accelerated stochastic matrix inversion: general theory and speeding up BFGS rules for faster second-order optimization. arXiv:1802.04079, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04079"
        },
        {
            "id": "15",
            "entry": "[15] Robert M Gower and Peter Richtarik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660\u20131690, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gower%2C%20Robert%20M.%20Richtarik%2C%20Peter%20Randomized%20iterative%20methods%20for%20linear%20systems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gower%2C%20Robert%20M.%20Richtarik%2C%20Peter%20Randomized%20iterative%20methods%20for%20linear%20systems%202015"
        },
        {
            "id": "16",
            "entry": "[16] Robert M Gower and Peter Richtarik. Stochastic dual ascent for solving linear systems. arXiv preprint arXiv:1512.06890, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.06890"
        },
        {
            "id": "17",
            "entry": "[17] Robert M Gower and Peter Richtarik. Linearly convergent randomized iterative methods for computing the pseudoinverse. arXiv:1612.06255, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.06255"
        },
        {
            "id": "18",
            "entry": "[18] Robert M Gower and Peter Richtarik. Randomized quasi-Newton updates are linearly convergent matrix inversion algorithms. SIAM Journal on Matrix Analysis and Applications, 38(4):1380\u20131409, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gower%2C%20Robert%20M.%20Richtarik%2C%20Peter%20Randomized%20quasi-Newton%20updates%20are%20linearly%20convergent%20matrix%20inversion%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gower%2C%20Robert%20M.%20Richtarik%2C%20Peter%20Randomized%20quasi-Newton%20updates%20are%20linearly%20convergent%20matrix%20inversion%20algorithms%202017"
        },
        {
            "id": "19",
            "entry": "[19] Robert M Gower, Peter Richtarik, and Francis Bach. Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching. arXiv preprint arXiv:1805.02632, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.02632"
        },
        {
            "id": "20",
            "entry": "[20] Filip Hanzely and Peter Richtarik. Accelerated coordinate descent with arbitrary sampling and best rates for minibatches. arXiv preprint arXiv:1809.09354, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.09354"
        },
        {
            "id": "21",
            "entry": "[21] Robert Hooke and Terry A Jeeves. \u201cDirect search\u201d solution of numerical and statistical problems. Journal of the ACM (JACM), 8(2):212\u2013229, 1961.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robert%20Hooke%20and%20Terry%20A%20Jeeves.%20%E2%80%9CDirect%20search%E2%80%9D%20solution%20of%20numerical%20and%20statistical%20problems%201961",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robert%20Hooke%20and%20Terry%20A%20Jeeves.%20%E2%80%9CDirect%20search%E2%80%9D%20solution%20of%20numerical%20and%20statistical%20problems%201961"
        },
        {
            "id": "22",
            "entry": "[22] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "23",
            "entry": "[23] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the Polyak-Lojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795\u2013811.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20Polyak-Lojasiewicz%20condition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximalgradient%20methods%20under%20the%20Polyak-Lojasiewicz%20condition"
        },
        {
            "id": "24",
            "entry": "[24] Tamara G Kolda, Robert M Lewis, and Virginia Torczon. Optimization by direct search: New perspectives on some classical and modern methods. SIAM Review, 45(3):385\u2013482, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Lewis%2C%20Robert%20M.%20Torczon%2C%20Virginia%20Optimization%20by%20direct%20search%3A%20New%20perspectives%20on%20some%20classical%20and%20modern%20methods%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20Tamara%20G.%20Lewis%2C%20Robert%20M.%20Torczon%2C%20Virginia%20Optimization%20by%20direct%20search%3A%20New%20perspectives%20on%20some%20classical%20and%20modern%20methods%202003"
        },
        {
            "id": "25",
            "entry": "[25] Jakub Konecnyand Peter Richtarik. Simple complexity analysis of simplified direct search. arXiv preprint arXiv:1410.0390, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1410.0390"
        },
        {
            "id": "26",
            "entry": "[26] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization. In Advances in Neural Information Processing Systems, pages 3384\u20133392, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20universal%20catalyst%20for%20first-order%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Hongzhou%20Mairal%2C%20Julien%20Harchaoui%2C%20Zaid%20A%20universal%20catalyst%20for%20first-order%20optimization%202015"
        },
        {
            "id": "27",
            "entry": "[27] Qihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient method. In Advances in Neural Information Processing Systems, pages 3059\u20133067, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Qihang%20Lu%2C%20Zhaosong%20Xiao%2C%20Lin%20An%20accelerated%20proximal%20coordinate%20gradient%20method%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Qihang%20Lu%2C%20Zhaosong%20Xiao%2C%20Lin%20An%20accelerated%20proximal%20coordinate%20gradient%20method%202014"
        },
        {
            "id": "28",
            "entry": "[28] Nicolas Loizou and Peter Richtarik. Linearly convergent stochastic heavy ball method for minimizing generalization error. In NIPS Workshop on Optimization for Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loizou%2C%20Nicolas%20Richtarik%2C%20Peter%20Linearly%20convergent%20stochastic%20heavy%20ball%20method%20for%20minimizing%20generalization%20error%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loizou%2C%20Nicolas%20Richtarik%2C%20Peter%20Linearly%20convergent%20stochastic%20heavy%20ball%20method%20for%20minimizing%20generalization%20error%202017"
        },
        {
            "id": "29",
            "entry": "[29] Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods. arXiv:1712.09677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09677"
        },
        {
            "id": "30",
            "entry": "[30] Ion Necoara, Peter Richtarik, and Andrei Patrascu. Randomized projection methods for convex feasibility problems: conditioning and convergence rates. arXiv:1801.04873, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.04873"
        },
        {
            "id": "31",
            "entry": "[31] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). Soviet Mathematics Doklady, 27(2):372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20O%281/k2%29%201983"
        },
        {
            "id": "32",
            "entry": "[32] Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Kluwer Academic Publishers, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%202004"
        },
        {
            "id": "33",
            "entry": "[33] Yurii Nesterov. Smooth minimization of nonsmooth functions. Mathematical Programming, 103:127\u2013152, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Smooth%20minimization%20of%20nonsmooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Smooth%20minimization%20of%20nonsmooth%20functions%202005"
        },
        {
            "id": "34",
            "entry": "[34] Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Efficiency%20of%20coordinate%20descent%20methods%20on%20huge-scale%20optimization%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Efficiency%20of%20coordinate%20descent%20methods%20on%20huge-scale%20optimization%20problems%202012"
        },
        {
            "id": "35",
            "entry": "[35] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2613\u20132621. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Lam%20M.%20Liu%2C%20Jie%20Scheinberg%2C%20Katya%20Takac%2C%20Martin%20SARAH%3A%20A%20novel%20method%20for%20machine%20learning%20problems%20using%20stochastic%20recursive%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Lam%20M.%20Liu%2C%20Jie%20Scheinberg%2C%20Katya%20Takac%2C%20Martin%20SARAH%3A%20A%20novel%20method%20for%20machine%20learning%20problems%20using%20stochastic%20recursive%20gradient%202017"
        },
        {
            "id": "36",
            "entry": "[36] Zheng Qu and Peter Richtarik. Coordinate descent with arbitrary sampling I: Algorithms and complexity. Optimization Methods and Software, 31(5):829\u2013857, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20I%3A%20Algorithms%20and%20complexity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20I%3A%20Algorithms%20and%20complexity%202016"
        },
        {
            "id": "37",
            "entry": "[37] Zheng Qu and Peter Richtarik. Coordinate descent with arbitrary sampling I: Algorithms and complexity. Optimization Methods and Software, 31(5):829\u2013857, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20I%3A%20Algorithms%20and%20complexity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20I%3A%20Algorithms%20and%20complexity%202016"
        },
        {
            "id": "38",
            "entry": "[38] Zheng Qu and Peter Richtarik. Coordinate descent with arbitrary sampling II: Expected separable overapproximation. Optimization Methods and Software, 31(5):858\u2013884, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20II%3A%20Expected%20separable%20overapproximation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Coordinate%20descent%20with%20arbitrary%20sampling%20II%3A%20Expected%20separable%20overapproximation%202016"
        },
        {
            "id": "39",
            "entry": "[39] Zheng Qu, Peter Richtarik, Martin Takac, and Olivier Fercoq. SDNA: Stochastic dual Newton ascent for empirical risk minimization. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1823\u2013 1832. PMLR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Takac%2C%20Martin%20Fercoq%2C%20Olivier%20SDNA%3A%20Stochastic%20dual%20Newton%20ascent%20for%20empirical%20risk%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Takac%2C%20Martin%20Fercoq%2C%20Olivier%20SDNA%3A%20Stochastic%20dual%20Newton%20ascent%20for%20empirical%20risk%20minimization%202016"
        },
        {
            "id": "40",
            "entry": "[40] Zheng Qu, Peter Richtarik, and Tong Zhang. Quartz: Randomized dual coordinate ascent with arbitrary sampling. In Advances in Neural Information Processing Systems, pages 865\u2013873, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Zhang%2C%20Tong%20Quartz%3A%20Randomized%20dual%20coordinate%20ascent%20with%20arbitrary%20sampling%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Zheng%20Richtarik%2C%20Peter%20Zhang%2C%20Tong%20Quartz%3A%20Randomized%20dual%20coordinate%20ascent%20with%20arbitrary%20sampling%202015"
        },
        {
            "id": "41",
            "entry": "[41] Peter Richtarik and Martin Takac. On optimal probabilities in stochastic coordinate descent methods. Optimization Letters, 10(6):1233\u20131243, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richtarik%2C%20Peter%20Takac%2C%20Martin%20On%20optimal%20probabilities%20in%20stochastic%20coordinate%20descent%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richtarik%2C%20Peter%20Takac%2C%20Martin%20On%20optimal%20probabilities%20in%20stochastic%20coordinate%20descent%20methods%202016"
        },
        {
            "id": "42",
            "entry": "[42] Peter Richtarik and Martin Takac. Stochastic reformulations of linear systems: algorithms and convergence theory. arXiv:1706.01108, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01108"
        },
        {
            "id": "43",
            "entry": "[43] Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20Herbert%20Monro%2C%20Sutton%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "44",
            "entry": "[44] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In Advances in Neural Information Processing Systems, pages 2663\u20132671, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20Nicolas%20Le%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20Nicolas%20Le%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "45",
            "entry": "[45] Mark Schmidt, Nicolas Le Roux, and Francis R Bach. Convergence rates of inexact proximalgradient methods for convex optimization. In Advances in Neural Information Processing Systems, pages 1458\u20131466, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20Mark%20Roux%2C%20Nicolas%20Le%20Bach%2C%20Francis%20R.%20Convergence%20rates%20of%20inexact%20proximalgradient%20methods%20for%20convex%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20Mark%20Roux%2C%20Nicolas%20Le%20Bach%2C%20Francis%20R.%20Convergence%20rates%20of%20inexact%20proximalgradient%20methods%20for%20convex%20optimization%202011"
        },
        {
            "id": "46",
            "entry": "[46] Shai Shalev-Shwartz and Tong Zhang. Proximal stochastic dual coordinate ascent. arXiv preprint arXiv:1211.2717, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1211.2717"
        },
        {
            "id": "47",
            "entry": "[47] Stephen Tu, Shivaram Venkataraman, Ashia C. Wilson, Alex Gittens, Michael I. Jordan, and Benjamin Recht. Breaking locality accelerates block Gauss-Seidel. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3482\u20133491. PMLR, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tu%2C%20Stephen%20Venkataraman%2C%20Shivaram%20Wilson%2C%20Ashia%20C.%20Gittens%2C%20Alex%20Breaking%20locality%20accelerates%20block%20Gauss-Seidel%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tu%2C%20Stephen%20Venkataraman%2C%20Shivaram%20Wilson%2C%20Ashia%20C.%20Gittens%2C%20Alex%20Breaking%20locality%20accelerates%20block%20Gauss-Seidel%202017"
        }
    ]
}
