{
    "filename": "8224-glow-generative-flow-with-invertible-1x1-convolutions.pdf",
    "metadata": {
        "title": "Glow: Generative Flow with Invertible 1x1 Convolutions",
        "author": "Durk P. Kingma, Prafulla Dhariwal",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Flow-based generative models (<a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\">Dinh et al., 2014</a>) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 \u21e5 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow."
    },
    "keywords": [
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "processing unit",
            "url": "https://en.wikipedia.org/wiki/processing_unit"
        },
        {
            "term": "residual network",
            "url": "https://en.wikipedia.org/wiki/residual_network"
        },
        {
            "term": "synthesis",
            "url": "https://en.wikipedia.org/wiki/synthesis"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "recurrent neural network",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_network"
        },
        {
            "term": "density estimation",
            "url": "https://en.wikipedia.org/wiki/density_estimation"
        }
    ],
    "highlights": [
        "Flow-based Generative Models<br/><br/>Let x be a high-dimensional random vector with unknown true distribution x \u21e0 p\u21e4(x)",
        "Flow-based generative models like Glow are efficient to parallelize for both inference and synthesis",
        "In this paper we propose a new a generative flow coined Glow, with various new elements as described in Section 3",
        "In (<a class=\"ref-link\" id=\"cPapamakarios_et+al_2017_a\" href=\"#rPapamakarios_et+al_2017_a\">Papamakarios et al, 2017</a>) (MAF), the authors propose a generative flow based on IAF (<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a>); since synthesis from MAF is non-parallelizable and inefficient, we omit it from comparisons",
        "Consistent with earlier work on likelihood-based generative models, we found that sampling from a reduced-temperature model (<a class=\"ref-link\" id=\"cParmar_et+al_2018_a\" href=\"#rParmar_et+al_2018_a\">Parmar et al, 2018</a>) often results in higher-quality samples",
        "We propose a new type of generative flow and demonstrate improved quantitative performance in terms of log-likelihood on standard image modeling benchmarks"
    ],
    "key_statements": [
        "Flow-based Generative Models<br/><br/>Let x be a high-dimensional random vector with unknown true distribution x \u21e0 p\u21e4(x)",
        "Flow-based generative models like Glow are efficient to parallelize for both inference and synthesis",
        "In this paper we propose a new a generative flow coined Glow, with various new elements as described in Section 3",
        "In (<a class=\"ref-link\" id=\"cPapamakarios_et+al_2017_a\" href=\"#rPapamakarios_et+al_2017_a\">Papamakarios et al, 2017</a>) (MAF), the authors propose a generative flow based on IAF (<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a>); since synthesis from MAF is non-parallelizable and inefficient, we omit it from comparisons",
        "Consistent with earlier work on likelihood-based generative models, we found that sampling from a reduced-temperature model (<a class=\"ref-link\" id=\"cParmar_et+al_2018_a\" href=\"#rParmar_et+al_2018_a\">Parmar et al, 2018</a>) often results in higher-quality samples",
        "We propose a new type of generative flow and demonstrate improved quantitative performance in terms of log-likelihood on standard image modeling benchmarks"
    ],
    "summary": [
        "Flow-based Generative Models<br/><br/>Let x be a high-dimensional random vector with unknown true distribution x \u21e0 p\u21e4(x).",
        "3. Flow-based generative models, first described in NICE (<a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\">Dinh et al, 2014</a></a></a></a>) and extended in RealNVP (<a class=\"ref-link\" id=\"cDinh_et+al_2016_a\" href=\"#rDinh_et+al_2016_a\">Dinh et al, 2016</a>).",
        "Flow-based generative models like Glow are efficient to parallelize for both inference and synthesis.",
        "In most flow-based generative models (<a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\">Dinh et al, 2014</a></a></a></a>, 2016), the generative process is defined as: z \u21e0 p\u2713(z)",
        "Each step of flow consists of actnorm (Section 3.1) followed by an invertible 1 \u21e5 1 convolution (Section 3.2), followed by a coupling layer (Section 3.3).",
        "3.1 Actnorm: scale and bias layer with data dependent initialization In Dinh et al (2016), the authors propose the use of batch normalization (<a class=\"ref-link\" id=\"cIoffe_2015_a\" href=\"#rIoffe_2015_a\">Ioffe and Szegedy, 2015</a>) to alleviate the problems encountered when training deep models.",
        "We propose an actnorm layer, that performs an affine transformation of the activations using a scale and bias parameter per channel, similar to batch normalization.",
        "(<a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\">Dinh et al, 2014</a></a></a></a>, 2016) proposed a flow containing the equivalent of a permutation that reverses the ordering of the channels.",
        "We initialize the last convolution of each NN() with zeros, such that each affine coupling layer initially performs an identity function; we found that this helps training very deep networks.",
        "The type of permutation specifically done in (<a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\"><a class=\"ref-link\" id=\"cDinh_et+al_2014_a\" href=\"#rDinh_et+al_2014_a\">Dinh et al, 2014</a></a></a></a>, 2016) is equivalent to reversing the ordering of the channels before performing an additive coupling layer.",
        "In (<a class=\"ref-link\" id=\"cPapamakarios_et+al_2017_a\" href=\"#rPapamakarios_et+al_2017_a\">Papamakarios et al, 2017</a>) (MAF), the authors propose a generative flow based on IAF (<a class=\"ref-link\" id=\"cKingma_et+al_2016_a\" href=\"#rKingma_et+al_2016_a\">Kingma et al, 2016</a>); since synthesis from MAF is non-parallelizable and inefficient, we omit it from comparisons.",
        "We apply our model on other standard datasets and compare log-likelihoods against previous generative models.",
        "We choose the architecture described in Section 3, and consider three variations for the permutation of the channel variables - a reversing operation as described in the RealNVP, a fixed random permutation, and our invertible 1 \u21e5 1 convolution.",
        "For both additive and affine couplings, the invertible 1 \u21e5 1 convolution achieves a lower negative log likelihood and converges faster.",
        "We include the bits/dimension for our model trained on 256 \u21e5 256 CelebA HQ used in our qualitative experiments.3 As we see in Table 2, our model achieves a significant improvement on all the datasets.",
        "Consistent with earlier work on likelihood-based generative models, we found that sampling from a reduced-temperature model (<a class=\"ref-link\" id=\"cParmar_et+al_2018_a\" href=\"#rParmar_et+al_2018_a\">Parmar et al, 2018</a>) often results in higher-quality samples.",
        "We propose a new type of generative flow and demonstrate improved quantitative performance in terms of log-likelihood on standard image modeling benchmarks.",
        "We demonstrate that when trained on high-resolution faces, our model is able to synthesize realistic images"
    ],
    "headline": "In this paper we propose Glow, a simple type of generative flow using an invertible 1 \u21e5 1 convolution",
    "reference_links": [
        {
            "id": "Deco_1995_a",
            "entry": "Deco, G. and Brauer, W. (1995). Higher order statistical decorrelation without information loss. Advances in Neural Information Processing Systems, pages 247\u2013254. 3For 128 \u21e5 128 and 96 \u21e5 96 versions, we centre cropped the original image, and downsampled. For 64 \u21e5 64 version, we took random crops from the 96 \u21e5 96 downsampled image as done in Dinh et al. (2016)",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deco%2C%20G.%20Brauer%2C%20W.%20Higher%20order%20statistical%20decorrelation%20without%20information%20loss%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deco%2C%20G.%20Brauer%2C%20W.%20Higher%20order%20statistical%20decorrelation%20without%20information%20loss%201995"
        },
        {
            "id": "Dinh_et+al_2014_a",
            "entry": "Dinh, L., Krueger, D., and Bengio, Y. (2014). Nice: non-linear independent components estimation. arXiv preprint arXiv:1410.8516.",
            "arxiv_url": "https://arxiv.org/pdf/1410.8516"
        },
        {
            "id": "Dinh_et+al_2016_a",
            "entry": "Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2016). Density estimation using Real NVP. arXiv preprint arXiv:1605.08803.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08803"
        },
        {
            "id": "Gomez_et+al_2017_a",
            "entry": "Gomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. (2017). The reversible residual network: Backpropagation without storing activations. In Advances in Neural Information Processing Systems, pages 2211\u20132221.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gomez%2C%20A.N.%20Ren%2C%20M.%20Urtasun%2C%20R.%20Grosse%2C%20R.B.%20The%20reversible%20residual%20network%3A%20Backpropagation%20without%20storing%20activations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gomez%2C%20A.N.%20Ren%2C%20M.%20Urtasun%2C%20R.%20Grosse%2C%20R.B.%20The%20reversible%20residual%20network%3A%20Backpropagation%20without%20storing%20activations%202017"
        },
        {
            "id": "Goodfellow_et+al_2014_a",
            "entry": "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672\u20132680.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "Graves_2013_a",
            "entry": "Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.",
            "arxiv_url": "https://arxiv.org/pdf/1308.0850"
        },
        {
            "id": "Grover_et+al_2018_a",
            "entry": "Grover, A., Dhar, M., and Ermon, S. (2018). Flow-gan: Combining maximum likelihood and adversarial learning in generative models. In AAAI Conference on Artificial Intelligence.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grover%2C%20A.%20Dhar%2C%20M.%20Ermon%2C%20S.%20Flow-gan%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grover%2C%20A.%20Dhar%2C%20M.%20Ermon%2C%20S.%20Flow-gan%3A%20Combining%20maximum%20likelihood%20and%20adversarial%20learning%20in%20generative%20models%202018"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Identity mappings in deep residual networks. arXiv preprint arXiv:1603.05027.",
            "arxiv_url": "https://arxiv.org/pdf/1603.05027"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "Hochreiter, S. and Schmidhuber, J. (1997). Long Short-Term Memory. Neural computation, 9(8):1735\u20131780.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20Short-Term%20Memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20Short-Term%20Memory%201997"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.",
            "arxiv_url": "https://arxiv.org/pdf/1502.03167"
        },
        {
            "id": "Karras_et+al_2017_a",
            "entry": "Karras, T., Aila, T., Laine, S., and Lehtinen, J. (2017). Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10196"
        },
        {
            "id": "Kingma_2015_a",
            "entry": "Kingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. Proceedings of the International Conference on Learning Representations 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "Kingma_et+al_2016_a",
            "entry": "Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016). Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pages 4743\u20134751.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Jozefowicz%2C%20R.%20Chen%2C%20X.%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Salimans%2C%20T.%20Jozefowicz%2C%20R.%20Chen%2C%20X.%20Improved%20variational%20inference%20with%20inverse%20autoregressive%20flow%202016"
        },
        {
            "id": "Kingma_2013_a",
            "entry": "Kingma, D. P. and Welling, M. (2013). Auto-encoding variational Bayes. Proceedings of the 2nd International Conference on Learning Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Auto-encoding%20variational%20Bayes%202013"
        },
        {
            "id": "Kingma_2018_a",
            "entry": "Kingma, D. P. and Welling, M. (2018). Variational autoencoders. Under Review.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Welling%2C%20M.%20Variational%20autoencoders%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Welling%2C%20M.%20Variational%20autoencoders%202018"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Krizhevsky, A. (2009). Learning multiple layers of features from tiny images.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Oord_et+al_2016_a",
            "entry": "Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu, K. (2016). Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "V_et+al_2017_a",
            "entry": "v. d., Lockhart, E., Cobo, L. C., Stimberg, F., et al. (2017). Parallel wavenet: Fast high-fidelity speech synthesis. arXiv preprint arXiv:1711.10433.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10433"
        },
        {
            "id": "Papamakarios_et+al_2017_a",
            "entry": "Papamakarios, G., Murray, I., and Pavlakou, T. (2017). Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, pages 2335\u20132344.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papamakarios%2C%20G.%20Murray%2C%20I.%20Pavlakou%2C%20T.%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papamakarios%2C%20G.%20Murray%2C%20I.%20Pavlakou%2C%20T.%20Masked%20autoregressive%20flow%20for%20density%20estimation%202017"
        },
        {
            "id": "Parmar_et+al_2018_a",
            "entry": "Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, \u0141., Shazeer, N., and Ku, A. (2018). Image transformer. arXiv preprint arXiv:1802.05751.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05751"
        },
        {
            "id": "Reed_et+al_2017_a",
            "entry": "Reed, S., Oord, A. v. d., Kalchbrenner, N., Colmenarejo, S. G., Wang, Z., Belov, D., and de Freitas, N. (2017). Parallel multiscale autoregressive density estimation. arXiv preprint arXiv:1703.03664.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03664"
        },
        {
            "id": "Rezende_2015_a",
            "entry": "Rezende, D. and Mohamed, S. (2015). Variational inference with normalizing flows. In Proceedings of The 32nd International Conference on Machine Learning, pages 1530\u20131538.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rezende%2C%20D.%20Mohamed%2C%20S.%20Variational%20inference%20with%20normalizing%20flows%202015"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20O.%20Deng%2C%20J.%20Su%2C%20H.%20Krause%2C%20J.%20Imagenet%20large%20scale%20visual%20recognition%20challenge%202015"
        },
        {
            "id": "Salimans_2017_a",
            "entry": "Salimans, T. and Bulatov, Y. (2017). Gradient checkpointing. https://github.com/openai/gradient-checkpointing.",
            "url": "https://github.com/openai/gradient-checkpointing"
        },
        {
            "id": "Salimans_2016_a",
            "entry": "Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07868"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "Van_et+al_2016_b",
            "entry": "van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. (2016a). Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "Van_et+al_2016_c",
            "entry": "van den Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., and Kavukcuoglu, K. (2016b). Conditional image generation with PixelCNN decoders. arXiv preprint arXiv:1606.05328.",
            "arxiv_url": "https://arxiv.org/pdf/1606.05328"
        },
        {
            "id": "Yu_et+al_2015_a",
            "entry": "Yu, F., Zhang, Y., Song, S., Seff, A., and Xiao, J. (2015). Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        }
    ]
}
