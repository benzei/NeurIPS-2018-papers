{
    "filename": "7724-fast-greedy-algorithms-for-dictionary-selection-with-generalized-sparsity-constraints.pdf",
    "metadata": {
        "title": "Fast greedy algorithms for dictionary selection with generalized sparsity constraints",
        "author": "Kaito Fujii, Tasuku Soma",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7724-fast-greedy-algorithms-for-dictionary-selection-with-generalized-sparsity-constraints.pdf"
        },
        "abstract": "In dictionary selection, several atoms are selected from finite candidates that successfully approximate given data points in the sparse representation. We propose a novel efficient greedy algorithm for dictionary selection. Not only does our algorithm work much faster than the known methods, but it can also handle more complex sparsity constraints, such as average sparsity. Using numerical experiments, we show that our algorithm outperforms the known methods for dictionary selection, achieving competitive performances with dictionary learning algorithms in a smaller running time."
    },
    "keywords": [
        {
            "term": "submodular maximization",
            "url": "https://en.wikipedia.org/wiki/submodular_maximization"
        },
        {
            "term": "dictionary learning",
            "url": "https://en.wikipedia.org/wiki/dictionary_learning"
        },
        {
            "term": "greedy algorithm",
            "url": "https://en.wikipedia.org/wiki/greedy_algorithm"
        },
        {
            "term": "sparse representation",
            "url": "https://en.wikipedia.org/wiki/sparse_representation"
        }
    ],
    "highlights": [
        "Learning sparse representations of data and signals has been extensively studied for the past decades in machine learning and signal processing [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "Many studies have been devoted to dictionary learning, it is usually difficult to solve, requiring a non-convex optimization problem that often suffers from local minima",
        "We extend their approach to dictionary selection in the present work, with an additional improvement that exploits",
        "Generalized sparsity constraint Incorporating further prior knowledge on the data domain often improves the quality of dictionaries [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "We present an adaptation of to dictionary selection with generalized sparsity constraints"
    ],
    "key_statements": [
        "Learning sparse representations of data and signals has been extensively studied for the past decades in machine learning and signal processing [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "Many studies have been devoted to dictionary learning, it is usually difficult to solve, requiring a non-convex optimization problem that often suffers from local minima",
        "We extend their approach to dictionary selection in the present work, with an additional improvement that exploits",
        "Generalized sparsity constraint Incorporating further prior knowledge on the data domain often improves the quality of dictionaries [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>]",
        "We prove that can be applied for the generalized sparsity constraint with a slightly worse efficiency",
        "Replacement Greedy was first proposed as an algorithm for a different problem, two-stage submodular maximization [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "We present an adaptation of to dictionary selection with generalized sparsity constraints",
        "For all the online dictionary selection methods, the hedge algorithm is used as the k = 20 s = 5 subroutines"
    ],
    "summary": [
        "Learning sparse representations of data and signals has been extensively studied for the past decades in machine learning and signal processing [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "The dictionary selection finds a set of size that maximizes",
        "Challenges in h dictionary selection are that the evaluation of is NP-hard in general [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], and the objective function h is not submodular [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] and the well-known greedy algorithm [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] cannot be applied.",
        "Generalized sparsity constraint Incorporating further prior knowledge on the data domain often improves the quality of dictionaries [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "; in contrast, cannot be extended to the average sparsity setting because it can only handle local constraints on Zt, and yields an exponential running time.",
        "Example 3.2 The sparsity constraint for the standard dictionary selection can be written as",
        "This was proposed by [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] as a sparsity constraint for two-stage submodular maximization.",
        "Example 3.5 This sparsity imposes a constraint on the average number of used atoms among all data points.",
        "The replacement sparsity parameters of individual matroids, block sparsity, and average sparsity are upper-bounded by k, k, and 3k 1, respectively.",
        "[<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] and for dictionary selection with generalized sparsity constraints.",
        "Replacement Greedy was first proposed as an algorithm for a different problem, two-stage submodular maximization [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>].",
        "We present an adaptation of to dictionary selection with generalized sparsity constraints.",
        "X to with respect to each function ft, i.e., the algorithm selects a that maximizes max Z0 (",
        "That the sparsity constraint supports of an optimal dictionary after k0 steps satisfies",
        "We consider the 2-utility function and average sparsity constraint because it is the most complex constraint.",
        "Further assume that u is the2-utility function and I is the average sparsity constraint.",
        "We empirically evaluate our proposed algorithms on several dictionary selection2 problems with synthetic and real-world datasets.",
        "We randomly pick a dictionary with size out of the ground set, and generate sparse linear combinations of the columns of this dictionary.",
        ", we replace Ms,2 with some parameter that decreases as the size of the currepnt dictionary grows, which prevents the gains of all the atoms from being zero.",
        ". The parameter s=5 of sparsity constraints is set to .",
        "There we provide examples of image restoration, in which the average sparsity works better than the standard dictionary selection.",
        "For all the online dictionary selection methods, the hedge algorithm is used as the k = 20 s = 5 subroutines."
    ],
    "headline": "We propose a novel efficient greedy algorithm for dictionary selection",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Learning sparsely used overcomplete dictionaries via alternating minimization. SIAM Journal on Optimization, 26(4):2775\u20132799, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Anandkumar%2C%20A.%20Jain%2C%20P.%20Netrapalli%2C%20P.%20Learning%20sparsely%20used%20overcomplete%20dictionaries%20via%20alternating%20minimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Anandkumar%2C%20A.%20Jain%2C%20P.%20Netrapalli%2C%20P.%20Learning%20sparsely%20used%20overcomplete%20dictionaries%20via%20alternating%20minimization%202016"
        },
        {
            "id": "3",
            "entry": "[3] S. Arora, R. Ge, and A. Moitra. New algorithms for learning incoherent and overcomplete dictionaries. In Proceedings of the Conference on Learning Theory (COLT), pages 779\u2013806, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20S.%20Ge%2C%20R.%20Moitra%2C%20A.%20New%20algorithms%20for%20learning%20incoherent%20and%20overcomplete%20dictionaries%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20S.%20Ge%2C%20R.%20Moitra%2C%20A.%20New%20algorithms%20for%20learning%20incoherent%20and%20overcomplete%20dictionaries%202014"
        },
        {
            "id": "4",
            "entry": "[4] E. Balkanski, B. Mirzasoleiman, A. Krause, and Y. Singer. Learning sparse combinatorial representations via two-stage submodular maximization. In Proceedings of The 33rd International Conference on Machine Learning (ICML), pages 2207\u20132216, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balkanski%2C%20E.%20Mirzasoleiman%2C%20B.%20Krause%2C%20A.%20Singer%2C%20Y.%20Learning%20sparse%20combinatorial%20representations%20via%20two-stage%20submodular%20maximization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balkanski%2C%20E.%20Mirzasoleiman%2C%20B.%20Krause%2C%20A.%20Singer%2C%20Y.%20Learning%20sparse%20combinatorial%20representations%20via%20two-stage%20submodular%20maximization%202016"
        },
        {
            "id": "5",
            "entry": "[5] E. J. Candes and T. Tao. Decoding by linear programming. Theory, 51(12):4203\u20134215, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20E.J.%20Tao%2C%20T.%20Decoding%20by%20linear%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20E.J.%20Tao%2C%20T.%20Decoding%20by%20linear%20programming%202005"
        },
        {
            "id": "6",
            "entry": "[6] V. Cevher and A. Krause. Greedy dictionary selection for sparse representation. IEEE Journal of Selected Topics in Signal Processing, 5(5):979\u2013988, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cevher%2C%20V.%20Krause%2C%20A.%20Greedy%20dictionary%20selection%20for%20sparse%20representation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cevher%2C%20V.%20Krause%2C%20A.%20Greedy%20dictionary%20selection%20for%20sparse%20representation%202011"
        },
        {
            "id": "7",
            "entry": "[7] L. Chen, H. Hassani, and A. Karbasi. Online continuous submodular maximization. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS), volume 84, pages 1896\u20131905, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20L.%20Hassani%2C%20H.%20Karbasi%2C%20A.%20Online%20continuous%20submodular%20maximization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20L.%20Hassani%2C%20H.%20Karbasi%2C%20A.%20Online%20continuous%20submodular%20maximization%202018"
        },
        {
            "id": "8",
            "entry": "[8] Y. Cong, J. Yuan, and J. Luo. Towards scalable summarization of consumer videos via sparse , 14(1):66\u201375, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cong%2C%20Y.%20Yuan%2C%20J.%20Luo%2C%20J.%20Towards%20scalable%20summarization%20of%20consumer%20videos%20via%20sparse%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cong%2C%20Y.%20Yuan%2C%20J.%20Luo%2C%20J.%20Towards%20scalable%20summarization%20of%20consumer%20videos%20via%20sparse%202012"
        },
        {
            "id": "9",
            "entry": "[9] Y. Cong, J. Liu, G. Sun, Q. You, Y. Li, and J. Luo. Adaptive greedy dictionary selection for web media summarization. IEEE Transactions on Image Processing, 26(1):185\u2013195, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cong%2C%20Y.%20Liu%2C%20J.%20Sun%2C%20G.%20You%2C%20Q.%20Adaptive%20greedy%20dictionary%20selection%20for%20web%20media%20summarization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cong%2C%20Y.%20Liu%2C%20J.%20Sun%2C%20G.%20You%2C%20Q.%20Adaptive%20greedy%20dictionary%20selection%20for%20web%20media%20summarization%202017"
        },
        {
            "id": "10",
            "entry": "[10] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 1057\u20131064, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Das%2C%20A.%20Kempe%2C%20D.%20Submodular%20meets%20spectral%3A%20Greedy%20algorithms%20for%20subset%20selection%2C%20sparse%20approximation%20and%20dictionary%20selection%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Das%2C%20A.%20Kempe%2C%20D.%20Submodular%20meets%20spectral%3A%20Greedy%20algorithms%20for%20subset%20selection%2C%20sparse%20approximation%20and%20dictionary%20selection%202011"
        },
        {
            "id": "11",
            "entry": "[11] B. Dumitrescu and P. Irofti. Dictionary Learning Algorithms and Applications. Springer, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumitrescu%2C%20B.%20Irofti%2C%20P.%20Dictionary%20Learning%20Algorithms%20and%20Applications%202018"
        },
        {
            "id": "12",
            "entry": "[12] E. Elenberg, A. G. Dimakis, M. Feldman, and A. Karbasi. Streaming weak submodularity: Interpreting neural networks on the fly. In Advances in Neural Information Processing Systems (NIPS) 30, pages 4047\u20134057. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elenberg%2C%20E.%20Dimakis%2C%20A.G.%20Feldman%2C%20M.%20Karbasi%2C%20A.%20Streaming%20weak%20submodularity%3A%20Interpreting%20neural%20networks%20on%20the%20fly%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elenberg%2C%20E.%20Dimakis%2C%20A.G.%20Feldman%2C%20M.%20Karbasi%2C%20A.%20Streaming%20weak%20submodularity%3A%20Interpreting%20neural%20networks%20on%20the%20fly%202017"
        },
        {
            "id": "13",
            "entry": "[13] E. R. Elenberg, R. Khanna, and A. G. Dimakis. Restricted strong convexity implies weak submodularity. In Proceedings of NIPS Workshop on Learning in High Dimensions with Structure, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elenberg%2C%20E.R.%20Khanna%2C%20R.%20Dimakis%2C%20A.G.%20Restricted%20strong%20convexity%20implies%20weak%20submodularity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elenberg%2C%20E.R.%20Khanna%2C%20R.%20Dimakis%2C%20A.G.%20Restricted%20strong%20convexity%20implies%20weak%20submodularity%202016"
        },
        {
            "id": "14",
            "entry": "[14] K. Engan, S. O. Aase, and J. Hakon Husoy. Method of optimal directions for frame design. In Proceedings of the IEEE International Conference on the Acoustics, Speech, and Signal Processing , volume 05, pages 2443\u20132446, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engan%2C%20K.%20Aase%2C%20S.O.%20Husoy%2C%20J.Hakon%20Method%20of%20optimal%20directions%20for%20frame%20design%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engan%2C%20K.%20Aase%2C%20S.O.%20Husoy%2C%20J.Hakon%20Method%20of%20optimal%20directions%20for%20frame%20design%201999"
        },
        {
            "id": "15",
            "entry": "[15] M. Everingham, A. Zisserman, C. K. I. Williams, and L. Van Gool. The PASCAL Visual Object Classes Challenge 2006 (VOC2006) Results. http://www.pascalnetwork.org/challenges/VOC/voc2006/results.pdf.",
            "url": "http://www.pascalnetwork.org/challenges/VOC/voc2006/results.pdf"
        },
        {
            "id": "16",
            "entry": "[16] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Springer, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foucart%2C%20S.%20Rauhut%2C%20H.%20A%20Mathematical%20Introduction%20to%20Compressive%20Sensing%202013"
        },
        {
            "id": "17",
            "entry": "[17] S. Fujishige. Submodular Functions and Optimization. Elsevier, 2nd edition, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fujishige%2C%20S.%20Submodular%20Functions%20and%20Optimization%202005"
        },
        {
            "id": "18",
            "entry": "[18] G. H. Golub and C. F. Van Loan. Matrix Computations, volume 3. JHU Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=G%20H%20Golub%20and%20C%20F%20Van%20Loan%20Matrix%20Computations%20volume%203%20JHU%20Press%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=G%20H%20Golub%20and%20C%20F%20Van%20Loan%20Matrix%20Computations%20volume%203%20JHU%20Press%202012"
        },
        {
            "id": "19",
            "entry": "[19] J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity. The Journal of Machine Learning Research, 12:3371\u20133412, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20J.%20Zhang%2C%20T.%20Metaxas%2C%20D.%20Learning%20with%20structured%20sparsity%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20J.%20Zhang%2C%20T.%20Metaxas%2C%20D.%20Learning%20with%20structured%20sparsity%202009"
        },
        {
            "id": "20",
            "entry": "[20] S. Kale, Z. Karnin, T. Liang, and D. P\u00e1l. Adaptive feature selection: Computationally efficient Proceedings of the 34th International Conference online sparse linear regression under RIP. In on Machine Learning (ICML), pages 1\u201322, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kale%2C%20S.%20Karnin%2C%20Z.%20Liang%2C%20T.%20P%C3%A1l%2C%20D.%20Adaptive%20feature%20selection%3A%20Computationally%20efficient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kale%2C%20S.%20Karnin%2C%20Z.%20Liang%2C%20T.%20P%C3%A1l%2C%20D.%20Adaptive%20feature%20selection%3A%20Computationally%20efficient%202017"
        },
        {
            "id": "21",
            "entry": "[21] R. Khanna, E. Elenberg, A. Dimakis, J. Ghosh, and S. Neghaban. On approximation guarantees for greedy low rank optimization. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 1837\u20131846, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khanna%2C%20R.%20Elenberg%2C%20E.%20Dimakis%2C%20A.%20Ghosh%2C%20J.%20On%20approximation%20guarantees%20for%20greedy%20low%20rank%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khanna%2C%20R.%20Elenberg%2C%20E.%20Dimakis%2C%20A.%20Ghosh%2C%20J.%20On%20approximation%20guarantees%20for%20greedy%20low%20rank%20optimization%202017"
        },
        {
            "id": "22",
            "entry": "[22] A. Krause and V. Cevher. Submodular dictionary selection for sparse representation. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 567\u2013",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krause%2C%20A.%20Cevher%2C%20V.%20Submodular%20dictionary%20selection%20for%20sparse%20representation",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krause%2C%20A.%20Cevher%2C%20V.%20Submodular%20dictionary%20selection%20for%20sparse%20representation"
        },
        {
            "id": "23",
            "entry": "[23] E. Liberty and M. Sviridenko. Greedy minimization of weakly supermodular set functions. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2017) , volume 81, pages 19:1\u201319:11, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liberty%2C%20E.%20Sviridenko%2C%20M.%20Greedy%20minimization%20of%20weakly%20supermodular%20set%20functions.%20In%20Approximation%2C%20Randomization%2C%20and%20Combinatorial%20Optimization.%20Algorithms%20and%20Techniques%202017"
        },
        {
            "id": "24",
            "entry": "[24] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11:19\u201360, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20J.%20Bach%2C%20F.%20Ponce%2C%20J.%20Sapiro%2C%20G.%20Online%20learning%20for%20matrix%20factorization%20and%20sparse%20coding%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20J.%20Bach%2C%20F.%20Ponce%2C%20J.%20Sapiro%2C%20G.%20Online%20learning%20for%20matrix%20factorization%20and%20sparse%20coding%202010"
        },
        {
            "id": "25",
            "entry": "[25] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24(2):227\u2013234, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Natarajan%2C%20B.K.%20Sparse%20approximate%20solutions%20to%20linear%20systems%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Natarajan%2C%20B.K.%20Sparse%20approximate%20solutions%20to%20linear%20systems%201995"
        },
        {
            "id": "26",
            "entry": "[26] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for highdimensional analysis of m -estimators with decomposable regularizers. Science, (4):538\u2013557, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20S.N.%20Ravikumar%2C%20P.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20A%20unified%20framework%20for%20highdimensional%20analysis%20of%20m%20-estimators%20with%20decomposable%20regularizers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20S.N.%20Ravikumar%2C%20P.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20A%20unified%20framework%20for%20highdimensional%20analysis%20of%20m%20-estimators%20with%20decomposable%20regularizers%202012"
        },
        {
            "id": "27",
            "entry": "[27] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing , 14(1):265\u2013294, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Fisher%2C%20M.L.%20An%20analysis%20of%20approximations%20for%20maximizing%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Fisher%2C%20M.L.%20An%20analysis%20of%20approximations%20for%20maximizing%201978"
        },
        {
            "id": "28",
            "entry": "[28] R. Rubinstein, M. Zibulevsky, and M. Elad. Double sparsity: Learning sparse dictionaries for sparse signal approximation. IEEE Transactions on Signal Processing, 58(3):1553\u20131564, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rubinstein%2C%20R.%20Zibulevsky%2C%20M.%20Elad%2C%20M.%20Double%20sparsity%3A%20Learning%20sparse%20dictionaries%20for%20sparse%20signal%20approximation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rubinstein%2C%20R.%20Zibulevsky%2C%20M.%20Elad%2C%20M.%20Double%20sparsity%3A%20Learning%20sparse%20dictionaries%20for%20sparse%20signal%20approximation%202010"
        },
        {
            "id": "29",
            "entry": "[29] C. Rusu, B. Dumitrescu, and S. A. Tsaftaris. Explicit shift-invariant dictionary learning. IEEE Signal Processing Letters, 21(1):6\u20139, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rusu%2C%20C.%20Dumitrescu%2C%20B.%20Tsaftaris%2C%20S.A.%20Explicit%20shift-invariant%20dictionary%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rusu%2C%20C.%20Dumitrescu%2C%20B.%20Tsaftaris%2C%20S.A.%20Explicit%20shift-invariant%20dictionary%20learning%202014"
        },
        {
            "id": "30",
            "entry": "[30] S. Stan, M. Zadimoghaddam, A. Krause, and A. Karbasi. Probabilistic submodular maximization in sub-linear time. Proceedings of the 34th International Conference on Machine Learning (ICML), pages 3241\u20133250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stan%2C%20S.%20Zadimoghaddam%2C%20M.%20Krause%2C%20A.%20Karbasi%2C%20A.%20Probabilistic%20submodular%20maximization%20in%20sub-linear%20time%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stan%2C%20S.%20Zadimoghaddam%2C%20M.%20Krause%2C%20A.%20Karbasi%2C%20A.%20Probabilistic%20submodular%20maximization%20in%20sub-linear%20time%202017"
        },
        {
            "id": "31",
            "entry": "[31] M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In Advances in Neural Information Processing Systems (NIPS), pages 1577\u20131584, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Streeter%2C%20M.%20Golovin%2C%20D.%20An%20online%20algorithm%20for%20maximizing%20submodular%20functions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Streeter%2C%20M.%20Golovin%2C%20D.%20An%20online%20algorithm%20for%20maximizing%20submodular%20functions%202009"
        },
        {
            "id": "32",
            "entry": "[32] M. Zhou, H. Chen, L. Ren, G. Sapiro, L. Carin, and J. W. Paisley. Non-parametric bayesian Advances in Neural Information dictionary learning for sparse image representations. In Processing Systems (NIPS) 22, pages 2295\u20132303. 2009. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Chen%2C%20H.%20Ren%2C%20L.%20Sapiro%2C%20G.%20Non-parametric%20bayesian%20Advances%20in%20Neural%20Information%20dictionary%20learning%20for%20sparse%20image%20representations%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Chen%2C%20H.%20Ren%2C%20L.%20Sapiro%2C%20G.%20Non-parametric%20bayesian%20Advances%20in%20Neural%20Information%20dictionary%20learning%20for%20sparse%20image%20representations%202009"
        }
    ]
}
