{
    "filename": "7339-which-neural-net-architectures-give-rise-to-exploding-and-vanishing-gradients.pdf",
    "metadata": {
        "title": "Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?",
        "author": "Boris Hanin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7339-which-neural-net-architectures-give-rise-to-exploding-and-vanishing-gradients.pdf"
        },
        "abstract": "We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant , given by the sum of the reciprocals of the hidden layer widths. When is large, the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view, we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "highway network",
            "url": "https://en.wikipedia.org/wiki/highway_network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "field theory",
            "url": "https://en.wikipedia.org/wiki/field_theory"
        }
    ],
    "highlights": [
        "A fundamental obstacle in training deep neural nets using gradient based optimization is the exploding and vanishing gradient problem (EVGP), which has attracted much attention (e.g. [BSF94, HBF+01, MM15, XXP17, PSG17, PSG18]) after first being studied by Hochreiter [Hoc91]",
        "D and hidden layer widths give nets that suffer from the exploding and vanishing gradient problem at initialization",
        "Our main results, Theorems 1-3, prove that the exploding and vanishing gradient problem will occur in a ReLU net N if and only if a single scalar parameter, the sum",
        "Their precise definitions are given in \u00a73.2 and \u00a73.3, and we provide in \u00a73.1 a discussion of the relation between the different senses in which the exploding and vanishing gradient problem can occur",
        "We explain in exactly what sense we study the exploding and vanishing gradient problem and contrast our definition, which depends on the behavior of the entries of the input-output Jacobian JN , with the more usual definition, which depends on the behavior of its singular values",
        "At least for the ReLU nets we study, that a family of neural net architectures avoids the quenched exploding and vanishing gradient problem if and only if it avoids the annealed exploding and vanishing gradient problem holds)"
    ],
    "key_statements": [
        "A fundamental obstacle in training deep neural nets using gradient based optimization is the exploding and vanishing gradient problem (EVGP), which has attracted much attention (e.g. [BSF94, HBF+01, MM15, XXP17, PSG17, PSG18]) after first being studied by Hochreiter [Hoc91]",
        "D and hidden layer widths give nets that suffer from the exploding and vanishing gradient problem at initialization",
        "Our main results, Theorems 1-3, prove that the exploding and vanishing gradient problem will occur in a ReLU net N if and only if a single scalar parameter, the sum",
        "Instead of taking the singular value definition of the exploding and vanishing gradient problem as in [PSG17, PSG18], we propose two non-spectral formulations of the exploding and vanishing gradient problem, which we term annealed and quenched",
        "Their precise definitions are given in \u00a73.2 and \u00a73.3, and we provide in \u00a73.1 a discussion of the relation between the different senses in which the exploding and vanishing gradient problem can occur",
        "To avoid the exploding and vanishing gradient problem in deep feed-forward networks with ReLU activations, our results advise letting the widths of hidden layers grow as a function of the depth",
        "We explain in exactly what sense we study the exploding and vanishing gradient problem and contrast our definition, which depends on the behavior of the entries of the input-output Jacobian JN , with the more usual definition, which depends on the behavior of its singular values",
        "Architectures that avoid the exploding and vanishing gradient problem in the d annealed sense are ones where the typical magnitude of the partial derivatives Zp,q(d) have bounded fluctuations around a constant mean value",
        "At least for the ReLU nets we study, that a family of neural net architectures avoids the quenched exploding and vanishing gradient problem if and only if it avoids the annealed exploding and vanishing gradient problem holds)"
    ],
    "summary": [
        "A fundamental obstacle in training deep neural nets using gradient based optimization is the exploding and vanishing gradient problem (EVGP), which has attracted much attention (e.g. [BSF94, HBF+01, MM15, XXP17, PSG17, PSG18]) after first being studied by Hochreiter [Hoc91].",
        "D and hidden layer widths give nets that suffer from the EVGP at initialization.",
        "We derive new exact formulas for the joint even moments of the entries of the input-output Jacobian in a fully connected net with random weights and biases.",
        "2. We prove that the empirical variance of gradients in a fully connected net is exponential in the sum of the reciprocals of the hidden layer widths.",
        "Our main results, Theorems 1-3, prove that the EVGP will occur in a ReLU net N if and only if a single scalar parameter, the sum",
        "Nj denotes the width of the th j hidden layer, and we prove in Theorem 1 that the variance of entries in the input-output Jacobian of N",
        "These articles consider two senses in which a fully connected neural net N with random weights and biases can avoid the EVGP.",
        "Theorem 1 below implies, in the infinite width limit, that all ReLU nets avoid the EVGP in both the quenched and annealed sense.",
        "To avoid the EVGP in deep feed-forward networks with ReLU activations, our results advise letting the widths of hidden layers grow as a function of the depth.",
        "In \u00a73.2 and \u00a73.3, we define two precise senses, which we call annealed and quenched, in which that EVGP can occur, phrased directly in terms of the joint moments of Zp,q .",
        "We recall a simple relationship between the moments of the entries of the input-output Jacobian JN and the distribution of its singular values, which can be used to directly compare spectral and entrywise definitions of the EVGP.",
        "Entail that the average singular value for JN equals 1, and we prove in Theorem 1) that even at finite depth and width the average singular value for JN equals 1 for all the random ReLU nets we consider!",
        "Architectures that avoid the EVGP in the d annealed sense are ones where the typical magnitude of the partial derivatives Zp,q(d) have bounded fluctuations around a constant mean value.",
        "At least for the ReLU nets we study, that a family of neural net architectures avoids the quenched EVGP if and only if it avoids the annealed exploding and vanishing gradient problem holds).",
        "Nfully connected feed-forward nets with ReLU activations,o depth d, and whose th j hidden layer has width nj"
    ],
    "headline": "Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant , given by the sum of the reciprocals of the hidden layer widths",
    "reference_links": [
        {
            "id": "Arjovsky_et+al_2016_a",
            "entry": "[ASB16] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pages 1120\u20131128, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Martin%20Shah%2C%20Amar%20Bengio%2C%20Yoshua%20Unitary%20evolution%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "Balduzzi_et+al_2017_a",
            "entry": "[BFL+17] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? arXiv preprint arXiv:1702.08591, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08591"
        },
        {
            "id": "Bengio_et+al_1994_a",
            "entry": "Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157\u2013166, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Simard%2C%20Patrice%20Frasconi%2C%20Paolo%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Simard%2C%20Patrice%20Frasconi%2C%20Paolo%20Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult%201994"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "[CHM+15] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Hochreiter_et+al_2001_a",
            "entry": "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J\u00fcrgen Schmidhuber, et al. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Bengio%2C%20Yoshua%20Frasconi%2C%20Paolo%20Schmidhuber%2C%20J%C3%BCrgen%20Gradient%20flow%20in%20recurrent%20nets%3A%20the%20difficulty%20of%20learning%20long-term%20dependencies%202001"
        },
        {
            "id": "Hochreiter_1991_a",
            "entry": "Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universit\u00e4t M\u00fcnchen, 91, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Untersuchungen%20zu%20dynamischen%20neuronalen%20netzen%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Untersuchungen%20zu%20dynamischen%20neuronalen%20netzen%201991"
        },
        {
            "id": "Hanin_2018_a",
            "entry": "Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. In Advances in Neural Information Processing Systems 32, 2018. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanin%2C%20Boris%20Rolnick%2C%20David%20How%20to%20start%20training%3A%20The%20effect%20of%20initialization%20and%20architecture%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanin%2C%20Boris%20Rolnick%2C%20David%20How%20to%20start%20training%3A%20The%20effect%20of%20initialization%20and%20architecture%202018"
        },
        {
            "id": "Henaff_et+al_2016_a",
            "entry": "Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory tasks. In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 2034\u20132042, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Henaff%2C%20Mikael%20Szlam%2C%20Arthur%20LeCun%2C%20Yann%20Recurrent%20orthogonal%20networks%20and%20long-memory%20tasks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Henaff%2C%20Mikael%20Szlam%2C%20Arthur%20LeCun%2C%20Yann%20Recurrent%20orthogonal%20networks%20and%20long-memory%20tasks%202016"
        },
        {
            "id": "He_et+al_2015_a",
            "entry": "[HZRS15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "[HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Klambauer_et+al_2017_a",
            "entry": "[KUMH17] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Selfnormalizing neural networks. In Advances in Neural Information Processing Systems, pages 972\u2013981, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klambauer%2C%20G%C3%BCnter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Selfnormalizing%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klambauer%2C%20G%C3%BCnter%20Unterthiner%2C%20Thomas%20Mayr%2C%20Andreas%20Hochreiter%2C%20Sepp%20Selfnormalizing%20neural%20networks%202017"
        },
        {
            "id": "Mishkin_2015_a",
            "entry": "[MM15] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06422"
        },
        {
            "id": "Poole_et+al_2016_a",
            "entry": "[PLR+16] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances in neural information processing systems, pages 3360\u20133368, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Poole%2C%20Ben%20Lahiri%2C%20Subhaneil%20Raghu%2C%20Maithra%20Sohl-Dickstein%2C%20Jascha%20Exponential%20expressivity%20in%20deep%20neural%20networks%20through%20transient%20chaos%202016"
        },
        {
            "id": "Pennington_et+al_2017_a",
            "entry": "Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. In Advances in neural information processing systems, pages 4788\u20134798, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Schoenholz%2C%20Samuel%20Ganguli%2C%20Surya%20Resurrecting%20the%20sigmoid%20in%20deep%20learning%20through%20dynamical%20isometry%3A%20theory%20and%20practice%202017"
        },
        {
            "id": "Pennington_et+al_2018_a",
            "entry": "[PSG18] Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. The emergence of spectral universality in deep networks. arXiv preprint arXiv:1802.09979, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09979"
        },
        {
            "id": "Raghu_et+al_2017_a",
            "entry": "[RPK+17] Maithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 2847\u20132854, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20M.%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raghu%2C%20Maithra%20Poole%2C%20Ben%20Kleinberg%2C%20Jon%20M.%20Ganguli%2C%20Surya%20On%20the%20expressive%20power%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "Sepp_1991_a",
            "entry": "Sepp hochreiter\u2019s fundamental deep learning problem (1991). http://people.idsia.ch/~juergen/fundamentaldeeplearningproblem.html. Accessed:2017-12-26.",
            "url": "http://people.idsia.ch/~juergen/fundamentaldeeplearningproblem.html"
        },
        {
            "id": "Rupesh_2015_a",
            "entry": "Rupesh Kumar Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Highway networks. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rupesh%20Kumar%20Srivastava%2C%20Klaus%20Greff%20Schmidhuber%2C%20J%C3%BCrgen%20Highway%20networks%202015"
        },
        {
            "id": "Schoenholz_et+al_2017_a",
            "entry": "[SPSD17] Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. A correspondence between random neural networks and statistical field theory. arXiv preprint arXiv:1710.06570, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06570"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. arXiv preprint arXiv:1703.01827, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01827"
        }
    ]
}
