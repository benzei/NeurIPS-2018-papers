{
    "filename": "7286-efficient-algorithms-for-non-convex-isotonic-regression-through-submodular-optimization.pdf",
    "metadata": {
        "title": "Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization",
        "author": "Francis Bach",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7286-efficient-algorithms-for-non-convex-isotonic-regression-through-submodular-optimization.pdf"
        },
        "abstract": "We consider the minimization of submodular functions subject to ordering constraints. We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance. We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th, first, or higher order oracles; these algorithms also lead to improvements without isotonic constraints. Finally, our experiments show that non-convex loss functions can be much more robust to outliers for isotonic regression, while still being solvable in polynomial time."
    },
    "keywords": [
        {
            "term": "polynomial time algorithm",
            "url": "https://en.wikipedia.org/wiki/polynomial_time_algorithm"
        },
        {
            "term": "stochastic dominance",
            "url": "https://en.wikipedia.org/wiki/stochastic_dominance"
        },
        {
            "term": "submodular function",
            "url": "https://en.wikipedia.org/wiki/submodular_function"
        },
        {
            "term": "estimation problem",
            "url": "https://en.wikipedia.org/wiki/estimation_problem"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "isotonic regression",
            "url": "https://en.wikipedia.org/wiki/isotonic_regression"
        },
        {
            "term": "convex optimization problem",
            "url": "https://en.wikipedia.org/wiki/convex_optimization_problem"
        }
    ],
    "highlights": [
        "Shape constraints such as ordering constraints appear everywhere in estimation problems in machine learning, signal processing and statistics",
        "We focus on imposing ordering constraints into an estimation problem, a setting typically referred to as isotonic regression [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], and we aim to generalize the set of problems for which efficient algorithms exist",
        "\u2013 We show in Section 3 that minimizing a submodular function with isotonic constraints can be cast as a convex optimization problem on a space of uni-dimensional measures, with isotonic constraints corresponding to first-order stochastic dominance",
        "We have shown how submodularity could be leveraged to obtain polynomial-time algorithms for isotonic regressions with a submodular cost, based on convex optimization in a space of measures\u2014 based on convexity arguments, our algorithms apply to all separable nonconvex functions",
        "The final algorithms are based on discretization, with a new scheme that provides improvements based on smoothness ( without isotonic constraints)",
        "Our framework is worth extending in the following directions: (a) we currently consider a fixed discretization, it would be advantageous to consider adaptive schemes, potentially improving the dependence on the number of variables n and the precision \"; (b) other shape constraints can be consider in a similar submodular framework, such as xixj > 0 for certain pairs (i, j); (c) a direct convex formulation without discretization could probably be found for quadratic programming with submodular costs; (d) a statistical study of isotonic regression with adversarial corruption could rely on formulations with polynomial-time algorithms"
    ],
    "key_statements": [
        "Shape constraints such as ordering constraints appear everywhere in estimation problems in machine learning, signal processing and statistics",
        "We focus on imposing ordering constraints into an estimation problem, a setting typically referred to as isotonic regression [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], and we aim to generalize the set of problems for which efficient algorithms exist",
        "\u2013 We show in Section 3 that minimizing a submodular function with isotonic constraints can be cast as a convex optimization problem on a space of uni-dimensional measures, with isotonic constraints corresponding to first-order stochastic dominance",
        "We consider experiments aiming at (a) showing that the new possibility of minimizing submodular functions with isotonic constraints brings new possibilities and (b) that the new discretization algorithms are faster than the naive one",
        "We have shown how submodularity could be leveraged to obtain polynomial-time algorithms for isotonic regressions with a submodular cost, based on convex optimization in a space of measures\u2014 based on convexity arguments, our algorithms apply to all separable nonconvex functions",
        "The final algorithms are based on discretization, with a new scheme that provides improvements based on smoothness ( without isotonic constraints)",
        "Our framework is worth extending in the following directions: (a) we currently consider a fixed discretization, it would be advantageous to consider adaptive schemes, potentially improving the dependence on the number of variables n and the precision \"; (b) other shape constraints can be consider in a similar submodular framework, such as xixj > 0 for certain pairs (i, j); (c) a direct convex formulation without discretization could probably be found for quadratic programming with submodular costs; (d) a statistical study of isotonic regression with adversarial corruption could rely on formulations with polynomial-time algorithms"
    ],
    "summary": [
        "Shape constraints such as ordering constraints appear everywhere in estimation problems in machine learning, signal processing and statistics.",
        "\u2013 We show in Section 3 that minimizing a submodular function with isotonic constraints can be cast as a convex optimization problem on a space of uni-dimensional measures, with isotonic constraints corresponding to first-order stochastic dominance.",
        "K\u21e2kF2 becomes the one of minimizing min\u21e22S\\T 1 k\u21e2 + wkF2 , which is the problem of projecting on the intersection of two convex sets, for w2hich an accelerated Dykstra algorithm may be used [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], with convergence rate in",
        "The main idea behind our discretization scheme is to use high-order smoothness to approximate for any required z, the function value Hz .",
        "We consider experiments aiming at (a) showing that the new possibility of minimizing submodular functions with isotonic constraints brings new possibilities and (b) that the new discretization algorithms are faster than the naive one.",
        "We see that the square loss is highly non robust, while the absolute loss is slightly more robust, and the robust non-convex loss still approximates the decreasing function correctly with 50% of corrupted data when optimized globally, while the method with no guarantee does not converge to an acceptable solution.",
        "As shown in Section 4.2, discretized separable submodular optimization corresponds to the orthogonal projection of a matrix into the intersection of chain isotonic constraints in each row, and isotonic constraints in each column equal to the original set of isotonic constraints.",
        "Right: effect of discretization k on non-separable problems., and we plot the difference in function values after 50 steps of subgradient descent: in each plot, the quantity H is equal to",
        "We have shown how submodularity could be leveraged to obtain polynomial-time algorithms for isotonic regressions with a submodular cost, based on convex optimization in a space of measures\u2014 based on convexity arguments, our algorithms apply to all separable nonconvex functions.",
        "Our framework is worth extending in the following directions: (a) we currently consider a fixed discretization, it would be advantageous to consider adaptive schemes, potentially improving the dependence on the number of variables n and the precision \"; (b) other shape constraints can be consider in a similar submodular framework, such as xixj > 0 for certain pairs (i, j); (c) a direct convex formulation without discretization could probably be found for quadratic programming with submodular costs; (d) a statistical study of isotonic regression with adversarial corruption could rely on formulations with polynomial-time algorithms."
    ],
    "headline": "We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] F. Bach. Learning with Submodular Functions: A Convex Optimization Perspective, volume 6 of Foundations and Trends in Machine Learning. NOW, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Learning%20with%20Submodular%20Functions%3A%20A%20Convex%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Learning%20with%20Submodular%20Functions%3A%20A%20Convex%202013"
        },
        {
            "id": "2",
            "entry": "[2] F. Bach. Submodular functions: from discrete to continuous domains. Mathematical Programming, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.%20Submodular%20functions%3A%20from%20discrete%20to%20continuous%20domains%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.%20Submodular%20functions%3A%20from%20discrete%20to%20continuous%20domains%202018"
        },
        {
            "id": "3",
            "entry": "[3] D. P. Bertsekas. Nonlinear Programming. Athena Scientific Belmont, 2016. 3rd edition.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%20P%20Bertsekas%20Nonlinear%20Programming%20Athena%20Scientific%20Belmont%202016%203rd%20edition"
        },
        {
            "id": "4",
            "entry": "[4] M. J. Best and N. Chakravarti. Active set algorithms for isotonic regression: a unifying framework. Mathematical Programming, 47(1):425\u2013439, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Best%2C%20M.J.%20Chakravarti%2C%20N.%20Active%20set%20algorithms%20for%20isotonic%20regression%3A%20a%20unifying%20framework%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Best%2C%20M.J.%20Chakravarti%2C%20N.%20Active%20set%20algorithms%20for%20isotonic%20regression%3A%20a%20unifying%20framework%201990"
        },
        {
            "id": "5",
            "entry": "[5] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(9):1124\u20131137, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boykov%2C%20Y.%20Kolmogorov%2C%20V.%20An%20experimental%20comparison%20of%20min-cut/max-flow%20algorithms%20for%20energy%20minimization%20in%20vision%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boykov%2C%20Y.%20Kolmogorov%2C%20V.%20An%20experimental%20comparison%20of%20min-cut/max-flow%20algorithms%20for%20energy%20minimization%20in%20vision%202004"
        },
        {
            "id": "6",
            "entry": "[6] A. Chambolle and T. Pock. A remark on accelerated block coordinate descent for computing the proximity operators of a sum of convex functions. SMAI-Journal of computational mathematics, 1:29\u201454, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20remark%20on%20accelerated%20block%20coordinate%20descent%20for%20computing%20the%20proximity%20operators%20of%20a%20sum%20of%20convex%20functions.%20SMAI-Journal%20of%20computational%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20remark%20on%20accelerated%20block%20coordinate%20descent%20for%20computing%20the%20proximity%20operators%20of%20a%20sum%20of%20convex%20functions.%20SMAI-Journal%20of%20computational%202015"
        },
        {
            "id": "7",
            "entry": "[7] Xi Chen, Qihang Lin, and Bodhisattva Sen. On degrees of freedom of projection estimators with applications to multivariate shape restricted regression. Technical Report 1509.01877, arXiv, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Lin%2C%20Qihang%20Sen%2C%20Bodhisattva%20On%20degrees%20of%20freedom%20of%20projection%20estimators%20with%20applications%20to%20multivariate%20shape%20restricted%20regression%202015"
        },
        {
            "id": "8",
            "entry": "[8] Y. Chen and R. J. Samworth. Generalized additive and index models with shape constraints. Journal of the Royal Statistical Society Series B, 78(4):729\u2013754, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Samworth%2C%20R.J.%20Generalized%20additive%20and%20index%20models%20with%20shape%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Y.%20Samworth%2C%20R.J.%20Generalized%20additive%20and%20index%20models%20with%20shape%20constraints%202016"
        },
        {
            "id": "9",
            "entry": "[9] D. Dentcheva and A. Ruszczynski. Semi-infinite probabilistic optimization: first-order stochastic dominance constraint. Optimization, 53(5-6):583\u2013601, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dentcheva%2C%20D.%20Ruszczynski%2C%20A.%20Semi-infinite%20probabilistic%20optimization%3A%20first-order%20stochastic%20dominance%20constraint%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dentcheva%2C%20D.%20Ruszczynski%2C%20A.%20Semi-infinite%20probabilistic%20optimization%3A%20first-order%20stochastic%20dominance%20constraint%202004"
        },
        {
            "id": "10",
            "entry": "[10] M. M. Deza and M. Laurent. Geometry of Cuts and Metrics, volume 15.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20M%20Deza%20and%20M%20Laurent%20Geometry%20of%20Cuts%20and%20Metrics%20volume%2015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20M%20Deza%20and%20M%20Laurent%20Geometry%20of%20Cuts%20and%20Metrics%20volume%2015"
        },
        {
            "id": "11",
            "entry": "[11] G. Gallo, M. D. Grigoriadis, and R. E. Tarjan. A fast parametric maximum flow algorithm and applications. SIAM Journal on Computing, 18(1):30\u201355, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gallo%2C%20G.%20Grigoriadis%2C%20M.D.%20Tarjan%2C%20R.E.%20A%20fast%20parametric%20maximum%20flow%20algorithm%20and%20applications%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gallo%2C%20G.%20Grigoriadis%2C%20M.D.%20Tarjan%2C%20R.E.%20A%20fast%20parametric%20maximum%20flow%20algorithm%20and%20applications%201989"
        },
        {
            "id": "12",
            "entry": "[12] Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust Statistics: the Approach Based on Influence Functions, volume 196. John Wiley & Sons, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hampel%2C%20Frank%20R.%20Ronchetti%2C%20Elvezio%20M.%20Rousseeuw%2C%20Peter%20J.%20Stahel%2C%20Werner%20A.%20Robust%20Statistics%3A%20the%20Approach%20Based%20on%20Influence%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hampel%2C%20Frank%20R.%20Ronchetti%2C%20Elvezio%20M.%20Rousseeuw%2C%20Peter%20J.%20Stahel%2C%20Werner%20A.%20Robust%20Statistics%3A%20the%20Approach%20Based%20on%20Influence%202011"
        },
        {
            "id": "Research_2008_a",
            "entry": "Operations Research, 56(4):992\u20131009, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Operations%20Research%205649921009%202008"
        },
        {
            "id": "14",
            "entry": "[14] D. R. Hunter and K. Lange. A tutorial on MM algorithms. The American Statistician, 58(1):30\u2013",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hunter%2C%20D.R.%20Lange%2C%20K.%20A%20tutorial%20on%20MM%20algorithms",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hunter%2C%20D.R.%20Lange%2C%20K.%20A%20tutorial%20on%20MM%20algorithms"
        },
        {
            "id": "15",
            "entry": "[15] S. Jegelka, F. Bach, and S. Sra. Reflection methods for user-friendly submodular optimization. In Advances in Neural Information Processing Systems (NIPS), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jegelka%2C%20S.%20Bach%2C%20F.%20Sra%2C%20S.%20Reflection%20methods%20for%20user-friendly%20submodular%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jegelka%2C%20S.%20Bach%2C%20F.%20Sra%2C%20S.%20Reflection%20methods%20for%20user-friendly%20submodular%20optimization%202013"
        },
        {
            "id": "16",
            "entry": "[16] S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems (NIPS), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20S.M.%20Kanade%2C%20V.%20Shamir%2C%20O.%20Kalai%2C%20A.%20Efficient%20learning%20of%20generalized%20linear%20and%20single%20index%20models%20with%20isotonic%20regression%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20S.M.%20Kanade%2C%20V.%20Shamir%2C%20O.%20Kalai%2C%20A.%20Efficient%20learning%20of%20generalized%20linear%20and%20single%20index%20models%20with%20isotonic%20regression%202011"
        },
        {
            "id": "17",
            "entry": "[17] S. Karlin and Y. Rinott. Classes of orderings of measures and related correlation inequalities. i. multivariate totally positive distributions. Journal of Multivariate Analysis, 10(4):467\u2013498, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karlin%2C%20S.%20Rinott%2C%20Y.%20Classes%20of%20orderings%20of%20measures%20and%20related%20correlation%20inequalities.%20i.%20multivariate%20totally%20positive%20distributions%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karlin%2C%20S.%20Rinott%2C%20Y.%20Classes%20of%20orderings%20of%20measures%20and%20related%20correlation%20inequalities.%20i.%20multivariate%20totally%20positive%20distributions%201980"
        },
        {
            "id": "18",
            "entry": "[18] S. Kim and M. Kojima. Exact solutions of some nonconvex quadratic optimization problems via SDP and SOCP relaxations. Comp. Optimization and Applications, 26(2):143\u2013154, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.%20Kojima%2C%20M.%20Exact%20solutions%20of%20some%20nonconvex%20quadratic%20optimization%20problems%20via%20SDP%20and%20SOCP%20relaxations%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20S.%20Kojima%2C%20M.%20Exact%20solutions%20of%20some%20nonconvex%20quadratic%20optimization%20problems%20via%20SDP%20and%20SOCP%20relaxations%202003"
        },
        {
            "id": "19",
            "entry": "[19] E. L. Lehmann. Ordered families of distributions. The Annals of Mathematical Statistics, 26(3):399\u2013419, 1955.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehmann%2C%20E.L.%20Ordered%20families%20of%20distributions%201955",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehmann%2C%20E.L.%20Ordered%20families%20of%20distributions%201955"
        },
        {
            "id": "20",
            "entry": "[20] H. Levy. Stochastic dominance and expected utility: survey and analysis. Management science, 38(4):555\u2013593, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20H.%20Stochastic%20dominance%20and%20expected%20utility%3A%20survey%20and%20analysis%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20H.%20Stochastic%20dominance%20and%20expected%20utility%3A%20survey%20and%20analysis%201992"
        },
        {
            "id": "21",
            "entry": "[21] G. G. Lorentz. An inequality for rearrangements. Am. Math. Monthly, 60(3):176\u2013179, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lorentz%2C%20G.G.%20An%20inequality%20for%20rearrangements%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lorentz%2C%20G.G.%20An%20inequality%20for%20rearrangements%201953"
        },
        {
            "id": "22",
            "entry": "[22] R. Luss and S. Rosset. Generalized isotonic regression. Journal of Computational and Graphical Statistics, 23(1):192\u2013210, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luss%2C%20R.%20Rosset%2C%20S.%20Generalized%20isotonic%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luss%2C%20R.%20Rosset%2C%20S.%20Generalized%20isotonic%20regression%202014"
        },
        {
            "id": "23",
            "entry": "[23] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%3A%20a%20basic%20course%202004"
        },
        {
            "id": "24",
            "entry": "[24] W. Rudin. Real and complex analysis. McGraw-Hill, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudin%2C%20W.%20Real%20and%20complex%20analysis%201986"
        },
        {
            "id": "25",
            "entry": "[25] D. D. Sleator and R. E. Tarjan. A data structure for dynamic trees. Journal of Computer and System Sciences, 26(3):362\u2013391, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sleator%2C%20D.D.%20Tarjan%2C%20R.E.%20A%20data%20structure%20for%20dynamic%20trees%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sleator%2C%20D.D.%20Tarjan%2C%20R.E.%20A%20data%20structure%20for%20dynamic%20trees%201983"
        },
        {
            "id": "26",
            "entry": "[26] J. Spouge, H. Wan, and W. J. Wilbur. Least squares isotonic regression in two dimensions. Journal of Optimization Theory and Applications, 117(3):585\u2013605, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spouge%2C%20J.%20Wan%2C%20H.%20Wilbur%2C%20W.J.%20Least%20squares%20isotonic%20regression%20in%20two%20dimensions%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spouge%2C%20J.%20Wan%2C%20H.%20Wilbur%2C%20W.J.%20Least%20squares%20isotonic%20regression%20in%20two%20dimensions%202003"
        },
        {
            "id": "27",
            "entry": "[27] Q. F. Stout. Isotonic regression via partitioning. Algorithmica, 66(1):93\u2013112, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stout%2C%20Q.F.%20Isotonic%20regression%20via%20partitioning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stout%2C%20Q.F.%20Isotonic%20regression%20via%20partitioning%202013"
        },
        {
            "id": "28",
            "entry": "[28] R. Tarjan, J. Ward, B. Zhang, Y. Zhou, and J. Mao. Balancing applied to maximum network flow problems. In European Symposium on Algorithms, pages 612\u2013623.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tarjan%2C%20R.%20Ward%2C%20J.%20Zhang%2C%20B.%20Zhou%2C%20Y.%20Balancing%20applied%20to%20maximum%20network%20flow%20problems",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tarjan%2C%20R.%20Ward%2C%20J.%20Zhang%2C%20B.%20Zhou%2C%20Y.%20Balancing%20applied%20to%20maximum%20network%20flow%20problems"
        },
        {
            "id": "29",
            "entry": "[29] D. M. Topkis. Minimizing a submodular function on a lattice. Operations Research, 26(2):305\u2013 321, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Topkis%2C%20D.M.%20Minimizing%20a%20submodular%20function%20on%20a%20lattice%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Topkis%2C%20D.M.%20Minimizing%20a%20submodular%20function%20on%20a%20lattice%201978"
        },
        {
            "id": "30",
            "entry": "[30] Y.-L. Yu and E. P. Xing. Exact algorithms for isotonic regression and related. In Journal of Physics: Conference Series, volume 699. IOP Publishing, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Y.-L.%20Xing%2C%20E.P.%20Exact%20algorithms%20for%20isotonic%20regression%20and%20related%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Y.-L.%20Xing%2C%20E.P.%20Exact%20algorithms%20for%20isotonic%20regression%20and%20related%202016"
        }
    ]
}
