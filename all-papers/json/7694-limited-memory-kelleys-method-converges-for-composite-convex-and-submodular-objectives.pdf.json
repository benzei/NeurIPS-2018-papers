{
    "filename": "7694-limited-memory-kelleys-method-converges-for-composite-convex-and-submodular-objectives.pdf",
    "metadata": {
        "title": "Limited Memory Kelley's Method Converges for Composite Convex and Submodular Objectives",
        "author": "Song Zhou, Swati Gupta, Madeleine Udell",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7694-limited-memory-kelleys-method-converges-for-composite-convex-and-submodular-objectives.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The original simplicial method (OSM), a variant of the classic Kelley\u2019s cutting plane method, has been shown to converge to the minimizer of a composite convex and submodular objective, though no rate of convergence for this method was known. Moreover, OSM is required to solve subproblems in each iteration whose size grows linearly in the number of iterations. We propose a limited memory version of Kelley\u2019s method (L-KM) and of OSM that requires limited memory (at most n + 1 constraints for an n-dimensional problem) independent of the iteration. We prove convergence for L-KM when the convex part of the objective g is strongly convex and show it converges linearly when g is also smooth. Our analysis relies on duality between minimization of the composite convex and submodular objective and minimization of a convex function over the submodular base polytope. We introduce a limited memory version, L-FCFW, of the Fully-Corrective FrankWolfe (FCFW) method with approximate correction, to solve the dual problem. We show that L-FCFW and L-KM are dual algorithms that produce the same sequence of iterates; hence both converge linearly (when g is smooth and strongly convex) and with limited memory. We propose L-KM to minimize composite convex and submodular objectives; however, our results on L-FCFW hold for general polytopes and may be of independent interest."
    },
    "keywords": [
        {
            "term": "submodular function",
            "url": "https://en.wikipedia.org/wiki/submodular_function"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        }
    ],
    "highlights": [
        "Consider a ground set V of n elements on which the submodular function F : 2V ! R is defined",
        "We propose a variant of Kelley\u2019s method, LIMITED MEMORY KELLEY\u2019S METHOD (L-KM), to minimize the composite convex and submodular objective minimize g(x) + f (x), (P )",
        "We show L-Fully Corrective Frank-Wolfe is a limited memory version of the FULLY-CORRECTIVE FRANK-WOLFE (FCFW) method with approximate correction [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and converges linearly to a solution of (D)",
        "We show that LIMITED MEMORY KELLEY\u2019S METHOD and L-Fully Corrective Frank-Wolfe are dual algorithms in the sense that both algorithms produce the same sequence of primal iterates and lower and upper bounds on the objective",
        "We present our novel limited memory adaptation LIMITED MEMORY KELLEY\u2019S METHOD of the Original Simplicial Method (OSM)",
        "Limited Memory Kelley\u2019s Method: To address these two challenges \u2014 memory requirements and unknown convergence rate \u2014 we introduce and analyze a novel limited memory version LIMITED MEMORY KELLEY\u2019S METHOD of Original Simplicial Method which ensures that the number of cutting planes maintained by the algorithm at any iteration is bounded by n + 1"
    ],
    "key_statements": [
        "Consider a ground set V of n elements on which the submodular function F : 2V ! R is defined",
        "We propose a variant of Kelley\u2019s method, LIMITED MEMORY KELLEY\u2019S METHOD (L-KM), to minimize the composite convex and submodular objective minimize g(x) + f (x), (P )",
        "We show L-Fully Corrective Frank-Wolfe is a limited memory version of the FULLY-CORRECTIVE FRANK-WOLFE (FCFW) method with approximate correction [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and converges linearly to a solution of (D)",
        "We show that LIMITED MEMORY KELLEY\u2019S METHOD and L-Fully Corrective Frank-Wolfe are dual algorithms in the sense that both algorithms produce the same sequence of primal iterates and lower and upper bounds on the objective",
        "We present our novel limited memory adaptation LIMITED MEMORY KELLEY\u2019S METHOD of the Original Simplicial Method (OSM)",
        "Limited Memory Kelley\u2019s Method: To address these two challenges \u2014 memory requirements and unknown convergence rate \u2014 we introduce and analyze a novel limited memory version LIMITED MEMORY KELLEY\u2019S METHOD of Original Simplicial Method which ensures that the number of cutting planes maintained by the algorithm at any iteration is bounded by n + 1",
        "We have shown that LIMITED MEMORY KELLEY\u2019S METHOD solves a series of limited memory convex subproblems with no more than n + 1 linear constraints, and it produces strictly increasing lower bounds that converge to the optimal value",
        "We show bounds on the memory requirements and convergence rate, and demonstrate compelling performance in practice"
    ],
    "summary": [
        "Consider a ground set V of n elements on which the submodular function F : 2V ! R is defined.",
        "Kelley\u2019s method and its variants are a natural fit for problem involving a piecewise linear function, such as composite convex and submodular objectives.",
        "This paper defines a new limited memory version of Kelley\u2019s method adapted to composite convex and submodular objectives, and establishes the first convergence rate for such a method, solving the open problem proposed in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "This paper introduces L-KM, a variant of OSM that uses no more than n + 1 linear constraints in each approximation f(i) and provably converges when g is strongly convex.",
        "Section 3 describes L-KM, our proposed limited memory version of OSM, and shows that L-KM converges and solves a problem over Rn using subproblems with at most n + 1 constraints.",
        "Original Simplicial Method: To solve the primal problem (P ), it is natural to approximate the piecewise linear Lov\u00e1sz extension f with cutting planes derived from the function values and subgradients of the function at previous iterations, which results in piecewise linear lower approximations of f .",
        "Limited Memory Kelley\u2019s Method: To address these two challenges \u2014 memory requirements and unknown convergence rate \u2014 we introduce and analyze a novel limited memory version L-KM of OSM which ensures that the number of cutting planes maintained by the algorithm at any iteration is bounded by n + 1.",
        "The convex subproblem has the same solution and optimal value over the new active set A(i) as over the memory V(i 1): (i)",
        "We have shown that L-KM solves a series of limited memory convex subproblems with no more than n + 1 linear constraints, and it produces strictly increasing lower bounds that converge to the optimal value.",
        "Lacoste-Julien and Jaggi [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] show that FCFW converges linearly to an \u270f-suboptimal solution when g is smooth and strongly convex so long as the active set B(i) and iterate x(i) satisfy three conditions they call approximate correction(\u270f): 1.",
        "When g is smooth and strongly convex, OSM and vanilla FCFW are dual algorithms in the same sense when we choose B(i) = V(i 1).",
        "As shown in Figures (3)(b) and (c), L-FCFW and FCFW return better approximate solutions than Frank-Wolfe with away steps under the same optimality gap tolerance.",
        "Conclusion This paper defines a new limited memory version of Kelley\u2019s method adapted to composite convex and submodular objectives, and establishes the first convergence rate for such a method, solving the open problem proposed in [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "We show bounds on the memory requirements and convergence rate, and demonstrate compelling performance in practice"
    ],
    "headline": "We propose a limited memory version of Kelley\u2019s method  and of Original Simplicial Method that requires limited memory  independent of the iteration",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J. Audibert, S. Bubeck, and G. Lugosi. Regret in online combinatorial optimization. Mathematics of Operations Research, 39(1):31\u201345, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.%20Bubeck%2C%20S.%20Lugosi%2C%20G.%20Regret%20in%20online%20combinatorial%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.%20Bubeck%2C%20S.%20Lugosi%2C%20G.%20Regret%20in%20online%20combinatorial%20optimization%202013"
        },
        {
            "id": "2",
            "entry": "[2] Francis Bach. Duality between subgradient and conditional gradient methods. SIAM Journal on Optimization, 25(1):115\u2013129, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Duality%20between%20subgradient%20and%20conditional%20gradient%20methods%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Duality%20between%20subgradient%20and%20conditional%20gradient%20methods%202015"
        },
        {
            "id": "3",
            "entry": "[3] Francis Bach et al. Learning with submodular functions: A convex optimization perspective. Foundations and Trends R in Machine Learning, 6(2-3):145\u2013373, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Learning%20with%20submodular%20functions%3A%20A%20convex%20optimization%20perspective%202013"
        },
        {
            "id": "4",
            "entry": "[4] Francis R Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Information Processing Systems, pages 118\u2013126, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20R.%20Structured%20sparsity-inducing%20norms%20through%20submodular%20functions%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20R.%20Structured%20sparsity-inducing%20norms%20through%20submodular%20functions%202010"
        },
        {
            "id": "5",
            "entry": "[5] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Vandenberghe%2C%20L.%20Convex%20optimization%202009"
        },
        {
            "id": "6",
            "entry": "[6] E Ward Cheney and Allen A Goldstein. Newton\u2019s method for convex programming and Tchebycheff approximation. Numerische Mathematik, 1(1):253\u2013268, 1959.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheney%2C%20E.Ward%20Goldstein%2C%20Allen%20A.%20Newton%E2%80%99s%20method%20for%20convex%20programming%20and%20Tchebycheff%20approximation%201959",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheney%2C%20E.Ward%20Goldstein%2C%20Allen%20A.%20Newton%E2%80%99s%20method%20for%20convex%20programming%20and%20Tchebycheff%20approximation%201959"
        },
        {
            "id": "7",
            "entry": "[7] Josip Djolonga and Andreas Krause. From MAP to marginals: Variational inference in bayesian submodular models. In Advances in Neural Information Processing Systems, pages 244\u2013252, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Djolonga%2C%20Josip%20Krause%2C%20Andreas%20From%20MAP%20to%20marginals%3A%20Variational%20inference%20in%20bayesian%20submodular%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Djolonga%2C%20Josip%20Krause%2C%20Andreas%20From%20MAP%20to%20marginals%3A%20Variational%20inference%20in%20bayesian%20submodular%20models%202014"
        },
        {
            "id": "8",
            "entry": "[8] Yoel Drori and Marc Teboulle. An optimal variant of Kelley\u2019s cutting-plane method. Mathematical Programming, 160(1-2):321\u2013351, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drori%2C%20Yoel%20Teboulle%2C%20Marc%20An%20optimal%20variant%20of%20Kelley%E2%80%99s%20cutting-plane%20method%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drori%2C%20Yoel%20Teboulle%2C%20Marc%20An%20optimal%20variant%20of%20Kelley%E2%80%99s%20cutting-plane%20method%202016"
        },
        {
            "id": "9",
            "entry": "[9] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3(1-2):95\u2013110, 1956.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming%201956",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frank%2C%20Marguerite%20Wolfe%2C%20Philip%20An%20algorithm%20for%20quadratic%20programming%201956"
        },
        {
            "id": "10",
            "entry": "[10] D. Hearn, S. Lawphongpanich, and J. Ventura. Restricted simplicial decomposition: Computation and extensions. Mathematical Programming Study, 31:99\u2013118, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hearn%2C%20D.%20Lawphongpanich%2C%20S.%20Ventura%2C%20J.%20Restricted%20simplicial%20decomposition%3A%20Computation%20and%20extensions%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hearn%2C%20D.%20Lawphongpanich%2C%20S.%20Ventura%2C%20J.%20Restricted%20simplicial%20decomposition%3A%20Computation%20and%20extensions%201987"
        },
        {
            "id": "11",
            "entry": "[11] James E Kelley, Jr. The cutting-plane method for solving convex programs. Journal of the Society for Industrial and Applied Mathematics, 8(4):703\u2013712, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kelley%2C%20Jr%2C%20James%20E.%20The%20cutting-plane%20method%20for%20solving%20convex%20programs%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kelley%2C%20Jr%2C%20James%20E.%20The%20cutting-plane%20method%20for%20solving%20convex%20programs%201960"
        },
        {
            "id": "12",
            "entry": "[12] Krzysztof C Kiwiel. Proximal level bundle methods for convex nondifferentiable optimization, saddle-point problems and variational inequalities. Mathematical Programming, 69(1-3):89\u2013109, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kiwiel%2C%20Krzysztof%20C.%20Proximal%20level%20bundle%20methods%20for%20convex%20nondifferentiable%20optimization%2C%20saddle-point%20problems%20and%20variational%20inequalities%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kiwiel%2C%20Krzysztof%20C.%20Proximal%20level%20bundle%20methods%20for%20convex%20nondifferentiable%20optimization%2C%20saddle-point%20problems%20and%20variational%20inequalities%201995"
        },
        {
            "id": "13",
            "entry": "[13] W. M. Koolen, M. K. Warmuth, and J. Kivinen. Hedging structured concepts. COLT, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koolen%2C%20W.M.%20Warmuth%2C%20M.K.%20Kivinen%2C%20J.%20Hedging%20structured%20concepts%202010"
        },
        {
            "id": "14",
            "entry": "[14] Walid Krichene, Syrine Krichene, and Alexandre Bayen. Convergence of mirror descent dynamics in the routing game. In European Control Conference (ECC), pages 569\u2013574. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krichene%2C%20Walid%20Krichene%2C%20Syrine%20Bayen%2C%20Alexandre%20Convergence%20of%20mirror%20descent%20dynamics%20in%20the%20routing%20game%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krichene%2C%20Walid%20Krichene%2C%20Syrine%20Bayen%2C%20Alexandre%20Convergence%20of%20mirror%20descent%20dynamics%20in%20the%20routing%20game%202015"
        },
        {
            "id": "15",
            "entry": "[15] Simon Lacoste-Julien and Martin Jaggi. On the global linear convergence of Frank-Wolfe optimization variants. In Advances in Neural Information Processing Systems, pages 496\u2013504, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste-Julien%2C%20Simon%20Jaggi%2C%20Martin%20On%20the%20global%20linear%20convergence%20of%20Frank-Wolfe%20optimization%20variants%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lacoste-Julien%2C%20Simon%20Jaggi%2C%20Martin%20On%20the%20global%20linear%20convergence%20of%20Frank-Wolfe%20optimization%20variants%202015"
        },
        {
            "id": "16",
            "entry": "[16] Claude Lemar\u00e9chal, Arkadii Nemirovskii, and Yurii Nesterov. New variants of bundle methods. Mathematical programming, 69(1-3):111\u2013147, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lemar%C3%A9chal%2C%20Claude%20Nemirovskii%2C%20Arkadii%20Nesterov%2C%20Yurii%20New%20variants%20of%20bundle%20methods%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lemar%C3%A9chal%2C%20Claude%20Nemirovskii%2C%20Arkadii%20Nesterov%2C%20Yurii%20New%20variants%20of%20bundle%20methods%201995"
        },
        {
            "id": "17",
            "entry": "[17] L. Lov\u00e1sz. Submodular functions and convexity. Mathematical Programming: The State of the Art, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lov%C3%A1sz%2C%20L.%20Submodular%20functions%20and%20convexity%201983"
        },
        {
            "id": "18",
            "entry": "[18] Marko M\u00e4kel\u00e4. Survey of bundle methods for nonsmooth optimization. Optimization methods and software, 17(1):1\u201329, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A4kel%C3%A4%2C%20Marko%20Survey%20of%20bundle%20methods%20for%20nonsmooth%20optimization%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%A4kel%C3%A4%2C%20Marko%20Survey%20of%20bundle%20methods%20for%20nonsmooth%20optimization%202002"
        },
        {
            "id": "19",
            "entry": "[19] Kiyohito Nagano, Yoshinobu Kawahara, and Kazuyuki Aihara. Size-constrained submodular minimization through minimum norm base. In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 977\u2013984, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nagano%2C%20Kiyohito%20Kawahara%2C%20Yoshinobu%20Aihara%2C%20Kazuyuki%20Size-constrained%20submodular%20minimization%20through%20minimum%20norm%20base%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nagano%2C%20Kiyohito%20Kawahara%2C%20Yoshinobu%20Aihara%2C%20Kazuyuki%20Size-constrained%20submodular%20minimization%20through%20minimum%20norm%20base%202011"
        },
        {
            "id": "20",
            "entry": "[20] A. S. Nemirovski and D. B. Yudin. Problem complexity and method efficiency in optimization. WileyInterscience, New York, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20A.S.%20Yudin%2C%20D.B.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization.%20WileyInterscience%201983"
        },
        {
            "id": "21",
            "entry": "[21] Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Journal of Applied and Computational Mathematics, 15(1):3\u201343, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ryu%2C%20Ernest%20K.%20Boyd%2C%20Stephen%20Primer%20on%20monotone%20operator%20methods%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ryu%2C%20Ernest%20K.%20Boyd%2C%20Stephen%20Primer%20on%20monotone%20operator%20methods%202016"
        },
        {
            "id": "22",
            "entry": "[22] Madeleine Udell and Stephen Boyd. Bounding duality gap for separable problems with linear constraints. Computational Optimization and Applications, 64(2):355\u2013378, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Udell%2C%20Madeleine%20Boyd%2C%20Stephen%20Bounding%20duality%20gap%20for%20separable%20problems%20with%20linear%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Udell%2C%20Madeleine%20Boyd%2C%20Stephen%20Bounding%20duality%20gap%20for%20separable%20problems%20with%20linear%20constraints%202016"
        },
        {
            "id": "23",
            "entry": "[23] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge University Press, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20Roman%20High-dimensional%20probability%3A%20An%20introduction%20with%20applications%20in%20data%20science%2C%20volume%2047%202018"
        },
        {
            "id": "24",
            "entry": "[24] Balder Von Hohenbalken. Simplicial decomposition in nonlinear programming algorithms. Mathematical Programming, 13(1):49\u201368, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hohenbalken%2C%20Balder%20Von%20Simplicial%20decomposition%20in%20nonlinear%20programming%20algorithms%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hohenbalken%2C%20Balder%20Von%20Simplicial%20decomposition%20in%20nonlinear%20programming%20algorithms%201977"
        },
        {
            "id": "25",
            "entry": "[25] M. K. Warmuth and D. Kuzmin. Randomized PCA algorithms with regret bounds that are logarithmic in the dimension. In Advances in Neural Information Processing Systems, pages 1481\u20131488, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Warmuth%2C%20M.K.%20Kuzmin%2C%20D.%20Randomized%20PCA%20algorithms%20with%20regret%20bounds%20that%20are%20logarithmic%20in%20the%20dimension%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Warmuth%2C%20M.K.%20Kuzmin%2C%20D.%20Randomized%20PCA%20algorithms%20with%20regret%20bounds%20that%20are%20logarithmic%20in%20the%20dimension%202006"
        },
        {
            "id": "26",
            "entry": "[26] S. Yasutake, K. Hatano, S. Kijima, E. Takimoto, and M. Takeda. Online linear optimization over permutations. In Algorithms and Computation, pages 534\u2013543. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yasutake%2C%20S.%20Hatano%2C%20K.%20Kijima%2C%20S.%20Takimoto%2C%20E.%20Online%20linear%20optimization%20over%20permutations",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yasutake%2C%20S.%20Hatano%2C%20K.%20Kijima%2C%20S.%20Takimoto%2C%20E.%20Online%20linear%20optimization%20over%20permutations"
        }
    ]
}
