{
    "filename": "8134-pac-bayes-bounds-for-stable-algorithms-with-instance-dependent-priors.pdf",
    "metadata": {
        "title": "PAC-Bayes bounds for stable algorithms with instance-dependent priors",
        "author": "Omar Rivasplata, Csaba Szepesvari, John S. Shawe-Taylor, Emilio Parrado-Hernandez, Shiliang Sun",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8134-pac-bayes-bounds-for-stable-algorithms-with-instance-dependent-priors.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "PAC-Bayes bounds have been proposed to get risk estimates based on a training sample. In this paper the PAC-Bayes approach is combined with stability of the hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting is used with a Gaussian prior centered at the expected output. Thus a novelty of our paper is using priors defined in terms of the data-generating distribution. Our main result estimates the risk of the randomized algorithm in terms of the hypothesis stability coefficients. We also provide a new bound for the SVM classifier, which is compared to other known bounds experimentally. Ours appears to be the first uniform hypothesis stability-based bound that evaluates to non-trivial values."
    },
    "keywords": [
        {
            "term": "Support Vector Machine",
            "url": "https://en.wikipedia.org/wiki/Support_Vector_Machine"
        },
        {
            "term": "Alberta machine intelligence institute",
            "url": "https://en.wikipedia.org/wiki/Alberta_Machine_Intelligence_Institute"
        },
        {
            "term": "hilbert space",
            "url": "https://en.wikipedia.org/wiki/hilbert_space"
        },
        {
            "term": "Defence Science and Technology Laboratory",
            "url": "https://en.wikipedia.org/wiki/Defence_Science_and_Technology_Laboratory"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        }
    ],
    "highlights": [
        "This paper combines two directions of research: stability of learning algorithms, and PAC-Bayes bounds for algorithms that randomize with a data-dependent distribution",
        "We provide an analysis leading to a PAC-Bayes bound for randomized classifiers under Gaussian randomization",
        "Our Corollary 3, with Q = N (Wn, 2I), a Gaussian centered at Wn = Support Vector Machine (Sn) with randomization variance 2, gives the following risk bound which holds with probability 1 2 :\n1 \u25c62 1 n+1",
        "We have developed a PAC-Bayes bound for randomized classifiers",
        "We proceeded by investigating the stability of the hypothesis learned by a Hilbert space valued algorithm",
        "Our work can be viewed as contributing to a line of research that aims to develop \u2018self-bounding algorithms\u2019 (<a class=\"ref-link\" id=\"cFreund_1998_a\" href=\"#rFreund_1998_a\">Freund [1998</a>], <a class=\"ref-link\" id=\"cLangford_2003_a\" href=\"#rLangford_2003_a\">Langford and Blum [2003</a>]) in the sense that besides producing a predictor the algorithm creates a performance certificate based on the available data"
    ],
    "key_statements": [
        "This paper combines two directions of research: stability of learning algorithms, and PAC-Bayes bounds for algorithms that randomize with a data-dependent distribution",
        "We provide an analysis leading to a PAC-Bayes bound for randomized classifiers under Gaussian randomization",
        "Our Corollary 3, with Q = N (Wn, 2I), a Gaussian centered at Wn = Support Vector Machine (Sn) with randomization variance 2, gives the following risk bound which holds with probability 1 2 :\n1 \u25c62 1 n+1",
        "The PAC-Bayes approach applied to Support Vector Machine says that instead of committing to the weight vector Wn = Support Vector Machine(Sn) we will randomize by choosing a fresh W 2 H according to some probability distribution on H for each prediction",
        "As the qualitative results were insensitive to the split, results for a single \u201crandom\u201d split are shown only",
        "Upon investigating this we found that this is because the hinge loss takes much larger values than the training error rate unless C takes large values (cf",
        "Fig. 1 show the difference between the P@O bound and the test error of the underlying respective randomized classifiers as a function of (C, ) while Fig. 2 shows the difference between the P@EW bound and the test error of the underlying randomized classifier. (Figs. 7 and 9 in the appendix show the test errors for these classifiers, while Figs. 6 and 8 shows the bound.) The meticulous reader may worry about that it appears that on the smaller dataset, PIM, the difference shown for P@O is sometimes negative",
        "From the figures the most obvious difference between the bounds is that the P@EW bound is sensitive to the value of C and it becomes loose for larger values of C",
        "In the appendix we show the advantage of the P@EW bound over the P@O bound on Fig. 10",
        "Two comments are in order in connection to this: (i) We find it remarkable that a stability-based bound can be competitive with the P@O bound, which is known as one of the best bounds available. While comparing bounds is interesting for learning about their qualities, the bounds can be used together",
        "We have developed a PAC-Bayes bound for randomized classifiers",
        "We proceeded by investigating the stability of the hypothesis learned by a Hilbert space valued algorithm",
        "Our work can be viewed as contributing to a line of research that aims to develop \u2018self-bounding algorithms\u2019 (<a class=\"ref-link\" id=\"cFreund_1998_a\" href=\"#rFreund_1998_a\">Freund [1998</a>], <a class=\"ref-link\" id=\"cLangford_2003_a\" href=\"#rLangford_2003_a\">Langford and Blum [2003</a>]) in the sense that besides producing a predictor the algorithm creates a performance certificate based on the available data"
    ],
    "summary": [
        "This paper combines two directions of research: stability of learning algorithms, and PAC-Bayes bounds for algorithms that randomize with a data-dependent distribution.",
        "Our work derives PAC-Bayes bounds for hypothesis stable Hilbert space valued algorithms.",
        "In this paper stability is measured by the sensitivity coefficients of the hypothesis learned by a Hilbert space valued algorithm.",
        "2.1 Main theorem: a PAC-Bayes bound for stable algorithms with Gaussian randomization",
        "2.2 Application: a PAC-Bayes bound for SVM with Gaussian randomization",
        "With this identification we can regard an SVM as a Hilbert space7 valued mapping that based on a training sample Sn learns a weight vector Wn = SVM(Sn) 2 H.",
        "8See Section 5 about the interpretation of Gaussian randomization for a Hilbert space valued algorithm.",
        "Our Corollary 3, with Q = N (Wn, 2I), a Gaussian centered at Wn = SVM (Sn) with randomization variance 2, gives the following risk bound which holds with probability 1 2 :",
        "The PAC-Bayes bound Theorem 4 again with Q = N (Wn, 2I), gives the following risk bound which holds with probability 1 : 8 2, KL+(R01(Q, Pn)kR01(Q, P ))",
        "The proof of our Theorem 2 combines stability of the learned hypothesis and a well-known PAC-Bayes bound, quoted for reference: Theorem 4.",
        "The proof of our concentration inequality is based on an extension of the bounded differences theorem of McDiarmid to vector-valued functions discussed .",
        "The PAC-Bayes approach applied to SVMs says that instead of committing to the weight vector Wn = SVM(Sn) we will randomize by choosing a fresh W 2 H according to some probability distribution on H for each prediction.",
        "Model parameters Following the procedure suggested in Section 2.3.1 of <a class=\"ref-link\" id=\"cChapelle_2005_a\" href=\"#rChapelle_2005_a\">Chapelle and Zien [2005</a>], we set up a geometric 7 \u21e5 7 grid over the (C, )-parameter space where C ranges between 2 8C0 and 22C0 and ranges between 2 3 0 and 23 0, where 0 is the median of the Euclidean distance between pairs of data points of the training set, and given 0, C0 is obtained as the reciprocal value of the empirical variance of data in feature space underlying the RBF kernel with width 0.",
        "We proceeded by investigating the stability of the hypothesis learned by a Hilbert space valued algorithm.",
        "A special case being SVMs. We applied our main theorem to SVMs, leading to our P@EW bound, and we compared it to other stability-based bounds and to a previously known PAC-Bayes bound."
    ],
    "headline": "We provide a new bound for the Support Vector Machine classifier, which is compared to other known bounds experimentally",
    "reference_links": [
        {
            "id": "Abou-Moustafa_2017_a",
            "entry": "Karim Abou-Moustafa and Csaba Szepesvari. An a priori exponential tail bound for k-folds crossvalidation. ArXiv e-prints, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abou-Moustafa%2C%20Karim%20Szepesvari%2C%20Csaba%20An%20a%20priori%20exponential%20tail%20bound%20for%20k-folds%20crossvalidation.%20ArXiv%20e-prints%202017"
        },
        {
            "id": "Bogachev_1998_a",
            "entry": "Vladimir I. Bogachev. Gaussian Measures. American Mathematical Society, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bogachev%2C%20Vladimir%20I.%20Gaussian%20Measures%201998"
        },
        {
            "id": "Bousquet_2002_a",
            "entry": "Olivier Bousquet and Andre Elisseeff. Stability and generalisation. Journal of Machine Learning Research, 2:499\u2013526, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andre%20Stability%20and%20generalisation%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andre%20Stability%20and%20generalisation%202002"
        },
        {
            "id": "Catoni_2007_a",
            "entry": "Olivier Catoni. PAC-Bayesian supervised classification: the thermodynamics of statistical learning. Technical report, Institute of Mathematical Statistics, Beachwood, Ohio, USA, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Catoni%2C%20Olivier%20PAC-Bayesian%20supervised%20classification%3A%20the%20thermodynamics%20of%20statistical%20learning%202007"
        },
        {
            "id": "Celisse_2016_a",
            "entry": "Alain Celisse and Benjamin Guedj. Stability revisited: new generalisation bounds for the leave-oneout. arXiv preprint arXiv:1608.06412, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06412"
        },
        {
            "id": "Chapelle_2005_a",
            "entry": "Olivier Chapelle and Alexander Zien. Semi-supervised classification by low density separation. In AISTATS, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20Olivier%20Zien%2C%20Alexander%20Semi-supervised%20classification%20by%20low%20density%20separation%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20Olivier%20Zien%2C%20Alexander%20Semi-supervised%20classification%20by%20low%20density%20separation%202005"
        },
        {
            "id": "Dziugaite_2017_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In UAI, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20Gintare%20Karolina%20Roy%2C%20Daniel%20M.%20Computing%20nonvacuous%20generalization%20bounds%20for%20deep%20%28stochastic%29%20neural%20networks%20with%20many%20more%20parameters%20than%20training%20data%202017"
        },
        {
            "id": "Dziugaite_2018_a",
            "entry": "Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential privacy. arXiv preprint arXiv:1802.09583, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.09583"
        },
        {
            "id": "Freund_1998_a",
            "entry": "Yoav Freund. Self bounding learning algorithms. In Proc. of the 11th annual conference on Computational Learning Theory, pages 247\u2013258. ACM, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Yoav%20Self%20bounding%20learning%20algorithms%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Yoav%20Self%20bounding%20learning%20algorithms%201998"
        },
        {
            "id": "Germain_et+al_2009_a",
            "entry": "Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. PAC-Bayesian learning of linear classifiers. In Proc. of the 26th International Conference on Machine Learning, pages 353\u2013360. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Pascal%20Lacasse%2C%20Alexandre%20Laviolette%2C%20Francois%20Marchand%2C%20Mario%20PAC-Bayesian%20learning%20of%20linear%20classifiers%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Pascal%20Lacasse%2C%20Alexandre%20Laviolette%2C%20Francois%20Marchand%2C%20Mario%20PAC-Bayesian%20learning%20of%20linear%20classifiers%202009"
        },
        {
            "id": "Germain_et+al_2015_a",
            "entry": "Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Jean-Francis Roy. Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning algorithm. Journal of Machine Learning Research, 16:787\u2013860, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20Pascal%20Lacasse%2C%20Alexandre%20Laviolette%2C%20Francois%20Marchand%2C%20Mario%20Risk%20bounds%20for%20the%20majority%20vote%3A%20From%20a%20PAC-Bayesian%20analysis%20to%20a%20learning%20algorithm%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20Pascal%20Lacasse%2C%20Alexandre%20Laviolette%2C%20Francois%20Marchand%2C%20Mario%20Risk%20bounds%20for%20the%20majority%20vote%3A%20From%20a%20PAC-Bayesian%20analysis%20to%20a%20learning%20algorithm%202015"
        },
        {
            "id": "Kontorovich_2014_a",
            "entry": "Aryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. In Proc. of the 31st International Conference on Machine Learning, pages 28\u201336, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kontorovich%2C%20Aryeh%20Concentration%20in%20unbounded%20metric%20spaces%20and%20algorithmic%20stability%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kontorovich%2C%20Aryeh%20Concentration%20in%20unbounded%20metric%20spaces%20and%20algorithmic%20stability%202014"
        },
        {
            "id": "Langford_2005_a",
            "entry": "John Langford. Tutorial on practical prediction theory for classification. Journal of Machine Learning Research, 6(Mar):273\u2013306, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Tutorial%20on%20practical%20prediction%20theory%20for%20classification%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Tutorial%20on%20practical%20prediction%20theory%20for%20classification%202005"
        },
        {
            "id": "Langford_2003_a",
            "entry": "John Langford and Avrim Blum. Microchoice bounds and self bounding learning algorithms. Machine Learning, 51(2):165\u2013179, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20John%20Blum%2C%20Avrim%20Microchoice%20bounds%20and%20self%20bounding%20learning%20algorithms%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20John%20Blum%2C%20Avrim%20Microchoice%20bounds%20and%20self%20bounding%20learning%20algorithms%202003"
        },
        {
            "id": "Liu_et+al_2017_a",
            "entry": "Tongliang Liu, Gabor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis complexity. In Proc. of the 34th International Conference on Machine Learning, pages 2159\u2013 2167, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Tongliang%20Lugosi%2C%20Gabor%20Neu%2C%20Gergely%20Tao%2C%20Dacheng%20Algorithmic%20stability%20and%20hypothesis%20complexity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Tongliang%20Lugosi%2C%20Gabor%20Neu%2C%20Gergely%20Tao%2C%20Dacheng%20Algorithmic%20stability%20and%20hypothesis%20complexity%202017"
        },
        {
            "id": "London_2017_a",
            "entry": "Ben London. A PAC-Bayesian analysis of randomized learning with application to stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 2931\u20132940, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=London%2C%20Ben%20A%20PAC-Bayesian%20analysis%20of%20randomized%20learning%20with%20application%20to%20stochastic%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=London%2C%20Ben%20A%20PAC-Bayesian%20analysis%20of%20randomized%20learning%20with%20application%20to%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "London_et+al_2013_a",
            "entry": "Ben London, Bert Huang, Ben Taskar, and Lise Getoor. Collective stability in structured prediction: Generalization from one example. In Proc. of the 30th International Conference on Machine Learning, pages 828\u2013836, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=London%2C%20Ben%20Huang%2C%20Bert%20Taskar%2C%20Ben%20Getoor%2C%20Lise%20Collective%20stability%20in%20structured%20prediction%3A%20Generalization%20from%20one%20example%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=London%2C%20Ben%20Huang%2C%20Bert%20Taskar%2C%20Ben%20Getoor%2C%20Lise%20Collective%20stability%20in%20structured%20prediction%3A%20Generalization%20from%20one%20example%202013"
        },
        {
            "id": "Lugosi_1994_a",
            "entry": "Gabor Lugosi and Miroslaw Pawlak. On the posterior-probability estimate of the error rate of nonparametric classification rules. IEEE Transactions on Information Theory, 40(2):475\u2013481, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lugosi%2C%20Gabor%20Pawlak%2C%20Miroslaw%20On%20the%20posterior-probability%20estimate%20of%20the%20error%20rate%20of%20nonparametric%20classification%20rules%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lugosi%2C%20Gabor%20Pawlak%2C%20Miroslaw%20On%20the%20posterior-probability%20estimate%20of%20the%20error%20rate%20of%20nonparametric%20classification%20rules%201994"
        },
        {
            "id": "Mcallester_1999_a",
            "entry": "David A McAllester. Some PAC-Bayesian theorems. Machine Learning, 37(3):355\u2013363, 1999a. David A McAllester. PAC-Bayesian model averaging. In Proc. of the 12th annual conference on",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20David%20A.%20Some%20PAC-Bayesian%20theorems%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20David%20A.%20Some%20PAC-Bayesian%20theorems%201999"
        },
        {
            "id": "C_1999_a",
            "entry": "Computational Learning Theory, pages 164\u2013170. ACM, 1999b. Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-Bayes bounds with data dependent priors. Journal of Machine Learning Research, 13:3507\u20133531, 2012. John C. Platt. Fast training of support vector machines using sequential minimal optimization. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C.%20Computational%20Learning%20Theory%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C.%20Computational%20Learning%20Theory%201999"
        },
        {
            "id": "Scholkopf_1999_a",
            "entry": "B. Scholkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods \u2013 Support Vector Learning, pages 185\u2013208. MIT Press, Cambridge MA, 1999. John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, Cambridge, UK, 2004. John Shawe-Taylor and Robert C Williamson. A PAC analysis of a Bayesian estimator. In Proc. of the 10th annual conference on Computational Learning Theory, pages 2\u20139. ACM, 1997. Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Advances%20in%20Kernel%20Methods%20%E2%80%93%20Support%20Vector%20Learning%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Advances%20in%20Kernel%20Methods%20%E2%80%93%20Support%20Vector%20Learning%201999"
        }
    ]
}
