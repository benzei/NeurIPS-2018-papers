{
    "filename": "7296-supervised-autoencoders-improving-generalization-performance-with-unsupervised-regularizers.pdf",
    "metadata": {
        "title": "Supervised autoencoders: Improving generalization performance with unsupervised regularizers",
        "author": "Lei Le, Andrew Patterson, Martha White",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7296-supervised-autoencoders-improving-generalization-performance-with-unsupervised-regularizers.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Generalization performance is a central goal in machine learning, with explicit generalization strategies needed when training over-parametrized models, like large neural networks. There is growing interest in using multiple, potentially auxiliary tasks, as one strategy towards this goal. In this work, we theoretically and empirically analyze one such model, called a supervised auto-encoder: a neural network that jointly predicts targets and inputs (reconstruction). We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error\u2014particularly as an improvement on simplistic regularization such as norms. We then demonstrate empirically that, across an array of architectures with a different number of hidden units and activation functions, the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can improve generalization."
    },
    "keywords": [
        {
            "term": "auto encoder",
            "url": "https://en.wikipedia.org/wiki/auto_encoder"
        },
        {
            "term": "linear function",
            "url": "https://en.wikipedia.org/wiki/linear_function"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Generalization is a central concept in machine learning: learning functions from a finite set of data, that can perform well on new data",
        "We provide a novel uniform stability result, showing that linear supervised auto-encoder\u2014which consists of the addition of reconstruction error to a linear neural network\u2014 provides uniform stability and so a bound on generalization error",
        "The overall conclusion is that supervised auto-encoder can benefit much more from model complexity given by kernel representations, than NNs",
        "We systematically investigated supervised auto-encoders (SAEs), as an approach to using unsupervised auxiliary tasks to improve generalization performance",
        "We showed theoretically that the addition of reconstruction error improves generalization performance, for linear supervised auto-encoder",
        "Across four different datasets, with a variety of architectures, that supervised auto-encoder never harms performance but in some cases can significantly improve performance, particularly when using kernels and under ReLu activations, for both shallow and deep architectures"
    ],
    "key_statements": [
        "Generalization is a central concept in machine learning: learning functions from a finite set of data, that can perform well on new data",
        "Understanding generalization performance is particularly critical for powerful function classes, such as neural networks",
        "We investigate an auxiliary-task model for which we can make generalization guarantees, called a supervised auto-encoder (SAE)",
        "A supervised auto-encoder is a neural network that predicts both inputs and outputs, and has been previously shown empirically to provide significant improvements when used in a semi-supervised setting [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]",
        "We provide a novel uniform stability result, showing that linear supervised auto-encoder\u2014which consists of the addition of reconstruction error to a linear neural network\u2014 provides uniform stability and so a bound on generalization error",
        "We show that linear supervised auto-encoders are uniformly stable, which means that there is a small difference between models learned for any subsample of data, which differ in only one instance",
        "The overall conclusion is that supervised auto-encoder can benefit much more from model complexity given by kernel representations, than NNs",
        "In Table 1, the most striking difference between supervised auto-encoder and NNs with kernels occurs for the Deterding dataset",
        "We show the performance of supervised auto-encoder with decreasing weight on the predictive loss, which increases the effect of the reconstruction error",
        "We systematically investigated supervised auto-encoders (SAEs), as an approach to using unsupervised auxiliary tasks to improve generalization performance",
        "We showed theoretically that the addition of reconstruction error improves generalization performance, for linear supervised auto-encoder",
        "Across four different datasets, with a variety of architectures, that supervised auto-encoder never harms performance but in some cases can significantly improve performance, particularly when using kernels and under ReLu activations, for both shallow and deep architectures"
    ],
    "summary": [
        "Generalization is a central concept in machine learning: learning functions from a finite set of data, that can perform well on new data.",
        "Multi-task learning [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] has been shown to improve generalization performance, from early work showing learning tasks jointly reduces the required number of samples [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and later work particularly focused on trace-norm regularization on the weights of a linear, single hidden-layer neural network for a set of tasks [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>\u2013<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>].",
        "We investigate an auxiliary-task model for which we can make generalization guarantees, called a supervised auto-encoder (SAE).",
        "A SAE is a neural network that predicts both inputs and outputs, and has been previously shown empirically to provide significant improvements when used in a semi-supervised setting [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "We demonstrate empirically that adding reconstruction error never harms performance compared to the corresponding neural network model, and in some cases can significantly improve classification accuracy.",
        "Solely training a representation according to the supervised tasks, like learning hidden layers in an neural network, is likely an under-constrained problem, and will find solutions that can well fit the data but that do not find underlying patterns in the data and do not generalize well.",
        "For linear SAEs, the hidden layer size k < d, as otherwise trivial solutions like the replication of the input are able to minimize the reconstruction error.",
        "There are at least two alternative strategies that could be considered to theoretically analyze these models: using a multi-task analysis and characterizing the Rademacher complexity of the supervised auto-encoder function class.",
        "We empirically test the utility of incorporating the reconstruction error into NNs, as a method for regularization to improve generalization performance.",
        "For SUSY, SAE and NNs were essentially tied; but for that dataset, all the nonlinear architectures performed very suggesting little improvement could be gained.",
        "We fixed the hidden dimension to 10% of the number of centers, to see if the SAE could still learn an appropriate model even with an aggressive bottleneck, namely a hidden layer with a relatively very small size, making it hard to reduce the reconstruction error.",
        "We show the performance of SAE with decreasing weight on the predictive loss, which increases the effect of the reconstruction error.",
        "We showed theoretically that the addition of reconstruction error improves generalization performance, for linear SAEs. We showed empirically, across four different datasets, with a variety of architectures, that SAE never harms performance but in some cases can significantly improve performance, particularly when using kernels and under ReLu activations, for both shallow and deep architectures."
    ],
    "headline": "We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error\u2014particularly as an improvement on simplistic regularization such as norms",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization. In Advances in Neural Information Processing Systems, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20Complexity%20of%20Linear%20Prediction%3A%20Risk%20Bounds%2C%20Margin%20Bounds%2C%20and%20Regularization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Sridharan%2C%20Karthik%20Tewari%2C%20Ambuj%20On%20the%20Complexity%20of%20Linear%20Prediction%3A%20Risk%20Bounds%2C%20Margin%20Bounds%2C%20and%20Regularization%202008"
        },
        {
            "id": "2",
            "entry": "[2] Mehryar Mohri, Afshin Rostamizadeh, and Dmitry Storcheus. Generalization Bounds for Supervised Dimensionality Reduction. In NIPS Workshop Feature Extraction Modern Questions and Challenges, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Storcheus%2C%20Dmitry%20Generalization%20Bounds%20for%20Supervised%20Dimensionality%20Reduction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Storcheus%2C%20Dmitry%20Generalization%20Bounds%20for%20Supervised%20Dimensionality%20Reduction%202015"
        },
        {
            "id": "3",
            "entry": "[3] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality reduction. Theoretical Computer Science, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gottlieb%2C%20Lee-Ad%20Kontorovich%2C%20Aryeh%20Krauthgamer%2C%20Robert%20Adaptive%20metric%20dimensionality%20reduction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gottlieb%2C%20Lee-Ad%20Kontorovich%2C%20Aryeh%20Krauthgamer%2C%20Robert%20Adaptive%20metric%20dimensionality%20reduction%202016"
        },
        {
            "id": "4",
            "entry": "[4] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: risk bounds and structural results. The Journal of Machine Learning Research, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20risk%20bounds%20and%20structural%20results%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Peter%20L.%20Mendelson%2C%20Shahar%20Rademacher%20and%20gaussian%20complexities%3A%20risk%20bounds%20and%20structural%20results%202002"
        },
        {
            "id": "5",
            "entry": "[5] Olivier Bousquet and Andr\u00e9 Elisseeff. Stability and Generalization. Journal of Machine Learning Research, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andr%C3%A9%20Stability%20and%20Generalization%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20Olivier%20Elisseeff%2C%20Andr%C3%A9%20Stability%20and%20Generalization%202002"
        },
        {
            "id": "6",
            "entry": "[6] Tong Zhang. Covering Number Bounds of Certain Regularized Linear Function Classes. Journal of Machine Learning Research, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Covering%20Number%20Bounds%20of%20Certain%20Regularized%20Linear%20Function%20Classes%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Covering%20Number%20Bounds%20of%20Certain%20Regularized%20Linear%20Function%20Classes%202002"
        },
        {
            "id": "7",
            "entry": "[7] Stefan Wager, Sida Wang, and Percy S Liang. Dropout Training as Adaptive Regularization. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wager%2C%20Stefan%20Wang%2C%20Sida%20Liang%2C%20Percy%20S.%20Dropout%20Training%20as%20Adaptive%20Regularization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wager%2C%20Stefan%20Wang%2C%20Sida%20Liang%2C%20Percy%20S.%20Dropout%20Training%20as%20Adaptive%20Regularization%202013"
        },
        {
            "id": "8",
            "entry": "[8] N Srivastava, G Hinton, A Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting . Journal of Machine Learning Research, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from%20Overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Hinton%2C%20G.%20Krizhevsky%2C%20A.%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from%20Overfitting%202014"
        },
        {
            "id": "9",
            "entry": "[9] Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, and Bohyung Han. Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noh%2C%20Hyeonwoo%20You%2C%20Tackgeun%20Mun%2C%20Jonghwan%20Han%2C%20Bohyung%20Regularizing%20Deep%20Neural%20Networks%20by%20Noise%3A%20Its%20Interpretation%20and%20Optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noh%2C%20Hyeonwoo%20You%2C%20Tackgeun%20Mun%2C%20Jonghwan%20Han%2C%20Bohyung%20Regularizing%20Deep%20Neural%20Networks%20by%20Noise%3A%20Its%20Interpretation%20and%20Optimization%202017"
        },
        {
            "id": "10",
            "entry": "[10] N Morgan, H Bourlard, and 1990. Generalization and parameter estimation in feedforward nets: Some experiments. In Advances in Neural Information Processing Systems, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=N%20Morgan%2C%20H%20Bourlard%201990%20Generalization%20and%20parameter%20estimation%20in%20feedforward%20nets%3A%20Some%20experiments%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=N%20Morgan%2C%20H%20Bourlard%201990%20Generalization%20and%20parameter%20estimation%20in%20feedforward%20nets%3A%20Some%20experiments%201990"
        },
        {
            "id": "11",
            "entry": "[11] Larry Yaeger, Richard Lyon, and Brandyn Webb. Effective Training of a Neural Network Character Classifier for Word Recognition. In Advances in Neural Information Processing Systems, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yaeger%2C%20Larry%20Lyon%2C%20Richard%20Webb%2C%20Brandyn%20Effective%20Training%20of%20a%20Neural%20Network%20Character%20Classifier%20for%20Word%20Recognition%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yaeger%2C%20Larry%20Lyon%2C%20Richard%20Webb%2C%20Brandyn%20Effective%20Training%20of%20a%20Neural%20Network%20Character%20Classifier%20for%20Word%20Recognition%201997"
        },
        {
            "id": "12",
            "entry": "[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%202012"
        },
        {
            "id": "13",
            "entry": "[13] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool - A Simple and Accurate Method to Fool Deep Neural Networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%20-%20A%20Simple%20and%20Accurate%20Method%20to%20Fool%20Deep%20Neural%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20Seyed-Mohsen%20Fawzi%2C%20Alhussein%20Frossard%2C%20Pascal%20DeepFool%20-%20A%20Simple%20and%20Accurate%20Method%20to%20Fool%20Deep%20Neural%20Networks%202016"
        },
        {
            "id": "14",
            "entry": "[14] Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, and Xin Geng. Deep Label Distribution Learning With Label Ambiguity. IEEE Transactions on Image Processing, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Bin-Bin%20Xing%2C%20Chao%20Xie%2C%20Chen-Wei%20Wu%2C%20Jianxin%20Deep%20Label%20Distribution%20Learning%20With%20Label%20Ambiguity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Bin-Bin%20Xing%2C%20Chao%20Xie%2C%20Chen-Wei%20Wu%2C%20Jianxin%20Deep%20Label%20Distribution%20Learning%20With%20Label%20Ambiguity%202017"
        },
        {
            "id": "15",
            "entry": "[15] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Lamblin%2C%20Pascal%20Popovici%2C%20Dan%20Larochelle%2C%20Hugo%20Greedy%20layer-wise%20training%20of%20deep%20networks%202007"
        },
        {
            "id": "16",
            "entry": "[16] Marc\u2019Aurelio Ranzato and Martin Szummer. Semi-supervised learning of compact document representations with deep networks. In International Conference on Machine Learning, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20Marc%E2%80%99Aurelio%20Szummer%2C%20Martin%20Semi-supervised%20learning%20of%20compact%20document%20representations%20with%20deep%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20Marc%E2%80%99Aurelio%20Szummer%2C%20Martin%20Semi-supervised%20learning%20of%20compact%20document%20representations%20with%20deep%20networks%202008"
        },
        {
            "id": "17",
            "entry": "[17] Miguel \u00c1 Carreira-Perpi\u00f1\u00e1n and Weiran Wang. Distributed optimization of deeply nested systems. In International Conference on Artificial Intelligence and Statistics, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira-Perpi%C3%B1%C3%A1n%2C%20Miguel%20%C3%81.%20Wang%2C%20Weiran%20Distributed%20optimization%20of%20deeply%20nested%20systems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira-Perpi%C3%B1%C3%A1n%2C%20Miguel%20%C3%81.%20Wang%2C%20Weiran%20Distributed%20optimization%20of%20deeply%20nested%20systems%202014"
        },
        {
            "id": "18",
            "entry": "[18] Rich Caruana. Multitask Learning. Machine Learning, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20Rich%20Multitask%20Learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20Rich%20Multitask%20Learning%201997"
        },
        {
            "id": "19",
            "entry": "[19] Jonathan Baxter. Learning internal representations. In Annual Conference on Learning Theory, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jonathan%20Baxter%20Learning%20internal%20representations%20In%20Annual%20Conference%20on%20Learning%20Theory%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jonathan%20Baxter%20Learning%20internal%20representations%20In%20Annual%20Conference%20on%20Learning%20Theory%201995"
        },
        {
            "id": "20",
            "entry": "[20] Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baxter%2C%20Jonathan%20A%20model%20of%20inductive%20bias%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baxter%2C%20Jonathan%20A%20model%20of%20inductive%20bias%20learning%202000"
        },
        {
            "id": "21",
            "entry": "[21] Andreas Maurer. Bounds for Linear Multi-Task Learning. Journal of Machine Learning Research, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20Andreas%20Bounds%20for%20Linear%20Multi-Task%20Learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20Bounds%20for%20Linear%20Multi-Task%20Learning%202006"
        },
        {
            "id": "22",
            "entry": "[22] Andreas Maurer and Massimiliano Pontil. Excess risk bounds for multitask learning with trace norm regularization. In Annual Conference on Learning Theory, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maurer%2C%20Andreas%20Pontil%2C%20Massimiliano%20Excess%20risk%20bounds%20for%20multitask%20learning%20with%20trace%20norm%20regularization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maurer%2C%20Andreas%20Pontil%2C%20Massimiliano%20Excess%20risk%20bounds%20for%20multitask%20learning%20with%20trace%20norm%20regularization%202013"
        },
        {
            "id": "23",
            "entry": "[23] Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The Benefit of Multitask Representation Learning. arXiv:1509.01240v2, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.01240v2"
        },
        {
            "id": "24",
            "entry": "[24] Tongliang Liu, Dacheng Tao, Mingli Song, and Stephen J Maybank. Algorithm-Dependent Generalization Bounds for Multi-Task Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Tongliang%20Tao%2C%20Dacheng%20Song%2C%20Mingli%20Maybank%2C%20Stephen%20J.%20Algorithm-Dependent%20Generalization%20Bounds%20for%20Multi-Task%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Tongliang%20Tao%2C%20Dacheng%20Song%2C%20Mingli%20Maybank%2C%20Stephen%20J.%20Algorithm-Dependent%20Generalization%20Bounds%20for%20Multi-Task%20Learning%202017"
        },
        {
            "id": "25",
            "entry": "[25] Jason Weston, Fr\u00e9d\u00e9ric Ratle, and Ronan Collobert. Deep learning via semi-supervised embedding. In International Conference on Machine Learning, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jason%20Weston%2C%20Fr%C3%A9d%C3%A9ric%20Ratle%20Collobert%2C%20Ronan%20Deep%20learning%20via%20semi-supervised%20embedding%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jason%20Weston%2C%20Fr%C3%A9d%C3%A9ric%20Ratle%20Collobert%2C%20Ronan%20Deep%20learning%20via%20semi-supervised%20embedding%202008"
        },
        {
            "id": "26",
            "entry": "[26] Alexander G Ororbia II, C Lee Giles, and David Reitter. Learning a Deep Hybrid Model for SemiSupervised Text Classification. EMNLP, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ororbia%2C%20II%2C%20Alexander%20G.%20Giles%2C%20C.Lee%20Reitter%2C%20David%20Learning%20a%20Deep%20Hybrid%20Model%20for%20SemiSupervised%20Text%20Classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ororbia%2C%20II%2C%20Alexander%20G.%20Giles%2C%20C.Lee%20Reitter%2C%20David%20Learning%20a%20Deep%20Hybrid%20Model%20for%20SemiSupervised%20Text%20Classification%202015"
        },
        {
            "id": "27",
            "entry": "[27] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised Learning with Ladder Networks. In Advances in Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semi-supervised%20Learning%20with%20Ladder%20Networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmus%2C%20Antti%20Berglund%2C%20Mathias%20Honkala%2C%20Mikko%20Valpola%2C%20Harri%20Semi-supervised%20Learning%20with%20Ladder%20Networks%202015"
        },
        {
            "id": "28",
            "entry": "[28] Yaser S Abu-Mostafa. Learning from hints in neural networks. J. Complexity, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abu-Mostafa%2C%20Yaser%20S.%20Learning%20from%20hints%20in%20neural%20networks%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abu-Mostafa%2C%20Yaser%20S.%20Learning%20from%20hints%20in%20neural%20networks%201990"
        },
        {
            "id": "29",
            "entry": "[29] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20Pierre%20Hornik%2C%20Kurt%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20Pierre%20Hornik%2C%20Kurt%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989"
        },
        {
            "id": "30",
            "entry": "[30] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohri%2C%20Mehryar%20Rostamizadeh%2C%20Afshin%20Talwalkar%2C%20Ameet%20Foundations%20of%20Machine%20Learning%202012"
        },
        {
            "id": "31",
            "entry": "[31] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. Journal of Machine Learning Research, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20Denoising%20Autoencoders%3A%20Learning%20Useful%20Representations%20in%20a%20Deep%20Network%20with%20a%20Local%20Denoising%20Criterion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20Pascal%20Larochelle%2C%20Hugo%20Lajoie%2C%20Isabelle%20Bengio%2C%20Yoshua%20Stacked%20Denoising%20Autoencoders%3A%20Learning%20Useful%20Representations%20in%20a%20Deep%20Network%20with%20a%20Local%20Denoising%20Criterion%202010"
        },
        {
            "id": "32",
            "entry": "[32] Marc\u2019Aurelio Ranzato, Christopher S Poultney, Sumit Chopra, and Yann LeCun. Efficient Learning of Sparse Representations with an Energy-Based Model. In Advances in Neural Information Processing Systems, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranzato%2C%20Marc%E2%80%99Aurelio%20Poultney%2C%20Christopher%20S.%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Efficient%20Learning%20of%20Sparse%20Representations%20with%20an%20Energy-Based%20Model%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranzato%2C%20Marc%E2%80%99Aurelio%20Poultney%2C%20Christopher%20S.%20Chopra%2C%20Sumit%20LeCun%2C%20Yann%20Efficient%20Learning%20of%20Sparse%20Representations%20with%20an%20Energy-Based%20Model%202006"
        },
        {
            "id": "33",
            "entry": "[33] Anupriya Gogna and Angshul Majumdar. Semi Supervised Autoencoder. In Neural Information Processing. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anupriya%20Gogna%20and%20Angshul%20Majumdar%20Semi%20Supervised%20Autoencoder%20In%20Neural%20Information%20Processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anupriya%20Gogna%20and%20Angshul%20Majumdar%20Semi%20Supervised%20Autoencoder%20In%20Neural%20Information%20Processing%202016"
        },
        {
            "id": "34",
            "entry": "[34] R Caruana and V R De Sa. Promoting poor features to supervisors: Some inputs work better as outputs. In Advances in Neural Information Processing Systems, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20R.%20Sa%2C%20V.R.De%20Promoting%20poor%20features%20to%20supervisors%3A%20Some%20inputs%20work%20better%20as%20outputs%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20R.%20Sa%2C%20V.R.De%20Promoting%20poor%20features%20to%20supervisors%3A%20Some%20inputs%20work%20better%20as%20outputs%201997"
        },
        {
            "id": "35",
            "entry": "[35] S Ben-David and R Schuller. Exploiting task relatedness for multiple task learning. Lecture Notes in Computer Science, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ben-David%2C%20S.%20Schuller%2C%20R.%20Exploiting%20task%20relatedness%20for%20multiple%20task%20learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ben-David%2C%20S.%20Schuller%2C%20R.%20Exploiting%20task%20relatedness%20for%20multiple%20task%20learning%202003"
        },
        {
            "id": "36",
            "entry": "[36] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for Exotic Particles in High-Energy Physics with Deep Learning. arXiv:1509.01240v2, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1509.01240v2"
        },
        {
            "id": "37",
            "entry": "[37] David Henry Deterding. Speaker normalisation for automatic speech recognition. PhD thesis, University of Cambridge, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deterding%2C%20David%20Henry%20Speaker%20normalisation%20for%20automatic%20speech%20recognition%201990"
        },
        {
            "id": "38",
            "entry": "[38] A Krizhevsky and G Hinton. Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images%202009"
        },
        {
            "id": "39",
            "entry": "[39] Y LeCun, L Bottou, Y Bengio, and P Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "40",
            "entry": "[40] Benjamin Graham. Fractional Max-Pooling. arXiv:1411.4000v2 [cs.LG], 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.4000v2"
        },
        {
            "id": "41",
            "entry": "[41] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        }
    ]
}
