{
    "filename": "8230-robust-learning-of-fixed-structure-bayesian-networks.pdf",
    "metadata": {
        "title": "Robust Learning of Fixed-Structure Bayesian Networks",
        "author": "Yu Cheng, Ilias Diakonikolas, Daniel Kane, Alistair Stewart",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8230-robust-learning-of-fixed-structure-bayesian-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We investigate the problem of learning Bayesian networks in a robust model where an -fraction of the samples are adversarially corrupted. In this work, we study the fully observable discrete case where the structure of the network is given. Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees. We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees. Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples. Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice."
    },
    "keywords": [
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "density estimation",
            "url": "https://en.wikipedia.org/wiki/density_estimation"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "linear time",
            "url": "https://en.wikipedia.org/wiki/linear_time"
        },
        {
            "term": "ising model",
            "url": "https://en.wikipedia.org/wiki/ising_model"
        },
        {
            "term": "polynomial time",
            "url": "https://en.wikipedia.org/wiki/polynomial_time"
        },
        {
            "term": "probability distribution",
            "url": "https://en.wikipedia.org/wiki/probability_distribution"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "bayesian networks",
            "url": "https://en.wikipedia.org/wiki/bayesian_networks"
        }
    ],
    "highlights": [
        "Probabilistic graphical models [KF09] provide an appealing and unifying formalism to succinctly represent structured high-dimensional distributions",
        "We need this to show that a good approximation to the conditional probability table implies that the corresponding Bayesian network is close in total variation distance",
        "We say that a Bayesian network is c-balanced, for some c > 0, if all coordinates of the corresponding conditional probability table are between c and 1 \u2212 c",
        "We first note that the sample complexity of our algorithm is near-optimal for learning Bayesian networks with known structure",
        "We described a computationally efficient algorithm for robustly learning Bayesian networks with a known topology, under some mild assumptions on the conditional probability table",
        "We evaluate our algorithm experimentally, and we view our experiments as a proof of concept demonstration that our techniques can be practical for learning fixed-structure Bayesian networks"
    ],
    "key_statements": [
        "Probabilistic graphical models [KF09] provide an appealing and unifying formalism to succinctly represent structured high-dimensional distributions",
        "We need this to show that a good approximation to the conditional probability table implies that the corresponding Bayesian network is close in total variation distance",
        "We say that a Bayesian network is c-balanced, for some c > 0, if all coordinates of the corresponding conditional probability table are between c and 1 \u2212 c",
        "We first note that the sample complexity of our algorithm is near-optimal for learning Bayesian networks with known structure",
        "We described a computationally efficient algorithm for robustly learning Bayesian networks with a known topology, under some mild assumptions on the conditional probability table",
        "We evaluate our algorithm experimentally, and we view our experiments as a proof of concept demonstration that our techniques can be practical for learning fixed-structure Bayesian networks"
    ],
    "summary": [
        "Probabilistic graphical models [KF09] provide an appealing and unifying formalism to succinctly represent structured high-dimensional distributions.",
        "We give the first efficient robust learning algorithm for Bayesian networks with a known graph G.",
        "We need this to show that a good approximation to the conditional probability table implies that the corresponding Bayesian network is close in total variation distance.",
        "We first note that the sample complexity of our algorithm is near-optimal for learning Bayesian networks with known structure.",
        "The worst-case sample complexity of learning BN d,f , within total variation distance and with probability 9/10, is \u03a9(2f \u00b7 d/ 2) for all f \u2264 d/2 when the graph structure is known.",
        "Our algorithm works as follows: We draw an -corrupted set of samples from a",
        "Bayesian network P with known structure, and iteratively remove samples until we can return the empirical conditional probability table.",
        "The structure of this section is as follows: First, we bound the total variation distance between two Bayes nets in terms of their conditional probability tables.",
        "Lemma 5 says that to learn a balanced fixed-structure Bayesian network, it is sufficient to learn all the relevant conditional means.",
        "For the case of binary product distributions in [DKK+16], (1) is trivial because the coordinates are independent; but for Bayesian networks we need to expand the dimension of the samples and fill in the missing entries properly.",
        "S satisfies the conditions of Proposition 9, and the algorithm outputs a smaller set S of samples that satisfies the conditions of the proposition, or else outputs a Bayesian network Q with small dTV (P, Q) that satisfies Theorem 3.",
        "Algorithm 1 computes a matrix M , and shows that: either M 2 is small, and we can output the empirical conditional probabilities, or M 2 is large, and we can use the top eigenvector of M to remove bad samples.",
        "Our experiments show that our filtering algorithm works very well in this setting, even when the assumptions under which we can prove theoretical guarantees are not satisfied.",
        "We use the error of the empirical conditional mean without noise (i.e., MLE estimator with only good samples) as the gold standard, since this is the best one could hope for even if all the corrupted samples are identified.",
        "In the semi-synthetic experiments, we apply our algorithm to robustly learn real-world Bayesian networks.",
        "In Figure 2, the noise distribution is a Bayes net with random dependency graphs and conditional probabilities drawn from, and graphical models with hidden variables.",
        "There is a wealth of natural probabilistic models that merit investigation in the robust setting, including undirected graphical models (e.g., Ising models), and graphical models with hidden variables"
    ],
    "headline": "We investigate the problem of learning Bayesian networks in a robust model where an -fraction of the samples are adversarially corrupted",
    "reference_links": [
        {
            "id": "Acharya_et+al_2016_a",
            "entry": "[ADLS16] J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Fast algorithms for segmented regression. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, pages 2878\u20132886, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Acharya%2C%20J.%20Diakonikolas%2C%20I.%20Li%2C%20J.%20Schmidt%2C%20L.%20Fast%20algorithms%20for%20segmented%20regression%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Acharya%2C%20J.%20Diakonikolas%2C%20I.%20Li%2C%20J.%20Schmidt%2C%20L.%20Fast%20algorithms%20for%20segmented%20regression%202016"
        },
        {
            "id": "Acharya_et+al_2017_a",
            "entry": "[ADLS17] J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-optimal density estimation in nearly-linear time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, pages 1278\u20131289, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Acharya%2C%20J.%20Diakonikolas%2C%20I.%20Li%2C%20J.%20Schmidt%2C%20L.%20Sample-optimal%20density%20estimation%20in%20nearly-linear%20time%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Acharya%2C%20J.%20Diakonikolas%2C%20I.%20Li%2C%20J.%20Schmidt%2C%20L.%20Sample-optimal%20density%20estimation%20in%20nearly-linear%20time%202017"
        },
        {
            "id": "Anandkumar_et+al_2012_a",
            "entry": "[AHHK12] A. Anandkumar, D. J. Hsu, F. Huang, and S. Kakade. Learning mixtures of tree graphical models. In Proc. 27th Annual Conference on Neural Information Processing Systems (NIPS), pages 1061\u20131069, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20A.%20Hsu%2C%20D.J.%20Huang%2C%20F.%20Kakade%2C%20S.%20Learning%20mixtures%20of%20tree%20graphical%20models%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20A.%20Hsu%2C%20D.J.%20Huang%2C%20F.%20Kakade%2C%20S.%20Learning%20mixtures%20of%20tree%20graphical%20models%202012"
        },
        {
            "id": "Abbeel_et+al_2006_a",
            "entry": "[AKN06] P. Abbeel, D. Koller, and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity. J. Mach. Learn. Res., 7:1743\u20131788, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abbeel%2C%20P.%20Koller%2C%20D.%20Ng%2C%20A.Y.%20Learning%20factor%20graphs%20in%20polynomial%20time%20and%20sample%20complexity%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abbeel%2C%20P.%20Koller%2C%20D.%20Ng%2C%20A.Y.%20Learning%20factor%20graphs%20in%20polynomial%20time%20and%20sample%20complexity%202006"
        },
        {
            "id": "Balakrishnan_et+al_2017_a",
            "entry": "[BDLS17] S. Balakrishnan, S. S. Du, J. Li, and A. Singh. Computationally efficient robust sparse estimation in high dimensions. In Proc. 30th Annual Conference on Learning Theory (COLT), pages 169\u2013212, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balakrishnan%2C%20S.%20Du%2C%20S.S.%20Li%2C%20J.%20Singh%2C%20A.%20Computationally%20efficient%20robust%20sparse%20estimation%20in%20high%20dimensions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balakrishnan%2C%20S.%20Du%2C%20S.S.%20Li%2C%20J.%20Singh%2C%20A.%20Computationally%20efficient%20robust%20sparse%20estimation%20in%20high%20dimensions%202017"
        },
        {
            "id": "Bernholt_2006_a",
            "entry": "[Ber06] T. Bernholt. Robust estimators are hard to compute. Technical report, University of Dortmund, Germany, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bernholt%2C%20T.%20Robust%20estimators%20are%20hard%20to%20compute%202006"
        },
        {
            "id": "Bresler_et+al_2014_a",
            "entry": "[BGS14] G. Bresler, D. Gamarnik, and D. Shah. Structure learning of antiferromagnetic Ising models. In NIPS, pages 2852\u20132860, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bresler%2C%20G.%20Gamarnik%2C%20D.%20Shah%2C%20D.%20Structure%20learning%20of%20antiferromagnetic%20Ising%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bresler%2C%20G.%20Gamarnik%2C%20D.%20Shah%2C%20D.%20Structure%20learning%20of%20antiferromagnetic%20Ising%20models%202014"
        },
        {
            "id": "Bresler_et+al_2013_a",
            "entry": "[BMS13] G. Bresler, E. Mossel, and A. Sly. Reconstruction of Markov random fields from samples: Some observations and algorithms. SIAM J. Comput., 42(2):563\u2013578, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bresler%2C%20G.%20Mossel%2C%20E.%20Sly%2C%20A.%20Reconstruction%20of%20Markov%20random%20fields%20from%20samples%3A%20Some%20observations%20and%20algorithms%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bresler%2C%20G.%20Mossel%2C%20E.%20Sly%2C%20A.%20Reconstruction%20of%20Markov%20random%20fields%20from%20samples%3A%20Some%20observations%20and%20algorithms%202013"
        },
        {
            "id": "Bresler_2015_a",
            "entry": "[Bre15] G. Bresler. Efficiently learning Ising models on arbitrary graphs. In Proc. 47th Annual ACM Symposium on Theory of Computing (STOC), pages 771\u2013782, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bresler%2C%20G.%20Efficiently%20learning%20Ising%20models%20on%20arbitrary%20graphs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bresler%2C%20G.%20Efficiently%20learning%20Ising%20models%20on%20arbitrary%20graphs%202015"
        },
        {
            "id": "Beinlich_et+al_1989_a",
            "entry": "[BSCC89] I. A. Beinlich, H. J. Suermondt, R. M. Chavez, and G. F. Cooper. The ALARM Monitoring System: A Case Study with two Probabilistic Inference Techniques for Belief Networks. Springer, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beinlich%2C%20I.A.%20Suermondt%2C%20H.J.%20Chavez%2C%20R.M.%20Cooper%2C%20G.F.%20The%20ALARM%20Monitoring%20System%3A%20A%20Case%20Study%20with%20two%20Probabilistic%20Inference%20Techniques%20for%20Belief%20Networks%201989"
        },
        {
            "id": "Canonne_et+al_2017_a",
            "entry": "[CDKS17] C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing Bayesian networks. In Proc. 30th Annual Conference on Learning Theory (COLT), pages 370\u2013448, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Canonne%2C%20C.L.%20Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20Testing%20Bayesian%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Canonne%2C%20C.L.%20Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20Testing%20Bayesian%20networks%202017"
        },
        {
            "id": "Chan_et+al_2013_a",
            "entry": "[CDSS13] S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions over discrete domains. In Proc. 24th Annual Symposium on Discrete Algorithms (SODA), pages 1380\u20131394, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20S.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.%20Sun%2C%20X.%20Learning%20mixtures%20of%20structured%20distributions%20over%20discrete%20domains%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20S.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.%20Sun%2C%20X.%20Learning%20mixtures%20of%20structured%20distributions%20over%20discrete%20domains%202013"
        },
        {
            "id": "Chan_et+al_2014_a",
            "entry": "[CDSS14a] S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise polynomial approximation. In Proc. 46th Annual ACM Symposium on Theory of Computing (STOC), pages 604\u2013613, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20S.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.%20Sun%2C%20X.%20Efficient%20density%20estimation%20via%20piecewise%20polynomial%20approximation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20S.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.%20Sun%2C%20X.%20Efficient%20density%20estimation%20via%20piecewise%20polynomial%20approximation%202014"
        },
        {
            "id": "Chan_et+al_2014_b",
            "entry": "[CDSS14b] S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear time using variable-width histograms. In Proc. 29th Annual Conference on Neural Information Processing Systems (NIPS), pages 1844\u20131852, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20S.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.%20Sun%2C%20X.%20Near-optimal%20density%20estimation%20in%20near-linear%20time%20using%20variable-width%20histograms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20S.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.%20Sun%2C%20X.%20Near-optimal%20density%20estimation%20in%20near-linear%20time%20using%20variable-width%20histograms%202014"
        },
        {
            "id": "Chen_et+al_2015_a",
            "entry": "[CGR15] M. Chen, C. Gao, and Z. Ren. Robust covariance and scatter matrix estimation under Huber\u2019s contamination model. CoRR, abs/1506.00691, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.00691"
        },
        {
            "id": "Chen_et+al_2016_a",
            "entry": "[CGR16] M. Chen, C. Gao, and Z. Ren. A general decision theory for Huber\u2019s -contamination model. Electronic Journal of Statistics, 10(2):3752\u20133774, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20M.%20Gao%2C%20C.%20Ren%2C%20Z.%20A%20general%20decision%20theory%20for%20Huber%E2%80%99s%20-contamination%20model%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20M.%20Gao%2C%20C.%20Ren%2C%20Z.%20A%20general%20decision%20theory%20for%20Huber%E2%80%99s%20-contamination%20model%202016"
        },
        {
            "id": "Chow_1968_a",
            "entry": "[CL68] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Trans. Inf. Theor., 14(3):462\u2013467, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chow%2C%20C.%20Liu%2C%20C.%20Approximating%20discrete%20probability%20distributions%20with%20dependence%20trees%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chow%2C%20C.%20Liu%2C%20C.%20Approximating%20discrete%20probability%20distributions%20with%20dependence%20trees%201968"
        },
        {
            "id": "Dasgupta_1997_a",
            "entry": "[Das97] S. Dasgupta. The sample complexity of learning fixed-structure Bayesian networks. Machine Learning, 29(2-3):165\u2013180, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20S.%20The%20sample%20complexity%20of%20learning%20fixed-structure%20Bayesian%20networks%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20S.%20The%20sample%20complexity%20of%20learning%20fixed-structure%20Bayesian%20networks%201997"
        },
        {
            "id": "Daskalakis_et+al_2014_a",
            "entry": "[DDS14] C. Daskalakis, I. Diakonikolas, and R. A. Servedio. Learning k-modal distributions via testing. Theory of Computing, 10(20):535\u2013570, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20C.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.A.%20Learning%20k-modal%20distributions%20via%20testing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20C.%20Diakonikolas%2C%20I.%20Servedio%2C%20R.A.%20Learning%20k-modal%20distributions%20via%20testing%202014"
        },
        {
            "id": "Diakonikolas_et+al_2016_a",
            "entry": "[DKK+16] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust estimators in high dimensions without the computational intractability. In Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Robust%20estimators%20in%20high%20dimensions%20without%20the%20computational%20intractability%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Robust%20estimators%20in%20high%20dimensions%20without%20the%20computational%20intractability%202016"
        },
        {
            "id": "Diakonikolas_et+al_2017_a",
            "entry": "[DKK+17] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Being robust (in high dimensions) can be practical. In Proc. 34th International Conference on Machine Learning (ICML), pages 999\u20131008, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Being%20robust%20%28in%20high%20dimensions%29%20can%20be%20practical%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Being%20robust%20%28in%20high%20dimensions%29%20can%20be%20practical%202017"
        },
        {
            "id": "Diakonikolas_et+al_2018_a",
            "entry": "[DKK+18a] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robustly learning a Gaussian: Getting optimal error, efficiently. In Proc. 29th ACM-SIAM Symposium on Discrete Algorithms (SODA), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Robustly%20learning%20a%20Gaussian%3A%20Getting%20optimal%20error%2C%20efficiently%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kamath%2C%20G.%20Kane%2C%20D.M.%20Li%2C%20J.%20Robustly%20learning%20a%20Gaussian%3A%20Getting%20optimal%20error%2C%20efficiently%202018"
        },
        {
            "id": "Diakonikolas_et+al_2018_b",
            "entry": "[DKK+18b] I. Diakonikolas, G. Kamath, D. M Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02815"
        },
        {
            "id": "Diakonikolas_et+al_2017_b",
            "entry": "[DKS17] I. Diakonikolas, D. M. Kane, and A. Stewart. Statistical query lower bounds for robust estimation of high-dimensional Gaussians and Gaussian mixtures. In Proc. 58th IEEE Symposium on Foundations of Computer Science (FOCS), pages 73\u201384, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20Statistical%20query%20lower%20bounds%20for%20robust%20estimation%20of%20high-dimensional%20Gaussians%20and%20Gaussian%20mixtures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20Statistical%20query%20lower%20bounds%20for%20robust%20estimation%20of%20high-dimensional%20Gaussians%20and%20Gaussian%20mixtures%202017"
        },
        {
            "id": "Diakonikolas_et+al_2018_c",
            "entry": "[DKS18a] I. Diakonikolas, D. M. Kane, and A. Stewart. Learning geometric concepts with nasty noise. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1061\u20131073, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20Learning%20geometric%20concepts%20with%20nasty%20noise%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20Learning%20geometric%20concepts%20with%20nasty%20noise%202018"
        },
        {
            "id": "Diakonikolas_et+al_2018_d",
            "entry": "[DKS18b] I. Diakonikolas, D. M. Kane, and A. Stewart. List-decodable robust mean estimation and learning mixtures of spherical Gaussians. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1047\u20131060, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20List-decodable%20robust%20mean%20estimation%20and%20learning%20mixtures%20of%20spherical%20Gaussians%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Kane%2C%20D.M.%20Stewart%2C%20A.%20List-decodable%20robust%20mean%20estimation%20and%20learning%20mixtures%20of%20spherical%20Gaussians%202018"
        },
        {
            "id": "Diakonikolas_et+al_2018_e",
            "entry": "[DKS18c] I. Diakonikolas, W. Kong, and A. Stewart. Efficient algorithms and lower bounds for robust linear regression. CoRR, abs/1806.00040, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.00040"
        },
        {
            "id": "Diakonikolas_et+al_2018_f",
            "entry": "[DLS18] I. Diakonikolas, J. Li, and L. Schmidt. Fast and sample near-optimal algorithms for learning multidimensional histograms. In Conference On Learning Theory, COLT 2018, pages 819\u2013842, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diakonikolas%2C%20I.%20Li%2C%20J.%20Schmidt%2C%20L.%20Fast%20and%20sample%20near-optimal%20algorithms%20for%20learning%20multidimensional%20histograms.%20In%20Conference%20On%20Learning%20Theory%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20I.%20Li%2C%20J.%20Schmidt%2C%20L.%20Fast%20and%20sample%20near-optimal%20algorithms%20for%20learning%20multidimensional%20histograms.%20In%20Conference%20On%20Learning%20Theory%202018"
        },
        {
            "id": "Daly_et+al_2011_a",
            "entry": "[DSA11] R. Daly, Q. Shen, and S. Aitken. Learning Bayesian networks: approaches and issues. The Knowledge Engineering Review, 26:99\u2013157, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daly%2C%20R.%20Shen%2C%20Q.%20Aitken%2C%20S.%20Learning%20Bayesian%20networks%3A%20approaches%20and%20issues%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daly%2C%20R.%20Shen%2C%20Q.%20Aitken%2C%20S.%20Learning%20Bayesian%20networks%3A%20approaches%20and%20issues%202011"
        },
        {
            "id": "Hopkins_2018_a",
            "entry": "[HL18] S. B. Hopkins and J. Li. Mixture models, robustness, and sum of squares proofs. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1021\u20131034, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hopkins%2C%20S.B.%20Li%2C%20J.%20Mixture%20models%2C%20robustness%2C%20and%20sum%20of%20squares%20proofs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hopkins%2C%20S.B.%20Li%2C%20J.%20Mixture%20models%2C%20robustness%2C%20and%20sum%20of%20squares%20proofs%202018"
        },
        {
            "id": "Hardt_2013_a",
            "entry": "[HM13] M. Hardt and A. Moitra. Algorithms and hardness for robust subspace recovery. In Proc. 26th Annual Conference on Learning Theory (COLT), pages 354\u2013375, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20M.%20Moitra%2C%20A.%20Algorithms%20and%20hardness%20for%20robust%20subspace%20recovery%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20M.%20Moitra%2C%20A.%20Algorithms%20and%20hardness%20for%20robust%20subspace%20recovery%202013"
        },
        {
            "id": "Huber_2009_a",
            "entry": "[HR09] P. J. Huber and E. M. Ronchetti. Robust statistics. Wiley New York, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huber%2C%20P.J.%20Ronchetti%2C%20E.M.%20Robust%20statistics%202009"
        },
        {
            "id": "Hampel_et+al_1986_a",
            "entry": "[HRRS86] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. Robust statistics: The approach based on influence functions. Wiley New York, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hampel%2C%20F.R.%20Ronchetti%2C%20E.M.%20Rousseeuw%2C%20P.J.%20Stahel%2C%20W.A.%20Robust%20statistics%3A%20The%20approach%20based%20on%20influence%20functions%201986"
        },
        {
            "id": "Publishing_2007_a",
            "entry": "Publishing Company, Incorporated, 2nd edition, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Publishing%20Company%20Incorporated%202nd%20edition%202007"
        },
        {
            "id": "Computer_1978_a",
            "entry": "Computer Science, 6:93\u2013107, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Computer%20Science%20693107%201978"
        },
        {
            "id": "-Adaptive_2009_a",
            "entry": "-Adaptive Computation and Machine Learning. The MIT Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Adaptive%20Computation%20and%20Machine%20Learning%202009"
        },
        {
            "id": "Klivans_et+al_2018_a",
            "entry": "[KKM18] A. Klivans, P. Kothari, and R. Meka. Efficient algorithms for outlier-robust regression. In Proc. 31st Annual Conference on Learning Theory (COLT), pages 1420\u20131430, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klivans%2C%20A.%20Kothari%2C%20P.%20Meka%2C%20R.%20Efficient%20algorithms%20for%20outlier-robust%20regression%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klivans%2C%20A.%20Kothari%2C%20P.%20Meka%2C%20R.%20Efficient%20algorithms%20for%20outlier-robust%20regression%202018"
        },
        {
            "id": "Kothari_et+al_2018_a",
            "entry": "[KSS18] P. K. Kothari, J. Steinhardt, and D. Steurer. Robust moment estimation and improved clustering via sum of squares. In Proc. 50th Annual ACM Symposium on Theory of Computing (STOC), pages 1035\u20131046, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kothari%2C%20P.K.%20Steinhardt%2C%20J.%20Steurer%2C%20D.%20Robust%20moment%20estimation%20and%20improved%20clustering%20via%20sum%20of%20squares%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kothari%2C%20P.K.%20Steinhardt%2C%20J.%20Steurer%2C%20D.%20Robust%20moment%20estimation%20and%20improved%20clustering%20via%20sum%20of%20squares%202018"
        },
        {
            "id": "Lai_et+al_2016_a",
            "entry": "[LRV16] K. A. Lai, A. B. Rao, and S. Vempala. Agnostic estimation of mean and covariance. In Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lai%2C%20K.A.%20Rao%2C%20A.B.%20Vempala%2C%20S.%20Agnostic%20estimation%20of%20mean%20and%20covariance%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lai%2C%20K.A.%20Rao%2C%20A.B.%20Vempala%2C%20S.%20Agnostic%20estimation%20of%20mean%20and%20covariance%202016"
        },
        {
            "id": "Liu_et+al_2018_a",
            "entry": "[LSLC18] L. Liu, Y. Shen, T. Li, and C. Caramanis. High dimensional robust sparse regression. CoRR, abs/1805.11643, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11643"
        },
        {
            "id": "Loh_2012_a",
            "entry": "[LW12] P. L. Loh and M. J. Wainwright. Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses. In NIPS, pages 2096\u20132104, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loh%2C%20P.L.%20Wainwright%2C%20M.J.%20Structure%20estimation%20for%20discrete%20graphical%20models%3A%20Generalized%20covariance%20matrices%20and%20their%20inverses%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loh%2C%20P.L.%20Wainwright%2C%20M.J.%20Structure%20estimation%20for%20discrete%20graphical%20models%3A%20Generalized%20covariance%20matrices%20and%20their%20inverses%202012"
        },
        {
            "id": "Neapolitan_2003_a",
            "entry": "[Nea03] R. E. Neapolitan. Learning Bayesian Networks. Prentice-Hall, Inc., 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neapolitan%2C%20R.E.%20Learning%20Bayesian%20Networks%202003"
        },
        {
            "id": "Prasad_et+al_2018_a",
            "entry": "[PSBR18] A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06485"
        },
        {
            "id": "Santhanam_2012_a",
            "entry": "[SW12] N. P. Santhanam and M. J. Wainwright. Information-theoretic limits of selecting binary graphical models in high dimensions. IEEE Trans. Information Theory, 58(7):4117\u2013 4134, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santhanam%2C%20N.P.%20Wainwright%2C%20M.J.%20Information-theoretic%20limits%20of%20selecting%20binary%20graphical%20models%20in%20high%20dimensions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Santhanam%2C%20N.P.%20Wainwright%2C%20M.J.%20Information-theoretic%20limits%20of%20selecting%20binary%20graphical%20models%20in%20high%20dimensions%202012"
        },
        {
            "id": "Tukey_1975_a",
            "entry": "[Tuk75] J. W. Tukey. Mathematics and the picturing of data. In Proceedings of the International Congress of Mathematicians, volume 6, pages 523\u2013531, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tukey%2C%20J.W.%20Mathematics%20and%20the%20picturing%20of%20data%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tukey%2C%20J.W.%20Mathematics%20and%20the%20picturing%20of%20data%201975"
        },
        {
            "id": "Wainwright_2008_a",
            "entry": "[WJ08] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1\u2013305, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20M.J.%20Jordan%2C%20M.I.%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008"
        },
        {
            "id": "Wainwright_et+al_2006_a",
            "entry": "[WRL06] M. J. Wainwright, P. Ravikumar, and J. D. Lafferty. High-dimensional graphical model selection using 1-regularized logistic regression. In Proc. 20th Annual Conference on Neural Information Processing Systems (NIPS), pages 1465\u20131472, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20M.J.%20Ravikumar%2C%20P.%20Lafferty%2C%20J.D.%20High-dimensional%20graphical%20model%20selection%20using%201-regularized%20logistic%20regression%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20M.J.%20Ravikumar%2C%20P.%20Lafferty%2C%20J.D.%20High-dimensional%20graphical%20model%20selection%20using%201-regularized%20logistic%20regression%202006"
        }
    ]
}
