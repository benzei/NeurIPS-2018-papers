{
    "filename": "7726-a-smoother-way-to-train-structured-prediction-models.pdf",
    "metadata": {
        "title": "A Smoother Way to Train Structured Prediction Models",
        "author": "Venkata Krishna Pillutla, Vincent Roulet, Sham M. Kakade, Zaid Harchaoui",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7726-a-smoother-way-to-train-structured-prediction-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a framework to train a structured prediction model by performing smoothing on the inference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum margin structured prediction objective, and paves the way for the use of fast primal gradient-based optimization algorithms. We illustrate the proposed framework by developing a novel primal incremental optimization algorithm for the structural support vector machine. The proposed algorithm blends an extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study several practical variants. We present experimental results on two real-world problems, namely named entity recognition and visual object localization. The experimental results show that the proposed framework allows us to build upon efficient inference algorithms to develop large-scale optimization algorithms for structured prediction which can achieve competitive performance on the two real-world problems."
    },
    "keywords": [
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "average precision",
            "url": "https://en.wikipedia.org/wiki/average_precision"
        },
        {
            "term": "dynamic programming",
            "url": "https://en.wikipedia.org/wiki/dynamic_programming"
        },
        {
            "term": "conditional random field",
            "url": "https://en.wikipedia.org/wiki/conditional_random_field"
        }
    ],
    "highlights": [
        "If the score function is smooth, one could take advantage of the composite structure of the structural hinge loss f = h \u25e6 g by using the prox-linear algorithm [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "Batch nonsmooth optimization algorithms such as cutting plane methods are appropriate for problems with small or moderate sample sizes [<a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "We define smooth inference oracles in the context of black-box first-order optimization and establish worst-case complexity bounds for incremental optimization algorithms making calls to these oracles",
        "We focus here on primal optimization algorithms in order to be able to train structured prediction models with affine or nonlinear mappings with a unified approach, and on incremental optimization algorithms in order to be able to scale to large datasets",
        "We introduced a general notion of smooth inference oracles in the context of black-box first-order optimization. This allows us to set the scene to extend the scope of fast incremental optimization algorithms to structured prediction problems owing to a careful blend of a smoothing strategy and an acceleration scheme",
        "This work paves the way to faster incremental primal optimization algorithms for deep structured prediction models explored in more detail in [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]"
    ],
    "key_statements": [
        "If the score function is smooth, one could take advantage of the composite structure of the structural hinge loss f = h \u25e6 g by using the prox-linear algorithm [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "Batch nonsmooth optimization algorithms such as cutting plane methods are appropriate for problems with small or moderate sample sizes [<a class=\"ref-link\" id=\"c62\" href=\"#r62\">62</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "We introduce a general framework that allows us to bring the power of accelerated incremental optimization algorithms to the realm of structured prediction problems",
        "We focus on the problem of training a structural support vector machine (SSVM)",
        "We show how to shade off the inherent non-smoothness of the objective while still being able to rely on efficient inference algorithms",
        "We present a new algorithm built on top of SVRG, blending an extrapolation scheme for acceleration and an adaptive smoothing scheme",
        "We define smooth inference oracles in the context of black-box first-order optimization and establish worst-case complexity bounds for incremental optimization algorithms making calls to these oracles",
        "We focus here on primal optimization algorithms in order to be able to train structured prediction models with affine or nonlinear mappings with a unified approach, and on incremental optimization algorithms in order to be able to scale to large datasets",
        "We introduced a general notion of smooth inference oracles in the context of black-box first-order optimization. This allows us to set the scene to extend the scope of fast incremental optimization algorithms to structured prediction problems owing to a careful blend of a smoothing strategy and an acceleration scheme",
        "This work paves the way to faster incremental primal optimization algorithms for deep structured prediction models explored in more detail in [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>]"
    ],
    "summary": [
        "If the score function is smooth, one could take advantage of the composite structure of the structural hinge loss f = h \u25e6 g by using the prox-linear algorithm [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>].",
        "The plots compare two non-smooth optimization methods, (a) SGD, which is a primal stochastic subgradient method with step-sizes chosen as \u03b3t = \u03b30/(1 + t/t0), where \u03b30, t0 are parameters to be tuned, and returns the averaged iterate wt = 2/t(t + 1)",
        "The validation F1 score and the train loss are used as the tuning criteria for named entity recognition and visual object localization respectively.",
        "All smooth optimization methods turned out to be robust to the choice of K for the top-K oracle (Fig. 3) - we use K = 5 for named entity recognition and K = 10 for visual object localization.",
        "We framed here a general notion of smooth inference oracles in the context of first-order optimization.",
        "They do not establish complexity bounds for optimization algorithms making calls to the resulting smooth inference oracles.",
        "We define smooth inference oracles in the context of black-box first-order optimization and establish worst-case complexity bounds for incremental optimization algorithms making calls to these oracles.",
        "We relate the amount of smoothing controlled by \u03bc to the resulting complexity of the optimization algorithms relying on smooth inference oracles.",
        "[<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>] proposed a block coordinate Frank-Wolfe (BCFW) algorithm to optimize the dual formulation of structural support vector machines; see [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] for variants and extensions.",
        "We focus here on primal optimization algorithms in order to be able to train structured prediction models with affine or nonlinear mappings with a unified approach, and on incremental optimization algorithms in order to be able to scale to large datasets.",
        "We introduced a general notion of smooth inference oracles in the context of black-box first-order optimization.",
        "This allows us to set the scene to extend the scope of fast incremental optimization algorithms to structured prediction problems owing to a careful blend of a smoothing strategy and an acceleration scheme.",
        "We illustrated the potential of our framework by proposing a new incremental optimization algorithm to train structural support vector machines both enjoying worstcase complexity bounds and demonstrating competitive performance on two real-world problems.",
        "This work paves the way to faster incremental primal optimization algorithms for deep structured prediction models explored in more detail in [<a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>].",
        "Instance-level improved algorithms along the lines of [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] could be interesting to explore"
    ],
    "headline": "We present a framework to train a structured prediction model by performing smoothing on the inference algorithm it builds upon",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Z. Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal of Machine Learning Research, 18:221:1\u2013221:51, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Z.%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Katyusha%3A%20The%20first%20direct%20acceleration%20of%20stochastic%20gradient%20methods%202017"
        },
        {
            "id": "2",
            "entry": "[2] A. Beck and M. Teboulle. Smoothing and first order methods: A unified framework. SIAM Journal on Optimization, 22(2):557\u2013580, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20A.%20Teboulle%2C%20M.%20Smoothing%20and%20first%20order%20methods%3A%20A%20unified%20framework%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20A.%20Teboulle%2C%20M.%20Smoothing%20and%20first%20order%20methods%3A%20A%20unified%20framework%202012"
        },
        {
            "id": "3",
            "entry": "[3] J. V. Burke. Descent methods for composite nondifferentiable optimization problems. Mathematical Programming, 33(3):260\u2013279, Dec 1985. ISSN 1436-4646. doi: 10.1007/BF01584377.",
            "crossref": "https://dx.doi.org/10.1007/BF01584377",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/BF01584377"
        },
        {
            "id": "4",
            "entry": "[4] M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L. Bartlett. Exponentiated gradient algorithms for conditional random fields and max-margin markov networks. Journal of Machine Learning Research, 9(Aug):1775\u20131822, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collins%2C%20M.%20Globerson%2C%20A.%20Koo%2C%20T.%20Carreras%2C%20X.%20Exponentiated%20gradient%20algorithms%20for%20conditional%20random%20fields%20and%20max-margin%20markov%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collins%2C%20M.%20Globerson%2C%20A.%20Koo%2C%20T.%20Carreras%2C%20X.%20Exponentiated%20gradient%20algorithms%20for%20conditional%20random%20fields%20and%20max-margin%20markov%20networks%202008"
        },
        {
            "id": "5",
            "entry": "[5] B. Cox, A. Juditsky, and A. Nemirovski. Dual subgradient algorithms for large-scale nonsmooth learning problems. Mathematical Programming, 148(1-2):143\u2013180, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cox%2C%20B.%20Juditsky%2C%20A.%20Nemirovski%2C%20A.%20Dual%20subgradient%20algorithms%20for%20large-scale%20nonsmooth%20learning%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cox%2C%20B.%20Juditsky%2C%20A.%20Nemirovski%2C%20A.%20Dual%20subgradient%20algorithms%20for%20large-scale%20nonsmooth%20learning%20problems%202014"
        },
        {
            "id": "6",
            "entry": "[6] A. Defazio. A simple practical accelerated method for finite sums. In Advances in Neural Information Processing Systems, pages 676\u2013684, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20A%20simple%20practical%20accelerated%20method%20for%20finite%20sums%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20A%20simple%20practical%20accelerated%20method%20for%20finite%20sums%202016"
        },
        {
            "id": "7",
            "entry": "[7] A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "8",
            "entry": "[8] D. Drusvyatskiy and C. Paquette. Efficiency of minimizing compositions of convex functions and smooth maps. Mathematical Programming, Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drusvyatskiy%2C%20D.%20Paquette%2C%20C.%20Efficiency%20of%20minimizing%20compositions%20of%20convex%20functions%20and%20smooth%20maps%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drusvyatskiy%2C%20D.%20Paquette%2C%20C.%20Efficiency%20of%20minimizing%20compositions%20of%20convex%20functions%20and%20smooth%20maps%202018-07"
        },
        {
            "id": "9",
            "entry": "[9] J. C. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using Combinatorial Optimization within Max-Product Belief Propagation. In Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pages 369\u2013376, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.C.%20Tarlow%2C%20D.%20Elidan%2C%20G.%20Koller%2C%20D.%20Using%20Combinatorial%20Optimization%20within%20Max-Product%20Belief%20Propagation%202006-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.C.%20Tarlow%2C%20D.%20Elidan%2C%20G.%20Koller%2C%20D.%20Using%20Combinatorial%20Optimization%20within%20Max-Product%20Belief%20Propagation%202006-12"
        },
        {
            "id": "10",
            "entry": "[10] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) challenge. International Journal of Computer Vision, 88(2):303\u2013338, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Everingham%2C%20M.%20Gool%2C%20L.Van%20Williams%2C%20C.K.%20Winn%2C%20J.%20The%20Pascal%20Visual%20Object%20Classes%20%28VOC%29%20challenge%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Everingham%2C%20M.%20Gool%2C%20L.Van%20Williams%2C%20C.K.%20Winn%2C%20J.%20The%20Pascal%20Visual%20Object%20Classes%20%28VOC%29%20challenge%202010"
        },
        {
            "id": "11",
            "entry": "[11] R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning, pages 2540\u20132548, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frostig%2C%20R.%20Ge%2C%20R.%20Kakade%2C%20S.%20Sidford%2C%20A.%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frostig%2C%20R.%20Ge%2C%20R.%20Kakade%2C%20S.%20Sidford%2C%20A.%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "12",
            "entry": "[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580\u2013587, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Girshick%2C%20R.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Malik%2C%20J.%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Girshick%2C%20R.%20Donahue%2C%20J.%20Darrell%2C%20T.%20Malik%2C%20J.%20Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation%202014"
        },
        {
            "id": "13",
            "entry": "[13] T. Hazan and R. Urtasun. A primal-dual message-passing algorithm for approximated large scale structured prediction. In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada., pages 838\u2013846, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20T.%20Urtasun%2C%20R.%20A%20primal-dual%20message-passing%20algorithm%20for%20approximated%20large%20scale%20structured%20prediction%202010-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20T.%20Urtasun%2C%20R.%20A%20primal-dual%20message-passing%20algorithm%20for%20approximated%20large%20scale%20structured%20prediction%202010-12"
        },
        {
            "id": "14",
            "entry": "[14] T. Hazan, A. G. Schwing, and R. Urtasun. Blending learning and inference in conditional random fields. Journal of Machine Learning Research, 17:237:1\u2013237:25, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20T.%20Schwing%2C%20A.G.%20Urtasun%2C%20R.%20Blending%20learning%20and%20inference%20in%20conditional%20random%20fields%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20T.%20Schwing%2C%20A.G.%20Urtasun%2C%20R.%20Blending%20learning%20and%20inference%20in%20conditional%20random%20fields%202016"
        },
        {
            "id": "15",
            "entry": "[15] L. He, K. Lee, M. Lewis, and L. Zettlemoyer. Deep semantic role labeling: What works and what\u2019s next. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 473\u2013483, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20L.%20Lee%2C%20K.%20Lewis%2C%20M.%20Zettlemoyer%2C%20L.%20Deep%20semantic%20role%20labeling%3A%20What%20works%20and%20what%E2%80%99s%20next%202017-07-30",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20L.%20Lee%2C%20K.%20Lewis%2C%20M.%20Zettlemoyer%2C%20L.%20Deep%20semantic%20role%20labeling%3A%20What%20works%20and%20what%E2%80%99s%20next%202017-07-30"
        },
        {
            "id": "16",
            "entry": "[16] N. He and Z. Harchaoui. Semi-proximal mirror-prox for nonsmooth composite minimization. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3411\u20133419, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20N.%20Harchaoui%2C%20Z.%20Semi-proximal%20mirror-prox%20for%20nonsmooth%20composite%20minimization%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20N.%20Harchaoui%2C%20Z.%20Semi-proximal%20mirror-prox%20for%20nonsmooth%20composite%20minimization%202015-12-07"
        },
        {
            "id": "17",
            "entry": "[17] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM Journal on computing, 22(5):1087\u20131116, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jerrum%2C%20M.%20Sinclair%2C%20A.%20Polynomial-time%20approximation%20algorithms%20for%20the%20Ising%20model%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jerrum%2C%20M.%20Sinclair%2C%20A.%20Polynomial-time%20approximation%20algorithms%20for%20the%20Ising%20model%201993"
        },
        {
            "id": "18",
            "entry": "[18] T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane training of structural SVMs. Machine Learning, 77(1):27\u201359, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joachims%2C%20T.%20Finley%2C%20T.%20Yu%2C%20C.-N.J.%20Cutting-plane%20training%20of%20structural%20SVMs%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joachims%2C%20T.%20Finley%2C%20T.%20Yu%2C%20C.-N.J.%20Cutting-plane%20training%20of%20structural%20SVMs%202009"
        },
        {
            "id": "19",
            "entry": "[19] J. K. Johnson. Convex relaxation methods for graphical models: Lagrangian and maximum entropy approaches. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.K.%20Convex%20relaxation%20methods%20for%20graphical%20models%3A%20Lagrangian%20and%20maximum%20entropy%20approaches%202008"
        },
        {
            "id": "20",
            "entry": "[20] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "21",
            "entry": "[21] V. Jojic, S. Gould, and D. Koller. Accelerated dual decomposition for MAP inference. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 503\u2013510, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jojic%2C%20V.%20Gould%2C%20S.%20Koller%2C%20D.%20Accelerated%20dual%20decomposition%20for%20MAP%20inference%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jojic%2C%20V.%20Gould%2C%20S.%20Koller%2C%20D.%20Accelerated%20dual%20decomposition%20for%20MAP%20inference%202010"
        },
        {
            "id": "22",
            "entry": "[22] D. Jurafsky, J. H. Martin, P. Norvig, and S. Russell. Speech and Language Processing. Pearson Education, 2014. ISBN 9780133252934.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jurafsky%2C%20D.%20Martin%2C%20J.H.%20Norvig%2C%20P.%20Russell%2C%20S.%20Speech%20and%20Language%20Processing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jurafsky%2C%20D.%20Martin%2C%20J.H.%20Norvig%2C%20P.%20Russell%2C%20S.%20Speech%20and%20Language%20Processing%202014"
        },
        {
            "id": "23",
            "entry": "[23] P. Kohli and P. H. Torr. Measuring uncertainty in graph cut solutions. Computer Vision and Image Understanding, 112(1):30\u201338, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohli%2C%20P.%20Torr%2C%20P.H.%20Measuring%20uncertainty%20in%20graph%20cut%20solutions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohli%2C%20P.%20Torr%2C%20P.H.%20Measuring%20uncertainty%20in%20graph%20cut%20solutions%202008"
        },
        {
            "id": "24",
            "entry": "[24] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE transactions on pattern analysis and machine intelligence, 26(2):147\u2013159, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolmogorov%2C%20V.%20Zabin%2C%20R.%20What%20energy%20functions%20can%20be%20minimized%20via%20graph%20cuts%3F%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolmogorov%2C%20V.%20Zabin%2C%20R.%20What%20energy%20functions%20can%20be%20minimized%20via%20graph%20cuts%3F%202004"
        },
        {
            "id": "25",
            "entry": "[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "26",
            "entry": "[26] S. Lacoste-Julien, M. Schmidt, and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1212.2002"
        },
        {
            "id": "27",
            "entry": "[27] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-Coordinate Frank-Wolfe Optimization for Structural SVMs. In ICML 2013 International Conference on Machine Learning, pages 53\u201361, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste-Julien%2C%20S.%20Jaggi%2C%20M.%20Schmidt%2C%20M.%20Pletscher%2C%20P.%20Block-Coordinate%20Frank-Wolfe%20Optimization%20for%20Structural%20SVMs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lacoste-Julien%2C%20S.%20Jaggi%2C%20M.%20Schmidt%2C%20M.%20Pletscher%2C%20P.%20Block-Coordinate%20Frank-Wolfe%20Optimization%20for%20Structural%20SVMs%202013"
        },
        {
            "id": "28",
            "entry": "[28] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001, pages 282\u2013289, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lafferty%2C%20J.%20McCallum%2C%20A.%20Pereira%2C%20F.C.%20Conditional%20random%20fields%3A%20Probabilistic%20models%20for%20segmenting%20and%20labeling%20sequence%20data%202001-06-28",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lafferty%2C%20J.%20McCallum%2C%20A.%20Pereira%2C%20F.C.%20Conditional%20random%20fields%3A%20Probabilistic%20models%20for%20segmenting%20and%20labeling%20sequence%20data%202001-06-28"
        },
        {
            "id": "29",
            "entry": "[29] C. H. Lampert, M. B. Blaschko, and T. Hofmann. Beyond sliding windows: Object localization by efficient subwindow search. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138. IEEE, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampert%2C%20C.H.%20Blaschko%2C%20M.B.%20Hofmann%2C%20T.%20Beyond%20sliding%20windows%3A%20Object%20localization%20by%20efficient%20subwindow%20search%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampert%2C%20C.H.%20Blaschko%2C%20M.B.%20Hofmann%2C%20T.%20Beyond%20sliding%20windows%3A%20Object%20localization%20by%20efficient%20subwindow%20search%202008"
        },
        {
            "id": "30",
            "entry": "[30] N. Le Roux, M. W. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets. In Advances in Neural Information Processing Systems 25, pages 2672\u20132680, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20N.Le%20Schmidt%2C%20M.W.%20Bach%2C%20F.R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20strongly-convex%20optimization%20with%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20N.Le%20Schmidt%2C%20M.W.%20Bach%2C%20F.R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20strongly-convex%20optimization%20with%20finite%20training%20sets%202012"
        },
        {
            "id": "31",
            "entry": "[31] M. Lewis and M. Steedman. A* CCG parsing with a supertag-factored model. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 990\u20131000, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lewis%2C%20M.%20Steedman%2C%20M.%20A%2A%20CCG%20parsing%20with%20a%20supertag-factored%20model%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lewis%2C%20M.%20Steedman%2C%20M.%20A%2A%20CCG%20parsing%20with%20a%20supertag-factored%20model%202014"
        },
        {
            "id": "32",
            "entry": "[32] H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In Advances in Neural Information Processing Systems, pages 3384\u20133392, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015"
        },
        {
            "id": "33",
            "entry": "[33] H. Lin, J. Mairal, and Z. Harchaoui. Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice. Journal of Machine Learning Research, 18(212):1\u201354, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20Catalyst%20Acceleration%20for%20First-order%20Convex%20Optimization%3A%20from%20Theory%20to%20Practice%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20Catalyst%20Acceleration%20for%20First-order%20Convex%20Optimization%3A%20from%20Theory%20to%20Practice%202018"
        },
        {
            "id": "34",
            "entry": "[34] J. Mairal. Incremental majorization-minimization optimization with application to large-scale machine learning. SIAM Journal on Optimization, 25(2):829\u2013855, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20J.%20Incremental%20majorization-minimization%20optimization%20with%20application%20to%20large-scale%20machine%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20J.%20Incremental%20majorization-minimization%20optimization%20with%20application%20to%20large-scale%20machine%20learning%202015"
        },
        {
            "id": "35",
            "entry": "[35] A. F. T. Martins and R. F. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1614\u20131623, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martins%2C%20A.F.T.%20Astudillo%2C%20R.F.%20From%20softmax%20to%20sparsemax%3A%20A%20sparse%20model%20of%20attention%20and%20multi-label%20classification%202016-06-19",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martins%2C%20A.F.T.%20Astudillo%2C%20R.F.%20From%20softmax%20to%20sparsemax%3A%20A%20sparse%20model%20of%20attention%20and%20multi-label%20classification%202016-06-19"
        },
        {
            "id": "36",
            "entry": "[36] A. Mensch and M. Blondel. Differentiable dynamic programming for structured prediction and attention. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, pages 3459\u20133468, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mensch%2C%20A.%20Blondel%2C%20M.%20Differentiable%20dynamic%20programming%20for%20structured%20prediction%20and%20attention%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mensch%2C%20A.%20Blondel%2C%20M.%20Differentiable%20dynamic%20programming%20for%20structured%20prediction%20and%20attention%202018-07"
        },
        {
            "id": "37",
            "entry": "[37] O. Meshi, D. Sontag, T. S. Jaakkola, and A. Globerson. Learning efficiently with approximate inference via dual losses. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 783\u2013790, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meshi%2C%20O.%20Sontag%2C%20D.%20Jaakkola%2C%20T.S.%20Globerson%2C%20A.%20Learning%20efficiently%20with%20approximate%20inference%20via%20dual%20losses%202010-06-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meshi%2C%20O.%20Sontag%2C%20D.%20Jaakkola%2C%20T.S.%20Globerson%2C%20A.%20Learning%20efficiently%20with%20approximate%20inference%20via%20dual%20losses%202010-06-21"
        },
        {
            "id": "38",
            "entry": "[38] O. Meshi, T. S. Jaakkola, and A. Globerson. Convergence rate analysis of MAP coordinate minimization algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 3023\u20133031, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meshi%2C%20O.%20Jaakkola%2C%20T.S.%20Globerson%2C%20A.%20Convergence%20rate%20analysis%20of%20MAP%20coordinate%20minimization%20algorithms%202012-12-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meshi%2C%20O.%20Jaakkola%2C%20T.S.%20Globerson%2C%20A.%20Convergence%20rate%20analysis%20of%20MAP%20coordinate%20minimization%20algorithms%202012-12-03"
        },
        {
            "id": "39",
            "entry": "[39] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization, 16(1):235\u2013249, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Excessive%20gap%20technique%20in%20nonsmooth%20convex%20minimization%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Excessive%20gap%20technique%20in%20nonsmooth%20convex%20minimization%202005"
        },
        {
            "id": "40",
            "entry": "[40] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103 (1):127\u2013152, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Smooth%20minimization%20of%20non-smooth%20functions.%20Mathematical%20programming%2C%20103%202005"
        },
        {
            "id": "41",
            "entry": "[41] V. Niculae, A. F. Martins, M. Blondel, and C. Cardie. SparseMAP: Differentiable Sparse Structured Inference. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, pages 3796\u20133805, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niculae%2C%20V.%20Martins%2C%20A.F.%20Blondel%2C%20M.%20Cardie%2C%20C.%20SparseMAP%3A%20Differentiable%20Sparse%20Structured%20Inference%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niculae%2C%20V.%20Martins%2C%20A.F.%20Blondel%2C%20M.%20Cardie%2C%20C.%20SparseMAP%3A%20Differentiable%20Sparse%20Structured%20Inference%202018-07"
        },
        {
            "id": "42",
            "entry": "[42] D. Nilsson. An efficient algorithm for finding the M most probable configurationsin probabilistic expert systems. Statistics and computing, 8(2):159\u2013173, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nilsson%2C%20D.%20An%20efficient%20algorithm%20for%20finding%20the%20M%20most%20probable%20configurationsin%20probabilistic%20expert%20systems%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nilsson%2C%20D.%20An%20efficient%20algorithm%20for%20finding%20the%20M%20most%20probable%20configurationsin%20probabilistic%20expert%20systems%201998"
        },
        {
            "id": "43",
            "entry": "[43] A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. Dokania, and S. Lacoste-Julien. Minding the gaps for block Frank-Wolfe optimization of structured SVMs. In International Conference on Machine Learning, pages 593\u2013602, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osokin%2C%20A.%20Alayrac%2C%20J.-B.%20Lukasewitz%2C%20I.%20Dokania%2C%20P.%20Minding%20the%20gaps%20for%20block%20Frank-Wolfe%20optimization%20of%20structured%20SVMs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osokin%2C%20A.%20Alayrac%2C%20J.-B.%20Lukasewitz%2C%20I.%20Dokania%2C%20P.%20Minding%20the%20gaps%20for%20block%20Frank-Wolfe%20optimization%20of%20structured%20SVMs%202016"
        },
        {
            "id": "44",
            "entry": "[44] B. Palaniappan and F. Bach. Stochastic variance reduction methods for saddle-point problems. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1408\u2013 1416, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Palaniappan%2C%20B.%20Bach%2C%20F.%20Stochastic%20variance%20reduction%20methods%20for%20saddle-point%20problems%202016-12-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Palaniappan%2C%20B.%20Bach%2C%20F.%20Stochastic%20variance%20reduction%20methods%20for%20saddle-point%20problems%202016-12-05"
        },
        {
            "id": "45",
            "entry": "[45] K. Pillutla, V. Roulet, S. M. Kakade, and Z. Harchaoui. A smoother way to train structured prediction models. arXiv preprint, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pillutla%2C%20K.%20Roulet%2C%20V.%20Kakade%2C%20S.M.%20Harchaoui%2C%20Z.%20A%20smoother%20way%20to%20train%20structured%20prediction%20models.%20arXiv%20p%202018"
        },
        {
            "id": "46",
            "entry": "[46] N. D. Ratliff, J. A. Bagnell, and M. Zinkevich. (Approximate) Subgradient Methods for Structured Prediction. In International Conference on Artificial Intelligence and Statistics, pages 380\u2013387, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ratliff%2C%20N.D.%20Bagnell%2C%20J.A.%20Zinkevich%2C%20M.%20%28Approximate%29%20Subgradient%20Methods%20for%20Structured%20Prediction%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ratliff%2C%20N.D.%20Bagnell%2C%20J.A.%20Zinkevich%2C%20M.%20%28Approximate%29%20Subgradient%20Methods%20for%20Structured%20Prediction%202007"
        },
        {
            "id": "47",
            "entry": "[47] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "48",
            "entry": "[48] B. Savchynskyy, J. H. Kappes, S. Schmidt, and C. Schn\u00f6rr. A study of Nesterov\u2019s scheme for Lagrangian decomposition and MAP labeling. In The 24th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011, Colorado Springs, CO, USA, 20-25 June 2011, pages 1817\u20131823, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Savchynskyy%2C%20B.%20Kappes%2C%20J.H.%20Schmidt%2C%20S.%20Schn%C3%B6rr%2C%20C.%20A%20study%20of%20Nesterov%E2%80%99s%20scheme%20for%20Lagrangian%20decomposition%20and%20MAP%20labeling%202011-06-20",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Savchynskyy%2C%20B.%20Kappes%2C%20J.H.%20Schmidt%2C%20S.%20Schn%C3%B6rr%2C%20C.%20A%20study%20of%20Nesterov%E2%80%99s%20scheme%20for%20Lagrangian%20decomposition%20and%20MAP%20labeling%202011-06-20"
        },
        {
            "id": "49",
            "entry": "[49] M. Schmidt, R. Babanezhad, M. Ahmed, A. Defazio, A. Clifton, and A. Sarkar. Non-uniform stochastic average gradient method for training conditional random fields. In artificial intelligence and statistics, pages 819\u2013828, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20M.%20Babanezhad%2C%20R.%20Ahmed%2C%20M.%20Defazio%2C%20A.%20Non-uniform%20stochastic%20average%20gradient%20method%20for%20training%20conditional%20random%20fields%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20M.%20Babanezhad%2C%20R.%20Ahmed%2C%20M.%20Defazio%2C%20A.%20Non-uniform%20stochastic%20average%20gradient%20method%20for%20training%20conditional%20random%20fields%202015"
        },
        {
            "id": "50",
            "entry": "[50] M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83\u2013112, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017"
        },
        {
            "id": "51",
            "entry": "[51] B. Seroussi and J. Golmard. An algorithm directly finding the K most probable configurations in Bayesian networks. International Journal of Approximate Reasoning, 11(3):205 \u2013 233, 1994. ISSN 0888-613X. doi: https://doi.org/10.1016/0888-613X(94)90031-0.",
            "crossref": "https://dx.doi.org/10.1016/0888-613X(94)90031-0",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/0888-613X%2894%2990031-0"
        },
        {
            "id": "52",
            "entry": "[52] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "53",
            "entry": "[53] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In International Conference on Machine Learning, pages 64\u201372, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014"
        },
        {
            "id": "54",
            "entry": "[54] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical programming, 127(1):3\u201330, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Cotter%2C%20A.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20SVM%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Singer%2C%20Y.%20Srebro%2C%20N.%20Cotter%2C%20A.%20Pegasos%3A%20Primal%20estimated%20sub-gradient%20solver%20for%20SVM%202011"
        },
        {
            "id": "55",
            "entry": "[55] H. O. Song, R. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui, and T. Darrell. On learning to localize objects with minimal supervision. arXiv preprint arXiv:1403.1024, pages 1611\u20131619, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1403.1024"
        },
        {
            "id": "56",
            "entry": "[56] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Advances in Neural Information Processing Systems, pages 25\u201332, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taskar%2C%20B.%20Guestrin%2C%20C.%20Koller%2C%20D.%20Max-margin%20Markov%20networks%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taskar%2C%20B.%20Guestrin%2C%20C.%20Koller%2C%20D.%20Max-margin%20Markov%20networks%202004"
        },
        {
            "id": "57",
            "entry": "[57] B. Taskar, S. Lacoste-Julien, and D. Klein. A discriminative matching approach to word alignment. In HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, 6-8 October 2005, Vancouver, British Columbia, Canada, pages 73\u201380, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taskar%2C%20B.%20Lacoste-Julien%2C%20S.%20Klein%2C%20D.%20A%20discriminative%20matching%20approach%20to%20word%20alignment%202005-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taskar%2C%20B.%20Lacoste-Julien%2C%20S.%20Klein%2C%20D.%20A%20discriminative%20matching%20approach%20to%20word%20alignment%202005-10"
        },
        {
            "id": "58",
            "entry": "[58] B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and Bregman projections. Journal of Machine Learning Research, 7(Jul):1627\u20131653, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taskar%2C%20B.%20Lacoste-Julien%2C%20S.%20Jordan%2C%20M.I.%20Structured%20prediction%2C%20dual%20extragradient%20and%20Bregman%20projections%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taskar%2C%20B.%20Lacoste-Julien%2C%20S.%20Jordan%2C%20M.I.%20Structured%20prediction%2C%20dual%20extragradient%20and%20Bregman%20projections%202006"
        },
        {
            "id": "59",
            "entry": "[59] C. H. Teo, S. Vishwanathan, A. Smola, and Q. V. Le. Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 1(55), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teo%2C%20C.H.%20Vishwanathan%2C%20S.%20Smola%2C%20A.%20Le%2C%20Q.V.%20Bundle%20methods%20for%20regularized%20risk%20minimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teo%2C%20C.H.%20Vishwanathan%2C%20S.%20Smola%2C%20A.%20Le%2C%20Q.V.%20Bundle%20methods%20for%20regularized%20risk%20minimization%202009"
        },
        {
            "id": "60",
            "entry": "[60] E. F. Tjong Kim Sang and F. De Meulder. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 142\u2013147. Association for Computational Linguistics, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sang%2C%20E.F.Tjong%20Kim%20Meulder%2C%20F.De%20Introduction%20to%20the%20conll-2003%20shared%20task%3A%20Languageindependent%20named%20entity%20recognition%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sang%2C%20E.F.Tjong%20Kim%20Meulder%2C%20F.De%20Introduction%20to%20the%20conll-2003%20shared%20task%3A%20Languageindependent%20named%20entity%20recognition%202003"
        },
        {
            "id": "61",
            "entry": "[61] M. Tkachenko and A. Simanovsky. Named entity recognition: Exploring features. In 11th Conference on Natural Language Processing, KONVENS 2012, Empirical Methods in Natural Language Processing, Vienna, Austria, September 19-21, 2012, pages 118\u2013127, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tkachenko%2C%20M.%20Simanovsky%2C%20A.%20Named%20entity%20recognition%3A%20Exploring%20features%202012-09-19",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tkachenko%2C%20M.%20Simanovsky%2C%20A.%20Named%20entity%20recognition%3A%20Exploring%20features%202012-09-19"
        },
        {
            "id": "62",
            "entry": "[62] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the 21st International Conference on Machine Learning, page 104. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsochantaridis%2C%20I.%20Hofmann%2C%20T.%20Joachims%2C%20T.%20Altun%2C%20Y.%20Support%20vector%20machine%20learning%20for%20interdependent%20and%20structured%20output%20spaces%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsochantaridis%2C%20I.%20Hofmann%2C%20T.%20Joachims%2C%20T.%20Altun%2C%20Y.%20Support%20vector%20machine%20learning%20for%20interdependent%20and%20structured%20output%20spaces%202004"
        },
        {
            "id": "63",
            "entry": "[63] K. E. Van de Sande, J. R. Uijlings, T. Gevers, and A. W. Smeulders. Segmentation as selective search for object recognition. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 1879\u20131886. IEEE, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20Sande%2C%20K.E.Van%20Uijlings%2C%20J.R.%20Gevers%2C%20T.%20W%2C%20A.%20Smeulders.%20Segmentation%20as%20selective%20search%20for%20object%20recognition%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20Sande%2C%20K.E.Van%20Uijlings%2C%20J.R.%20Gevers%2C%20T.%20W%2C%20A.%20Smeulders.%20Segmentation%20as%20selective%20search%20for%20object%20recognition%202011"
        },
        {
            "id": "64",
            "entry": "[64] B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives. In Advances in Neural Information Processing Systems, pages 3639\u20133647, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodworth%2C%20B.E.%20Srebro%2C%20N.%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodworth%2C%20B.E.%20Srebro%2C%20N.%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016"
        },
        {
            "id": "65",
            "entry": "[65] C. Yanover and Y. Weiss. Finding the M most probable configurations using loopy belief propagation. In Advances in Neural Information Processing Systems, pages 289\u2013296, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yanover%2C%20C.%20Weiss%2C%20Y.%20Finding%20the%20M%20most%20probable%20configurations%20using%20loopy%20belief%20propagation%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yanover%2C%20C.%20Weiss%2C%20Y.%20Finding%20the%20M%20most%20probable%20configurations%20using%20loopy%20belief%20propagation%202004"
        },
        {
            "id": "66",
            "entry": "[66] X. Zhang, A. Saha, and S. Vishwanathan. Accelerated training of max-margin markov networks with kernels. Theoretical Computer Science, 519:88\u2013102, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20X.%20Saha%2C%20A.%20Vishwanathan%2C%20S.%20Accelerated%20training%20of%20max-margin%20markov%20networks%20with%20kernels%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20X.%20Saha%2C%20A.%20Vishwanathan%2C%20S.%20Accelerated%20training%20of%20max-margin%20markov%20networks%20with%20kernels%202014"
        }
    ]
}
