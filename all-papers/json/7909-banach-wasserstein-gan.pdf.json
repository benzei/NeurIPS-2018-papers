{
    "filename": "7909-banach-wasserstein-gan.pdf",
    "metadata": {
        "title": "Banach Wasserstein GAN",
        "author": "Jonas Adler, Sebastian Lunz",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7909-banach-wasserstein-gan.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered 2 as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA."
    },
    "keywords": [
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        },
        {
            "term": "Jensen\u2013Shannon divergence",
            "url": "https://en.wikipedia.org/wiki/Jensen_Shannon_divergence"
        },
        {
            "term": "image feature",
            "url": "https://en.wikipedia.org/wiki/image_feature"
        },
        {
            "term": "banach space",
            "url": "https://en.wikipedia.org/wiki/banach_space"
        }
    ],
    "highlights": [
        "Generative adversarial networks<br/><br/>Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] perform generative modeling by learning a map G : Z \u2192 B from a low-dimensional latent space Z to image space B, mapping a fixed noise distribution PZ to a distribution of generated images PG.<br/><br/>In order to train the generative model G, a second network D is used to discriminate between original images drawn from a distribution of real images Pr and images drawn from PG",
        "In this work we extend the classical Wasserstein Generative Adversarial Networks theory to work on these and more general Banach spaces",
        "We introduce Banach Wasserstein Generative Adversarial Networks (BWGAN), extending Wasserstein Generative Adversarial Networks implemented via a gradient penalty (GP) term to any separable complete normed space",
        "We describe how Banach Wasserstein GAN can be efficiently implemented",
        "We introduced a generalization of Wasserstein Generative Adversarial Networks with gradient norm penalization to Banach spaces, allowing to implement Wasserstein Generative Adversarial Networks for a wide range of underlying norms on images",
        "On the CIFAR-10 and CelebA dataset, we demonstrated the impact a change in norm has on model performance"
    ],
    "key_statements": [
        "Generative adversarial networks<br/><br/>Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] perform generative modeling by learning a map G : Z \u2192 B from a low-dimensional latent space Z to image space B, mapping a fixed noise distribution PZ to a distribution of generated images PG.<br/><br/>In order to train the generative model G, a second network D is used to discriminate between original images drawn from a distribution of real images Pr and images drawn from PG",
        "In this work we extend the classical Wasserstein Generative Adversarial Networks theory to work on these and more general Banach spaces",
        "We introduce Banach Wasserstein Generative Adversarial Networks (BWGAN), extending Wasserstein Generative Adversarial Networks implemented via a gradient penalty (GP) term to any separable complete normed space",
        "We describe how Banach Wasserstein GAN can be efficiently implemented",
        "In terms of computational costs, the difference between general Banach Wasserstein Generative Adversarial Networks and the ones based on the 2 metric lies in the computation of the gradient of the dual norm",
        "We introduced a generalization of Wasserstein Generative Adversarial Networks with gradient norm penalization to Banach spaces, allowing to implement Wasserstein Generative Adversarial Networks for a wide range of underlying norms on images",
        "This opens up a new degree of freedom to design the algorithm to account for the image features relevant in a specific application",
        "On the CIFAR-10 and CelebA dataset, we demonstrated the impact a change in norm has on model performance"
    ],
    "summary": [
        "Generative adversarial networks<br/><br/>Generative Adversarial Networks (GANs) [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] perform generative modeling by learning a map G : Z \u2192 B from a low-dimensional latent space Z to image space B, mapping a fixed noise distribution PZ to a distribution of generated images PG.<br/><br/>In order to train the generative model G, a second network D is used to discriminate between original images drawn from a distribution of real images Pr and images drawn from PG.",
        "We introduce Banach Wasserstein GAN (BWGAN), extending WGAN implemented via a gradient penalty (GP) term to any separable complete normed space.",
        "Since taking the gradient is equivalent to multiplying with \u03be in the Fourier space, the concept of Sobolev spaces can be generalized to arbitrary derivative orders s if we use the norm p",
        "We generalize the loss (4) to separable Banach spaces, allowing us to effectively train a Wasserstein GAN using arbitrary norms.",
        "We will show that the characterization of \u03b3-Lipschitz functions via the norm of the differential can be extended from the 2 setting in (4) to arbitrary Banach spaces by considering the gradient as an element in the dual of B.",
        "For any Banach space B with norm \u00b7 B, we will derive the loss function",
        "The following theorem allows us to characterize all Lipschitz continuous functions according to the dual norm of the Fr\u00e9chet derivative.",
        "Using lemma 1 we see that a \u03b3-Lipschitz requirement in Banach spaces is equivalent to the dual norm of the Fr\u00e9chet derivative being less than \u03b3 everywhere.",
        "As shown in section 2.4, the dual norm can be readily computed for a range of interesting Banach spaces, but we need to compute \u2202f (x), preferably using readily available automatic differentiation software.",
        "In terms of computational costs, the difference between general Banach Wasserstein GANs and the ones based on the 2 metric lies in the computation of the gradient of the dual norm.",
        "If there is an efficient framework available to compute the gradient of the dual norm, as in the case of the Fourier transform used for Sobolev spaces, the computational expenses stay essentially the same independent of the choice of norm.",
        "To demonstrate computational feasibility and to show how the choice of norm can impact the trained generator, we implemented Banach Wasserstein GAN with various Sobolev and Lp norms, applied to CIFAR-10 and CelebA (64 \u00d7 64 pixels).",
        "This would potentially allow training Wasserstein GAN on general metric spaces by adding a penalty term of the form",
        "We introduced a generalization of WGANs with gradient norm penalization to Banach spaces, allowing to implement WGANs for a wide range of underlying norms on images.",
        "We computed FID scores for Banach Wasserstein GANs using different Sobolev spaces W s,p and found a correlation between the values of both s and p with model performance."
    ],
    "headline": "We demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Arjovsky and L\u00e9on Bottou. Towards Principled Methods for Training Generative Adversarial Networks. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20Mart%C3%ADn%20Bottou%2C%20L%C3%A9on%20Towards%20Principled%20Methods%20for%20Training%20Generative%20Adversarial%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20Mart%C3%ADn%20Bottou%2C%20L%C3%A9on%20Towards%20Principled%20Methods%20for%20Training%20Generative%20Adversarial%20Networks%202017"
        },
        {
            "id": "2",
            "entry": "[2] Mart\u00edn Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein Generative Adversarial Networks. International Conference on Machine Learning, ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mart%C3%ADn%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20Generative%20Adversarial%20Networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mart%C3%ADn%20Arjovsky%2C%20Soumith%20Chintala%20Bottou%2C%20L%C3%A9on%20Wasserstein%20Generative%20Adversarial%20Networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] Haim Brezis. Functional analysis, Sobolev spaces and partial differential equations. Springer Science & Business Media, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brezis%2C%20Haim%20Functional%20analysis%2C%20Sobolev%20spaces%20and%20partial%20differential%20equations%202010"
        },
        {
            "id": "4",
            "entry": "[4] Tony F Chan and Jianhong Jackie Shen. Image processing and analysis: variational, PDE, wavelet, and stochastic methods, volume 94.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20Tony%20F.%20Shen%2C%20Jianhong%20Jackie%20Image%20processing%20and%20analysis%3A%20variational%2C%20PDE%2C%20wavelet%2C%20and%20stochastic",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20Tony%20F.%20Shen%2C%20Jianhong%20Jackie%20Image%20processing%20and%20analysis%3A%20variational%2C%20PDE%2C%20wavelet%2C%20and%20stochastic"
        },
        {
            "id": "5",
            "entry": "[5] Michel Marie Deza and Elena Deza. Encyclopedia of Distances. Springer Berlin Heidelberg, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deza%2C%20Michel%20Marie%20Deza%2C%20Elena%20Encyclopedia%20of%20Distances%202009"
        },
        {
            "id": "6",
            "entry": "[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20Pouget-Abadie%2C%20Jean%20Mirza%2C%20Mehdi%20Xu%2C%20Bing%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "7",
            "entry": "[7] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "8",
            "entry": "[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6626\u20136637. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20Gans%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20local%20nash%20equilibrium%202017"
        },
        {
            "id": "9",
            "entry": "[9] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive Growing of GANs for Improved Quality, Stability, and Variation. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20Growing%20of%20GANs%20for%20Improved%20Quality%2C%20Stability%2C%20and%20Variation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karras%2C%20Tero%20Aila%2C%20Timo%20Laine%2C%20Samuli%20Lehtinen%2C%20Jaakko%20Progressive%20Growing%20of%20GANs%20for%20Improved%20Quality%2C%20Stability%2C%20and%20Variation%202018"
        },
        {
            "id": "10",
            "entry": "[10] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20Method%20for%20Stochastic%20Optimization%202015"
        },
        {
            "id": "11",
            "entry": "[11] Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and Convergence Properties of Generative Adversarial Learning. arXiv, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Shuang%20Bousquet%2C%20Olivier%20Chaudhuri%2C%20Kamalika%20Approximation%20and%20Convergence%20Properties%20of%20Generative%20Adversarial%20Learning%202017"
        },
        {
            "id": "12",
            "entry": "[12] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient descent with Warm Restarts. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20SGDR%3A%20Stochastic%20Gradient%20descent%20with%20Warm%20Restarts%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Loshchilov%2C%20Ilya%20Hutter%2C%20Frank%20SGDR%3A%20Stochastic%20Gradient%20descent%20with%20Warm%20Restarts%202017"
        },
        {
            "id": "13",
            "entry": "[13] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. IEEE International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20Xudong%20Li%2C%20Qing%20Xie%2C%20Haoran%20Lau%2C%20Raymond%20Y.K.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "14",
            "entry": "[14] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral Normalization for Generative Adversarial Networks. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20Normalization%20for%20Generative%20Adversarial%20Networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20Takeru%20Kataoka%2C%20Toshiki%20Koyama%2C%20Masanori%20Yoshida%2C%20Yuichi%20Spectral%20Normalization%20for%20Generative%20Adversarial%20Networks%202018"
        },
        {
            "id": "15",
            "entry": "[15] Henning Petzka, Asja Fischer, and Denis Lukovnikov. On the regularization of Wasserstein GANs. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Petzka%2C%20Henning%20Fischer%2C%20Asja%20Lukovnikov%2C%20Denis%20On%20the%20regularization%20of%20Wasserstein%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Petzka%2C%20Henning%20Fischer%2C%20Asja%20Lukovnikov%2C%20Denis%20On%20the%20regularization%20of%20Wasserstein%20GANs%202018"
        },
        {
            "id": "16",
            "entry": "[16] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20Representation%20Learning%20with%20Deep%20Convolutional%20Generative%20Adversarial%20Networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Radford%2C%20Alec%20Metz%2C%20Luke%20Chintala%2C%20Soumith%20Unsupervised%20Representation%20Learning%20with%20Deep%20Convolutional%20Generative%20Adversarial%20Networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] Walter Rudin. Functional analysis. International series in pure and applied mathematics, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudin%2C%20Walter%20Functional%20analysis.%20International%20series%20in%20pure%20and%20applied%20mathematics%201991"
        },
        {
            "id": "18",
            "entry": "[18] Calvin Seward, Thomas Unterthiner, Urs Bergmann, Nikolay Jetchev, and Sepp Hochreiter. First Order Generative Adversarial Networks. arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calvin%20Seward%20Thomas%20Unterthiner%20Urs%20Bergmann%20Nikolay%20Jetchev%20and%20Sepp%20Hochreiter%20First%20Order%20Generative%20Adversarial%20Networks%20arXiv%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calvin%20Seward%20Thomas%20Unterthiner%20Urs%20Bergmann%20Nikolay%20Jetchev%20and%20Sepp%20Hochreiter%20First%20Order%20Generative%20Adversarial%20Networks%20arXiv%202018"
        },
        {
            "id": "19",
            "entry": "[19] C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C%C3%A9dric%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338"
        },
        {
            "id": "20",
            "entry": "[20] Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing Gong. Improving the Improved Training of Wasserstein GANs. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Xiang%20Liu%2C%20Zixia%20Wang%2C%20Liqiang%20Gong%2C%20Boqing%20Improving%20the%20Improved%20Training%20of%20Wasserstein%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Xiang%20Liu%2C%20Zixia%20Wang%2C%20Liqiang%20Gong%2C%20Boqing%20Improving%20the%20Improved%20Training%20of%20Wasserstein%20GANs%202018"
        },
        {
            "id": "21",
            "entry": "[21] Junbo Jake Zhao, Micha\u00ebl Mathieu, and Yann LeCun. Energy-based Generative Adversarial Network. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Junbo%20Jake%20Zhao%2C%20Micha%C3%ABl%20Mathieu%20LeCun%2C%20Yann%20Energy-based%20Generative%20Adversarial%20Network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Junbo%20Jake%20Zhao%2C%20Micha%C3%ABl%20Mathieu%20LeCun%2C%20Yann%20Energy-based%20Generative%20Adversarial%20Network%202017"
        }
    ]
}
