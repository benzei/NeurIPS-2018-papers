{
    "filename": "7896-an-improved-analysis-of-alternating-minimization-for-structured-multi-response-regression.pdf",
    "metadata": {
        "title": "An Improved Analysis of Alternating Minimization for Structured Multi-Response Regression",
        "author": "Sheng Chen, Arindam Banerjee",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7896-an-improved-analysis-of-alternating-minimization-for-structured-multi-response-regression.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Multi-response linear models aggregate a set of vanilla linear models by assuming correlated noise across them, which has an unknown covariance structure. To find the coefficient vector, estimators with a joint approximation of the noise covariance are often preferred than the simple linear regression in view of their superior empirical performance, which can be generally solved by alternating-minimizationtype procedures. Due to the non-convex nature of such joint estimators, the theoretical justification of their efficiency is typically challenging. The existing analyses fail to fully explain the empirical observations due to the assumption of resampling on the alternating procedures, which requires access to fresh samples in each iteration. In this work, we present a resampling-free analysis for the alternating minimization algorithm applied to the multi-response regression. In particular, we focus on the high-dimensional setting of multi-response linear models with structured coefficient parameter, and the statistical error of the parameter can be expressed by the complexity measure, Gaussian width, which is related to the assumed structure. More importantly, to the best of our knowledge, our result reveals for the first time that the alternating minimization with random initialization can achieve the same performance as the well-initialized one when solving this multi-response regression problem. Experimental results support our theoretical developments."
    },
    "keywords": [
        {
            "term": "ordinary least squares",
            "url": "https://en.wikipedia.org/wiki/ordinary_least_squares"
        },
        {
            "term": "positive semidefinite",
            "url": "https://en.wikipedia.org/wiki/positive_semidefinite"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "linear regression",
            "url": "https://en.wikipedia.org/wiki/linear_regression"
        },
        {
            "term": "maximum likelihood estimator",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimator"
        }
    ],
    "highlights": [
        "The alternating minimization algorithm for multi-response regression was initially proposed by [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "The m responses share the same underlying parameter \u03b8\u2217 \u2208 Rp, which corresponds to the so-called pooled model [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "The statistical guarantees for non-convex alternating minimization procedures are often shown under the resampling assumption, which assumes that each iteration receives a fresh sample",
        "For structured multi-response regression, we will focus on the alternating minimization procedure shown in Algorithm 1",
        "We investigate the alternating minimization (AltMin) algorithm for high-dimensional multi-response linear models, which allow general structures of the underlying parameter",
        "The error bounds suggest that the arbitrarily-initialized alternating minimization is able to attain the same level of estimation error as the one with good initializations"
    ],
    "key_statements": [
        "The alternating minimization algorithm for multi-response regression was initially proposed by [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "The m responses share the same underlying parameter \u03b8\u2217 \u2208 Rp, which corresponds to the so-called pooled model [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>]",
        "The statistical guarantees for non-convex alternating minimization procedures are often shown under the resampling assumption, which assumes that each iteration receives a fresh sample",
        "We aim at a better way to bound the statistical error of the above alternating minimization procedure for general structure-inducing f",
        "In Section 3, we present the statistical guarantees for the alternating minimization algorithm under suitable probabilistic assumptions",
        "The basic idea for showing the statistical guarantees of alternating minimization is to derive the statistical error bounds for both the aand b-steps when the other parameter is fixed to the latest estimate",
        "For structured multi-response regression, we will focus on the alternating minimization procedure shown in Algorithm 1",
        "We investigate the alternating minimization (AltMin) algorithm for high-dimensional multi-response linear models, which allow general structures of the underlying parameter",
        "The error bounds suggest that the arbitrarily-initialized alternating minimization is able to attain the same level of estimation error as the one with good initializations"
    ],
    "summary": [
        "The AltMin algorithm for multi-response regression was initially proposed by [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>].",
        "The statistical guarantees for non-convex AltMin procedures are often shown under the resampling assumption, which assumes that each iteration receives a fresh sample.",
        "We aim at a better way to bound the statistical error of the above AltMin procedure for general structure-inducing f .",
        "The error for good initializations matches the resampling-based result up to some constant, which requires more fresh data to achieve such a bound.",
        "In Section 2, we outline our strategy for combating nonconvexity and present the algorithmic details of the AltMin procedure for structured multi-response regression.",
        "For many statistical estimation problems, we can construct the estimator of the underlying model parameter w\u2217, by minimizing certain loss function on the given sample D, w = argmin L(w; D) .",
        "The basic idea for showing the statistical guarantees of AltMin is to derive the statistical error bounds for both the aand b-steps when the other parameter is fixed to the latest estimate.",
        "The resampling-based result states that with any fixed b(t) (a(t+1)), given a fresh sample D(t) independent of b(t) (a(t+1)), the iterate a(t+1) (b(t+1)) satisfies the corresponding bound in (10) with high probability.",
        "For structured multi-response regression, we will focus on the AltMin procedure shown in Algorithm 1.",
        "The following theorem characterizes the sharpened error of AltMin under good initializations.",
        "The initialization condition is a result of setting a small value of e0, which yields an improved version of Lemma 2 so that we can obtain a better bound in Theorem 2.",
        "A reasonably good initialization of \u03b8(0) can be obtained by solving OLS \u03b8odn, whose error bound is given (27).",
        "The iterates obtained by running arbitrarily-initialized AltMin may satisfy the initialization requirements as Theorem 1 guarantees a moderate error.",
        "We run the AltMin initialized by both OLS and Gaussian random vector, where \u03b8-step is solved by the hard-thresholding pursuit (HTP) algorithm [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "We investigate the alternating minimization (AltMin) algorithm for high-dimensional multi-response linear models, which allow general structures of the underlying parameter.",
        "We present a resampling-free analysis for the statistical error of the non-convex AltMin procedure.",
        "Our error bound matches the resampling-based result up to some constant, which is of the same order as the oracle estimator.",
        "The error bounds suggest that the arbitrarily-initialized AltMin is able to attain the same level of estimation error as the one with good initializations"
    ],
    "headline": "We present a resampling-free analysis for the alternating minimization algorithm applied to the multi-response regression",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] T. W. Anderson. An introduction to multivariate statistical analysis. Wiley-Interscience, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anderson%2C%20T.W.%20An%20introduction%20to%20multivariate%20statistical%20analysis%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anderson%2C%20T.W.%20An%20introduction%20to%20multivariate%20statistical%20analysis%202003"
        },
        {
            "id": "2",
            "entry": "[2] A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20A.%20Chen%2C%20S.%20Fazayeli%2C%20F.%20Sivakumar%2C%20V.%20Estimation%20with%20norm%20regularization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20A.%20Chen%2C%20S.%20Fazayeli%2C%20F.%20Sivakumar%2C%20V.%20Estimation%20with%20norm%20regularization%202014"
        },
        {
            "id": "3",
            "entry": "[3] R. Barber and W. Ha. Gradient descent with nonconvex constraints: local concavity determines convergence. arXiv preprint arXiv:1703.07755, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07755"
        },
        {
            "id": "4",
            "entry": "[4] S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pages 3873\u20133881, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhojanapalli%2C%20S.%20Neyshabur%2C%20B.%20Srebro%2C%20N.%20Global%20optimality%20of%20local%20search%20for%20low%20rank%20matrix%20recovery%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhojanapalli%2C%20S.%20Neyshabur%2C%20B.%20Srebro%2C%20N.%20Global%20optimality%20of%20local%20search%20for%20low%20rank%20matrix%20recovery%202016"
        },
        {
            "id": "5",
            "entry": "[5] L. Breiman and J. H. Friedman. Predicting multivariate responses in multiple linear regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 59(1):3\u201354, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20L.%20Friedman%2C%20J.H.%20Predicting%20multivariate%20responses%20in%20multiple%20linear%20regression%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20L.%20Friedman%2C%20J.H.%20Predicting%20multivariate%20responses%20in%20multiple%20linear%20regression%201997"
        },
        {
            "id": "6",
            "entry": "[6] E. J Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985\u20132007, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Candes%2C%20E.J.%20Li%2C%20X.%20Soltanolkotabi%2C%20M.%20Phase%20retrieval%20via%20wirtinger%20flow%3A%20Theory%20and%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Candes%2C%20E.J.%20Li%2C%20X.%20Soltanolkotabi%2C%20M.%20Phase%20retrieval%20via%20wirtinger%20flow%3A%20Theory%20and%20algorithms%202015"
        },
        {
            "id": "7",
            "entry": "[7] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805\u2013849, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chandrasekaran%2C%20V.%20Recht%2C%20B.%20Parrilo%2C%20P.A.%20Willsky%2C%20A.S.%20The%20convex%20geometry%20of%20linear%20inverse%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chandrasekaran%2C%20V.%20Recht%2C%20B.%20Parrilo%2C%20P.A.%20Willsky%2C%20A.S.%20The%20convex%20geometry%20of%20linear%20inverse%20problems%202012"
        },
        {
            "id": "8",
            "entry": "[8] S. Chatterjee, S. Chen, and A. Banerjee. Generalized dantzig selector: Application to the k-support norm. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chatterjee%2C%20S.%20Chen%2C%20S.%20Banerjee%2C%20A.%20Generalized%20dantzig%20selector%3A%20Application%20to%20the%20k-support%20norm%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chatterjee%2C%20S.%20Chen%2C%20S.%20Banerjee%2C%20A.%20Generalized%20dantzig%20selector%3A%20Application%20to%20the%20k-support%20norm%202014"
        },
        {
            "id": "9",
            "entry": "[9] S. Chen and A. Banerjee. Structured estimation with atomic norms: General bounds and applications. In NIPS, pages 2908\u20132916, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20S.%20Banerjee%2C%20A.%20Structured%20estimation%20with%20atomic%20norms%3A%20General%20bounds%20and%20applications%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20S.%20Banerjee%2C%20A.%20Structured%20estimation%20with%20atomic%20norms%3A%20General%20bounds%20and%20applications%202015"
        },
        {
            "id": "10",
            "entry": "[10] S. Chen and A. Banerjee. Alternating estimation for structured high-dimensional multi-response models. In Advances in Neural Information Processing Systems, pages 2835\u20132844, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20S.%20Banerjee%2C%20A.%20Alternating%20estimation%20for%20structured%20high-dimensional%20multi-response%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20S.%20Banerjee%2C%20A.%20Alternating%20estimation%20for%20structured%20high-dimensional%20multi-response%20models%202017"
        },
        {
            "id": "11",
            "entry": "[11] S. Foucart. Hard thresholding pursuit: an algorithm for compressive sensing. SIAM Journal on Numerical Analysis, 49(6):2543\u20132563, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foucart%2C%20S.%20Hard%20thresholding%20pursuit%3A%20an%20algorithm%20for%20compressive%20sensing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foucart%2C%20S.%20Hard%20thresholding%20pursuit%3A%20an%20algorithm%20for%20compressive%20sensing%202011"
        },
        {
            "id": "12",
            "entry": "[12] R. Ge, C. Jin, and Y. Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00708"
        },
        {
            "id": "13",
            "entry": "[13] R. Ge, J. D Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pages 2973\u20132981, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Lee%2C%20J.D.%20Ma%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Lee%2C%20J.D.%20Ma%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016"
        },
        {
            "id": "14",
            "entry": "[14] A. Goncalves, P. Das, S. Chatterjee, V. Sivakumar, F. J. Von Zuben, and A. Banerjee. Multi-task sparse structure learning. In CIKM, pages 451\u2013460, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goncalves%2C%20A.%20Das%2C%20P.%20Chatterjee%2C%20S.%20Sivakumar%2C%20V.%20Multi-task%20sparse%20structure%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goncalves%2C%20A.%20Das%2C%20P.%20Chatterjee%2C%20S.%20Sivakumar%2C%20V.%20Multi-task%20sparse%20structure%20learning%202014"
        },
        {
            "id": "15",
            "entry": "[15] A. Goncalves, F. J Von Zuben, and A. Banerjee. Multi-task sparse structure learning with gaussian copula models. The Journal of Machine Learning Research, 17(1):1205\u20131234, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goncalves%2C%20A.%20Zuben%2C%20F.J.Von%20Banerjee%2C%20A.%20Multi-task%20sparse%20structure%20learning%20with%20gaussian%20copula%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goncalves%2C%20A.%20Zuben%2C%20F.J.Von%20Banerjee%2C%20A.%20Multi-task%20sparse%20structure%20learning%20with%20gaussian%20copula%20models%202016"
        },
        {
            "id": "16",
            "entry": "[16] Y. Gordon. Some inequalities for gaussian processes and applications. Israel Journal of Mathematics, 50(4):265\u2013289, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gordon%2C%20Y.%20Some%20inequalities%20for%20gaussian%20processes%20and%20applications%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gordon%2C%20Y.%20Some%20inequalities%20for%20gaussian%20processes%20and%20applications%201985"
        },
        {
            "id": "17",
            "entry": "[17] W. H. Greene. Econometric Analysis. Prentice Hall, 7. edition, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Greene%2C%20W.H.%20Econometric%20Analysis%202011"
        },
        {
            "id": "18",
            "entry": "[18] A. J. Izenman. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Izenman%2C%20A.J.%20Modern%20Multivariate%20Statistical%20Techniques%3A%20Regression%2C%20Classification%2C%20and%20Manifold%20Learning%202008"
        },
        {
            "id": "19",
            "entry": "[19] P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating minimization. In STOC, pages 665\u2013674, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20P.%20Netrapalli%2C%20P.%20Sanghavi%2C%20S.%20Low-rank%20matrix%20completion%20using%20alternating%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20P.%20Netrapalli%2C%20P.%20Sanghavi%2C%20S.%20Low-rank%20matrix%20completion%20using%20alternating%20minimization%202013"
        },
        {
            "id": "20",
            "entry": "[20] P. Jain and A. Tewari. Alternating minimization for regression problems with vector-valued outputs. In Advances in Neural Information Processing Systems (NIPS), pages 1126\u20131134, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20P.%20Tewari%2C%20A.%20Alternating%20minimization%20for%20regression%20problems%20with%20vector-valued%20outputs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20P.%20Tewari%2C%20A.%20Alternating%20minimization%20for%20regression%20problems%20with%20vector-valued%20outputs%202015"
        },
        {
            "id": "21",
            "entry": "[21] P. Jain, A. Tewari, and P. Kar. On iterative hard thresholding methods for high-dimensional m-estimation. In NIPS, pages 685\u2013693, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20P.%20Tewari%2C%20A.%20Kar%2C%20P.%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20P.%20Tewari%2C%20A.%20Kar%2C%20P.%20On%20iterative%20hard%20thresholding%20methods%20for%20high-dimensional%20m-estimation%202014"
        },
        {
            "id": "22",
            "entry": "[22] S. Kim and E. P. Xing. Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping. Ann. Appl. Stat., 6(3):1095\u20131117, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20S.%20Xing%2C%20E.P.%20Tree-guided%20group%20lasso%20for%20multi-response%20regression%20with%20structured%20sparsity%2C%20with%20an%20application%20to%20eqtl%20mapping%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20S.%20Xing%2C%20E.P.%20Tree-guided%20group%20lasso%20for%20multi-response%20regression%20with%20structured%20sparsity%2C%20with%20an%20application%20to%20eqtl%20mapping%202012"
        },
        {
            "id": "23",
            "entry": "[23] W. Lee and Y. Liu. Simultaneous multiple response regression and inverse covariance matrix estimation via penalized gaussian maximum likelihood. J. Multivar. Anal., 111:241\u2013255, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20W.%20Liu%2C%20Y.%20Simultaneous%20multiple%20response%20regression%20and%20inverse%20covariance%20matrix%20estimation%20via%20penalized%20gaussian%20maximum%20likelihood%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20W.%20Liu%2C%20Y.%20Simultaneous%20multiple%20response%20regression%20and%20inverse%20covariance%20matrix%20estimation%20via%20penalized%20gaussian%20maximum%20likelihood%202012"
        },
        {
            "id": "24",
            "entry": "[24] Q. Li and G. Tang. The nonconvex geometry of low-rank matrix optimizations with general objective functions. arXiv preprint arXiv:1611.03060, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03060"
        },
        {
            "id": "25",
            "entry": "[25] C. Ma, K. Wang, Y. Chi, and Y. Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10467"
        },
        {
            "id": "26",
            "entry": "[26] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for the analysis of regularized M -estimators. Statistical Science, 27(4):538\u2013557, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20S.%20Ravikumar%2C%20P.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20A%20unified%20framework%20for%20the%20analysis%20of%20regularized%20M%20-estimators%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20S.%20Ravikumar%2C%20P.%20Wainwright%2C%20M.J.%20Yu%2C%20B.%20A%20unified%20framework%20for%20the%20analysis%20of%20regularized%20M%20-estimators%202012"
        },
        {
            "id": "27",
            "entry": "[27] P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netrapalli%2C%20P.%20Jain%2C%20P.%20Sanghavi%2C%20S.%20Phase%20retrieval%20using%20alternating%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netrapalli%2C%20P.%20Jain%2C%20P.%20Sanghavi%2C%20S.%20Phase%20retrieval%20using%20alternating%20minimization%202013"
        },
        {
            "id": "28",
            "entry": "[28] W. Oberhofer and J. Kmenta. A general procedure for obtaining maximum likelihood estimates in generalized regression models. Econometrica: Journal of the Econometric Society, pages 579\u2013590, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oberhofer%2C%20W.%20Kmenta%2C%20J.%20A%20general%20procedure%20for%20obtaining%20maximum%20likelihood%20estimates%20in%20generalized%20regression%20models%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oberhofer%2C%20W.%20Kmenta%2C%20J.%20A%20general%20procedure%20for%20obtaining%20maximum%20likelihood%20estimates%20in%20generalized%20regression%20models%201974"
        },
        {
            "id": "29",
            "entry": "[29] S. Oymak, B. Recht, and M. Soltanolkotabi. Sharp time\u2013data tradeoffs for linear inverse problems. arXiv preprint arXiv:1507.04793, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.04793"
        },
        {
            "id": "30",
            "entry": "[30] Y. Plan, R. Vershynin, and E. Yudovina. High-dimensional estimation with geometric constraints. Information and Inference, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Plan%2C%20Y.%20Vershynin%2C%20R.%20Yudovina%2C%20E.%20High-dimensional%20estimation%20with%20geometric%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Plan%2C%20Y.%20Vershynin%2C%20R.%20Yudovina%2C%20E.%20High-dimensional%20estimation%20with%20geometric%20constraints%202016"
        },
        {
            "id": "31",
            "entry": "[31] P. Rai, A. Kumar, and H. Daume. Simultaneously leveraging output and task structures for multiple-output regression. In NIPS, pages 3185\u20133193, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rai%2C%20P.%20Kumar%2C%20A.%20H.%20Daume.%20Simultaneously%20leveraging%20output%20and%20task%20structures%20for%20multiple-output%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rai%2C%20P.%20Kumar%2C%20A.%20H.%20Daume.%20Simultaneously%20leveraging%20output%20and%20task%20structures%20for%20multiple-output%20regression%202012"
        },
        {
            "id": "32",
            "entry": "[32] A. J. Rothman, E. Levina, and J. Zhu. Sparse multivariate regression with covariance estimation. Journal of Computational and Graphical Statistics, 19(4):947\u2013962, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rothman%2C%20A.J.%20Levina%2C%20E.%20Zhu%2C%20J.%20Sparse%20multivariate%20regression%20with%20covariance%20estimation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rothman%2C%20A.J.%20Levina%2C%20E.%20Zhu%2C%20J.%20Sparse%20multivariate%20regression%20with%20covariance%20estimation%202010"
        },
        {
            "id": "33",
            "entry": "[33] J. Shen and P. Li. On the iteration complexity of support recovery via hard thresholding pursuit. In International Conference on Machine Learning, pages 3115\u20133124, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20J.%20Li%2C%20P.%20On%20the%20iteration%20complexity%20of%20support%20recovery%20via%20hard%20thresholding%20pursuit%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20J.%20Li%2C%20P.%20On%20the%20iteration%20complexity%20of%20support%20recovery%20via%20hard%20thresholding%20pursuit%202017"
        },
        {
            "id": "34",
            "entry": "[34] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. arXiv preprint arXiv:1602.06664, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.06664"
        },
        {
            "id": "35",
            "entry": "[35] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere i: Overview and the geometric picture. IEEE Transactions on Information Theory, 63(2):853\u2013884, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20Complete%20dictionary%20recovery%20over%20the%20sphere%20i%3A%20Overview%20and%20the%20geometric%20picture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20Complete%20dictionary%20recovery%20over%20the%20sphere%20i%3A%20Overview%20and%20the%20geometric%20picture%202017"
        },
        {
            "id": "36",
            "entry": "[36] R. Sun and Z.-Q. Luo. Guaranteed matrix completion via nonconvex factorization. In FOCS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20R.%20Luo%2C%20Z.-Q.%20Guaranteed%20matrix%20completion%20via%20nonconvex%20factorization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20R.%20Luo%2C%20Z.-Q.%20Guaranteed%20matrix%20completion%20via%20nonconvex%20factorization%202015"
        },
        {
            "id": "37",
            "entry": "[37] M. Talagrand. The Generic Chaining. Springer, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20M.%20The%20Generic%20Chaining%202005"
        },
        {
            "id": "38",
            "entry": "[38] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20M.%20Upper%20and%20Lower%20Bounds%20for%20Stochastic%20Processes%202014"
        },
        {
            "id": "39",
            "entry": "[39] S. Tu, R. Boczar, M. Simchowitz, M. Soltanolkotabi, and B. Recht. Low-rank solutions of linear matrix equations via procrustes flow. arXiv preprint arXiv:1507.03566, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.03566"
        },
        {
            "id": "40",
            "entry": "[40] V. Vapnik. Statistical learning theory. Wiley New York, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20V.%20Statistical%20learning%20theory%201998"
        },
        {
            "id": "41",
            "entry": "[41] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and G. Kutyniok, editors, Compressed Sensing, chapter 5, pages 210\u2013268. Cambridge University Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20R.%20Introduction%20to%20the%20non-asymptotic%20analysis%20of%20random%20matrices%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vershynin%2C%20R.%20Introduction%20to%20the%20non-asymptotic%20analysis%20of%20random%20matrices%202012"
        },
        {
            "id": "42",
            "entry": "[42] R. Vershynin. Estimation in High Dimensions: A Geometric Perspective, pages 3\u201366. Springer International Publishing, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20R.%20Estimation%20in%20High%20Dimensions%3A%20A%20Geometric%20Perspective%202015"
        },
        {
            "id": "43",
            "entry": "[43] Ir\u00e8ne Waldspurger. Phase retrieval with random gaussian sensing vectors by alternating projections. IEEE Trans. Information Theory, 64(5):3301\u20133312, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Waldspurger%2C%20Ir%C3%A8ne%20Phase%20retrieval%20with%20random%20gaussian%20sensing%20vectors%20by%20alternating%20projections%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Waldspurger%2C%20Ir%C3%A8ne%20Phase%20retrieval%20with%20random%20gaussian%20sensing%20vectors%20by%20alternating%20projections%202018"
        },
        {
            "id": "44",
            "entry": "[44] X. Yi, C. Caramanis, and S. Sanghavi. Alternating minimization for mixed linear regression. In ICML, pages 613\u2013621, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20X.%20Caramanis%2C%20C.%20Sanghavi%2C%20S.%20Alternating%20minimization%20for%20mixed%20linear%20regression%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yi%2C%20X.%20Caramanis%2C%20C.%20Sanghavi%2C%20S.%20Alternating%20minimization%20for%20mixed%20linear%20regression%202014"
        },
        {
            "id": "45",
            "entry": "[45] Q. Zheng and J. Lafferty. A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements. In Advances in Neural Information Processing Systems, pages 109\u2013117, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Q.%20Lafferty%2C%20J.%20A%20convergent%20gradient%20descent%20algorithm%20for%20rank%20minimization%20and%20semidefinite%20programming%20from%20random%20linear%20measurements%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Q.%20Lafferty%2C%20J.%20A%20convergent%20gradient%20descent%20algorithm%20for%20rank%20minimization%20and%20semidefinite%20programming%20from%20random%20linear%20measurements%202015"
        }
    ]
}
