{
    "filename": "7803-occams-razor-is-insufficient-to-infer-the-preferences-of-irrational-agents.pdf",
    "metadata": {
        "title": "Occam's razor is insufficient to infer the preferences of irrational agents",
        "author": "Stuart Armstrong, S\u00f6ren Mindermann",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7803-occams-razor-is-insufficient-to-infer-the-preferences-of-irrational-agents.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent\u2019s policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam\u2019s razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple \u2018normative\u2019 assumptions, which cannot be deduced exclusively from observations."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "occams razor",
            "url": "https://en.wikipedia.org/wiki/occams_razor"
        },
        {
            "term": "Inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Inverse_reinforcement_learning"
        },
        {
            "term": "reward function",
            "url": "https://en.wikipedia.org/wiki/reward_function"
        }
    ],
    "highlights": [
        "In today\u2019s reinforcement learning systems, a simple reward function is often hand-crafted, and still sometimes leads to undesired behaviors on the part of RL agent, as the reward function is not well aligned with the operator\u2019s true goals4",
        "Supervised learning of long-range and goal-directed behavior is often difficult without the reward function [Ratliff et al, 2006]",
        "Previous work has shown that noisy rationality is too strong an assumption as it does not account for bias; we tried the weaker assumption of simplicity, strong enough to avoid typical No Free Lunch results, but it is insufficient here",
        "Staying close to agnostic is desirable in some settings: for example, a misspecified model of the human reward function can lead to disastrous decisions with high confidence [<a class=\"ref-link\" id=\"cMilli_et+al_2017_a\" href=\"#rMilli_et+al_2017_a\">Milli et al, 2017</a>]"
    ],
    "key_statements": [
        "In today\u2019s reinforcement learning systems, a simple reward function is often hand-crafted, and still sometimes leads to undesired behaviors on the part of RL agent, as the reward function is not well aligned with the operator\u2019s true goals4",
        "Supervised learning of long-range and goal-directed behavior is often difficult without the reward function [Ratliff et al, 2006]",
        "Previous work has shown that noisy rationality is too strong an assumption as it does not account for bias; we tried the weaker assumption of simplicity, strong enough to avoid typical No Free Lunch results, but it is insufficient here",
        "Staying close to agnostic is desirable in some settings: for example, a misspecified model of the human reward function can lead to disastrous decisions with high confidence [<a class=\"ref-link\" id=\"cMilli_et+al_2017_a\" href=\"#rMilli_et+al_2017_a\">Milli et al, 2017</a>]"
    ],
    "summary": [
        "In today\u2019s reinforcement learning systems, a simple reward function is often hand-crafted, and still sometimes leads to undesired behaviors on the part of RL agent, as the reward function is not well aligned with the operator\u2019s true goals4.",
        "A pair (p, R) is defined to be compatible with \u03c0 , if p(R) = \u03c0 \u2014 that pair is a possible candidate for decomposing the human policy into the human\u2019s planner and reward function.",
        "The results show that without assumptions about the rationality of the human, all attempts to optimise their reward function are essentially futile.",
        "The human reward function cannot be inferred from behavior without assumptions about the planning algorithm p.",
        "Let (p, R ) be a \u2018reasonable\u2019 planner-reward pair that captures our judgements about the biases and rationality of a human with policy \u03c0 = p(R ).",
        "Simplicity in algorithmic information theory is relative to the computer language L used [<a class=\"ref-link\" id=\"cMing_2014_a\" href=\"#rMing_2014_a\">Ming and Vitanyi, 2014</a>, <a class=\"ref-link\" id=\"cCalude_2002_a\" href=\"#rCalude_2002_a\">Calude, 2002</a>], and there exists languages in which the theorem is clearly false: one could choose a degenerate language in which (p, R ) is encoded by the string \u20180\u2019, for example, and all other planner-reward pairs are of extremely long length.",
        "If \u03c0is a human policy, and L is a \u2018reasonable\u2019 computer language, there exists degenerate planner-reward pairs amongst the pairs of lowest complexity compatible with \u03c0 .",
        "If \u03c0is a human policy, and L is a \u2018reasonable\u2019 computer language with (p, R ) a compatible planner-reward pair, there exist a pair (p , \u2212R ) of comparable complexity to (p, R ), but opposite reward function.",
        "If \u03c0is the human policy, c defines a reasonable measure of comparable complexity, and L is a c-reasonable language for F , the degenerate planner-reward pairs,, and (\u2212pg, \u2212R\u03c0 ) are amongst the pairs of lowest complexity among the pairs compatible with \u03c0 .",
        "Section 5 demonstrated that there are degenerate planner-reward pairs close to the minimum complexity among all pairs compatible with \u03c0 .",
        "Any given (p, R ) can quantify the form and extent of these biases by computing objects like the regret function Reg(p, R )(s) := Reg(p(R ), R )(s) = VR\u2217 \u0307 (s) \u2212 VRp(R )(s), which measures the divergence between the expected value of the actual and optimal human policies12.",
        "If \u03c0is a human policy, and L is a \u2018reasonable\u2019 computer language with (p, R ) a \u2018reasonable\u2019 compatible planner-reward pair, the complexity of (p, R ) is not close to minimal amongst the pairs compatible with \u03c0 .",
        "Staying close to agnostic is desirable in some settings: for example, a misspecified model of the human reward function can lead to disastrous decisions with high confidence [<a class=\"ref-link\" id=\"cMilli_et+al_2017_a\" href=\"#rMilli_et+al_2017_a\"><a class=\"ref-link\" id=\"cMilli_et+al_2017_a\" href=\"#rMilli_et+al_2017_a\">Milli et al, 2017</a></a>]. <a class=\"ref-link\" id=\"cAnonymous_2019_a\" href=\"#rAnonymous_2019_a\"><a class=\"ref-link\" id=\"cAnonymous_2019_a\" href=\"#rAnonymous_2019_a\">Anonymous [2019</a></a>] makes a promising first try \u2014 a high-dimensional parametric planner is initialized to noisy rationality and adapts to fit the behavior of a systematically irrational agent"
    ],
    "headline": "We have shown that some degenerate planner-reward decompositions of a human policy have nearminimal description length and argued that decompositions we would endorse do not",
    "reference_links": [
        {
            "id": "Pieter_2004_a",
            "entry": "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Pieter%20Abbeel%20and%20Andrew%20Y%20Ng.%20Apprenticeship%20learning%20via%20inverse%20reinforcement%20learning%202004"
        },
        {
            "id": "Amin_2016_a",
            "entry": "Kareem Amin and Satinder Singh. Towards resolving unidentifiability in inverse reinforcement learning. arXiv preprint arXiv:1601.06569, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06569"
        },
        {
            "id": "Amodei_et+al_2016_a",
            "entry": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete Problems in AI Safety. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dario%20Amodei%20Chris%20Olah%20Jacob%20Steinhardt%20Paul%20Christiano%20John%20Schulman%20and%20Dan%20Mane%20Concrete%20Problems%20in%20AI%20Safety%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dario%20Amodei%20Chris%20Olah%20Jacob%20Steinhardt%20Paul%20Christiano%20John%20Schulman%20and%20Dan%20Mane%20Concrete%20Problems%20in%20AI%20Safety%202016"
        },
        {
            "id": "Anonymous_2019_a",
            "entry": "Anonymous. Inferring reward functions from demonstrators with unknown biases. In Submitted to International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rkgqCiRqKQ.under review.",
            "url": "https://openreview.net/forum?id=rkgqCiRqKQ.under",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anonymous%20Inferring%20reward%20functions%20from%20demonstrators%20with%20unknown%20biases%202019"
        },
        {
            "id": "Ariely_et+al_2004_a",
            "entry": "Dan Ariely, George Loewenstein, and Drazen Prelec. Arbitrarily coherent preferences. The psychology of economic decisions, 2:131\u2013161, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ariely%2C%20Dan%20Loewenstein%2C%20George%20Prelec%2C%20Drazen%20Arbitrarily%20coherent%20preferences.%20The%20psychology%20of%20economic%20decisions%2C%202%202004"
        },
        {
            "id": "Bostrom_2014_a",
            "entry": "Nick Bostrom. Superintelligence: Paths, dangers, strategies., 2014a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nick%20Bostrom%20Superintelligence%20Paths%20dangers%20strategies%202014a"
        },
        {
            "id": "Bostrom_2014_b",
            "entry": "Nick Bostrom. Superintelligence: Paths, dangers, strategies. Oxford University Press, 2014b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bostrom%2C%20Nick%20Superintelligence%3A%20Paths%2C%20dangers%2C%20strategies%202014"
        },
        {
            "id": "Boularias_et+al_2011_a",
            "entry": "Abdeslam Boularias, Jens Kober, and Jan Peters. Relative Entropy Inverse Reinforcement Learning, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abdeslam%20Boularias%20Jens%20Kober%20and%20Jan%20Peters%20Relative%20Entropy%20Inverse%20Reinforcement%20Learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abdeslam%20Boularias%20Jens%20Kober%20and%20Jan%20Peters%20Relative%20Entropy%20Inverse%20Reinforcement%20Learning%202011"
        },
        {
            "id": "Bruck_1999_a",
            "entry": "Joanna Bruck. Ritual and rationality: some problems of interpretation in european archaeology. European journal of archaeology, 2(3):313\u2013344, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bruck%2C%20Joanna%20Ritual%20and%20rationality%3A%20some%20problems%20of%20interpretation%20in%20european%20archaeology%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bruck%2C%20Joanna%20Ritual%20and%20rationality%3A%20some%20problems%20of%20interpretation%20in%20european%20archaeology%201999"
        },
        {
            "id": "Calude_2002_a",
            "entry": "Cristian Calude. Information and randomness : an algorithmic perspective. Springer, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calude%2C%20Cristian%20Information%20and%20randomness%20%3A%20an%20algorithmic%20perspective%202002"
        },
        {
            "id": "Cundy_2018_a",
            "entry": "Chris Cundy and Daniel Filan. Exploring hierarchy-aware inverse reinforcement learning. arXiv preprint arXiv:1807.05037, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.05037"
        },
        {
            "id": "Evans_et+al_2015_a",
            "entry": "Owain Evans, Andreas Stuhlmueller, and Noah D. Goodman. Learning the Preferences of Ignorant, Inconsistent Agents. Thirtieth AAAI Conference on Artificial Intelligence, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evans%2C%20Owain%20Stuhlmueller%2C%20Andreas%20Goodman%2C%20Noah%20D.%20Learning%20the%20Preferences%20of%20Ignorant%2C%20Inconsistent%20Agents%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evans%2C%20Owain%20Stuhlmueller%2C%20Andreas%20Goodman%2C%20Noah%20D.%20Learning%20the%20Preferences%20of%20Ignorant%2C%20Inconsistent%20Agents%202015"
        },
        {
            "id": "Evans_et+al_0000_a",
            "entry": "Owain Evans, Andreas Stuhlmuller, and Noah D Goodman. Learning the preferences of bounded agents. NIPS Workshop on Bounded Optimality, pages 16\u201322, 2015b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evans%2C%20Owain%20Stuhlmuller%2C%20Andreas%20Goodman%2C%20Noah%20D.%20Learning%20the%20preferences%20of%20bounded%20agents",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Evans%2C%20Owain%20Stuhlmuller%2C%20Andreas%20Goodman%2C%20Noah%20D.%20Learning%20the%20preferences%20of%20bounded%20agents"
        },
        {
            "id": "Everitt_et+al_2014_a",
            "entry": "Tom Everitt, Tor Lattimore, and Marcus Hutter. Free Lunch for optimisation under the universal distribution. In Proceedings of the 2014 IEEE Congress on Evolutionary Computation, CEC 2014, pages 167\u2013174, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Everitt%2C%20Tom%20Lattimore%2C%20Tor%20Free%2C%20Marcus%20Hutter%20Lunch%20for%20optimisation%20under%20the%20universal%20distribution%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Everitt%2C%20Tom%20Lattimore%2C%20Tor%20Free%2C%20Marcus%20Hutter%20Lunch%20for%20optimisation%20under%20the%20universal%20distribution%202014"
        },
        {
            "id": "Everitt_et+al_2017_a",
            "entry": "Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg. Reinforcement Learning with a Corrupted Reward Channel. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Everitt%2C%20Tom%20Krakovna%2C%20Victoria%20Orseau%2C%20Laurent%20Hutter%2C%20Marcus%20Reinforcement%20Learning%20with%20a%20Corrupted%20Reward%20Channel%202017"
        },
        {
            "id": "Glimcher_et+al_2009_a",
            "entry": "Paul W Glimcher, Colin F Camerer, Ernst Fehr, and Russell A Poldrack. Neuroeconomics: Decision making and the brain, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glimcher%2C%20Paul%20W.%20Camerer%2C%20Colin%20F.%20Fehr%2C%20Ernst%20and%20Russell%20A%20Poldrack.%20Neuroeconomics%3A%20Decision%20making%20and%20the%20brain%202009"
        },
        {
            "id": "Griffiths_et+al_2015_a",
            "entry": "Thomas L. Griffiths, Falk Lieder, and Noah D. Goodman. Rational Use of Cognitive Resources: Levels of Analysis Between the Computational and the Algorithmic. Topics in Cognitive Science, 7(2):217\u2013229, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffiths%2C%20Thomas%20L.%20Lieder%2C%20Falk%20Goodman%2C%20Noah%20D.%20Rational%20Use%20of%20Cognitive%20Resources%3A%20Levels%20of%20Analysis%20Between%20the%20Computational%20and%20the%20Algorithmic%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffiths%2C%20Thomas%20L.%20Lieder%2C%20Falk%20Goodman%2C%20Noah%20D.%20Rational%20Use%20of%20Cognitive%20Resources%3A%20Levels%20of%20Analysis%20Between%20the%20Computational%20and%20the%20Algorithmic%202015"
        },
        {
            "id": "Hadfield-Menell_et+al_2016_a",
            "entry": "Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. Cooperative Inverse Reinforcement Learning. arXiv:1606.03137 [cs], 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03137"
        },
        {
            "id": "Hadfield-Menell_et+al_2017_a",
            "entry": "Dylan Hadfield-Menell, Smitha Milli, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Inverse reward design. In Advances in Neural Information Processing Systems, pages 6749\u20136758, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dylan%20HadfieldMenell%20Smitha%20Milli%20Stuart%20J%20Russell%20Pieter%20Abbeel%20and%20Anca%20Dragan%20Inverse%20reward%20design%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2067496758%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dylan%20HadfieldMenell%20Smitha%20Milli%20Stuart%20J%20Russell%20Pieter%20Abbeel%20and%20Anca%20Dragan%20Inverse%20reward%20design%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2067496758%202017"
        },
        {
            "id": "Andreas_2015_a",
            "entry": "Andreas Hula, P. Read Montague, and Peter Dayan. Monte Carlo Planning Method Estimates Planning Horizons during Interactive Social Exchange. PLOS Computational Biology, 11(6): e1004254, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%20Hula%2C%20P.Read%20Montague%20Dayan%2C%20Peter%20Monte%20Carlo%20Planning%20Method%20Estimates%20Planning%20Horizons%20during%20Interactive%20Social%20Exchange%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%20Hula%2C%20P.Read%20Montague%20Dayan%2C%20Peter%20Monte%20Carlo%20Planning%20Method%20Estimates%20Planning%20Horizons%20during%20Interactive%20Social%20Exchange%202015"
        },
        {
            "id": "David_0000_a",
            "entry": "David Hume. Treatise on Human Nature Ed Selby-bigge, L a. 1888.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=David%20Hume%20Treatise%20on%20Human%20Nature%20Ed%20Selbybigge%20L%20a%201888"
        },
        {
            "id": "Farrar_2011_a",
            "entry": "Farrar, Straus and Giroux New York, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Farrar%20Straus%20and%20Giroux%202011"
        },
        {
            "id": "Kahneman_2013_a",
            "entry": "Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In Handbook of the fundamentals of financial decision making: Part I, pages 99\u2013127. World Scientific, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kahneman%2C%20Daniel%20Tversky%2C%20Amos%20Prospect%20theory%3A%20An%20analysis%20of%20decision%20under%20risk%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kahneman%2C%20Daniel%20Tversky%2C%20Amos%20Prospect%20theory%3A%20An%20analysis%20of%20decision%20under%20risk%202013"
        },
        {
            "id": "Kahneman_et+al_2016_a",
            "entry": "Daniel Kahneman, Andrew M Rosenfield, Linnea Gandhi, and Tom Blaser. Noise: How to overcome the high, hidden cost of inconsistent decision making. Harvard business review, 94(10):38\u201346, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kahneman%2C%20Daniel%20Rosenfield%2C%20Andrew%20M.%20Gandhi%2C%20Linnea%20Blaser%2C%20Tom%20Noise%3A%20How%20to%20overcome%20the%20high%2C%20hidden%20cost%20of%20inconsistent%20decision%20making%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kahneman%2C%20Daniel%20Rosenfield%2C%20Andrew%20M.%20Gandhi%2C%20Linnea%20Blaser%2C%20Tom%20Noise%3A%20How%20to%20overcome%20the%20high%2C%20hidden%20cost%20of%20inconsistent%20decision%20making%202016"
        },
        {
            "id": "Kolmogorov_1965_a",
            "entry": "Andrei N Kolmogorov. Three approaches to the quantitative definition ofinformation\u2019. Problems of information transmission, 1(1):1\u20137, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolmogorov%2C%20Andrei%20N.%20Three%20approaches%20to%20the%20quantitative%20definition%20ofinformation%E2%80%99%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolmogorov%2C%20Andrei%20N.%20Three%20approaches%20to%20the%20quantitative%20definition%20ofinformation%E2%80%99%201965"
        },
        {
            "id": "Lazar_et+al_2018_a",
            "entry": "Daniel A Lazar, Kabir Chandrasekher, Ramtin Pedarsani, and Dorsa Sadigh. Maximizing road capacity using cars that influence people. arXiv preprint arXiv:1807.04414, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04414"
        },
        {
            "id": "Leike_et+al_2017_a",
            "entry": "Jan Leike, Miljan Martic, Victoria Krakovna, Pedro Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. Ai safety gridworlds. arXiv preprint arXiv:1711.09883, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09883"
        },
        {
            "id": "Levin_1984_a",
            "entry": "Leonid A Levin. Randomness conservation inequalities; information and independence in mathematical theories. Information and Control, 61(1):15\u201337, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levin%2C%20Leonid%20A.%20Randomness%20conservation%20inequalities%3B%20information%20and%20independence%20in%20mathematical%20theories%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levin%2C%20Leonid%20A.%20Randomness%20conservation%20inequalities%3B%20information%20and%20independence%20in%20mathematical%20theories%201984"
        },
        {
            "id": "Lewis_et+al_2014_a",
            "entry": "Richard L Lewis, Andrew Howes, and Satinder Singh. Computational Rationality: Linking Mechanism and Behavior Through Bounded Utility Maximization. Topics in Cognitive Science, 6: 279\u2013311, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lewis%2C%20Richard%20L.%20Howes%2C%20Andrew%20Singh%2C%20Satinder%20Computational%20Rationality%3A%20Linking%20Mechanism%20and%20Behavior%20Through%20Bounded%20Utility%20Maximization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lewis%2C%20Richard%20L.%20Howes%2C%20Andrew%20Singh%2C%20Satinder%20Computational%20Rationality%3A%20Linking%20Mechanism%20and%20Behavior%20Through%20Bounded%20Utility%20Maximization%202014"
        },
        {
            "id": "Miller_1984_a",
            "entry": "Joan G Miller. Culture and the development of everyday social explanation. Journal of personality and social psychology, 46(5):961, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miller%2C%20Joan%20G.%20Culture%20and%20the%20development%20of%20everyday%20social%20explanation%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miller%2C%20Joan%20G.%20Culture%20and%20the%20development%20of%20everyday%20social%20explanation%201984"
        },
        {
            "id": "Milli_et+al_2017_a",
            "entry": "Smitha Milli, Dylan Hadfield-Menell, Anca Dragan, and Stuart Russell. Should robots be obedient? arXiv preprint arXiv:1705.09990, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09990"
        },
        {
            "id": "Ming_2014_a",
            "entry": "LI Ming and Paul MB Vitanyi. Kolmogorov complexity and its applications. Algorithms and Complexity, 1:187, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ming%2C%20L.I.%20Vitanyi%2C%20Paul%20M.B.%20Kolmogorov%20complexity%20and%20its%20applications%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ming%2C%20L.I.%20Vitanyi%2C%20Paul%20M.B.%20Kolmogorov%20complexity%20and%20its%20applications%202014"
        },
        {
            "id": "Minsky_1984_a",
            "entry": "Marvin Minsky. Afterword to Vernor Vinge\u2019s novel, \u201cTrue names.\u201d Unpublished manuscript. 1984. URL http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html.",
            "url": "http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minsky%2C%20Marvin%20Afterword%20to%20Vernor%20Vinge%E2%80%99s%20novel%2C%20%E2%80%9CTrue%20names.%E2%80%9D%201984"
        },
        {
            "id": "Moravec_1988_a",
            "entry": "Hans Moravec. Mind children: The future of robot and human intelligence. Harvard University Press, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moravec%2C%20Hans%20Mind%20children%3A%20The%20future%20of%20robot%20and%20human%20intelligence%201988"
        },
        {
            "id": "Muller_2010_a",
            "entry": "Markus Muller. Stationary algorithmic probability. Theoretical Computer Science, 411(1):113\u2013130, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muller%2C%20Markus%20Stationary%20algorithmic%20probability%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muller%2C%20Markus%20Stationary%20algorithmic%20probability%202010"
        },
        {
            "id": "Ng_2000_a",
            "entry": "Andrew Ng and Stuart Russell. Algorithms for inverse reinforcement learning. Proceedings of the Seventeenth International Conference on Machine Learning, pages 663\u2013670, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20Andrew%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20Andrew%20Russell%2C%20Stuart%20Algorithms%20for%20inverse%20reinforcement%20learning%202000"
        },
        {
            "id": "Nisbett_et+al_2001_a",
            "entry": "Richard E Nisbett, Kaiping Peng, Incheol Choi, and Ara Norenzayan. Culture and systems of thought: holistic versus analytic cognition. Psychological review, 108(2):291, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nisbett%2C%20Richard%20E.%20Peng%2C%20Kaiping%20Choi%2C%20Incheol%20Norenzayan%2C%20Ara%20Culture%20and%20systems%20of%20thought%3A%20holistic%20versus%20analytic%20cognition%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nisbett%2C%20Richard%20E.%20Peng%2C%20Kaiping%20Choi%2C%20Incheol%20Norenzayan%2C%20Ara%20Culture%20and%20systems%20of%20thought%3A%20holistic%20versus%20analytic%20cognition%202001"
        },
        {
            "id": "Ratliff_2006_a",
            "entry": "Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In Proceedings of the 23rd international conference on Machine learning - ICML \u201906, pages 729\u2013736, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20A%20Zinkevich.%20Maximum%20margin%20planning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20A%20Zinkevich.%20Maximum%20margin%20planning%202006"
        },
        {
            "id": "Rawls_1971_a",
            "entry": "John Rawls. A Theory of Justice. Cambridge, Massachusetts: Belknap Press, 1971. ISBN 0-67400078-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rawls%2C%20John%20A%20Theory%20of%20Justice%201971"
        },
        {
            "id": "Russell_2016_a",
            "entry": "Stuart Russell. Rationality and Intelligence: A Brief Update. In Fundamental Issues of Artificial Intelligence, pages 7\u201328. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russell%2C%20Stuart%20Rationality%20and%20Intelligence%3A%20A%20Brief%20Update%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russell%2C%20Stuart%20Rationality%20and%20Intelligence%3A%20A%20Brief%20Update%202016"
        },
        {
            "id": "Russell_et+al_2015_a",
            "entry": "Stuart Russell, Daniel Dewey, and Max Tegmark. Research Priorities for Robust and Beneficial Artificial Intelligence. AI Magazine, 36(4):105, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russell%2C%20Stuart%20Dewey%2C%20Daniel%20Tegmark%2C%20Max%20Research%20Priorities%20for%20Robust%20and%20Beneficial%20Artificial%20Intelligence%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russell%2C%20Stuart%20Dewey%2C%20Daniel%20Tegmark%2C%20Max%20Research%20Priorities%20for%20Robust%20and%20Beneficial%20Artificial%20Intelligence%202015"
        },
        {
            "id": "Samuelson_1948_a",
            "entry": "Paul A Samuelson. Consumption theory in terms of revealed preference. Economica, 15(60):243\u2013253, 1948.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samuelson%2C%20Paul%20A.%20Consumption%20theory%20in%20terms%20of%20revealed%20preference%201948",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samuelson%2C%20Paul%20A.%20Consumption%20theory%20in%20terms%20of%20revealed%20preference%201948"
        },
        {
            "id": "Schuman_1983_a",
            "entry": "Howard Schuman and Jacob Ludwig. The norm of even-handedness in surveys as in life. American Sociological Review, pages 112\u2013120, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schuman%2C%20Howard%20Ludwig%2C%20Jacob%20The%20norm%20of%20even-handedness%20in%20surveys%20as%20in%20life%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schuman%2C%20Howard%20Ludwig%2C%20Jacob%20The%20norm%20of%20even-handedness%20in%20surveys%20as%20in%20life%201983"
        },
        {
            "id": "Scopelliti_et+al_2015_a",
            "entry": "Irene Scopelliti, Carey K Morewedge, Erin McCormick, H Lauren Min, Sophie Lebrecht, and Karim S Kassam. Bias blind spot: Structure, measurement, and consequences. Management Science, 61(10):2468\u20132486, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scopelliti%2C%20Irene%20Morewedge%2C%20Carey%20K.%20Erin%20McCormick%2C%20H.Lauren%20Min%20Lebrecht%2C%20Sophie%20Bias%20blind%20spot%3A%20Structure%2C%20measurement%2C%20and%20consequences%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scopelliti%2C%20Irene%20Morewedge%2C%20Carey%20K.%20Erin%20McCormick%2C%20H.Lauren%20Min%20Lebrecht%2C%20Sophie%20Bias%20blind%20spot%3A%20Structure%2C%20measurement%2C%20and%20consequences%202015"
        },
        {
            "id": "Shiarlis_et+al_2017_a",
            "entry": "Kyriacos Shiarlis, Joao Messias, and Shimon Whiteson. Rapidly exploring learning trees. In Proceedings - IEEE International Conference on Robotics and Automation, pages 1541\u20131548, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shiarlis%2C%20Kyriacos%20Messias%2C%20Joao%20Whiteson%2C%20Shimon%20Rapidly%20exploring%20learning%20trees%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shiarlis%2C%20Kyriacos%20Messias%2C%20Joao%20Whiteson%2C%20Shimon%20Rapidly%20exploring%20learning%20trees%202017"
        },
        {
            "id": "Slovic_1974_a",
            "entry": "Paul Slovic and Amos Tversky. Who accepts savage\u2019s axiom? Behavioral science, 19(6):368\u2013373, 1974.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Slovic%2C%20Paul%20Tversky%2C%20Amos%20Who%20accepts%20savage%E2%80%99s%20axiom%3F%201974",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Slovic%2C%20Paul%20Tversky%2C%20Amos%20Who%20accepts%20savage%E2%80%99s%20axiom%3F%201974"
        },
        {
            "id": "Tversky_1975_a",
            "entry": "Amos Tversky and Daniel Kahneman. Judgment under Uncertainty: Heuristics and Biases. In Utility, Probability, and Human Decision Making, pages 141\u2013162. Springer Netherlands, Dordrecht, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tversky%2C%20Amos%20Kahneman%2C%20Daniel%20Judgment%20under%20Uncertainty%3A%20Heuristics%20and%20Biases.%20In%20Utility%2C%20Probability%2C%20and%20Human%20Decision%20Making%201975"
        },
        {
            "id": "Vitanyi_1997_a",
            "entry": "Paul MB Vitanyi and Ming Li. An introduction to Kolmogorov complexity and its applications, volume 34. Springer Heidelberg, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vitanyi%2C%20Paul%20M.B.%20Li%2C%20Ming%20An%20introduction%20to%20Kolmogorov%20complexity%20and%20its%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vitanyi%2C%20Paul%20M.B.%20Li%2C%20Ming%20An%20introduction%20to%20Kolmogorov%20complexity%20and%20its%201997"
        },
        {
            "id": "Wolpert_1997_a",
            "entry": "David H. Wolpert and William G. Macready. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1):67\u201382, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolpert%2C%20David%20H.%20Macready%2C%20William%20G.%20No%20free%20lunch%20theorems%20for%20optimization%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolpert%2C%20David%20H.%20Macready%2C%20William%20G.%20No%20free%20lunch%20theorems%20for%20optimization%201997"
        },
        {
            "id": "Ziebart_et+al_2008_a",
            "entry": "Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum Entropy Inverse Reinforcement Learning. In AAAI Conference on Artificial Intelligence, pages 1433\u20131438, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20K.%20Maximum%20Entropy%20Inverse%20Reinforcement%20Learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20Brian%20D.%20Andrew%20Maas%2C%20J.Andrew%20Bagnell%20Dey%2C%20Anind%20K.%20Maximum%20Entropy%20Inverse%20Reinforcement%20Learning%202008"
        }
    ]
}
