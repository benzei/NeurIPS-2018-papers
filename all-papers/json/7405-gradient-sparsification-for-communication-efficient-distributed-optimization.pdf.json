{
    "filename": "7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf",
    "metadata": {
        "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization",
        "author": "Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computational architectures. A key bottleneck is the communication overhead for exchanging information such as stochastic gradients among different workers. In this paper, to reduce the communication cost, we propose a convex optimization formulation to minimize the coding length of stochastic gradients. The key idea is to randomly drop out coordinates of the stochastic gradient vectors and amplify the remaining coordinates appropriately to ensure the sparsified gradient to be unbiased. To solve the optimal sparsification efficiently, a simple and fast algorithm is proposed for an approximate solution, with a theoretical guarantee for sparseness. Experiments on 2-regularized logistic regression, support vector machines and convolutional neural networks validate our sparsification approaches."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        },
        {
            "term": "component analysis",
            "url": "https://en.wikipedia.org/wiki/component_analysis"
        },
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "single instruction multiple data",
            "url": "https://en.wikipedia.org/wiki/Single_Instruction_Multiple_Data"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Scaling stochastic optimization algorithms [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] to distributed computational architectures [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] or multicore systems [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] is a crucial problem for large-scale machine learning",
        "Other existing works on distributed machine learning include two directions: 1) how to design communication efficient algorithms to reduce the round of communications among workers [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], and 2) how to use large mini-batches without compromising the convergence speed [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "The optimization algorithm converges even when the sparsity ratio is about \u03ba = 0.004, and the communication cost is significantly reduced in this setting",
        "We propose a gradient sparsification technique to reduce the communication cost for large-scale distributed machine learning",
        "We propose a convex optimization formulation to minimize the coding length of stochastic gradients given the variance budget that monotonically depends on the computational complexity, with efficient algorithms and a theoretical guarantee",
        "Comprehensive experiments on distributed and parallel optimization of multiple models proved our algorithm can effectively reduce the communication cost during training or reduce conflicts among multiple threads"
    ],
    "key_statements": [
        "Scaling stochastic optimization algorithms [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] to distributed computational architectures [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] or multicore systems [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] is a crucial problem for large-scale machine learning",
        "Other existing works on distributed machine learning include two directions: 1) how to design communication efficient algorithms to reduce the round of communications among workers [<a class=\"ref-link\" id=\"c37\" href=\"#r37\">37</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>], and 2) how to use large mini-batches without compromising the convergence speed [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>]",
        "The optimization algorithm converges even when the sparsity ratio is about \u03ba = 0.004, and the communication cost is significantly reduced in this setting",
        "We propose a gradient sparsification technique to reduce the communication cost for large-scale distributed machine learning",
        "We propose a convex optimization formulation to minimize the coding length of stochastic gradients given the variance budget that monotonically depends on the computational complexity, with efficient algorithms and a theoretical guarantee",
        "Comprehensive experiments on distributed and parallel optimization of multiple models proved our algorithm can effectively reduce the communication cost during training or reduce conflicts among multiple threads"
    ],
    "summary": [
        "Scaling stochastic optimization algorithms [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] to distributed computational architectures [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] or multicore systems [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] is a crucial problem for large-scale machine learning.",
        "We consider how to reduce the communication cost in distributed machine learning by using a sparsified gradient gt, denoted by Q(g), such that Q) is unbiased, and has a relatively small variance.",
        "As shown in Section 2.3, we only need to use one floating-point number to encode the gradient values in Skc, so there is a further reduction in communication when considering the total number of bits transmitted, this is characterized by the Theorem below.",
        "If the gradient g \u2208 Rd of the loss function is (\u03c1, s)-approximately sparse as in Definition 2, and a floating-point number costs b bits, the coding length of Q(g) in Lemma 3 can be bounded by s(b + log2 d) + min + b.",
        "The coding length of the original gradient vector g is db, by considering the slightly increased number of iterations to reach the same optimization accuracy, the total communication cost is reduced by a factor of at least (1 + \u03c1)((s + 1)b + log2 d)/db.",
        "The sparsification technique shows strong improvement over the uniform sampling approach as a baseline, the iteration complexity is only slightly increased as we strongly reduce the communication costs.",
        "The probability vector p is calculated by Algorithm 3 and set the maximum iterations to be 2, which generates good enough high-quality approximation of the optimal p vector.",
        "We compare our algorithm with a uniform sampling method as baseline, where each element of the probability vector is set to be pi = \u03ba, and a similar sparsification follows to apply.",
        "Our algorithm is denoted by \u2018GSpar\u2019, and the uniform sampling method is denoted by \u2018UniSp\u2019, and the SGD/SVRG algorithm with non-sparsified communication is denoted by \u2018baseline\u2019, indicating the original distributed optimization algorithm.",
        "The optimization algorithm converges even when the sparsity ratio is about \u03ba = 0.004, and the communication cost is significantly reduced in this setting.",
        "We propose a gradient sparsification technique to reduce the communication cost for large-scale distributed machine learning.",
        "We propose a convex optimization formulation to minimize the coding length of stochastic gradients given the variance budget that monotonically depends on the computational complexity, with efficient algorithms and a theoretical guarantee.",
        "Comprehensive experiments on distributed and parallel optimization of multiple models proved our algorithm can effectively reduce the communication cost during training or reduce conflicts among multiple threads."
    ],
    "headline": "To reduce the communication cost, we propose a convex optimization formulation to minimize the coding length of stochastic gradients",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 440\u2013445, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aji%2C%20Alham%20Fikri%20Heafield%2C%20Kenneth%20Sparse%20communication%20for%20distributed%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aji%2C%20Alham%20Fikri%20Heafield%2C%20Kenneth%20Sparse%20communication%20for%20distributed%20gradient%20descent%202017"
        },
        {
            "id": "2",
            "entry": "[2] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communicationefficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1707\u20131718, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alistarh%2C%20Dan%20Grubic%2C%20Demjan%20Li%2C%20Jerry%20Tomioka%2C%20Ryota%20QSGD%3A%20Communicationefficient%20SGD%20via%20gradient%20quantization%20and%20encoding%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alistarh%2C%20Dan%20Grubic%2C%20Demjan%20Li%2C%20Jerry%20Tomioka%2C%20Ryota%20QSGD%3A%20Communicationefficient%20SGD%20via%20gradient%20quantization%20and%20encoding%202017"
        },
        {
            "id": "3",
            "entry": "[3] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. In Advances in Neural Information Processing Systems, pages 1756\u20131764, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjevani%2C%20Yossi%20Shamir%2C%20Ohad%20Communication%20complexity%20of%20distributed%20convex%20learning%20and%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjevani%2C%20Yossi%20Shamir%2C%20Ohad%20Communication%20complexity%20of%20distributed%20convex%20learning%20and%20optimization%202015"
        },
        {
            "id": "4",
            "entry": "[4] L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010, pages 177\u2013186.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent"
        },
        {
            "id": "5",
            "entry": "[5] Peter B\u00fchlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science & Business Media, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%BChlmann%2C%20Peter%20Geer%2C%20Sara%20Van%20De%20Statistics%20for%20high-dimensional%20data%3A%20methods%2C%20theory%20and%20applications%202011"
        },
        {
            "id": "6",
            "entry": "[6] Jiecao Chen, He Sun, David Woodruff, and Qin Zhang. Communication-optimal distributed clustering. In Advances in Neural Information Processing Systems, pages 3727\u20133735, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jiecao%20Sun%2C%20He%20Woodruff%2C%20David%20Zhang%2C%20Qin%20Communication-optimal%20distributed%20clustering%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Jiecao%20Sun%2C%20He%20Woodruff%2C%20David%20Zhang%2C%20Qin%20Communication-optimal%20distributed%20clustering%202016"
        },
        {
            "id": "7",
            "entry": "[7] Shang-Tse Chen, Maria-Florina Balcan, and Duen Horng Chau. Communication efficient distributed agnostic boosting. In Artificial Intelligence and Statistics, pages 1299\u20131307, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Shang-Tse%20Balcan%2C%20Maria-Florina%20Chau%2C%20Duen%20Horng%20Communication%20efficient%20distributed%20agnostic%20boosting%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Shang-Tse%20Balcan%2C%20Maria-Florina%20Chau%2C%20Duen%20Horng%20Communication%20efficient%20distributed%20agnostic%20boosting%202016"
        },
        {
            "id": "8",
            "entry": "[8] Christopher De Sa, Matthew Feldman, Christopher R\u00e9, and Kunle Olukotun. Understanding and optimizing asynchronous low-precision stochastic gradient descent. In Proceedings of the 44th Annual International Symposium on Computer Architecture, pages 561\u2013574. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20Christopher%20De%20Feldman%2C%20Matthew%20R%C3%A9%2C%20Christopher%20Olukotun%2C%20Kunle%20Understanding%20and%20optimizing%20asynchronous%20low-precision%20stochastic%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20Christopher%20De%20Feldman%2C%20Matthew%20R%C3%A9%2C%20Christopher%20Olukotun%2C%20Kunle%20Understanding%20and%20optimizing%20asynchronous%20low-precision%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "9",
            "entry": "[9] Christopher De Sa, Ce Zhang, Kunle Olukotun, and Christopher R\u00e9. Taming the wild: A unified analysis of hogwild-style algorithms. In Advances in Neural Information Processing Systems, pages 2674\u20132682, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20Christopher%20De%20Zhang%2C%20Ce%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20hogwild-style%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20Christopher%20De%20Zhang%2C%20Ce%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20hogwild-style%20algorithms%202015"
        },
        {
            "id": "10",
            "entry": "[10] Jeffrey Dean and Sanjay Ghemawat. MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107\u2013113, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dean%2C%20Jeffrey%20Ghemawat%2C%20Sanjay%20MapReduce%3A%20simplified%20data%20processing%20on%20large%20clusters%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dean%2C%20Jeffrey%20Ghemawat%2C%20Sanjay%20MapReduce%3A%20simplified%20data%20processing%20on%20large%20clusters%202008"
        },
        {
            "id": "11",
            "entry": "[11] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20SAGA%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "12",
            "entry": "[12] Martin Jaggi, Virginia Smith, Martin Tak\u00e1c, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 3068\u20133076, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Tak%C3%A1c%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Tak%C3%A1c%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014"
        },
        {
            "id": "13",
            "entry": "[13] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International Conference on Machine Learning, pages 1724\u20131732, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Ge%2C%20Rong%20Netrapalli%2C%20Praneeth%20Kakade%2C%20Sham%20M.%20How%20to%20escape%20saddle%20points%20efficiently%202017"
        },
        {
            "id": "14",
            "entry": "[14] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "15",
            "entry": "[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "16",
            "entry": "[16] Jason D Lee, Qiang Liu, Yuekai Sun, and Jonathan E Taylor. Communication-efficient sparse regression. Journal of Machine Learning Research, 18(5):1\u201330, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Liu%2C%20Qiang%20Sun%2C%20Yuekai%20Taylor%2C%20Jonathan%20E.%20Communication-efficient%20sparse%20regression%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Liu%2C%20Qiang%20Sun%2C%20Yuekai%20Taylor%2C%20Jonathan%20E.%20Communication-efficient%20sparse%20regression%202017"
        },
        {
            "id": "17",
            "entry": "[17] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on Operating Systems Design and Implementation, pages 583\u2013598, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Mu%20Andersen%2C%20David%20G.%20Park%2C%20Jun%20Woo%20Smola%2C%20Alexander%20J.%20Scaling%20distributed%20machine%20learning%20with%20the%20parameter%20server%202014"
        },
        {
            "id": "18",
            "entry": "[18] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 661\u2013670. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Mu%20Zhang%2C%20Tong%20Chen%2C%20Yuqiang%20Smola%2C%20Alexander%20J.%20Efficient%20mini-batch%20training%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Mu%20Zhang%2C%20Tong%20Chen%2C%20Yuqiang%20Smola%2C%20Alexander%20J.%20Efficient%20mini-batch%20training%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "19",
            "entry": "[19] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737\u20132745, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20Xiangru%20Huang%2C%20Yijun%20Li%2C%20Yuncheng%20Liu%2C%20Ji%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20Xiangru%20Huang%2C%20Yijun%20Li%2C%20Yuncheng%20Liu%2C%20Ji%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015"
        },
        {
            "id": "20",
            "entry": "[20] Yingyu Liang, Maria-Florina F Balcan, Vandana Kanchanapally, and David Woodruff. Improved distributed principal component analysis. In Advances in Neural Information Processing Systems, pages 3113\u20133121, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Yingyu%20Balcan%2C%20Maria-Florina%20F.%20Kanchanapally%2C%20Vandana%20Woodruff%2C%20David%20Improved%20distributed%20principal%20component%20analysis%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Yingyu%20Balcan%2C%20Maria-Florina%20F.%20Kanchanapally%2C%20Vandana%20Woodruff%2C%20David%20Improved%20distributed%20principal%20component%20analysis%202014"
        },
        {
            "id": "21",
            "entry": "[21] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Yujun%20Han%2C%20Song%20Mao%2C%20Huizi%20Wang%2C%20Yu%20Deep%20gradient%20compression%3A%20Reducing%20the%20communication%20bandwidth%20for%20distributed%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Yujun%20Han%2C%20Song%20Mao%2C%20Huizi%20Wang%2C%20Yu%20Deep%20gradient%20compression%3A%20Reducing%20the%20communication%20bandwidth%20for%20distributed%20training%202018"
        },
        {
            "id": "22",
            "entry": "[22] Ji Liu, Stephen J Wright, Christopher R\u00e9, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. The Journal of Machine Learning Research, 16(1):285\u2013322, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015"
        },
        {
            "id": "23",
            "entry": "[23] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693\u2013701, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "24",
            "entry": "[24] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming: Series A and B, 162(1-2):83\u2013112, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20Mark%20Roux%2C%20Nicolas%20Le%20Bach%2C%20Francis%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20Mark%20Roux%2C%20Nicolas%20Le%20Bach%2C%20Francis%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202017"
        },
        {
            "id": "25",
            "entry": "[25] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seide%2C%20Frank%20Fu%2C%20Hao%20Droppo%2C%20Jasha%20Li%2C%20Gang%201-bit%20stochastic%20gradient%20descent%20and%20its%20application%20to%20data-parallel%20distributed%20training%20of%20speech%20dnns%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seide%2C%20Frank%20Fu%2C%20Hao%20Droppo%2C%20Jasha%20Li%2C%20Gang%201-bit%20stochastic%20gradient%20descent%20and%20its%20application%20to%20data-parallel%20distributed%20training%20of%20speech%20dnns%202014"
        },
        {
            "id": "26",
            "entry": "[26] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "27",
            "entry": "[27] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In International Conference on Machine Learning, pages 1000\u20131008, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Zhang%2C%20Tong%20Communication-efficient%20distributed%20optimization%20using%20an%20approximate%20newton-type%20method%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Srebro%2C%20Nati%20Zhang%2C%20Tong%20Communication-efficient%20distributed%20optimization%20using%20an%20approximate%20newton-type%20method%202014"
        },
        {
            "id": "28",
            "entry": "[28] Ananda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean estimation with limited communication. In International Conference on Machine Learning, pages 3329\u20133337, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ananda%20Theertha%20Suresh%2C%20X.Yu%20Felix%20Kumar%2C%20Sanjiv%20McMahan%2C%20H.Brendan%20Distributed%20mean%20estimation%20with%20limited%20communication%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ananda%20Theertha%20Suresh%2C%20X.Yu%20Felix%20Kumar%2C%20Sanjiv%20McMahan%2C%20H.Brendan%20Distributed%20mean%20estimation%20with%20limited%20communication%202017"
        },
        {
            "id": "29",
            "entry": "[29] Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and H. Brendan McMahan. Distributed mean estimation with limited communication. In International Conference on Machine Learning, pages 3329\u2013 3337, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Suresh%2C%20Ananda%20Theertha%20Yu%2C%20Felix%20X.%20Kumar%2C%20Sanjiv%20McMahan%2C%20H.Brendan%20Distributed%20mean%20estimation%20with%20limited%20communication%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suresh%2C%20Ananda%20Theertha%20Yu%2C%20Felix%20X.%20Kumar%2C%20Sanjiv%20McMahan%2C%20H.Brendan%20Distributed%20mean%20estimation%20with%20limited%20communication%202017"
        },
        {
            "id": "30",
            "entry": "[30] John N Tsitsiklis and Zhi-Quan Luo. Communication complexity of convex optimization. Journal of Complexity, 3(3):231\u2013243, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsitsiklis%2C%20John%20N.%20Luo%2C%20Zhi-Quan%20Communication%20complexity%20of%20convex%20optimization%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsitsiklis%2C%20John%20N.%20Luo%2C%20Zhi-Quan%20Communication%20complexity%20of%20convex%20optimization%201987"
        },
        {
            "id": "31",
            "entry": "[31] Jialei Wang, Weiran Wang, and Nathan Srebro. Memory and communication efficient distributed stochastic optimization with minibatch prox. In Conference on Learning Theory, pages 1882\u20131919, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jialei%20Wang%2C%20Weiran%20Srebro%2C%20Nathan%20Memory%20and%20communication%20efficient%20distributed%20stochastic%20optimization%20with%20minibatch%20prox%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jialei%20Wang%2C%20Weiran%20Srebro%2C%20Nathan%20Memory%20and%20communication%20efficient%20distributed%20stochastic%20optimization%20with%20minibatch%20prox%202017"
        },
        {
            "id": "32",
            "entry": "[32] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. TernGrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1509\u20131519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20TernGrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20TernGrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017"
        },
        {
            "id": "33",
            "entry": "[33] Eric P Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu. Petuum: A new platform for distributed machine learning on big data. IEEE Transactions on Big Data, 1(2):49\u201367, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xing%2C%20Eric%20P.%20Ho%2C%20Qirong%20Dai%2C%20Wei%20Kim%2C%20Jin%20Kyu%20Petuum%3A%20A%20new%20platform%20for%20distributed%20machine%20learning%20on%20big%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xing%2C%20Eric%20P.%20Ho%2C%20Qirong%20Dai%2C%20Wei%20Kim%2C%20Jin%20Kyu%20Petuum%3A%20A%20new%20platform%20for%20distributed%20machine%20learning%20on%20big%20data%202015"
        },
        {
            "id": "34",
            "entry": "[34] Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear models with end-to-end low precision, and a little bit of deep learning. International Conference on Machine Learning, page 4035\u20134043, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Hantian%20Li%2C%20Jerry%20Kara%2C%20Kaan%20Alistarh%2C%20Dan%20ZipML%3A%20Training%20linear%20models%20with%20end-to-end%20low%20precision%2C%20and%20a%20little%20bit%20of%20deep%20learning",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Hantian%20Li%2C%20Jerry%20Kara%2C%20Kaan%20Alistarh%2C%20Dan%20ZipML%3A%20Training%20linear%20models%20with%20end-to-end%20low%20precision%2C%20and%20a%20little%20bit%20of%20deep%20learning"
        },
        {
            "id": "35",
            "entry": "[35] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In International Conference on Machine Learning, page 116, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Solving%20large%20scale%20linear%20prediction%20problems%20using%20stochastic%20gradient%20descent%20algorithms%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Solving%20large%20scale%20linear%20prediction%20problems%20using%20stochastic%20gradient%20descent%20algorithms%202004"
        },
        {
            "id": "36",
            "entry": "[36] Yuchen Zhang and Xiao Lin. DISCO: Distributed optimization for self-concordant empirical loss. In International Conference on Machine Learning, pages 362\u2013370, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20DISCO%3A%20Distributed%20optimization%20for%20self-concordant%20empirical%20loss%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20DISCO%3A%20Distributed%20optimization%20for%20self-concordant%20empirical%20loss%202015"
        },
        {
            "id": "37",
            "entry": "[37] Yuchen Zhang, Martin J Wainwright, and John C Duchi. Communication-efficient algorithms for statistical optimization. In Advances in Neural Information Processing Systems, pages 1502\u20131510, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Wainwright%2C%20Martin%20J.%20Duchi%2C%20John%20C.%20Communication-efficient%20algorithms%20for%20statistical%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Wainwright%2C%20Martin%20J.%20Duchi%2C%20John%20C.%20Communication-efficient%20algorithms%20for%20statistical%20optimization%202012"
        }
    ]
}
