{
    "filename": "7831-diverse-ensemble-evolution-curriculum-data-model-marriage.pdf",
    "metadata": {
        "title": "Diverse Ensemble Evolution: Curriculum Data-Model Marriage",
        "author": "Tianyi Zhou, Shengjie Wang, Jeff A. Bilmes",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7831-diverse-ensemble-evolution-curriculum-data-model-marriage.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study a new method (\u201cDiverse Ensemble Evolution (DivE2)\u201d) to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model\u2019s current expertise and an intraand inter-model diversity reward. DivE2 schedules, over the course of training epochs, the relative importance of these characteristics; it starts by selecting easy samples for each model, and then gradually adjusts towards the models having specialized and complementary expertise on subsets of the training data, thereby encouraging high accuracy of the ensemble. We utilize an intra-model diversity term on data assigned to each model, and an inter-model diversity term on data assigned to pairs of models, to penalize both within-model and cross-model redundancy. We formulate the data-model marriage problem as a generalized bipartite matching, represented as submodular maximization subject to two matroid constraints. DivE2 solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data-model marriage while the continuous part updates model parameters based on the assignments. In experiments, DivE2 outperforms other ensemble training methods under a variety of model aggregation techniques, while also maintaining competitive efficiency."
    },
    "keywords": [
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "submodular set function",
            "url": "https://en.wikipedia.org/wiki/submodular_set_function"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "Semiconductor Research Corporation",
            "url": "https://en.wikipedia.org/wiki/Semiconductor_Research_Corporation"
        }
    ],
    "highlights": [
        "Ensemble methods [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] are simple and powerful machine learning approaches to obtain improved performance by aggregating predictions over multiple models",
        "State-of-the-art results on many contemporary competitions/benchmarks are achieved via ensembles of deep neural networks (DNNs), e.g., ImageNet [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], SQuAD [<a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>], and the Kaggle competitions",
        "We propose an efficient meta-algorithm \u201cdiverse ensemble evolution (DivE2)\u201d, that \u201cevolves\u201d the ensemble adaptively by changing over stages both the diversity encouragement and each model\u2019s expertise, and this is based on information available during ensemble training",
        "Dynamic classifier selection (DCS) [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>, <a class=\"ref-link\" id=\"c73\" href=\"#r73\">73</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] selects different subsets of models to be aggregated for each sample",
        "All Average: evenly average the outputs of all m models. Random-k Average: randomly select k models and average their outputs. Top-k Confidence: select the top-k models with the highest confidence on the given sample, and average their outputs. Top-k dynamic classifier selection-KNN: apply an KNN based dynamic classifier selection method, i.e., find the K nearest neighbors of the given sample from the training data, select the top-k models assigned to the K nearest neighbors by Top-k Oracle, and average their outputs. Top-k neural networks-LossPredict: train an L2-regression neural nets with m outputs to predict the per-sample losses on the m models by using a training set composed of all training samples and their losses on the trained models",
        "DivE2:Top-k Confidence DivE2:Top-k dynamic classifier selection-KNN DivE2:Top-k neural networks-L.P.\n90.18 78.95 78.59 79.38 79.23 80.49 single models achieved by DivE2 is usually lower than those obtained by other baselines, the test accuracy on the ensemble is better"
    ],
    "key_statements": [
        "Ensemble methods [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] are simple and powerful machine learning approaches to obtain improved performance by aggregating predictions over multiple models",
        "State-of-the-art results on many contemporary competitions/benchmarks are achieved via ensembles of deep neural networks (DNNs), e.g., ImageNet [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], SQuAD [<a class=\"ref-link\" id=\"c55\" href=\"#r55\">55</a>], and the Kaggle competitions",
        "We propose an efficient meta-algorithm \u201cdiverse ensemble evolution (DivE2)\u201d, that \u201cevolves\u201d the ensemble adaptively by changing over stages both the diversity encouragement and each model\u2019s expertise, and this is based on information available during ensemble training",
        "By optimizing the objective G(A, W ), we explicitly encourage model diversity in the ensemble, while ensuring every sample gets learned by k models so that the ensemble can generate correct predictions",
        "Dynamic classifier selection (DCS) [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c47\" href=\"#r47\">47</a>, <a class=\"ref-link\" id=\"c73\" href=\"#r73\">73</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] selects different subsets of models to be aggregated for each sample",
        "All Average: evenly average the outputs of all m models. Random-k Average: randomly select k models and average their outputs. Top-k Confidence: select the top-k models with the highest confidence on the given sample, and average their outputs. Top-k dynamic classifier selection-KNN: apply an KNN based dynamic classifier selection method, i.e., find the K nearest neighbors of the given sample from the training data, select the top-k models assigned to the K nearest neighbors by Top-k Oracle, and average their outputs. Top-k neural networks-LossPredict: train an L2-regression neural nets with m outputs to predict the per-sample losses on the m models by using a training set composed of all training samples and their losses on the trained models",
        "DivE2:Top-k Confidence DivE2:Top-k dynamic classifier selection-KNN DivE2:Top-k neural networks-L.P.\n90.18 78.95 78.59 79.38 79.23 80.49 single models achieved by DivE2 is usually lower than those obtained by other baselines, the test accuracy on the ensemble is better"
    ],
    "summary": [
        "Ensemble methods [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c57\" href=\"#r57\">57</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] are simple and powerful machine learning approaches to obtain improved performance by aggregating predictions over multiple models.",
        "We apply DivE2 to four benchmark datasets, and show that it improves over randomization-based ensemble training methods on a variety of approaches to aggregate ensemble models into a single prediction.",
        "By optimizing the objective G(A, W ), we explicitly encourage model diversity in the ensemble, while ensuring every sample gets learned by k models so that the ensemble can generate correct predictions.",
        "When(\u00b7; wi) is non-convex, e.g., each model is a deep neural networks, there exist many practical and provable algorithms that can achieve a local optimal solution, say, by backpropagation.",
        "We compute an approximate solution A \u2713 E using submodular maximization SUBMODULARMAX; in lines 7-9 we compare Awith the old A on G(\u00b7, W ) and choose the better one; lines 10-13 run an optimizer \u21e1(\u00b7; \u2318) to update each model wi according to its assigned data.",
        "During later stages when the \u201csample selecting model\u201d constraint (Iv) dominates and and are almost 0, the inner modular maximization is be exactly solved (\u21b5 = 1) and greedy algorithm degenerates to simple sorting.",
        "Each stage uses SELECTLEARN (Algorithm 1) to solve a continuous-combinatorial optimization in the form of Eq (3) with pre-specified values of (k, p, , ) and initialization {wit 1}im=1 from the previous episode as a warm start.",
        "Static model selection methods [<a class=\"ref-link\" id=\"c72\" href=\"#r72\">72</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>] choose a subset of models from the ensemble and apply it to all samples.",
        "DivE2 assigns different subsets of samples to different models, so for aggregation, we may benefit more from using sample-specific model selection methods.",
        "Top-k DCS-KNN: apply an KNN based DCS method, i.e., find the K nearest neighbors of the given sample from the training data, select the top-k models assigned to the K nearest neighbors by Top-k Oracle, and average their outputs.",
        "Select the top-k models with the smallest predicted losses on the given sample, and average their outputs.",
        "Top-k Oracle Table 2: The highest test accuracy (%) achieved by different combinations of is always the best, and ensemble training and aggregation methods on four datasets, with k = 3.",
        "RND:Top-k NN-L.P. DivE2:Top-k Oracle (Cheat) DivE2:All Average DivE2:Random-k Avg. DivE2:Top-k Confidence DivE2:Top-k DCS-KNN DivE2:Top-k NN-L.P. 90.18 78.95 78.59 79.38 79.23 80.49 single models achieved by DivE2 is usually lower than those obtained by other baselines, the test accuracy on the ensemble is better."
    ],
    "headline": "We study a new method \u201d) to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model\u2019s current expertise and an intraand inter-model diversity reward",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Wenruo Bai, Jeffrey Bilmes, and William S. Noble. Bipartite matching generalizations for peptide identification in tandem mass spectrometry. In 7th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB), ACM SIGBio, Seattle, WA, October 2016. ACM, ACM SIGBio.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bai%2C%20Wenruo%20Bilmes%2C%20Jeffrey%20Noble%2C%20William%20S.%20Bipartite%20matching%20generalizations%20for%20peptide%20identification%20in%20tandem%20mass%20spectrometry.%20In%207th%202016-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bai%2C%20Wenruo%20Bilmes%2C%20Jeffrey%20Noble%2C%20William%20S.%20Bipartite%20matching%20generalizations%20for%20peptide%20identification%20in%20tandem%20mass%20spectrometry.%20In%207th%202016-10"
        },
        {
            "id": "2",
            "entry": "[2] Sumit Basu and Janara Christensen. Teaching classification boundaries to humans. In AAAI, pages 109\u2013115, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Basu%2C%20Sumit%20Christensen%2C%20Janara%20Teaching%20classification%20boundaries%20to%20humans%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Basu%2C%20Sumit%20Christensen%2C%20Janara%20Teaching%20classification%20boundaries%20to%20humans%202013"
        },
        {
            "id": "3",
            "entry": "[3] Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse m-best solutions in markov random fields. In ECCV, pages 1\u201316, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Batra%2C%20Dhruv%20Yadollahpour%2C%20Payman%20Guzman-Rivera%2C%20Abner%20Shakhnarovich%2C%20Gregory%20Diverse%20m-best%20solutions%20in%20markov%20random%20fields%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Batra%2C%20Dhruv%20Yadollahpour%2C%20Payman%20Guzman-Rivera%2C%20Abner%20Shakhnarovich%2C%20Gregory%20Diverse%20m-best%20solutions%20in%20markov%20random%20fields%202012"
        },
        {
            "id": "4",
            "entry": "[4] Yoshua Bengio. Evolving Culture Versus Local Minima, pages 109\u2013138. Springer Berlin Heidelberg, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Evolving%20Culture%20Versus%20Local%20Minima%202014"
        },
        {
            "id": "5",
            "entry": "[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u2013 1828, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Vincent%2C%20Pascal%20Representation%20learning%3A%20A%20review%20and%20new%20perspectives%202013"
        },
        {
            "id": "6",
            "entry": "[6] Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, pages 41\u201348, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Louradour%2C%20J%C3%A9r%C3%B4me%20Collobert%2C%20Ronan%20Weston%2C%20Jason%20Curriculum%20learning%202009"
        },
        {
            "id": "7",
            "entry": "[7] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123\u2013140, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Bagging%20predictors%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Bagging%20predictors%201996"
        },
        {
            "id": "8",
            "entry": "[8] Leo Breiman. Random forests. Machine Learning, 45(1):5\u201332, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Breiman%2C%20Leo%20Random%20forests%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Breiman%2C%20Leo%20Random%20forests%202001"
        },
        {
            "id": "9",
            "entry": "[9] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 535\u2013541, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bucilua%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bucilua%2C%20Cristian%20Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Model%20compression%202006"
        },
        {
            "id": "10",
            "entry": "[10] Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Crew%2C%20Geoff%20Ksikes%2C%20Alex%20Ensemble%20selection%20from%20libraries%20of%20models%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caruana%2C%20Rich%20Niculescu-Mizil%2C%20Alexandru%20Crew%2C%20Geoff%20Ksikes%2C%20Alex%20Ensemble%20selection%20from%20libraries%20of%20models%202004"
        },
        {
            "id": "11",
            "entry": "[11] Paulo R. Cavalin, Robert Sabourin, and Ching Y. Suen. Dynamic selection approaches for multiple classifier systems. Neural Computing and Applications, 22(3):673\u2013688, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cavalin%2C%20Paulo%20R.%20Sabourin%2C%20Robert%20Suen%2C%20Ching%20Y.%20Dynamic%20selection%20approaches%20for%20multiple%20classifier%20systems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cavalin%2C%20Paulo%20R.%20Sabourin%2C%20Robert%20Suen%2C%20Ching%20Y.%20Dynamic%20selection%20approaches%20for%20multiple%20classifier%20systems%202013"
        },
        {
            "id": "12",
            "entry": "[12] Adam Coates, Honglak Lee, and Andrew Y. Ng. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, pages 215\u2013223, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Coates%2C%20Adam%20Lee%2C%20Honglak%20Ng%2C%20Andrew%20Y.%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Coates%2C%20Adam%20Lee%2C%20Honglak%20Ng%2C%20Andrew%20Y.%20An%20analysis%20of%20single-layer%20networks%20in%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "13",
            "entry": "[13] M. Conforti and G. Cornuejols. Submodular set functions, matroids and the greedy algorithm: tight worst-case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Mathematics, 7(3):251\u2013274, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conforti%2C%20M.%20Cornuejols%2C%20G.%20Submodular%20set%20functions%2C%20matroids%20and%20the%20greedy%20algorithm%3A%20tight%20worst-case%20bounds%20and%20some%20generalizations%20of%20the%20Rado-Edmonds%20theorem%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conforti%2C%20M.%20Cornuejols%2C%20G.%20Submodular%20set%20functions%2C%20matroids%20and%20the%20greedy%20algorithm%3A%20tight%20worst-case%20bounds%20and%20some%20generalizations%20of%20the%20Rado-Edmonds%20theorem%201984"
        },
        {
            "id": "14",
            "entry": "[14] G. Cornu\u00e9jols, M. Fisher, and G.L. Nemhauser. On the uncapacitated location problem. Annals of Discrete Mathematics, 1:163\u2013177, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cornu%C3%A9jols%2C%20G.%20Fisher%2C%20M.%20Nemhauser%2C%20G.L.%20On%20the%20uncapacitated%20location%20problem%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cornu%C3%A9jols%2C%20G.%20Fisher%2C%20M.%20Nemhauser%2C%20G.L.%20On%20the%20uncapacitated%20location%20problem%201977"
        },
        {
            "id": "15",
            "entry": "[15] Andrew Cotter, Mahdi Milani Fard, Seungil You, Maya Gupta, and Jeff Bilmes. Constrained interacting submodular groupings. In International Conference on Machine Learning (ICML), Stockholm, Sweden, July 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cotter%2C%20Andrew%20Fard%2C%20Mahdi%20Milani%20You%2C%20Seungil%20Gupta%2C%20Maya%20Constrained%20interacting%20submodular%20groupings%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cotter%2C%20Andrew%20Fard%2C%20Mahdi%20Milani%20You%2C%20Seungil%20Gupta%2C%20Maya%20Constrained%20interacting%20submodular%20groupings%202018-07"
        },
        {
            "id": "16",
            "entry": "[16] Rafael M.O. Cruz, Robert Sabourin, and George D.C. Cavalcanti. Dynamic classifier selection: Recent advances and perspectives. Information Fusion, 41:195\u2013216, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cruz%2C%20Rafael%20M.O.%20Sabourin%2C%20Robert%20Cavalcanti%2C%20George%20D.C.%20Dynamic%20classifier%20selection%3A%20Recent%20advances%20and%20perspectives%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cruz%2C%20Rafael%20M.O.%20Sabourin%2C%20Robert%20Cavalcanti%2C%20George%20D.C.%20Dynamic%20classifier%20selection%3A%20Recent%20advances%20and%20perspectives%202018"
        },
        {
            "id": "17",
            "entry": "[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "18",
            "entry": "[18] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "19",
            "entry": "[19] B. Efron. Bootstrap methods: Another look at the jackknife. The Annals of Statistics, 7(1):1\u201326, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Efron%2C%20B.%20Bootstrap%20methods%3A%20Another%20look%20at%20the%20jackknife%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Efron%2C%20B.%20Bootstrap%20methods%3A%20Another%20look%20at%20the%20jackknife%201979"
        },
        {
            "id": "20",
            "entry": "[20] Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian J. Goodfellow, and Jascha Sohl-Dickstein. Adversarial examples that fool both human and computer vision. arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elsayed%2C%20Gamaleldin%20F.%20Shankar%2C%20Shreya%20Cheung%2C%20Brian%20Papernot%2C%20Nicolas%20Adversarial%20examples%20that%20fool%20both%20human%20and%20computer%20vision%202018"
        },
        {
            "id": "21",
            "entry": "[21] M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey. An analysis of approximations for maximizing submodular set functions-II. Mathematical Programming Studies, 8, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fisher%2C%20M.L.%20Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20An%20analysis%20of%20approximations%20for%20maximizing%20submodular%20set%20functions-II%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fisher%2C%20M.L.%20Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20An%20analysis%20of%20approximations%20for%20maximizing%20submodular%20set%20functions-II%201978"
        },
        {
            "id": "22",
            "entry": "[22] Madalina Fiterau and Artur Dubrawski. Projection retrieval for classification. In Advances in Neural Information Processing Systems 25, pages 3023\u20133031. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fiterau%2C%20Madalina%20Dubrawski%2C%20Artur%20Projection%20retrieval%20for%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fiterau%2C%20Madalina%20Dubrawski%2C%20Artur%20Projection%20retrieval%20for%20classification%202012"
        },
        {
            "id": "23",
            "entry": "[23] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119\u2013139, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Yoav%20Schapire%2C%20Robert%20E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997"
        },
        {
            "id": "24",
            "entry": "[24] Satoru Fujishige. Submodular functions and optimization. Annals of discrete mathematics. Elsevier, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fujishige%2C%20Satoru%20Submodular%20functions%20and%20optimization.%20Annals%20of%20discrete%20mathematics%202005"
        },
        {
            "id": "25",
            "entry": "[25] Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Near-optimal map inference for determinantal point processes. In NIPS, pages 2735\u20132743, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gillenwater%2C%20Jennifer%20Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Near-optimal%20map%20inference%20for%20determinantal%20point%20processes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gillenwater%2C%20Jennifer%20Kulesza%2C%20Alex%20Taskar%2C%20Ben%20Near-optimal%20map%20inference%20for%20determinantal%20point%20processes%202012"
        },
        {
            "id": "26",
            "entry": "[26] Abner Guzm\u00e1n-rivera, Dhruv Batra, and Pushmeet Kohli. Multiple choice learning: Learning to produce multiple structured outputs. In Advances in Neural Information Processing Systems 25, pages 1799\u20131807. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guzm%C3%A1n-rivera%2C%20Abner%20Batra%2C%20Dhruv%20Kohli%2C%20Pushmeet%20Multiple%20choice%20learning%3A%20Learning%20to%20produce%20multiple%20structured%20outputs%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guzm%C3%A1n-rivera%2C%20Abner%20Batra%2C%20Dhruv%20Kohli%2C%20Pushmeet%20Multiple%20choice%20learning%3A%20Learning%20to%20produce%20multiple%20structured%20outputs%202012"
        },
        {
            "id": "27",
            "entry": "[27] Shizhong Han, Zibo Meng, AHMED-SHEHAB KHAN, and Yan Tong. Incremental boosting convolutional neural network for facial action unit recognition. In Advances in Neural Information Processing Systems (NIPS), pages 109\u2013117. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Shizhong%20Zibo%20Meng%2C%20A.H.M.E.D.-S.H.E.H.A.B.K.H.A.N.%20Tong%2C%20Yan%20Incremental%20boosting%20convolutional%20neural%20network%20for%20facial%20action%20unit%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Shizhong%20Zibo%20Meng%2C%20A.H.M.E.D.-S.H.E.H.A.B.K.H.A.N.%20Tong%2C%20Yan%20Incremental%20boosting%20convolutional%20neural%20network%20for%20facial%20action%20unit%20recognition%202016"
        },
        {
            "id": "28",
            "entry": "[28] L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(10):993\u20131001, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hansen%2C%20L.K.%20Salamon%2C%20P.%20Neural%20network%20ensembles%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hansen%2C%20L.K.%20Salamon%2C%20P.%20Neural%20network%20ensembles%201990"
        },
        {
            "id": "29",
            "entry": "[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "30",
            "entry": "[30] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Vinyals%2C%20Oriol%20Dean%2C%20Jeffrey%20Distilling%20the%20knowledge%20in%20a%20neural%20network.%20In%20NIPS%20Deep%20Learning%20and%20Representation%20Learning%20Workshop%202015"
        },
        {
            "id": "31",
            "entry": "[31] Tin Kam Ho. Random decision forests. In Proceedings of 3rd International Conference on Document Analysis and Recognition, volume 1, pages 278\u2013282, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Tin%20Kam%20Random%20decision%20forests%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Tin%20Kam%20Random%20decision%20forests%201995"
        },
        {
            "id": "32",
            "entry": "[32] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: Train 1, get M for free. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Li%2C%20Yixuan%20Pleiss%2C%20Geoff%20Liu%2C%20Zhuang%20Snapshot%20ensembles%202017"
        },
        {
            "id": "33",
            "entry": "[33] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computing, 3(1):79\u201387, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacobs%2C%20Robert%20A.%20Jordan%2C%20Michael%20I.%20Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Adaptive%20mixtures%20of%20local%20experts%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacobs%2C%20Robert%20A.%20Jordan%2C%20Michael%20I.%20Nowlan%2C%20Steven%20J.%20Hinton%2C%20Geoffrey%20E.%20Adaptive%20mixtures%20of%20local%20experts%201991"
        },
        {
            "id": "34",
            "entry": "[34] Michael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural Computing, 6(2):181\u2013214, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20Michael%20I.%20Jacobs%2C%20Robert%20A.%20Hierarchical%20mixtures%20of%20experts%20and%20the%20em%20algorithm%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20Michael%20I.%20Jacobs%2C%20Robert%20A.%20Hierarchical%20mixtures%20of%20experts%20and%20the%20em%20algorithm%201994"
        },
        {
            "id": "35",
            "entry": "[35] Faisal Khan, Xiaojin (Jerry) Zhu, and Bilge Mutlu. How do humans teach: On curriculum learning and teaching dimension. In NIPS, pages 1449\u20131457, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Faisal%20Khan%2C%20Xiaojin%20%28Jerry%29%20Zhu%20Mutlu%2C%20Bilge%20How%20do%20humans%20teach%3A%20On%20curriculum%20learning%20and%20teaching%20dimension%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Faisal%20Khan%2C%20Xiaojin%20%28Jerry%29%20Zhu%20Mutlu%2C%20Bilge%20How%20do%20humans%20teach%3A%20On%20curriculum%20learning%20and%20teaching%20dimension%202011"
        },
        {
            "id": "36",
            "entry": "[36] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "37",
            "entry": "[37] Albert H. R. Ko, Robert Sabourin, and Alceu Souza Britto, Jr. From dynamic classifier selection to dynamic ensemble selection. Pattern Recognition, 41(5):1718\u20131731, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ko%2C%20Albert%20H.R.%20Sabourin%2C%20Robert%20Britto%2C%20Jr%2C%20Alceu%20Souza%20From%20dynamic%20classifier%20selection%20to%20dynamic%20ensemble%20selection%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ko%2C%20Albert%20H.R.%20Sabourin%2C%20Robert%20Britto%2C%20Jr%2C%20Alceu%20Souza%20From%20dynamic%20classifier%20selection%20to%20dynamic%20ensemble%20selection%202008"
        },
        {
            "id": "38",
            "entry": "[38] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "39",
            "entry": "[39] Anders Krogh and Jesper Vedelsby. Neural network ensembles, cross validation, and active learning. In Advances in Neural Information Processing Systems (NIPS), pages 231\u2013238. 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krogh%2C%20Anders%20Vedelsby%2C%20Jesper%20Neural%20network%20ensembles%2C%20cross%20validation%2C%20and%20active%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krogh%2C%20Anders%20Vedelsby%2C%20Jesper%20Neural%20network%20ensembles%2C%20cross%20validation%2C%20and%20active%20learning%201995"
        },
        {
            "id": "40",
            "entry": "[40] M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS, pages 1189\u20131197, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-paced%20learning%20for%20latent%20variable%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kumar%2C%20M.Pawan%20Packer%2C%20Benjamin%20Koller%2C%20Daphne%20Self-paced%20learning%20for%20latent%20variable%20models%202010"
        },
        {
            "id": "41",
            "entry": "[41] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David J. Crandall, and Dhruv Batra. Why m heads are better than one: Training a diverse ensemble of deep networks. arXiv, abs/1511.06314, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06314"
        },
        {
            "id": "42",
            "entry": "[42] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael Cogswell, Viresh Ranjan, David Crandall, and Dhruv Batra. Stochastic multiple choice learning for training diverse deep ensembles. In Advances in Neural Information Processing Systems 29, pages 2119\u20132127. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Stefan%20Prakash%2C%20Senthil%20Purushwalkam%20Shiva%20Cogswell%2C%20Michael%20Ranjan%2C%20Viresh%20Stochastic%20multiple%20choice%20learning%20for%20training%20diverse%20deep%20ensembles%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Stefan%20Prakash%2C%20Senthil%20Purushwalkam%20Shiva%20Cogswell%2C%20Michael%20Ranjan%2C%20Viresh%20Stochastic%20multiple%20choice%20learning%20for%20training%20diverse%20deep%20ensembles%202016"
        },
        {
            "id": "43",
            "entry": "[43] Hui Lin and Jeff Bilmes. Word alignment via submodular maximization over matroids. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 170\u2013175. Association for Computational Linguistics, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Hui%20Bilmes%2C%20Jeff%20Word%20alignment%20via%20submodular%20maximization%20over%20matroids%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Hui%20Bilmes%2C%20Jeff%20Word%20alignment%20via%20submodular%20maximization%20over%20matroids%202011"
        },
        {
            "id": "44",
            "entry": "[44] Hui Lin and Jeff A. Bilmes. A class of submodular functions for document summarization. In ACL, pages 510\u2013520, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Hui%20Bilmes%2C%20Jeff%20A.%20A%20class%20of%20submodular%20functions%20for%20document%20summarization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Hui%20Bilmes%2C%20Jeff%20A.%20A%20class%20of%20submodular%20functions%20for%20document%20summarization%202011"
        },
        {
            "id": "45",
            "entry": "[45] Hui Lin, Jeff A. Bilmes, and Shasha Xie. Graph-based submodular selection for extractive summarization. In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), Merano, Italy, December 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Hui%20Bilmes%2C%20Jeff%20A.%20Xie%2C%20Shasha%20Graph-based%20submodular%20selection%20for%20extractive%20summarization%202009-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Hui%20Bilmes%2C%20Jeff%20A.%20Xie%2C%20Shasha%20Graph-based%20submodular%20selection%20for%20extractive%20summarization%202009-12"
        },
        {
            "id": "46",
            "entry": "[46] S. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory (TIT), 28(2):129\u2013137, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lloyd%2C%20S.%20Least%20squares%20quantization%20in%20pcm%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lloyd%2C%20S.%20Least%20squares%20quantization%20in%20pcm%201982"
        },
        {
            "id": "47",
            "entry": "[47] Christopher J. Merz. Dynamical Selection of Learning Algorithms, pages 281\u2013290. Springer New York, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merz%2C%20Christopher%20J.%20Dynamical%20Selection%20of%20Learning%20Algorithms%201996"
        },
        {
            "id": "48",
            "entry": "[48] Michel Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization Techniques, volume 7 of Lecture Notes in Control and Information Sciences, chapter 27, pages 234\u2013243. Springer Berlin Heidelberg, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minoux%2C%20Michel%20Accelerated%20greedy%20algorithms%20for%20maximizing%20submodular%20set%20functions%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minoux%2C%20Michel%20Accelerated%20greedy%20algorithms%20for%20maximizing%20submodular%20set%20functions%201978"
        },
        {
            "id": "49",
            "entry": "[49] Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondr\u00e1k, and Andreas Krause. Lazier than lazy greedy. In AAAI, pages 1812\u20131818, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mirzasoleiman%2C%20Baharan%20Badanidiyuru%2C%20Ashwinkumar%20Karbasi%2C%20Amin%20Vondr%C3%A1k%2C%20Jan%20Lazier%20than%20lazy%20greedy%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mirzasoleiman%2C%20Baharan%20Badanidiyuru%2C%20Ashwinkumar%20Karbasi%2C%20Amin%20Vondr%C3%A1k%2C%20Jan%20Lazier%20than%20lazy%20greedy%202015"
        },
        {
            "id": "50",
            "entry": "[50] Mohammad Moghimi, Mohammad Saberian, Jian Yang, Li-Jia Li, Nuno Vasconcelos, and Serge Belongie. Boosted convolutional neural networks. In British Machine Vision Conference (BMVC), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moghimi%2C%20Mohammad%20Saberian%2C%20Mohammad%20Yang%2C%20Jian%20Li%2C%20Li-Jia%20Boosted%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moghimi%2C%20Mohammad%20Saberian%2C%20Mohammad%20Yang%2C%20Jian%20Li%2C%20Li-Jia%20Boosted%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "51",
            "entry": "[51] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions-I. Mathematical Programming, 14(1):265\u2013294, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Fisher%2C%20M.L.%20An%20analysis%20of%20approximations%20for%20maximizing%20submodular%20set%20functions-I%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemhauser%2C%20G.L.%20Wolsey%2C%20L.A.%20Fisher%2C%20M.L.%20An%20analysis%20of%20approximations%20for%20maximizing%20submodular%20set%20functions-I%201978"
        },
        {
            "id": "52",
            "entry": "[52] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127\u2013152, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Smooth%20minimization%20of%20non-smooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20Smooth%20minimization%20of%20non-smooth%20functions%202005"
        },
        {
            "id": "53",
            "entry": "[53] Ioannis Partalas, Grigorios Tsoumakas, and Ioannis Vlahavas. Focused ensemble selection: A diversity-based method for greedy ensemble selection. In European Conference on Artificial Intelligence (ECML), pages 117\u2013121, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Partalas%2C%20Ioannis%20Tsoumakas%2C%20Grigorios%20Vlahavas%2C%20Ioannis%20Focused%20ensemble%20selection%3A%20A%20diversity-based%20method%20for%20greedy%20ensemble%20selection%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Partalas%2C%20Ioannis%20Tsoumakas%2C%20Grigorios%20Vlahavas%2C%20Ioannis%20Focused%20ensemble%20selection%3A%20A%20diversity-based%20method%20for%20greedy%20ensemble%20selection%202008"
        },
        {
            "id": "54",
            "entry": "[54] Adarsh Prasad, Stefanie Jegelka, and Dhruv Batra. Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets. In NIPS, pages 2645\u20132653, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prasad%2C%20Adarsh%20Jegelka%2C%20Stefanie%20Batra%2C%20Dhruv%20Submodular%20meets%20structured%3A%20Finding%20diverse%20subsets%20in%20exponentially-large%20structured%20item%20sets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prasad%2C%20Adarsh%20Jegelka%2C%20Stefanie%20Batra%2C%20Dhruv%20Submodular%20meets%20structured%3A%20Finding%20diverse%20subsets%20in%20exponentially-large%20structured%20item%20sets%202014"
        },
        {
            "id": "55",
            "entry": "[55] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajpurkar%2C%20Pranav%20Zhang%2C%20Jian%20Lopyrev%2C%20Konstantin%20Liang%2C%20Percy%20Squad%3A%20100%2C%20000%2B%20questions%20for%20machine%20comprehension%20of%20text%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajpurkar%2C%20Pranav%20Zhang%2C%20Jian%20Lopyrev%2C%20Konstantin%20Liang%2C%20Percy%20Squad%3A%20100%2C%20000%2B%20questions%20for%20machine%20comprehension%20of%20text%202016"
        },
        {
            "id": "56",
            "entry": "[56] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. arXiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sandler%2C%20Mark%20Howard%2C%20Andrew%20G.%20Zhu%2C%20Menglong%20Zhmoginov%2C%20Andrey%20Inverted%20residuals%20and%20linear%20bottlenecks%3A%20Mobile%20networks%20for%20classification%2C%20detection%20and%20segmentation%202018"
        },
        {
            "id": "57",
            "entry": "[57] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197\u2013227, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schapire%2C%20Robert%20E.%20The%20strength%20of%20weak%20learnability%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schapire%2C%20Robert%20E.%20The%20strength%20of%20weak%20learnability%201990"
        },
        {
            "id": "58",
            "entry": "[58] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shazeer%2C%20Noam%20Mirhoseini%2C%20Azalia%20Maziarz%2C%20Krzysztof%20Davis%2C%20Andy%20Outrageously%20large%20neural%20networks%3A%20The%20sparsely-gated%20mixture-of-experts%20layer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shazeer%2C%20Noam%20Mirhoseini%2C%20Azalia%20Maziarz%2C%20Krzysztof%20Davis%2C%20Andy%20Outrageously%20large%20neural%20networks%3A%20The%20sparsely-gated%20mixture-of-experts%20layer%202017"
        },
        {
            "id": "59",
            "entry": "[59] Saurabh Singh, Derek Hoiem, and David Forsyth. Swapout: Learning an ensemble of deep architectures. In Advances in Neural Information Processing Systems (NIPS), pages 28\u201336. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Saurabh%20Hoiem%2C%20Derek%20Forsyth%2C%20David%20Swapout%3A%20Learning%20an%20ensemble%20of%20deep%20architectures%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Saurabh%20Hoiem%2C%20Derek%20Forsyth%2C%20David%20Swapout%3A%20Learning%20an%20ensemble%20of%20deep%20architectures%202016"
        },
        {
            "id": "60",
            "entry": "[60] Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. Baby Steps: How \u201cLess is More\u201d in unsupervised dependency parsing. In NIPS 2009 Workshop on Grammar Induction, Representation of Language and Language Learning, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Spitkovsky%2C%20Valentin%20I.%20Alshawi%2C%20Hiyan%20Jurafsky%2C%20Daniel%20Baby%20Steps%3A%20How%20%E2%80%9CLess%20is%20More%E2%80%9D%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Spitkovsky%2C%20Valentin%20I.%20Alshawi%2C%20Hiyan%20Jurafsky%2C%20Daniel%20Baby%20Steps%3A%20How%20%E2%80%9CLess%20is%20More%E2%80%9D%202009"
        },
        {
            "id": "61",
            "entry": "[61] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research (JMLR), 15:1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "62",
            "entry": "[62] James Steven Supancic III and Deva Ramanan. Self-paced learning for long-term tracking. In CVPR, pages 2379\u20132386, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Supancic%2C%20III%2C%20James%20Steven%20Ramanan%2C%20Deva%20Self-paced%20learning%20for%20long-term%20tracking%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Supancic%2C%20III%2C%20James%20Steven%20Ramanan%2C%20Deva%20Self-paced%20learning%20for%20long-term%20tracking%202013"
        },
        {
            "id": "63",
            "entry": "[63] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Conference on Computer Vision and Pattern Recognition (CVPR), volume 00, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Liu%2C%20Wei%20Yangqing%20Jia%2C%20P.Sermanet%20Reed%2C%20S.%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Liu%2C%20Wei%20Yangqing%20Jia%2C%20P.Sermanet%20Reed%2C%20S.%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "64",
            "entry": "[64] Kevin Tang, Vignesh Ramanathan, Li Fei-fei, and Daphne Koller. Shifting weights: Adapting object detectors from image to video. In NIPS, pages 638\u2013646, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Kevin%20Ramanathan%2C%20Vignesh%20Fei-fei%2C%20Li%20Koller%2C%20Daphne%20Shifting%20weights%3A%20Adapting%20object%20detectors%20from%20image%20to%20video%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Kevin%20Ramanathan%2C%20Vignesh%20Fei-fei%2C%20Li%20Koller%2C%20Daphne%20Shifting%20weights%3A%20Adapting%20object%20detectors%20from%20image%20to%20video%202012"
        },
        {
            "id": "65",
            "entry": "[65] Ye Tang, Yu-Bin Yang, and Yang Gao. Self-paced dictionary learning for image classification. In MM, pages 833\u2013836, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Ye%20Yang%2C%20Yu-Bin%20Gao%2C%20Yang%20Self-paced%20dictionary%20learning%20for%20image%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Ye%20Yang%2C%20Yu-Bin%20Gao%2C%20Yang%20Self-paced%20dictionary%20learning%20for%20image%20classification%202012"
        },
        {
            "id": "66",
            "entry": "[66] F. Tram\u00e8r, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tram%C3%A8r%2C%20F.%20Kurakin%2C%20A.%20Papernot%2C%20N.%20Goodfellow%2C%20I.%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tram%C3%A8r%2C%20F.%20Kurakin%2C%20A.%20Papernot%2C%20N.%20Goodfellow%2C%20I.%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018"
        },
        {
            "id": "67",
            "entry": "[67] Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks. In Advances in Neural Information Processing Systems (NIPS), pages 550\u2013558. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veit%2C%20Andreas%20Wilber%2C%20Michael%20J.%20Belongie%2C%20Serge%20Residual%20networks%20behave%20like%20ensembles%20of%20relatively%20shallow%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veit%2C%20Andreas%20Wilber%2C%20Michael%20J.%20Belongie%2C%20Serge%20Residual%20networks%20behave%20like%20ensembles%20of%20relatively%20shallow%20networks%202016"
        },
        {
            "id": "68",
            "entry": "[68] K. Woods, W. P. Kegelmeyer, and K. Bowyer. Combination of multiple classifiers using local accuracy estimates. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):405\u2013410, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woods%2C%20K.%20Kegelmeyer%2C%20W.P.%20Bowyer%2C%20K.%20Combination%20of%20multiple%20classifiers%20using%20local%20accuracy%20estimates%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woods%2C%20K.%20Kegelmeyer%2C%20W.P.%20Bowyer%2C%20K.%20Combination%20of%20multiple%20classifiers%20using%20local%20accuracy%20estimates%201997"
        },
        {
            "id": "69",
            "entry": "[69] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Han%20Rasul%2C%20Kashif%20Vollgraf%2C%20Roland%20Fashion-mnist%3A%20a%20novel%20image%20dataset%20for%20benchmarking%20machine%20learning%20algorithms%202017"
        },
        {
            "id": "70",
            "entry": "[70] Tianyi Zhou and Jeff Bilmes. Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Tianyi%20Bilmes%2C%20Jeff%20Minimax%20curriculum%20learning%3A%20Machine%20teaching%20with%20desirable%20difficulties%20and%20scheduled%20diversity%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Tianyi%20Bilmes%2C%20Jeff%20Minimax%20curriculum%20learning%3A%20Machine%20teaching%20with%20desirable%20difficulties%20and%20scheduled%20diversity%202018"
        },
        {
            "id": "71",
            "entry": "[71] Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Supplementary material for diverse ensemble evolution. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Tianyi%20Wang%2C%20Shengjie%20Bilmes%2C%20Jeff%20Supplementary%20material%20for%20diverse%20ensemble%20evolution%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Tianyi%20Wang%2C%20Shengjie%20Bilmes%2C%20Jeff%20Supplementary%20material%20for%20diverse%20ensemble%20evolution%202018"
        },
        {
            "id": "72",
            "entry": "[72] Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: Many could be better than all. Artificial Intelligence, 137(1):239\u2013263, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Zhi-Hua%20Wu%2C%20Jianxin%20Tang%2C%20Wei%20Ensembling%20neural%20networks%3A%20Many%20could%20be%20better%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Zhi-Hua%20Wu%2C%20Jianxin%20Tang%2C%20Wei%20Ensembling%20neural%20networks%3A%20Many%20could%20be%20better%202002"
        },
        {
            "id": "73",
            "entry": "[73] Xingquan Zhu, Xindong Wu, and Ying Yang. Dynamic classifier selection for effective mining from noisy data streams. In Data Mining, 2004. ICDM \u201904. Fourth IEEE International Conference on, pages 305\u2013312, 2004. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xingquan%20Zhu%20Xindong%20Wu%20and%20Ying%20Yang%20Dynamic%20classifier%20selection%20for%20effective%20mining%20from%20noisy%20data%20streams%20In%20Data%20Mining%202004%20ICDM%2004%20Fourth%20IEEE%20International%20Conference%20on%20pages%20305312%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xingquan%20Zhu%20Xindong%20Wu%20and%20Ying%20Yang%20Dynamic%20classifier%20selection%20for%20effective%20mining%20from%20noisy%20data%20streams%20In%20Data%20Mining%202004%20ICDM%2004%20Fourth%20IEEE%20International%20Conference%20on%20pages%20305312%202004"
        }
    ]
}
