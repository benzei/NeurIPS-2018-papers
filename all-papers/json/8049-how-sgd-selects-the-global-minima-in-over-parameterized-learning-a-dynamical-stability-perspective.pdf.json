{
    "filename": "8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf",
    "metadata": {
        "title": "How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective",
        "author": "Lei Wu, Chao Ma, Weinan E",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The question of which global minima are accessible by a stochastic gradient decent (SGD) algorithm with specific learning rate and batch size is studied from the perspective of dynamical stability. The concept of non-uniformity is introduced, which, together with sharpness, characterizes the stability property of a global minimum and hence the accessibility of a particular SGD algorithm to that global minimum. In particular, this analysis shows that learning rate and batch size play different roles in minima selection. Extensive empirical results seem to correlate well with the theoretical findings and provide further support to these claims."
    },
    "keywords": [
        {
            "term": "global minimum",
            "url": "https://en.wikipedia.org/wiki/global_minimum"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "In machine learning we have always faced with the following dilemma: The function that we minimize is the empirical risk but the one that we are really interested in is the population risk",
        "We show that both the sharpness and the non-uniformity are important for the selection of the global minima, we do observe in experiments that these two quantities are strongly correlated for deep neural network models",
        "The global minimum x\u2217 is linearly stable for stochastic gradient decent with learning rate \u03b7 and batch size B if the following condition is satisfied \u03bbmax",
        "From the sharpness-non-uniformity diagram we see that, when the learning rate is fixed, the set of global minima that are linearly stable for stochastic gradient decent is much smaller than that for GD",
        "First we study how the learning rate affects the sharpness of the solutions",
        "It was observed empirically that non-uniformity is roughly proportional to the sharpness"
    ],
    "key_statements": [
        "In machine learning we have always faced with the following dilemma: The function that we minimize is the empirical risk but the one that we are really interested in is the population risk",
        "We show that both the sharpness and the non-uniformity are important for the selection of the global minima, we do observe in experiments that these two quantities are strongly correlated for deep neural network models",
        "The global minimum x\u2217 is linearly stable for stochastic gradient decent with learning rate \u03b7 and batch size B if the following condition is satisfied \u03bbmax",
        "From the sharpness-non-uniformity diagram we see that, when the learning rate is fixed, the set of global minima that are linearly stable for stochastic gradient decent is much smaller than that for GD",
        "First we study how the learning rate affects the sharpness of the solutions",
        "It should be remarked that the same phenomenon is not expected to hold for very small learning rates, since there are not that many really sharp global minima either.\n4.2",
        "To investigate the accuracy of global minima selection criteria introduced in Section 3, we trained a large number of models, and display the results of the sharpness and non-uniformity in Figure 6",
        "It was observed empirically that non-uniformity is roughly proportional to the sharpness"
    ],
    "summary": [
        "In machine learning we have always faced with the following dilemma: The function that we minimize is the empirical risk but the one that we are really interested in is the population risk.",
        "We show that both the sharpness and the non-uniformity are important for the selection of the global minima, we do observe in experiments that these two quantities are strongly correlated for deep neural network models.",
        "For the overparametrized learning (OPL) problems of interest, all the global minima of f (x) are fixed points of the popular optimizers such as SGD, Adam, etc..",
        "The global minimum x\u2217 is linearly stable for SGD with learning rate \u03b7 and batch size B if the following condition is satisfied \u03bbmax",
        "The sharpness-non-uniformity diagram of SGD assume that the learning rate \u03b7 is fixed.",
        "From the sharpness-non-uniformity diagram we see that, when the learning rate is fixed, the set of global minima that are linearly stable for SGD is much smaller than that for GD.",
        "As shown in Figure 3, increasing the learning rate forces the SGD to choose global minima closer to the origin in the sharpness-non-uniformity diagram, which means smaller sharpness and smaller non-uniformity.",
        "Decreasing batch size only forces SGD to choose global minima with smaller non-uniformity.",
        "It should be remarked that the same phenomenon is not expected to hold for very small learning rates, since there are not that many really sharp global minima either.",
        "We trained a large number of models with different learning rates and batch sizes, and show the results in Figure 5.",
        "In the CIFAR10 experiment, with a fixed learning rate \u03b7 = 0.01, the non-uniformity of GD solutions is about 350, the same quantity is only about 100 for SGD with batch size B = 4.",
        "To investigate the accuracy of global minima selection criteria introduced in Section 3, we trained a large number of models, and display the results of the sharpness and non-uniformity in Figure 6.",
        "This partially explains why SGD tends to select flatter minima shown in Figure 5b, instead of sharp minima with low non-uniformity.",
        "To further explore of the generality of this observed correlation, we trained many more models with a variety of different learning rates, batch sizes and initialization.",
        "GD with large learning rates and SGD with small batch sizes converge to solutions that generalize better.",
        "For both GD and SGD, larger learning rates give rise to flatter solutions, and SGD tends to select minima with smaller non-uniformity than GD.",
        "This might explain why SGD tends to select flatter minima than GD in deep learning"
    ],
    "headline": "We show that both the sharpness and the non-uniformity are important for the selection of the global minima, we do observe in experiments that these two quantities are strongly correlated for deep neural network models",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Crispin Gardiner. Stochastic methods, volume 4. springer Berlin, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crispin%20Gardiner%20Stochastic%20methods%20volume%204%20springer%20Berlin%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crispin%20Gardiner%20Stochastic%20methods%20volume%204%20springer%20Berlin%202009"
        },
        {
            "id": "2",
            "entry": "[2] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "3",
            "entry": "[3] S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1\u201342, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Flat%20minima%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Flat%20minima%201997"
        },
        {
            "id": "4",
            "entry": "[4] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pages 1729\u20131739, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of nonconvex stochastic gradient descent. arXiv preprint arXiv:1705.07562, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07562"
        },
        {
            "id": "6",
            "entry": "[6] Stanis\u0142aw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.04623"
        },
        {
            "id": "7",
            "entry": "[7] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Keskar%2C%20N.S.%20Mudigere%2C%20D.%20Nocedal%2C%20J.%20Smelyanskiy%2C%20M.%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Keskar%2C%20N.S.%20Mudigere%2C%20D.%20Nocedal%2C%20J.%20Smelyanskiy%2C%20M.%20On%20large-batch%20training%20for%20deep%20learning%3A%20Generalization%20gap%20and%20sharp%20minima%202017"
        },
        {
            "id": "8",
            "entry": "[8] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pages 2101\u20132110. PMLR, Aug 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Qianxiao%20Tai%2C%20Cheng%20E%2C%20Weinan%20Stochastic%20modified%20equations%20and%20adaptive%20stochastic%20gradient%20algorithms%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Qianxiao%20Tai%2C%20Cheng%20E%2C%20Weinan%20Stochastic%20modified%20equations%20and%20adaptive%20stochastic%20gradient%20algorithms%202017-08"
        },
        {
            "id": "9",
            "entry": "[9] Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 3325\u20133334. PMLR, Jul 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20Siyuan%20Bassily%2C%20Raef%20Belkin%2C%20Mikhail%20The%20power%20of%20interpolation%3A%20Understanding%20the%20effectiveness%20of%20SGD%20in%20modern%20over-parametrized%20learning%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20Siyuan%20Bassily%2C%20Raef%20Belkin%2C%20Mikhail%20The%20power%20of%20interpolation%3A%20Understanding%20the%20effectiveness%20of%20SGD%20in%20modern%20over-parametrized%20learning%202018-07"
        },
        {
            "id": "10",
            "entry": "[10] Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Samuel%20L.%20Le%2C%20Quoc%20V.%20A%20bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20Samuel%20L.%20Le%2C%20Quoc%20V.%20A%20bayesian%20perspective%20on%20generalization%20and%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "11",
            "entry": "[11] Steven H Strogatz. Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering. CRC Press, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strogatz%2C%20Steven%20H.%20Nonlinear%20dynamics%20and%20chaos%3A%20with%20applications%20to%20physics%2C%20biology%2C%20chemistry%2C%20and%20engineering%202018"
        },
        {
            "id": "12",
            "entry": "[12] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pages 4151\u20134161, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2041514161%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashia%20C%20Wilson%20Rebecca%20Roelofs%20Mitchell%20Stern%20Nati%20Srebro%20and%20Benjamin%20Recht%20The%20marginal%20value%20of%20adaptive%20gradient%20methods%20in%20machine%20learning%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2041514161%202017"
        },
        {
            "id": "13",
            "entry": "[13] Lei Wu, Zhanxing Zhu, and Weinan E. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.10239"
        },
        {
            "id": "14",
            "entry": "[14] Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In International Conference on Artificial Intelligence and Statistics, pages 1998\u20132007, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yin%2C%20Dong%20Pananjady%2C%20Ashwin%20Lam%2C%20Max%20Papailiopoulos%2C%20Dimitris%20Gradient%20diversity%3A%20a%20key%20ingredient%20for%20scalable%20distributed%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yin%2C%20Dong%20Pananjady%2C%20Ashwin%20Lam%2C%20Max%20Papailiopoulos%2C%20Dimitris%20Gradient%20diversity%3A%20a%20key%20ingredient%20for%20scalable%20distributed%20learning%201998"
        },
        {
            "id": "15",
            "entry": "[15] Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from minima and regularization effects. arXiv preprint arXiv:1803.00195, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1803.00195"
        }
    ]
}
