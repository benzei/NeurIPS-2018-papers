{
    "filename": "8250-deep-generative-models-with-learnable-knowledge-constraints.pdf",
    "metadata": {
        "title": "Deep Generative Models with Learnable Knowledge Constraints",
        "author": "Zhiting Hu, Zichao Yang, Ruslan R. Salakhutdinov, LIANHUI Qin, Xiaodan Liang, Haoye Dong, Eric P. Xing",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8250-deep-generative-models-with-learnable-knowledge-constraints.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is modelagnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models."
    },
    "keywords": [
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "domain knowledge",
            "url": "https://en.wikipedia.org/wiki/domain_knowledge"
        },
        {
            "term": "Inverse reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Inverse_reinforcement_learning"
        },
        {
            "term": "Maximum entropy",
            "url": "https://en.wikipedia.org/wiki/Maximum_entropy"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "text generation",
            "url": "https://en.wikipedia.org/wiki/text_generation"
        }
    ],
    "highlights": [
        "Generative models provide a powerful mechanism for learning data distributions and simulating samples",
        "It is usually difficult to exploit in these various deep generative models rich problem structures and domain knowledge",
        "It is highly desirable to have a general means of incorporating arbitrary structured knowledge with any types of deep generative models in a principled way",
        "This paper develops a practical algorithm that is generally applicable to any deep generative models and any learnable constraints on arbitrary target space",
        "We adopt the maximum-entropy Inverse reinforcement learning formulation to derive the constraint learning objective in our algorithm, and leverage the unique structure of posterior regularization for efficient importance sampling estimation, which differs from these previous approaches.\n3 Connecting Posterior Regularization to Reinforcement Learning\n3.1",
        "We revealed the connections between posterior regularization and reinforcement learning, which motivates to learn the knowledge constraints in posterior regularization as reward learning in reinforcement learning"
    ],
    "key_statements": [
        "Generative models provide a powerful mechanism for learning data distributions and simulating samples",
        "It is usually difficult to exploit in these various deep generative models rich problem structures and domain knowledge",
        "It is highly desirable to have a general means of incorporating arbitrary structured knowledge with any types of deep generative models in a principled way",
        "Though the posterior regularization framework and the reinforcement learning are apparently distinct paradigms applied in different context, we show mathematical correspondence between the model and constraints in posterior regularization with the policy and reward in entropyregularized policy optimization [<a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], respectively",
        "We demonstrate the effectiveness of the proposed approach in both image and text generation (Figure 1)",
        "Leveraging domain knowledge of structure-preserving constraints, the resulting models improve over base generative models.\n2 Related Work",
        "This paper develops a practical algorithm that is generally applicable to any deep generative models and any learnable constraints on arbitrary target space",
        "We adopt the maximum-entropy Inverse reinforcement learning formulation to derive the constraint learning objective in our algorithm, and leverage the unique structure of posterior regularization for efficient importance sampling estimation, which differs from these previous approaches.\n3 Connecting Posterior Regularization to Reinforcement Learning\n3.1",
        "It is worth noting that though we present in the generative model context, the formulations, including the algorithm developed later, can straightforwardly be extended to other settings such as discriminative models",
        "We show that with an added body part consistency constraint, a plain end-to-end generative model can be trained to produce highly competitive results, significantly improving over base models that do not incorporate the problem structure",
        "Table 2 compares the full model with the base model (Row 4) and the one regularized with the constraint that is fixed after pre-training (Row 5)",
        "Human survey is performed by asking annotators to rank the quality of images generated by the three models on each of 200 test cases, and the percentages of ranked as the best are reported (Tied ranking is treated as negative result)",
        "The model with fixed constraint fails, partially because pre-training on external data does not necessarily fit to the current problem domain",
        "Row 2 is the base model with an additional binary discriminator that adversarial distinguishes between the generated sentence and the ground truth (i.e., a Generative Adversarial Networks model)",
        "We revealed the connections between posterior regularization and reinforcement learning, which motivates to learn the knowledge constraints in posterior regularization as reward learning in reinforcement learning",
        "The resulting algorithm is generally applicable to any deep generative models, and flexible to learn the constraints and model jointly"
    ],
    "summary": [
        "Generative models provide a powerful mechanism for learning data distributions and simulating samples.",
        "Leveraging domain knowledge of structure-preserving constraints, the resulting models improve over base generative models.",
        "For structured probabilistic models, posterior regularization (PR) and related frameworks [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] provide a general means to impose knowledge constraints during model estimation.",
        "We adopt the maximum-entropy IRL formulation to derive the constraint learning objective in our algorithm, and leverage the unique structure of PR for efficient importance sampling estimation, which differs from these previous approaches.",
        "We can see the close resemblance between Eq(6) with the PR objective in Eq(1), where the generative model p\u03b8(x) in PR corresponds to the reference policy p\u03c0(x), while the constraint f (x) corresponds to the reward R(x).",
        "With the unified view of these approaches, we derive a practical algorithm for arbitrary learnable constraints on any deep generative models.",
        "Algorithm 1 Joint Learning of Deep Generative Model and Constraints",
        "The constraint f\u03c6(x) can be seen as being optimized to assign lower energy) to real examples from pd(x), and higher energy to fake samples from q(x) which is the regularized model of the generator p\u03b8(x).",
        "Note that here fake samples are generated from q(x) and p\u03b8(x) in the two learning phases, respectively, which differs from previous adversarial methods for energy-based model estimation that simulate only from a generator.",
        "Distinct from the discriminator-centric view of the previous work [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>, <a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>], we primarily aim at improving the generative model by incorporating learned constraints.",
        "We show that with an added body part consistency constraint, a plain end-to-end generative model can be trained to produce highly competitive results, significantly improving over base models that do not incorporate the problem structure.",
        "With this connection and given that the generator in the task is an implicit generative model, here we can apply and learn the structured consistency constraint using GANs, which is equivalent to replacing q(x) in Eq(8) with p\u03b8(x).",
        "The resulting algorithm is generally applicable to any deep generative models, and flexible to learn the constraints and model jointly.",
        "The proposed algorithm, along with the previous work (e.g., [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>]), represents a general means of adding knowledge to black-box neural networks by devising knowledge-inspired losses/constraints that drive the model to learn the desired structures.",
        "Compared to the conventional end-to-end maximum likelihood learning that usually requires fully-annotated or paired data, the knowledge-aware losses provide additional supervisions based on, e.g., structured rules [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], other models [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], and datasets for other related tasks."
    ],
    "headline": "Though the posterior regularization framework and the reinforcement learning are apparently distinct paradigms applied in different context, we show mathematical correspondence between the model and constraints in posterior regularization with the policy and reward in entropyregularized policy optimization , respectively",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. Maximum a posteriori policy optimisation. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abdolmaleki%2C%20A.%20Springenberg%2C%20J.T.%20Tassa%2C%20Y.%20Munos%2C%20R.%20Maximum%20a%20posteriori%20policy%20optimisation.%20In%20ICLR%202018"
        },
        {
            "id": "2",
            "entry": "[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning to compose neural networks for question answering. arXiv preprint arXiv:1601.01705, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.01705"
        },
        {
            "id": "3",
            "entry": "[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "4",
            "entry": "[4] K. Bellare, G. Druck, and A. McCallum. Alternating projections for learning with expectation constraints. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 43\u201350. AUAI Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellare%2C%20K.%20Druck%2C%20G.%20McCallum%2C%20A.%20Alternating%20projections%20for%20learning%20with%20expectation%20constraints%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellare%2C%20K.%20Druck%2C%20G.%20McCallum%2C%20A.%20Alternating%20projections%20for%20learning%20with%20expectation%20constraints%202009"
        },
        {
            "id": "5",
            "entry": "[5] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Duan%2C%20Y.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20InfoGAN%3A%20Interpretable%20representation%20learning%20by%20information%20maximizing%20generative%20adversarial%20nets%202016"
        },
        {
            "id": "6",
            "entry": "[6] P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural Computation, 9(2):271\u2013278, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dayan%2C%20P.%20Hinton%2C%20G.E.%20Using%20expectation-maximization%20for%20reinforcement%20learning%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dayan%2C%20P.%20Hinton%2C%20G.E.%20Using%20expectation-maximization%20for%20reinforcement%20learning%201997"
        },
        {
            "id": "7",
            "entry": "[7] M. P. Deisenroth, G. Neumann, J. Peters, et al. A survey on policy search for robotics. Foundations and Trends R in Robotics, 2(1\u20132):1\u2013142, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deisenroth%2C%20M.P.%20Neumann%2C%20G.%20Peters%2C%20J.%20A%20survey%20on%20policy%20search%20for%20robotics%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deisenroth%2C%20M.P.%20Neumann%2C%20G.%20Peters%2C%20J.%20A%20survey%20on%20policy%20search%20for%20robotics%202013"
        },
        {
            "id": "8",
            "entry": "[8] Q. Diao, M. Qiu, C.-Y. Wu, A. J. Smola, J. Jiang, and C. Wang. Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS). In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 193\u2013202. ACM, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diao%2C%20Q.%20Qiu%2C%20M.%20Wu%2C%20C.-Y.%20Smola%2C%20A.J.%20Jointly%20modeling%20aspects%2C%20ratings%20and%20sentiments%20for%20movie%20recommendation%20%28JMARS%29%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diao%2C%20Q.%20Qiu%2C%20M.%20Wu%2C%20C.-Y.%20Smola%2C%20A.J.%20Jointly%20modeling%20aspects%2C%20ratings%20and%20sentiments%20for%20movie%20recommendation%20%28JMARS%29%202014"
        },
        {
            "id": "9",
            "entry": "[9] W. Fedus, I. Goodfellow, and A. M. Dai. MaskGAN: Better text generation via filling in the _. arXiv preprint arXiv:1801.07736, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07736"
        },
        {
            "id": "10",
            "entry": "[10] C. Finn, P. Christiano, P. Abbeel, and S. Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03852"
        },
        {
            "id": "11",
            "entry": "[11] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, pages 49\u201358, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20C.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20C.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Guided%20cost%20learning%3A%20Deep%20inverse%20optimal%20control%20via%20policy%20optimization%202016"
        },
        {
            "id": "12",
            "entry": "[12] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11248"
        },
        {
            "id": "13",
            "entry": "[13] K. Ganchev, J. Gillenwater, B. Taskar, et al. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11(Jul):2001\u20132049, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganchev%2C%20K.%20Gillenwater%2C%20J.%20Taskar%2C%20B.%20Posterior%20regularization%20for%20structured%20latent%20variable%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganchev%2C%20K.%20Gillenwater%2C%20J.%20Taskar%2C%20B.%20Posterior%20regularization%20for%20structured%20latent%20variable%20models%202010"
        },
        {
            "id": "14",
            "entry": "[14] K. Gong, X. Liang, X. Shen, and L. Lin. Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing. In CVPR, pages 6757\u20136765, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20K.%20Liang%2C%20X.%20Shen%2C%20X.%20Lin%2C%20L.%20Look%20into%20person%3A%20Self-supervised%20structure-sensitive%20learning%20and%20a%20new%20benchmark%20for%20human%20parsing%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20K.%20Liang%2C%20X.%20Shen%2C%20X.%20Lin%2C%20L.%20Look%20into%20person%3A%20Self-supervised%20structure-sensitive%20learning%20and%20a%20new%20benchmark%20for%20human%20parsing%202017"
        },
        {
            "id": "15",
            "entry": "[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "16",
            "entry": "[16] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.",
            "url": "http://www.deeplearningbook.org"
        },
        {
            "id": "17",
            "entry": "[17] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08165"
        },
        {
            "id": "18",
            "entry": "[18] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "19",
            "entry": "[19] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The \u201cwake-sleep\u201d algorithm for unsupervised neural networks. Science, 268(5214):1158, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Dayan%2C%20P.%20Frey%2C%20B.J.%20Neal%2C%20R.M.%20The%20%E2%80%9Cwake-sleep%E2%80%9D%20algorithm%20for%20unsupervised%20neural%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Dayan%2C%20P.%20Frey%2C%20B.J.%20Neal%2C%20R.M.%20The%20%E2%80%9Cwake-sleep%E2%80%9D%20algorithm%20for%20unsupervised%20neural%20networks%201995"
        },
        {
            "id": "20",
            "entry": "[20] A. Holtzman, J. Buys, M. Forbes, A. Bosselut, D. Golub, and Y. Choi. Learning to write with cooperative discriminators. In Proceedings of the Association for Computational Linguistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holtzman%2C%20A.%20Buys%2C%20J.%20Forbes%2C%20M.%20Bosselut%2C%20A.%20Learning%20to%20write%20with%20cooperative%20discriminators%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holtzman%2C%20A.%20Buys%2C%20J.%20Forbes%2C%20M.%20Bosselut%2C%20A.%20Learning%20to%20write%20with%20cooperative%20discriminators%202018"
        },
        {
            "id": "21",
            "entry": "[21] Z. Hu, X. Ma, Z. Liu, E. Hovy, and E. Xing. Harnessing deep neural networks with logic rules. In ACL, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Ma%2C%20X.%20Liu%2C%20Z.%20Hovy%2C%20E.%20Harnessing%20deep%20neural%20networks%20with%20logic%20rules%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Ma%2C%20X.%20Liu%2C%20Z.%20Hovy%2C%20E.%20Harnessing%20deep%20neural%20networks%20with%20logic%20rules%202016"
        },
        {
            "id": "22",
            "entry": "[22] Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing. Deep neural networks with massive learned knowledge. In EMNLP, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Yang%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20Deep%20neural%20networks%20with%20massive%20learned%20knowledge%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Yang%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20Deep%20neural%20networks%20with%20massive%20learned%20knowledge%202016"
        },
        {
            "id": "23",
            "entry": "[23] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Yang%2C%20Z.%20Liang%2C%20X.%20Salakhutdinov%2C%20R.%20Toward%20controlled%20generation%20of%20text%202017"
        },
        {
            "id": "24",
            "entry": "[24] Z. Hu, H. Shi, Z. Yang, B. Tan, T. Zhao, J. He, W. Wang, X. Yu, L. Qin, D. Wang, et al. Texar: A modularized, versatile, and extensible toolkit for text generation. arXiv preprint arXiv:1809.00794, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.00794"
        },
        {
            "id": "25",
            "entry": "[25] Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing. On unifying deep generative models. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20Z.%20Yang%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20On%20unifying%20deep%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20Z.%20Yang%2C%20Z.%20Salakhutdinov%2C%20R.%20Xing%2C%20E.P.%20On%20unifying%20deep%20generative%20models%202018"
        },
        {
            "id": "26",
            "entry": "[26] T. Kim and Y. Bengio. Deep directed generative models with energy-based probability estimation. arXiv preprint arXiv:1606.03439, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03439"
        },
        {
            "id": "27",
            "entry": "[27] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "28",
            "entry": "[28] M. J. Kusner, B. Paige, and J. M. Hern\u00e1ndez-Lobato. Grammar variational autoencoder. arXiv preprint arXiv:1703.01925, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.01925"
        },
        {
            "id": "29",
            "entry": "[29] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larochelle%2C%20H.%20Murray%2C%20I.%20The%20neural%20autoregressive%20distribution%20estimator%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20H.%20Murray%2C%20I.%20The%20neural%20autoregressive%20distribution%20estimator%202011"
        },
        {
            "id": "30",
            "entry": "[30] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00909"
        },
        {
            "id": "31",
            "entry": "[31] C. Li, J. Zhu, T. Shi, and B. Zhang. Max-margin deep generative models. In Advances in neural information processing systems, pages 1837\u20131845, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Zhu%2C%20J.%20Shi%2C%20T.%20Zhang%2C%20B.%20Max-margin%20deep%20generative%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Zhu%2C%20J.%20Shi%2C%20T.%20Zhang%2C%20B.%20Max-margin%20deep%20generative%20models%202015"
        },
        {
            "id": "32",
            "entry": "[32] P. Liang, M. I. Jordan, and D. Klein. Learning from measurements in exponential families. In Proc. of ICML, pages 641\u2013648. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20P.%20Jordan%2C%20M.I.%20Klein%2C%20D.%20Learning%20from%20measurements%20in%20exponential%20families%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20P.%20Jordan%2C%20M.I.%20Klein%2C%20D.%20Learning%20from%20measurements%20in%20exponential%20families%202009"
        },
        {
            "id": "33",
            "entry": "[33] X. Liang, Z. Hu, H. Zhang, C. Gan, and E. P. Xing. Recurrent topic-transition GAN for visual paragraph generation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20X.%20Hu%2C%20Z.%20Zhang%2C%20H.%20Gan%2C%20C.%20Recurrent%20topic-transition%20GAN%20for%20visual%20paragraph%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20X.%20Hu%2C%20Z.%20Zhang%2C%20H.%20Gan%2C%20C.%20Recurrent%20topic-transition%20GAN%20for%20visual%20paragraph%20generation%202017"
        },
        {
            "id": "34",
            "entry": "[34] X. Liang, Z. Hu, and E. Xing. Symbolic graph reasoning meets convolutions. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20X.%20Hu%2C%20Z.%20Xing%2C%20E.%20Symbolic%20graph%20reasoning%20meets%20convolutions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20X.%20Hu%2C%20Z.%20Xing%2C%20E.%20Symbolic%20graph%20reasoning%20meets%20convolutions%202018"
        },
        {
            "id": "35",
            "entry": "[35] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1096\u20131104, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Z.%20Luo%2C%20P.%20Qiu%2C%20S.%20Wang%2C%20X.%20Deepfashion%3A%20Powering%20robust%20clothes%20recognition%20and%20retrieval%20with%20rich%20annotations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Z.%20Luo%2C%20P.%20Qiu%2C%20S.%20Wang%2C%20X.%20Deepfashion%3A%20Powering%20robust%20clothes%20recognition%20and%20retrieval%20with%20rich%20annotations%202016"
        },
        {
            "id": "36",
            "entry": "[36] D. Lopez-Paz, L. Bottou, B. Sch\u00f6lkopf, and V. Vapnik. Unifying distillation and privileged information. arXiv preprint arXiv:1511.03643, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.03643"
        },
        {
            "id": "37",
            "entry": "[37] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool. Pose guided person image generation. In Advances in Neural Information Processing Systems, pages 405\u2013415, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20L.%20Jia%2C%20X.%20Sun%2C%20Q.%20Schiele%2C%20B.%20Pose%20guided%20person%20image%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20L.%20Jia%2C%20X.%20Sun%2C%20Q.%20Schiele%2C%20B.%20Pose%20guided%20person%20image%20generation%202017"
        },
        {
            "id": "38",
            "entry": "[38] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and M. Fritz. Disentangled person image generation. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20L.%20Sun%2C%20Q.%20Georgoulis%2C%20S.%20Gool%2C%20L.Van%20Disentangled%20person%20image%20generation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20L.%20Sun%2C%20Q.%20Georgoulis%2C%20S.%20Gool%2C%20L.Van%20Disentangled%20person%20image%20generation%202018"
        },
        {
            "id": "39",
            "entry": "[39] S. Mei, J. Zhu, and J. Zhu. Robust regBayes: Selectively incorporating first-order logic domain knowledge into bayesian models. In International Conference on Machine Learning, pages 253\u2013261, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20S.%20Zhu%2C%20J.%20Zhu%2C%20J.%20Robust%20regBayes%3A%20Selectively%20incorporating%20first-order%20logic%20domain%20knowledge%20into%20bayesian%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20S.%20Zhu%2C%20J.%20Zhu%2C%20J.%20Robust%20regBayes%3A%20Selectively%20incorporating%20first-order%20logic%20domain%20knowledge%20into%20bayesian%20models%202014"
        },
        {
            "id": "40",
            "entry": "[40] S. Mohamed and B. Lakshminarayanan. Learning in implicit generative models. arXiv preprint arXiv:1610.03483, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.03483"
        },
        {
            "id": "41",
            "entry": "[41] G. Neumann et al. Variational inference for policy search in changing situations. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, pages 817\u2013824, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neumann%2C%20G.%20Variational%20inference%20for%20policy%20search%20in%20changing%20situations%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neumann%2C%20G.%20Variational%20inference%20for%20policy%20search%20in%20changing%20situations%202011"
        },
        {
            "id": "42",
            "entry": "[42] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1601.06759"
        },
        {
            "id": "43",
            "entry": "[43] J. Peters, K. M\u00fclling, and Y. Altun. Relative entropy policy search. In AAAI, pages 1607\u20131612.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20J.%20M%C3%BClling%2C%20K.%20Altun%2C%20Y.%20Relative%20entropy%20policy%20search",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20J.%20M%C3%BClling%2C%20K.%20Altun%2C%20Y.%20Relative%20entropy%20policy%20search"
        },
        {
            "id": "44",
            "entry": "[44] A. Pumarola, A. Agudo, A. Sanfeliu, and F. Moreno-Noguer. Unsupervised person image synthesis in arbitrary poses. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pumarola%2C%20A.%20Agudo%2C%20A.%20Sanfeliu%2C%20A.%20Moreno-Noguer%2C%20F.%20Unsupervised%20person%20image%20synthesis%20in%20arbitrary%20poses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pumarola%2C%20A.%20Agudo%2C%20A.%20Sanfeliu%2C%20A.%20Moreno-Noguer%2C%20F.%20Unsupervised%20person%20image%20synthesis%20in%20arbitrary%20poses%202018"
        },
        {
            "id": "45",
            "entry": "[45] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "46",
            "entry": "[46] J. Schulman, X. Chen, and P. Abbeel. Equivalence between policy gradients and soft Q-learning. arXiv preprint arXiv:1704.06440, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.06440"
        },
        {
            "id": "47",
            "entry": "[47] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%2C%20volume%201%201998"
        },
        {
            "id": "48",
            "entry": "[48] B. Tan, Z. Hu, Z. Yang, R. Salakhutdinov, and E. Xing. Connecting the dots between MLE and RL for text generation. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tan%2C%20B.%20Hu%2C%20Z.%20Yang%2C%20Z.%20Salakhutdinov%2C%20R.%20Connecting%20the%20dots%20between%20MLE%20and%20RL%20for%20text%20generation%202018"
        },
        {
            "id": "49",
            "entry": "[49] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In Advances in neural information processing systems, pages 25\u201332, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taskar%2C%20B.%20Guestrin%2C%20C.%20Koller%2C%20D.%20Max-margin%20Markov%20networks%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taskar%2C%20B.%20Guestrin%2C%20C.%20Koller%2C%20D.%20Max-margin%20Markov%20networks%202004"
        },
        {
            "id": "50",
            "entry": "[50] D. Wang and Q. Liu. Learning to draw samples: With application to amortized MLE for generative adversarial learning. arXiv preprint arXiv:1611.01722, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01722"
        },
        {
            "id": "51",
            "entry": "[51] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. arXiv preprint arXiv:1711.11585, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.11585"
        },
        {
            "id": "52",
            "entry": "[52] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004"
        },
        {
            "id": "53",
            "entry": "[53] Z. Yang, Z. Hu, C. Dyer, E. Xing, and T. Berg-Kirkpatrick. Unsupervised text style transfer using language models as discriminators. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Hu%2C%20Z.%20Dyer%2C%20C.%20Xing%2C%20E.%20Unsupervised%20text%20style%20transfer%20using%20language%20models%20as%20discriminators%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Hu%2C%20Z.%20Dyer%2C%20C.%20Xing%2C%20E.%20Unsupervised%20text%20style%20transfer%20using%20language%20models%20as%20discriminators%202018"
        },
        {
            "id": "54",
            "entry": "[54] S. Zhai, Y. Cheng, R. Feris, and Z. Zhang. Generative adversarial networks as variational training of energy based models. arXiv preprint arXiv:1611.01799, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01799"
        },
        {
            "id": "55",
            "entry": "[55] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.03126"
        },
        {
            "id": "56",
            "entry": "[56] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ziebart%2C%20B.D.%20Maas%2C%20A.L.%20Bagnell%2C%20J.A.%20Dey%2C%20A.K.%20Maximum%20entropy%20inverse%20reinforcement%20learning%202008"
        },
        {
            "id": "57",
            "entry": "[57] G. Zweig and C. J. Burges. The Microsoft Research sentence completion challenge. Technical report, Citeseer, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zweig%2C%20G.%20Burges%2C%20C.J.%20The%20Microsoft%20Research%20sentence%20completion%20challenge%202011"
        }
    ]
}
