{
    "filename": "8125-bayesian-multi-domain-learning-for-cancer-subtype-discovery-from-next-generation-sequencing-count-data.pdf",
    "metadata": {
        "title": "Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data",
        "author": "Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh, Mingyuan Zhou, Xiaoning Qian",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8125-bayesian-multi-domain-learning-for-cancer-subtype-discovery-from-next-generation-sequencing-count-data.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without \u201cnegative transfer\u201d effects often seen in existing multi-task learning and transfer learning methods."
    },
    "keywords": [
        {
            "term": "hierarchical Dirichlet process",
            "url": "https://en.wikipedia.org/wiki/hierarchical_Dirichlet_process"
        },
        {
            "term": "cancer genome atlas",
            "url": "https://en.wikipedia.org/wiki/Cancer_Genome_Atlas"
        },
        {
            "term": "Multi-task learning",
            "url": "https://en.wikipedia.org/wiki/Multi-task_learning"
        },
        {
            "term": "next-generation sequencing",
            "url": "https://en.wikipedia.org/wiki/next-generation_sequencing"
        },
        {
            "term": "negative binomial",
            "url": "https://en.wikipedia.org/wiki/negative_binomial"
        },
        {
            "term": "Markov chain Monte Carlo",
            "url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"
        },
        {
            "term": "standard deviation",
            "url": "https://en.wikipedia.org/wiki/standard_deviation"
        },
        {
            "term": "The Cancer Genome Atlas",
            "url": "https://en.wikipedia.org/wiki/The_Cancer_Genome_Atlas"
        },
        {
            "term": "support vector machine",
            "url": "https://en.wikipedia.org/wiki/support_vector_machine"
        },
        {
            "term": "transfer learning",
            "url": "https://en.wikipedia.org/wiki/transfer_learning"
        },
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        },
        {
            "term": "complex disease",
            "url": "https://en.wikipedia.org/wiki/complex_disease"
        },
        {
            "term": "Dirichlet process",
            "url": "https://en.wikipedia.org/wiki/Dirichlet_process"
        },
        {
            "term": "Indian Buffet Process",
            "url": "https://en.wikipedia.org/wiki/Indian_Buffet_Process"
        }
    ],
    "highlights": [
        "We study Bayesian Multi-Domain Learning (BMDL) for analyzing count data from -generation sequencing (NGS) experiments, with the goal of enhancing cancer subtyping in the target domain with a limited number of next-generation sequencing samples by leveraging surrogate data from other domains, for example relevant data from other well-studied cancer types",
        "To verify the advantages of our Bayesian Multi-Domain Learning model with the flexibility to capture the varying sample relevance across domains with both domain-specific and globally shared latent factors, we have designed experiments based on both simulated data and RNA-seq count data from The Cancer Genome Atlas [The Cancer Genome Atlas Research Network et al, 2008]",
        "We have developed a multi-domain negative binomial latent factorization model, tailored for Bayesian multi-domain learning of next-generation sequencing count data\u2014Bayesian Multi-Domain Learning",
        "As Bayesian Multi-Domain Learning learns domain relevance based on given samples across domains and enables the flexibly of sharing useful information through common latent factors, Bayesian Multi-Domain Learning performs consistently better than single-domain learning regardless of the domain relevance level",
        "Our experiments have shown the promising potential of Bayesian Multi-Domain Learning in accurate and reproducible cancer subtyping with \u201csmall\u201d data through effective multi-domain learning by taking advantage of available data from different sources"
    ],
    "key_statements": [
        "We study Bayesian Multi-Domain Learning (BMDL) for analyzing count data from -generation sequencing (NGS) experiments, with the goal of enhancing cancer subtyping in the target domain with a limited number of next-generation sequencing samples by leveraging surrogate data from other domains, for example relevant data from other well-studied cancer types",
        "We construct a Bayesian Multi-Domain Learning (BMDL) framework based on a domain-dependent latent negative binomial (NB) factor model for next-generation sequencing counts so that (1) over-dispersion is appropriately modeled and ad-hoc pre-processing is not needed; (2) low-dimensional representations of counts in different domains can help achieve more robust subtyping results; and most importantly, (3) the sample relevance across domains can be explicitly learned to guarantee the effectiveness of joint learning across multiple domains",
        "To verify the advantages of our Bayesian Multi-Domain Learning model with the flexibility to capture the varying sample relevance across domains with both domain-specific and globally shared latent factors, we have designed experiments based on both simulated data and RNA-seq count data from The Cancer Genome Atlas [The Cancer Genome Atlas Research Network et al, 2008]",
        "negative binomial-hierarchical Dirichlet process [<a class=\"ref-link\" id=\"cZhou_2012_a\" href=\"#rZhou_2012_a\">Zhou and Carin, 2012</a>], for which all domains are assumed to share a set of latent factors",
        "We illustrate that Bayesian Multi-Domain Learning leads to more effective target domain learning compared to both hierarchical Dirichlet process and hierarchical gammanegative binomial process based models by assigning domain-specific latent factors to domains given observed samples, while learning the latent representations globally in a similar fashion as hierarchical Dirichlet process and hierarchical gammanegative binomial process",
        "Compared to the hierarchical Dirichlet process based methods, Bayesian Multi-Domain Learning can achieve up to 16% improvement in the highly-related setup due to the benefits brought by the gamma process modeling of count data instead of using Dirichlet process in hierarchical Dirichlet process models, which forces negative correlation and restricts the distribution over latent factor abundance [<a class=\"ref-link\" id=\"cWilliamson_et+al_2010_a\" href=\"#rWilliamson_et+al_2010_a\">Williamson et al, 2010</a>]",
        "We have developed a multi-domain negative binomial latent factorization model, tailored for Bayesian multi-domain learning of next-generation sequencing count data\u2014Bayesian Multi-Domain Learning",
        "As Bayesian Multi-Domain Learning learns domain relevance based on given samples across domains and enables the flexibly of sharing useful information through common latent factors, Bayesian Multi-Domain Learning performs consistently better than single-domain learning regardless of the domain relevance level",
        "Our experiments have shown the promising potential of Bayesian Multi-Domain Learning in accurate and reproducible cancer subtyping with \u201csmall\u201d data through effective multi-domain learning by taking advantage of available data from different sources"
    ],
    "summary": [
        "We study Bayesian Multi-Domain Learning (BMDL) for analyzing count data from -generation sequencing (NGS) experiments, with the goal of enhancing cancer subtyping in the target domain with a limited number of NGS samples by leveraging surrogate data from other domains, for example relevant data from other well-studied cancer types.",
        "We construct a Bayesian Multi-Domain Learning (BMDL) framework based on a domain-dependent latent negative binomial (NB) factor model for NGS counts so that (1) over-dispersion is appropriately modeled and ad-hoc pre-processing is not needed; (2) low-dimensional representations of counts in different domains can help achieve more robust subtyping results; and most importantly, (3) the sample relevance across domains can be explicitly learned to guarantee the effectiveness of joint learning across multiple domains.",
        "To verify the advantages of our BMDL model with the flexibility to capture the varying sample relevance across domains with both domain-specific and globally shared latent factors, we have designed experiments based on both simulated data and RNA-seq count data from TCGA [The Cancer Genome Atlas Research Network et al, 2008].",
        "We illustrate that BMDL leads to more effective target domain learning compared to both HDP and hGNBP based models by assigning domain-specific latent factors to domains given observed samples, while learning the latent representations globally in a similar fashion as HDP and hGNBP.",
        "When comparing the performance of BMDL and hGNBP, domain-specific latent factor assignment using the beta-Bernoulli process can be considered as the main reason for the superior performance of BMDL, especially in the low-related setup.",
        "Compared to the HDP based methods, BMDL can achieve up to 16% improvement in the highly-related setup due to the benefits brought by the gamma process modeling of count data instead of using DP in HDP models, which forces negative correlation and restricts the distribution over latent factor abundance [<a class=\"ref-link\" id=\"cWilliamson_et+al_2010_a\" href=\"#rWilliamson_et+al_2010_a\">Williamson et al, 2010</a>].",
        "Compared to hGNBP, BMDL can achieve up to 4% and 6% accuracy improvement, respectively, in highlyand low-related setups due to domain-specific latent factor assignment using the beta-Bernoulli process.",
        "It shows the great potential for effective data integration and joint learning even in the low-related setup: the performance is better than competing methods as well as the baseline hGNBP-NBFA using only target samples and increasing the number of source samples does not hurt the performance.",
        "By introducing this hierarchical Bayesian model with selector variables to flexibly assign both domain-specific and globally shared latent factors to different domains, the derived latent representations of NGS data preserves predictive information in corresponding domains so that accurate cancer subtyping is possible even with a limited number of samples.",
        "Our experiments have shown the promising potential of BMDL in accurate and reproducible cancer subtyping with \u201csmall\u201d data through effective multi-domain learning by taking advantage of available data from different sources"
    ],
    "headline": "We develop a Bayesian Multi-Domain Learning  model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small",
    "reference_links": [
        {
            "id": "Argyriou_et+al_2007_a",
            "entry": "A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in neural information processing systems, pages 41\u201348, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argyriou%2C%20A.%20Evgeniou%2C%20T.%20Pontil%2C%20M.%20Multi-task%20feature%20learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argyriou%2C%20A.%20Evgeniou%2C%20T.%20Pontil%2C%20M.%20Multi-task%20feature%20learning%202007"
        },
        {
            "id": "Chelba_2006_a",
            "entry": "C. Chelba and A. Acero. Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech & Language, 20(4):382\u2013399, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chelba%2C%20C.%20Acero%2C%20A.%20Adaptation%20of%20maximum%20entropy%20capitalizer%3A%20Little%20data%20can%20help%20a%20lot%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chelba%2C%20C.%20Acero%2C%20A.%20Adaptation%20of%20maximum%20entropy%20capitalizer%3A%20Little%20data%20can%20help%20a%20lot%202006"
        },
        {
            "id": "Dadaneh_et+al_2018_a",
            "entry": "S. Z. Dadaneh, X. Qian, and M. Zhou. BNP-Seq: Bayesian nonparametric differential expression analysis of sequencing count data. Journal of the American Statistical Association, 113(521):81\u201394, 2018. doi: 10.1080/01621459.2017.1328358.",
            "crossref": "https://dx.doi.org/10.1080/01621459.2017.1328358",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1080/01621459.2017.1328358"
        },
        {
            "id": "Fan_et+al_2008_a",
            "entry": "R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. Journal of machine learning research, 9(Aug):1871\u20131874, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20R.-E.%20Chang%2C%20K.-W.%20Hsieh%2C%20C.-J.%20Wang%2C%20X.-R.%20Liblinear%3A%20A%20library%20for%20large%20linear%20classification%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20R.-E.%20Chang%2C%20K.-W.%20Hsieh%2C%20C.-J.%20Wang%2C%20X.-R.%20Liblinear%3A%20A%20library%20for%20large%20linear%20classification%202008"
        },
        {
            "id": "Ghahramani_2006_a",
            "entry": "Z. Ghahramani and T. L. Griffiths. Infinite latent feature models and the Indian buffet process. In Advances in neural information processing systems, pages 475\u2013482, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Z.%20Griffiths%2C%20T.L.%20Infinite%20latent%20feature%20models%20and%20the%20Indian%20buffet%20process%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghahramani%2C%20Z.%20Griffiths%2C%20T.L.%20Infinite%20latent%20feature%20models%20and%20the%20Indian%20buffet%20process%202006"
        },
        {
            "id": "Hajiramezanali_et+al_2018_a",
            "entry": "E. Hajiramezanali, S. Z. Dadaneh, P. de Figueiredo, S.-H. Sze, M. Zhou, and X. Qian. Differential expression analysis of dynamical sequencing count data with a gamma Markov chain. arXiv preprint arXiv:1803.02527, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02527"
        },
        {
            "id": "Jacob_et+al_2009_a",
            "entry": "L. Jacob, J.-P. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In Advances in neural information processing systems, pages 745\u2013752, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jacob%2C%20L.%20Vert%2C%20J.-P.%20Bach%2C%20F.R.%20Clustered%20multi-task%20learning%3A%20A%20convex%20formulation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jacob%2C%20L.%20Vert%2C%20J.-P.%20Bach%2C%20F.R.%20Clustered%20multi-task%20learning%3A%20A%20convex%20formulation%202009"
        },
        {
            "id": "Kang_et+al_2011_a",
            "entry": "Z. Kang, K. Grauman, and F. Sha. Learning with whom to share in multi-task feature learning. In ICML, pages 521\u2013528, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kang%2C%20Z.%20Grauman%2C%20K.%20Sha%2C%20F.%20Learning%20with%20whom%20to%20share%20in%20multi-task%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kang%2C%20Z.%20Grauman%2C%20K.%20Sha%2C%20F.%20Learning%20with%20whom%20to%20share%20in%20multi-task%20feature%20learning%202011"
        },
        {
            "id": "Karbalayghareh_et+al_2018_a",
            "entry": "A. Karbalayghareh, X. Qian, and E. R. Dougherty. Optimal Bayesian Transfer Learning. IEEE Transactions on Signal Processing, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karbalayghareh%2C%20A.%20Qian%2C%20X.%20Dougherty%2C%20E.R.%20Optimal%20Bayesian%20Transfer%20Learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karbalayghareh%2C%20A.%20Qian%2C%20X.%20Dougherty%2C%20E.R.%20Optimal%20Bayesian%20Transfer%20Learning%202018"
        },
        {
            "id": "Kumar_2012_a",
            "entry": "A. Kumar and H. Daume III. Learning task grouping and overlap in multi-task learning. arXiv preprint arXiv:1206.6417, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6417"
        },
        {
            "id": "Cam_1960_a",
            "entry": "L. Le Cam. An approximation theorem for the Poisson binomial distribution. Pacific Journal of Mathematics, 10(4):1181\u20131197, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cam%2C%20L.Le%20An%20approximation%20theorem%20for%20the%20Poisson%20binomial%20distribution%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cam%2C%20L.Le%20An%20approximation%20theorem%20for%20the%20Poisson%20binomial%20distribution%201960"
        },
        {
            "id": "Love_et+al_2014_a",
            "entry": "M. I. Love, W. Huber, and S. Anders. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome biology, 15(12):550, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Love%2C%20M.I.%20Huber%2C%20W.%20Anders%2C%20S.%20Moderated%20estimation%20of%20fold%20change%20and%20dispersion%20for%20RNA-seq%20data%20with%20DESeq2%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Love%2C%20M.I.%20Huber%2C%20W.%20Anders%2C%20S.%20Moderated%20estimation%20of%20fold%20change%20and%20dispersion%20for%20RNA-seq%20data%20with%20DESeq2%202014"
        },
        {
            "id": "Pan_2010_a",
            "entry": "S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pan%2C%20S.J.%20Yang%2C%20Q.%20A%20survey%20on%20transfer%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pan%2C%20S.J.%20Yang%2C%20Q.%20A%20survey%20on%20transfer%20learning%202010"
        },
        {
            "id": "Pardoe_0000_a",
            "entry": "D. Pardoe and P. Stone. Boosting for regression transfer. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 863\u2013870.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pardoe%2C%20D.%20Stone%2C%20P.%20Boosting%20for%20regression%20transfer",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pardoe%2C%20D.%20Stone%2C%20P.%20Boosting%20for%20regression%20transfer"
        },
        {
            "id": "Omnipress_2010_a",
            "entry": "Omnipress, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Omnipress%202010"
        },
        {
            "id": "Passos_et+al_2012_a",
            "entry": "A. Passos, P. Rai, J. Wainer, and H. Daume III. Flexible modeling of latent task structures in multitask learning. arXiv preprint arXiv:1206.6486, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.6486"
        },
        {
            "id": "Patel_et+al_2015_a",
            "entry": "V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53\u201369, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patel%2C%20V.M.%20Gopalan%2C%20R.%20Li%2C%20R.%20Chellappa%2C%20R.%20Visual%20domain%20adaptation%3A%20A%20survey%20of%20recent%20advances%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patel%2C%20V.M.%20Gopalan%2C%20R.%20Li%2C%20R.%20Chellappa%2C%20R.%20Visual%20domain%20adaptation%3A%20A%20survey%20of%20recent%20advances%202015"
        },
        {
            "id": "Rai_2010_a",
            "entry": "P. Rai and H. Daume III. Infinite predictor subspace models for multitask learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 613\u2013620, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rai%2C%20P.%20Daume%2C%20III%2C%20H.%20Infinite%20predictor%20subspace%20models%20for%20multitask%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rai%2C%20P.%20Daume%2C%20III%2C%20H.%20Infinite%20predictor%20subspace%20models%20for%20multitask%20learning%202010"
        },
        {
            "id": "Schoelkopf_2002_a",
            "entry": "B. Sch\u00f6lkopf and A. J. Smola. Learning with kernels: Support vector machines, regularization, optimization, and beyond. MIT press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20B.%20Smola%2C%20A.J.%20Learning%20with%20kernels%3A%20Support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202002"
        },
        {
            "id": "Teh_et+al_2005_a",
            "entry": "Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Sharing clusters among related groups: Hierarchical Dirichlet processes. In Advances in neural information processing systems, pages 1385\u20131392, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teh%2C%20Y.W.%20Jordan%2C%20M.I.%20Beal%2C%20M.J.%20Blei%2C%20D.M.%20Sharing%20clusters%20among%20related%20groups%3A%20Hierarchical%20Dirichlet%20processes%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teh%2C%20Y.W.%20Jordan%2C%20M.I.%20Beal%2C%20M.J.%20Blei%2C%20D.M.%20Sharing%20clusters%20among%20related%20groups%3A%20Hierarchical%20Dirichlet%20processes%202005"
        },
        {
            "id": "The_2008_a",
            "entry": "The Cancer Genome Atlas Research Network et al. Comprehensive genomic characterization defines human glioblastoma genes and core pathways. Nature, 455(7216):1061, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20The%20Cancer%20Genome%20Atlas%20Research%20Network%20et%20al.%20Comprehensive%20genomic%20characterization%20defines%20human%20glioblastoma%20genes%20and%20core%20pathways%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20The%20Cancer%20Genome%20Atlas%20Research%20Network%20et%20al.%20Comprehensive%20genomic%20characterization%20defines%20human%20glioblastoma%20genes%20and%20core%20pathways%202008"
        },
        {
            "id": "Thibaux_2007_a",
            "entry": "R. Thibaux and M. I. Jordan. Hierarchical beta processes and the Indian buffet process. In Artificial Intelligence and Statistics, pages 564\u2013571, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thibaux%2C%20R.%20Jordan%2C%20M.I.%20Hierarchical%20beta%20processes%20and%20the%20Indian%20buffet%20process%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thibaux%2C%20R.%20Jordan%2C%20M.I.%20Hierarchical%20beta%20processes%20and%20the%20Indian%20buffet%20process%202007"
        },
        {
            "id": "Williamson_et+al_2010_a",
            "entry": "S. Williamson, C. Wang, K. Heller, and D. Blei. The IBP compound Dirichlet process and its application to focused topic modeling. 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williamson%2C%20S.%20Wang%2C%20C.%20Heller%2C%20K.%20Blei%2C%20D.%20The%20IBP%20compound%20Dirichlet%20process%20and%20its%20application%20to%20focused%20topic%20modeling%202010"
        },
        {
            "id": "Xue_et+al_2007_a",
            "entry": "Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Multi-task learning for classification with Dirichlet process priors. Journal of Machine Learning Research, 8(Jan):35\u201363, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xue%2C%20Y.%20Liao%2C%20X.%20Carin%2C%20L.%20Krishnapuram%2C%20B.%20Multi-task%20learning%20for%20classification%20with%20Dirichlet%20process%20priors%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xue%2C%20Y.%20Liao%2C%20X.%20Carin%2C%20L.%20Krishnapuram%2C%20B.%20Multi-task%20learning%20for%20classification%20with%20Dirichlet%20process%20priors%202007"
        },
        {
            "id": "Zhou_2018_a",
            "entry": "M. Zhou. Nonparametric Bayesian negative binomial factor analysis. Bayesian Analysis, pages 1061\u20131089, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Nonparametric%20Bayesian%20negative%20binomial%20factor%20analysis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Nonparametric%20Bayesian%20negative%20binomial%20factor%20analysis%202018"
        },
        {
            "id": "Zhou_2012_a",
            "entry": "M. Zhou and L. Carin. Augment-and-conquer negative binomial processes. In Advances in Neural Information Processing Systems, pages 2546\u20132554, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Carin%2C%20L.%20Augment-and-conquer%20negative%20binomial%20processes%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Carin%2C%20L.%20Augment-and-conquer%20negative%20binomial%20processes%202012"
        },
        {
            "id": "Zhou_2015_a",
            "entry": "M. Zhou and L. Carin. Negative binomial process count and mixture modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2):307\u2013320, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Carin%2C%20L.%20Negative%20binomial%20process%20count%20and%20mixture%20modeling%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Carin%2C%20L.%20Negative%20binomial%20process%20count%20and%20mixture%20modeling%202015"
        },
        {
            "id": "Zhou_et+al_2009_a",
            "entry": "M. Zhou, H. Chen, L. Ren, G. Sapiro, L. Carin, and J. W. Paisley. Non-parametric Bayesian dictionary learning for sparse image representations. In Advances in neural information processing systems, pages 2295\u20132303, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Chen%2C%20H.%20Ren%2C%20L.%20Sapiro%2C%20G.%20Non-parametric%20Bayesian%20dictionary%20learning%20for%20sparse%20image%20representations%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Chen%2C%20H.%20Ren%2C%20L.%20Sapiro%2C%20G.%20Non-parametric%20Bayesian%20dictionary%20learning%20for%20sparse%20image%20representations%202009"
        }
    ]
}
