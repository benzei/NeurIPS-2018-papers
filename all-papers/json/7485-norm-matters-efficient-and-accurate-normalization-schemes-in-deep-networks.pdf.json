{
    "filename": "7485-norm-matters-efficient-and-accurate-normalization-schemes-in-deep-networks.pdf",
    "metadata": {
        "title": "Norm matters: efficient and accurate normalization schemes in deep networks",
        "author": "Elad Hoffer, Ron Banner, Itay Golan, Daniel Soudry",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7485-norm-matters-efficient-and-accurate-normalization-schemes-in-deep-networks.pdf"
        },
        "abstract": "Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weightdecay, as tools to decouple weights\u2019 norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used L2 batch-norm, using normalization in L1 and L\u221e spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks. 2"
    },
    "keywords": [
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "weight decay",
            "url": "https://en.wikipedia.org/wiki/weight_decay"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "low precision",
            "url": "https://en.wikipedia.org/wiki/low_precision"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Deep neural networks are known to benefit from normalization between consecutive layers",
        "We find the mechanism through which weight decay before BN affects learning dynamics: we demonstrate that by adjusting the learning rate or normalization method we can exactly mimic the effect of weight decay on the learning dynamics",
        "We show empirically that the accuracy gained by using weight decay can be achieved without it, only by adjusting 95 the learning rate",
        "Given statistics on norms of each channel from a training with weight decay and BN, similar 90 results can be achieved without weight decay by mimicking the effective step size using the following correction on the learning rate: Test accuracy \u03b7Correction = \u03b7 w\n2 2 w[weight decay on]",
        "We analyzed common normalization techniques used in deep learning models, with BN as their prime representative"
    ],
    "key_statements": [
        "Deep neural networks are known to benefit from normalization between consecutive layers",
        "Issues with current normalization methods Batch-normalization, despite its merits, suffers from several issues, as pointed out by previous work [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]. These issues are not yet solved in current normalization methods",
        "We find the mechanism through which weight decay before BN affects learning dynamics: we demonstrate that by adjusting the learning rate or normalization method we can exactly mimic the effect of weight decay on the learning dynamics",
        "We show empirically that the accuracy gained by using weight decay can be achieved without it, only by adjusting 95 the learning rate",
        "Given statistics on norms of each channel from a training with weight decay and BN, similar 90 results can be achieved without weight decay by mimicking the effective step size using the following correction on the learning rate: Test accuracy \u03b7Correction = \u03b7 w\n2 2 w[weight decay on]",
        "An additional modification of weight-norm called \"normalization propagation\" [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] adds additional multiplicative and additive corrections to address the change of activation distribution introduced by the ReLU non-linearity used between layers in the network",
        "We analyzed common normalization techniques used in deep learning models, with BN as their prime representative",
        "Such a weight normalization scheme improves computational costs and can enable improved learning in tasks that were not suited for previous methods such as reinforcement-learning and temporal modeling"
    ],
    "summary": [
        "Deep neural networks are known to benefit from normalization between consecutive layers.",
        "Such as Weight-Normalization [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] have a much smaller computational cost but typically achieve significantly lower accuracy when used in large-scale tasks such as ImageNet [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "We suggest this happens since certain normalization methods, such as a BN, disentangle the effect of weight vector norm on the following activation layers.",
        "We show that by bounding the norm in a weight-normalization scheme, we can significantly improve its performance in convnets, and improve baseline performance in LSTMs. This method can alleviate several task-specific limitations of BN, and reduce its computational and memory costs.",
        "Given statistics on norms of each channel from a training with WD and BN, similar 90 results can be achieved without WD by mimicking the effective step size using the following correction on the learning rate: Test accuracy \u03b7Correction = \u03b7 w",
        "We aim to replace the use of L2 norm with scale-invariant alternatives which are more appealing computationally and for low-precision implementations.",
        "As the above variance computation involves sums of squares, the quantization of the L2 batch norm for training on optimized hardware can lead to numerical instability as well as to arithmetic overflows when dealing with large values.",
        "As weight-norm requires an L2 normalization over the output channels of the weight matrix, it alleviates both computational and task-specific shortcomings of BN, ensuring no dependency on the current batch of sample activations within a layer.",
        "An additional modification of weight-norm called \"normalization propagation\" [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] adds additional multiplicative and additive corrections to address the change of activation distribution introduced by the ReLU non-linearity used between layers in the network.",
        "We find that weight-norm can be improved substantially, solving the stability issues for large-scale task observed by Gitman & Ginsburg [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and achieving comparable accuracy.",
        "This perspective allowed us to re-evaluate the necessity of regularization methods such as weight decay, and to suggest new methods for normalization, targeting the computational, numerical and task-specific deficiencies of current techniques.",
        "We showed that the use of L1 and L\u221e-based normalization schemes could provide similar results to the standard BN while allowing low-precision computation.",
        "Such a weight normalization scheme improves computational costs and can enable improved learning in tasks that were not suited for previous methods such as reinforcement-learning and temporal modeling.",
        "A strong connection appears between the batch-size used and the optimal learning rate regime [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] and between the weight-decay factor and learning-rate [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]."
    ],
    "headline": "We present a novel view on the purpose and function of normalization methods and weightdecay, as tools to decouple weights\u2019 norm from the underlying optimized objective",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Arpit, D., Zhou, Y., Kota, B., and Govindaraju, V. Normalization propagation: A parametric technique for removing internal covariate shift in deep networks. In International Conference on Machine Learning, pp. 1168\u20131176, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arpit%2C%20D.%20Zhou%2C%20Y.%20Kota%2C%20B.%20Govindaraju%2C%20V.%20Normalization%20propagation%3A%20A%20parametric%20technique%20for%20removing%20internal%20covariate%20shift%20in%20deep%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arpit%2C%20D.%20Zhou%2C%20Y.%20Kota%2C%20B.%20Govindaraju%2C%20V.%20Normalization%20propagation%3A%20A%20parametric%20technique%20for%20removing%20internal%20covariate%20shift%20in%20deep%20networks%202016"
        },
        {
            "id": "2",
            "entry": "[2] Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "3",
            "entry": "[3] Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "4",
            "entry": "[4] B\u00f6s, S. Optimal weight decay in a perceptron. In International Conference on Artificial Neural Networks, pp. 551\u2013556.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=B%C3%B6s%2C%20S.%20Optimal%20weight%20decay%20in%20a%20perceptron",
            "oa_query": "https://api.scholarcy.com/oa_version?query=B%C3%B6s%2C%20S.%20Optimal%20weight%20decay%20in%20a%20perceptron"
        },
        {
            "id": "5",
            "entry": "[5] Bos, S. and Chug, E. Using weight decay to optimize the generalization ability of a perceptron. In Neural Networks, 1996., IEEE International Conference on, volume 1, pp. 241\u2013246. IEEE, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bos%2C%20S.%20Chug%2C%20E.%20Using%20weight%20decay%20to%20optimize%20the%20generalization%20ability%20of%20a%20perceptron%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bos%2C%20S.%20Chug%2C%20E.%20Using%20weight%20decay%20to%20optimize%20the%20generalization%20ability%20of%20a%20perceptron%201996"
        },
        {
            "id": "6",
            "entry": "[6] Cooijmans, T., Ballas, N., Laurent, C., G\u00fcl\u00e7ehre, \u00c7., and Courville, A. Recurrent batch normalization. arXiv preprint arXiv:1603.09025, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.09025"
        },
        {
            "id": "7",
            "entry": "[7] Courbariaux, M., Bengio, Y., and David, J.-P. Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.7024"
        },
        {
            "id": "8",
            "entry": "[8] Das, D., Mellempudi, N., Mudigere, D., et al. Mixed precision training of convolutional neural networks using integer operations. arXiv preprint arXiv:1802.00930, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00930"
        },
        {
            "id": "9",
            "entry": "[9] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "10",
            "entry": "[10] Eidnes, L. and N\u00f8kland, A. Shifting mean activation towards zero with bipolar activation functions. arXiv preprint arXiv:1709.04054, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04054"
        },
        {
            "id": "11",
            "entry": "[11] Gitman, I. and Ginsburg, B. Comparison of batch normalization and weight normalization algorithms for the large-scale image classification. CoRR, abs/1709.08145, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.08145"
        },
        {
            "id": "12",
            "entry": "[12] Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20X.%20Bengio%2C%20Y.%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "13",
            "entry": "[13] He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "14",
            "entry": "[14] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "15",
            "entry": "[15] Hoffer, E., Hubara, I., and Soudry, D. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pp. 1729\u20131739, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20E.%20Hubara%2C%20I.%20Soudry%2C%20D.%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20E.%20Hubara%2C%20I.%20Soudry%2C%20D.%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] Huang, L., Liu, X., Lang, B., and Li, B. Projection based weight normalization for deep neural networks. arXiv preprint arXiv:1710.02338, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.02338"
        },
        {
            "id": "17",
            "entry": "[17] Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. Binarized neural networks. In Advances in neural information processing systems, pp. 4107\u20134115, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hubara%2C%20I.%20Courbariaux%2C%20M.%20Soudry%2C%20D.%20El-Yaniv%2C%20R.%20Binarized%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hubara%2C%20I.%20Courbariaux%2C%20M.%20Soudry%2C%20D.%20El-Yaniv%2C%20R.%20Binarized%20neural%20networks%202016"
        },
        {
            "id": "18",
            "entry": "[18] Ioffe, S. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In Advances in Neural Information Processing Systems, pp. 1942\u20131950, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Batch%20renormalization%3A%20Towards%20reducing%20minibatch%20dependence%20in%20batch-normalized%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Batch%20renormalization%3A%20Towards%20reducing%20minibatch%20dependence%20in%20batch-normalized%20models%202017"
        },
        {
            "id": "19",
            "entry": "[19] Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "20",
            "entry": "[20] Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "21",
            "entry": "[21] K\u00f6ster, U., Webb, T., Wang, X., et al. Flexpoint: An adaptive numerical format for efficient training of deep neural networks. In Advances in Neural Information Processing Systems, pp. 1740\u20131750, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%C3%B6ster%2C%20U.%20Webb%2C%20T.%20Wang%2C%20X.%20Flexpoint%3A%20An%20adaptive%20numerical%20format%20for%20efficient%20training%20of%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%C3%B6ster%2C%20U.%20Webb%2C%20T.%20Wang%2C%20X.%20Flexpoint%3A%20An%20adaptive%20numerical%20format%20for%20efficient%20training%20of%20deep%20neural%20networks%202017"
        },
        {
            "id": "22",
            "entry": "[22] Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "23",
            "entry": "[23] Krogh, A. and Hertz, J. A. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950\u2013957, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krogh%2C%20A.%20Hertz%2C%20J.A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krogh%2C%20A.%20Hertz%2C%20J.A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992"
        },
        {
            "id": "24",
            "entry": "[24] Micikevicius, P., Narang, S., Alben, J., et al. Mixed precision training. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Micikevicius%2C%20P.%20Narang%2C%20S.%20Alben%2C%20J.%20Mixed%20precision%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Micikevicius%2C%20P.%20Narang%2C%20S.%20Alben%2C%20J.%20Mixed%20precision%20training%202018"
        },
        {
            "id": "25",
            "entry": "[25] Rota Bul\u00f2, S., Porzi, L., and Kontschieder, P. In-place activated batchnorm for memoryoptimized training of dnns. arXiv preprint arXiv:1712.02616, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02616"
        },
        {
            "id": "26",
            "entry": "[26] Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pp. 901\u2013909, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Kingma%2C%20D.P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Kingma%2C%20D.P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "27",
            "entry": "[27] Salimans, T., Goodfellow, I., Zaremba, W., et al. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234\u20132242, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Improved%20techniques%20for%20training%20gans%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Goodfellow%2C%20I.%20Zaremba%2C%20W.%20Improved%20techniques%20for%20training%20gans%202016"
        },
        {
            "id": "28",
            "entry": "[28] Smith, S. L., Kindermans, P.-J., and Le, Q. V. Don\u2019t decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00489"
        },
        {
            "id": "29",
            "entry": "[29] Soudry, D., Hubara, I., and Meir, R. Expectation backpropagation: parameter-free training of multilayer neural networks with continuous or discrete weights. In Neural Information Processing Systems, volume 2, pp. 963\u2013971, dec 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soudry%2C%20D.%20Hubara%2C%20I.%20Meir%2C%20R.%20Expectation%20backpropagation%3A%20parameter-free%20training%20of%20multilayer%20neural%20networks%20with%20continuous%20or%20discrete%20weights",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soudry%2C%20D.%20Hubara%2C%20I.%20Meir%2C%20R.%20Expectation%20backpropagation%3A%20parameter-free%20training%20of%20multilayer%20neural%20networks%20with%20continuous%20or%20discrete%20weights"
        },
        {
            "id": "30",
            "entry": "[30] Soudry, D., Hoffer, E., and Srebro, N. The implicit bias of gradient descent on separable data. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soudry%2C%20D.%20Hoffer%2C%20E.%20Srebro%2C%20N.%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soudry%2C%20D.%20Hoffer%2C%20E.%20Srebro%2C%20N.%20The%20implicit%20bias%20of%20gradient%20descent%20on%20separable%20data%202018"
        },
        {
            "id": "31",
            "entry": "[31] Takeru Miyato, M. K. Y. Y., Toshiki Kataoka. Spectral normalization for generative adversarial networks. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takeru%20Miyato%2C%20M.K.Y.Y.%20Kataoka%2C%20Toshiki%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takeru%20Miyato%2C%20M.K.Y.Y.%20Kataoka%2C%20Toshiki%20Spectral%20normalization%20for%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "32",
            "entry": "[32] Ulyanov, D., Vedaldi, A., and Lempitsky, V. S. Instance normalization: The missing ingredient for fast stylization. CoRR, abs/1607.08022, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.08022"
        },
        {
            "id": "33",
            "entry": "[33] van Laarhoven, T. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.05350"
        },
        {
            "id": "34",
            "entry": "[34] Vaswani, A., Shazeer, N., Parmar, N., et al. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000\u20136010, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaswani%20A%20Shazeer%20N%20Parmar%20N%20et%20al%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaswani%20A%20Shazeer%20N%20Parmar%20N%20et%20al%20Attention%20is%20all%20you%20need%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pp%2060006010%202017"
        },
        {
            "id": "35",
            "entry": "[35] Venkatesh, G., Nurvitadhi, E., and Marr, D. Accelerating deep convolutional networks using low-precision and sparsity. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on, pp. 2861\u20132865. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Venkatesh%2C%20G.%20Nurvitadhi%2C%20E.%20Marr%2C%20D.%20Accelerating%20deep%20convolutional%20networks%20using%20low-precision%20and%20sparsity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Venkatesh%2C%20G.%20Nurvitadhi%2C%20E.%20Marr%2C%20D.%20Accelerating%20deep%20convolutional%20networks%20using%20low-precision%20and%20sparsity%202017"
        },
        {
            "id": "36",
            "entry": "[36] Wu, S., Li, G., Deng, L., et al. L1-Norm Batch Normalization for Efficient Training of Deep Neural Networks. ArXiv e-prints, February 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20S.%20Li%2C%20G.%20Deng%2C%20L.%20L1-Norm%20Batch%20Normalization%20for%20Efficient%20Training%20of%20Deep%20Neural%20Networks.%20ArXiv%20e-prints%202018-02"
        },
        {
            "id": "37",
            "entry": "[37] Wu, Y. and He, K. Group normalization. arXiv preprint arXiv:1803.08494, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08494"
        },
        {
            "id": "38",
            "entry": "[38] Xiang, S. and Li, H. On the effect of batch normalization and weight normalization in generative adversarial networks. arXiv preprint arXiv:1704.03971, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03971"
        },
        {
            "id": "39",
            "entry": "[39] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. ",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        }
    ]
}
