{
    "filename": "7932-hybrid-macromicro-level-backpropagation-for-training-deep-spiking-neural-networks.pdf",
    "metadata": {
        "title": "Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks",
        "author": "Yingyezhe Jin, Wenrui Zhang, Peng Li",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7932-hybrid-macromicro-level-backpropagation-for-training-deep-spiking-neural-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Spiking neural networks (SNNs) are positioned to enable spatio-temporal information processing and ultra-low power event-driven neuromorphic hardware. However, SNNs are yet to reach the same performances of conventional deep artificial neural networks (ANNs), a long-standing challenge due to complex dynamics and non-differentiable spike events encountered in training. The existing SNN error backpropagation (BP) methods are limited in terms of scalability, lack of proper handling of spiking discontinuities, and/or mismatch between the ratecoded loss function and computed gradient. We present a hybrid macro/micro level backpropagation (HM2-BP) algorithm for training multi-layer SNNs. The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level. The rate-coded errors are defined at the macroscopic level, computed and back-propagated across both macroscopic and microscopic levels. Different from existing BP methods, HM2-BP directly computes the gradient of the rate-coded loss function w.r.t tunable parameters. We evaluate the proposed HM2-BP algorithm by training deep fully connected and convolutional SNNs based on the static MNIST [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] and dynamic neuromorphic N-MNIST [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]. HM2-BP achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing SNN BP algorithms. Furthermore, the HM2-BP produces the highest accuracies based on SNNs for the EMNIST [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] dataset, and leads to high recognition accuracy for the 16-speaker spoken English letters of TI46 Corpus [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], a challenging patio-temporal speech recognition benchmark for which no prior success based on SNNs was reported. It also achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams."
    },
    "keywords": [
        {
            "term": "Semiconductor Research Corporation",
            "url": "https://en.wikipedia.org/wiki/Semiconductor_Research_Corporation"
        },
        {
            "term": "artificial neural networks",
            "url": "https://en.wikipedia.org/wiki/artificial_neural_networks"
        },
        {
            "term": "spike train",
            "url": "https://en.wikipedia.org/wiki/spike_train"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "spiking neural networks",
            "url": "https://en.wikipedia.org/wiki/spiking_neural_networks"
        },
        {
            "term": "micro level",
            "url": "https://en.wikipedia.org/wiki/micro_level"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "In spite of recent success in deep neural networks (DNNs) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], it is believed that biological brains operate rather differently",
        "Connected spiking neural networks for Extended MNIST-Balanced Table 4 shows that the HM2-BP outperforms the non-spiking artificial neural networks and the spike-based backpropagation rule reported in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] significantly with less training epochs",
        "We present a novel hybrid macro/micro level error backpropagation scheme to train deep spiking neural networks directly based on spiking activities",
        "Using our efficient GPU implementation of the proposed method, we demonstrate the best performances for both fully connected and convolutional spiking neural networks over the static MNIST, the dynamic N-MNIST and the more challenging Extended MNIST-Balanced and 16-speaker spoken English letters of TI46 datasets, outperforming the best previously reported spiking neural networks training techniques",
        "The proposed approach achieves competitive performances better than those of the conventional deep learning models when dealing with asynchronous spiking streams",
        "Orchestrating the information flow based on a combination of temporal effects and firing rate behaviors across the two levels in an interactive manner allows for the definition of the rate-coded loss function at the macro level, and backpropagation of errors from the macro level to the micro level, and back to the macro level"
    ],
    "key_statements": [
        "In spite of recent success in deep neural networks (DNNs) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], it is believed that biological brains operate rather differently",
        "There are theoretical evidences supporting that spiking neural networks possess greater computational power over traditional artificial neural networks (ANNs) [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "Our BP algorithm achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing spiking neural networks BP algorithms.\n2 Hybrid Macro-Micro Backpropagation",
        "More training settings are reported in the released source code",
        "Connected spiking neural networks for Extended MNIST-Balanced Table 4 shows that the HM2-BP outperforms the non-spiking artificial neural networks and the spike-based backpropagation rule reported in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] significantly with less training epochs",
        "We present a novel hybrid macro/micro level error backpropagation scheme to train deep spiking neural networks directly based on spiking activities",
        "We further propose a decoupled spike-train level post-synaptic potentials model to assist gradient computation at the micro-level",
        "In contrast to the previous methods, our hybrid approach directly computes the gradient of the rate-coded loss function with respect to tunable parameters",
        "Using our efficient GPU implementation of the proposed method, we demonstrate the best performances for both fully connected and convolutional spiking neural networks over the static MNIST, the dynamic N-MNIST and the more challenging Extended MNIST-Balanced and 16-speaker spoken English letters of TI46 datasets, outperforming the best previously reported spiking neural networks training techniques",
        "The proposed approach achieves competitive performances better than those of the conventional deep learning models when dealing with asynchronous spiking streams",
        "The performances achieved by the proposed BP method may be attributed to the fact that it addresses key challenges of spiking neural networks training in terms of scalability, handling of temporal effects, and gradient computation of loss functions with inherent discontinuities",
        "Coping with these difficulties through error backpropagation at both the macro and micro levels provides a unique perspective to training of spiking neural networks",
        "Orchestrating the information flow based on a combination of temporal effects and firing rate behaviors across the two levels in an interactive manner allows for the definition of the rate-coded loss function at the macro level, and backpropagation of errors from the macro level to the micro level, and back to the macro level"
    ],
    "summary": [
        "In spite of recent success in deep neural networks (DNNs) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>], it is believed that biological brains operate rather differently.",
        "Bp over firing rates bp over spike trains where ai is the aggregated membrane potential for the post-synaptic neuron i per (11).",
        "We further propose a decoupled model of the S-PSP for disentangling the effects of firing rates and spike-train timings to allow differentiation of the S-PSP w.r.t. pre and post-synaptic firing rates at the micro-level.",
        "Unlike [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], here decomposing the rate-coded error backpropagation into the macro and micro levels enables computation of the gradient of the actual loss function with respect to the tunable weights, leading to highly competitive performances.",
        "According to (9), the S-PSP eki|j is dependent on both rate and temporal information of the pre and post-synaptic spikes.",
        "We decompose eik|j into an asymptotic rate-dependent effect using the product of okj and oki and a correction factor \u03b1accounting for temporal correlations between the pre and post-synaptic spike trains eik|j = \u03b1, ti(f))ojk oki .",
        "We attribute the overall improvement to the hybrid macro-micro processing that handles the temporal effects and discontinuities at two levels in a way such that explicit back-propagation of the rate-coded error becomes possible and practical.",
        "Connected SNNs for EMNIST Table 4 shows that the HM2-BP outperforms the non-spiking ANN and the spike-based backpropagation rule reported in [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>] significantly with less training epochs.",
        "We present a novel hybrid macro/micro level error backpropagation scheme to train deep SNNs directly based on spiking activities.",
        "Using our efficient GPU implementation of the proposed method, we demonstrate the best performances for both fully connected and convolutional SNNs over the static MNIST, the dynamic N-MNIST and the more challenging EMNIST and 16-speaker spoken English letters of TI46 datasets, outperforming the best previously reported SNN training techniques.",
        "The performances achieved by the proposed BP method may be attributed to the fact that it addresses key challenges of SNN training in terms of scalability, handling of temporal effects, and gradient computation of loss functions with inherent discontinuities.",
        "Coping with these difficulties through error backpropagation at both the macro and micro levels provides a unique perspective to training of SNNs. More specifically, orchestrating the information flow based on a combination of temporal effects and firing rate behaviors across the two levels in an interactive manner allows for the definition of the rate-coded loss function at the macro level, and backpropagation of errors from the macro level to the micro level, and back to the macro level.",
        "The proposed approach achieves competitive performances better than those of the conventional deep learning models when dealing with asynchronous spiking streams"
    ],
    "headline": "We present a hybrid macro/micro level backpropagation  algorithm for training multi-layer spiking neural networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ben Varkey Benjamin, Peiran Gao, Emmett McQuinn, Swadesh Choudhary, Anand R Chandrasekaran, Jean-Marie Bussat, Rodrigo Alvarez-Icaza, John V Arthur, Paul A Merolla, and Kwabena Boahen. Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations. Proceedings of the IEEE, 102(5):699\u2013716, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Benjamin%2C%20Ben%20Varkey%20Gao%2C%20Peiran%20McQuinn%2C%20Emmett%20Choudhary%2C%20Swadesh%20Paul%20A%20Merolla%2C%20and%20Kwabena%20Boahen.%20Neurogrid%3A%20A%20mixed-analog-digital%20multichip%20system%20for%20large-scale%20neural%20simulations%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Benjamin%2C%20Ben%20Varkey%20Gao%2C%20Peiran%20McQuinn%2C%20Emmett%20Choudhary%2C%20Swadesh%20Paul%20A%20Merolla%2C%20and%20Kwabena%20Boahen.%20Neurogrid%3A%20A%20mixed-analog-digital%20multichip%20system%20for%20large-scale%20neural%20simulations%202014"
        },
        {
            "id": "2",
            "entry": "[2] Sander M Bohte, Joost N Kok, and Han La Poutre. Error-backpropagation in temporally encoded networks of spiking neurons. Neurocomputing, 48(1-4):17\u201337, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bohte%2C%20Sander%20M.%20Kok%2C%20Joost%20N.%20Poutre%2C%20Han%20La%20Error-backpropagation%20in%20temporally%20encoded%20networks%20of%20spiking%20neurons%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bohte%2C%20Sander%20M.%20Kok%2C%20Joost%20N.%20Poutre%2C%20Han%20La%20Error-backpropagation%20in%20temporally%20encoded%20networks%20of%20spiking%20neurons%202002"
        },
        {
            "id": "3",
            "entry": "[3] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00e9 van Schaik. EMNIST: an extension of mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05373"
        },
        {
            "id": "4",
            "entry": "[4] Gregory K Cohen, Garrick Orchard, Sio-Hoi Leng, Jonathan Tapson, Ryad B Benosman, and Andr\u00e9 Van Schaik. Skimming digits: neuromorphic classification of spike-encoded images. Frontiers in neuroscience, 10:184, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Gregory%20K.%20Orchard%2C%20Garrick%20Leng%2C%20Sio-Hoi%20Tapson%2C%20Jonathan%20Skimming%20digits%3A%20neuromorphic%20classification%20of%20spike-encoded%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Gregory%20K.%20Orchard%2C%20Garrick%20Leng%2C%20Sio-Hoi%20Tapson%2C%20Jonathan%20Skimming%20digits%3A%20neuromorphic%20classification%20of%20spike-encoded%20images%202016"
        },
        {
            "id": "5",
            "entry": "[5] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collobert%2C%20Ronan%20Weston%2C%20Jason%20A%20unified%20architecture%20for%20natural%20language%20processing%3A%20Deep%20neural%20networks%20with%20multitask%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collobert%2C%20Ronan%20Weston%2C%20Jason%20A%20unified%20architecture%20for%20natural%20language%20processing%3A%20Deep%20neural%20networks%20with%20multitask%20learning%202008"
        },
        {
            "id": "6",
            "entry": "[6] Peter U Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer. Fastclassifying, high-accuracy spiking deep networks through weight and threshold balancing. In Neural Networks (IJCNN), 2015 International Joint Conference on, pages 1\u20138. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diehl%2C%20Peter%20U.%20Neil%2C%20Daniel%20Binas%2C%20Jonathan%20Cook%2C%20Matthew%20Fastclassifying%2C%20high-accuracy%20spiking%20deep%20networks%20through%20weight%20and%20threshold%20balancing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diehl%2C%20Peter%20U.%20Neil%2C%20Daniel%20Binas%2C%20Jonathan%20Cook%2C%20Matthew%20Fastclassifying%2C%20high-accuracy%20spiking%20deep%20networks%20through%20weight%20and%20threshold%20balancing%202015"
        },
        {
            "id": "7",
            "entry": "[7] Steve K Esser, Rathinakumar Appuswamy, Paul Merolla, John V Arthur, and Dharmendra S Modha. Backpropagation for energy-efficient neuromorphic computing. In Advances in Neural Information Processing Systems, pages 1117\u20131125, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esser%2C%20Steve%20K.%20Appuswamy%2C%20Rathinakumar%20Merolla%2C%20Paul%20Arthur%2C%20John%20V.%20Backpropagation%20for%20energy-efficient%20neuromorphic%20computing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Esser%2C%20Steve%20K.%20Appuswamy%2C%20Rathinakumar%20Merolla%2C%20Paul%20Arthur%2C%20John%20V.%20Backpropagation%20for%20energy-efficient%20neuromorphic%20computing%202015"
        },
        {
            "id": "8",
            "entry": "[8] Wulfram Gerstner and Werner M Kistler. Spiking neuron models: Single neurons, populations, plasticity. Cambridge university press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gerstner%2C%20Wulfram%20Kistler%2C%20Werner%20M.%20Spiking%20neuron%20models%3A%20Single%20neurons%2C%20populations%2C%20plasticity%202002"
        },
        {
            "id": "9",
            "entry": "[9] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20Deng%2C%20Li%20Yu%2C%20Dong%20Dahl%2C%20George%20E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012"
        },
        {
            "id": "10",
            "entry": "[10] Eric Hunsberger and Chris Eliasmith. Spiking deep networks with lif neurons. arXiv preprint arXiv:1510.08829, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.08829"
        },
        {
            "id": "11",
            "entry": "[11] Eugene M Izhikevich and Gerald M Edelman. Large-scale model of mammalian thalamocortical systems. Proceedings of the national academy of sciences, 105(9):3593\u20133598, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Izhikevich%2C%20Eugene%20M.%20Edelman%2C%20Gerald%20M.%20Large-scale%20model%20of%20mammalian%20thalamocortical%20systems%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Izhikevich%2C%20Eugene%20M.%20Edelman%2C%20Gerald%20M.%20Large-scale%20model%20of%20mammalian%20thalamocortical%20systems%202008"
        },
        {
            "id": "12",
            "entry": "[12] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "13",
            "entry": "[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "14",
            "entry": "[14] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "15",
            "entry": "[15] Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. Training deep spiking neural networks using backpropagation. Frontiers in neuroscience, 10:508, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jun%20Haeng%20Delbruck%2C%20Tobi%20Pfeiffer%2C%20Michael%20Training%20deep%20spiking%20neural%20networks%20using%20backpropagation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jun%20Haeng%20Delbruck%2C%20Tobi%20Pfeiffer%2C%20Michael%20Training%20deep%20spiking%20neural%20networks%20using%20backpropagation%202016"
        },
        {
            "id": "16",
            "entry": "[16] Mark Liberman, Robert Amsler, Ken Church, Ed Fox, Carole Hafner, Judy Klavans, Mitch Marcus, Bob Mercer, Jan Pedersen, Paul Roossin, Don Walker, Susan Warwick, and Antonio Zampolli. The TI46 speech corpus. http://catalog.ldc.upenn.edu/LDC93S9. Accessed:2014-06-30.",
            "url": "http://catalog.ldc.upenn.edu/LDC93S9",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mark%20Liberman%20Robert%20Amsler%20Ken%20Church%20Ed%20Fox%20Carole%20Hafner%20Judy%20Klavans%20Mitch%20Marcus%20Bob%20Mercer%20Jan%20Pedersen%20Paul%20Roossin%20Don%20Walker%20Susan%20Warwick%20and%20Antonio%20Zampolli%20The%20TI46%20speech%20corpus%20httpcatalogldcupenneduLDC93S9%20Accessed20140630"
        },
        {
            "id": "17",
            "entry": "[17] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. A 128 \u00d7128 120 db 15\u03bcs latency asynchronous temporal contrast vision sensor. IEEE journal of solid-state circuits, 43(2):566\u2013576, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lichtsteiner%2C%20Patrick%20Posch%2C%20Christoph%20Delbruck%2C%20Tobi%20120%20db%2015%CE%BCs%20latency%20asynchronous%20temporal%20contrast%20vision%20sensor%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lichtsteiner%2C%20Patrick%20Posch%2C%20Christoph%20Delbruck%2C%20Tobi%20120%20db%2015%CE%BCs%20latency%20asynchronous%20temporal%20contrast%20vision%20sensor%202008"
        },
        {
            "id": "18",
            "entry": "[18] Richard F Lyon. A computational model of filtering, detection, and compression in the cochlea. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201982., volume 7, pages 1282\u20131285. IEEE, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lyon%2C%20Richard%20F.%20A%20computational%20model%20of%20filtering%2C%20detection%2C%20and%20compression%20in%20the%20cochlea%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lyon%2C%20Richard%20F.%20A%20computational%20model%20of%20filtering%2C%20detection%2C%20and%20compression%20in%20the%20cochlea%201982"
        },
        {
            "id": "19",
            "entry": "[19] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9):1659\u20131671, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maass%2C%20Wolfgang%20Networks%20of%20spiking%20neurons%3A%20the%20third%20generation%20of%20neural%20network%20models%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maass%2C%20Wolfgang%20Networks%20of%20spiking%20neurons%3A%20the%20third%20generation%20of%20neural%20network%20models%201997"
        },
        {
            "id": "20",
            "entry": "[20] Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197):668\u2013673, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Alvarez-Icaza%2C%20Rodrigo%20Cassidy%2C%20Andrew%20S.%20A%20million%20spiking-neuron%20integrated%20circuit%20with%20a%20scalable%20communication%20network%20and%20interface%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Merolla%2C%20Paul%20A.%20Arthur%2C%20John%20V.%20Alvarez-Icaza%2C%20Rodrigo%20Cassidy%2C%20Andrew%20S.%20A%20million%20spiking-neuron%20integrated%20circuit%20with%20a%20scalable%20communication%20network%20and%20interface%202014"
        },
        {
            "id": "21",
            "entry": "[21] Emre O Neftci, Charles Augustine, Somnath Paul, and Georgios Detorakis. Event-driven random backpropagation: Enabling neuromorphic deep learning machines. Frontiers in neuroscience, 11:324, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neftci%2C%20Emre%20O.%20Augustine%2C%20Charles%20Paul%2C%20Somnath%20Detorakis%2C%20Georgios%20Event-driven%20random%20backpropagation%3A%20Enabling%20neuromorphic%20deep%20learning%20machines%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neftci%2C%20Emre%20O.%20Augustine%2C%20Charles%20Paul%2C%20Somnath%20Detorakis%2C%20Georgios%20Event-driven%20random%20backpropagation%3A%20Enabling%20neuromorphic%20deep%20learning%20machines%202017"
        },
        {
            "id": "22",
            "entry": "[22] Daniel Neil and Shih-Chii Liu. Effective sensor fusion with event-based sensors and deep network architectures. In Circuits and Systems (ISCAS), 2016 IEEE International Symposium on, pages 2282\u20132285. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neil%2C%20Daniel%20Liu%2C%20Shih-Chii%20Effective%20sensor%20fusion%20with%20event-based%20sensors%20and%20deep%20network%20architectures%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neil%2C%20Daniel%20Liu%2C%20Shih-Chii%20Effective%20sensor%20fusion%20with%20event-based%20sensors%20and%20deep%20network%20architectures%202016"
        },
        {
            "id": "23",
            "entry": "[23] Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network training for long or event-based sequences. In Advances in Neural Information Processing Systems, pages 3882\u20133890, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neil%2C%20Daniel%20Pfeiffer%2C%20Michael%20Liu%2C%20Shih-Chii%20Phased%20lstm%3A%20Accelerating%20recurrent%20network%20training%20for%20long%20or%20event-based%20sequences%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neil%2C%20Daniel%20Pfeiffer%2C%20Michael%20Liu%2C%20Shih-Chii%20Phased%20lstm%3A%20Accelerating%20recurrent%20network%20training%20for%20long%20or%20event-based%20sequences%202016"
        },
        {
            "id": "24",
            "entry": "[24] Peter O\u2019Connor, Daniel Neil, Shih-Chii Liu, Tobi Delbruck, and Michael Pfeiffer. Real-time classification and sensor fusion with a spiking deep belief network. Frontiers in neuroscience, 7:178, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Connor%2C%20Peter%20Neil%2C%20Daniel%20Liu%2C%20Shih-Chii%20Delbruck%2C%20Tobi%20Real-time%20classification%20and%20sensor%20fusion%20with%20a%20spiking%20deep%20belief%20network%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=O%E2%80%99Connor%2C%20Peter%20Neil%2C%20Daniel%20Liu%2C%20Shih-Chii%20Delbruck%2C%20Tobi%20Real-time%20classification%20and%20sensor%20fusion%20with%20a%20spiking%20deep%20belief%20network%202013"
        },
        {
            "id": "25",
            "entry": "[25] Peter O\u2019Connor and Max Welling. Deep spiking networks. arXiv preprint arXiv:1602.08323, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.08323"
        },
        {
            "id": "26",
            "entry": "[26] Garrick Orchard, Ajinkya Jayawant, Gregory K Cohen, and Nitish Thakor. Converting static image datasets to spiking neuromorphic datasets using saccades. Frontiers in neuroscience, 9:437, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orchard%2C%20Garrick%20Jayawant%2C%20Ajinkya%20Cohen%2C%20Gregory%20K.%20Thakor%2C%20Nitish%20Converting%20static%20image%20datasets%20to%20spiking%20neuromorphic%20datasets%20using%20saccades%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orchard%2C%20Garrick%20Jayawant%2C%20Ajinkya%20Cohen%2C%20Gregory%20K.%20Thakor%2C%20Nitish%20Converting%20static%20image%20datasets%20to%20spiking%20neuromorphic%20datasets%20using%20saccades%202015"
        },
        {
            "id": "27",
            "entry": "[27] Bodo Rueckauer, Yuhuang Hu, Iulia-Alexandra Lungu, Michael Pfeiffer, and Shih-Chii Liu. Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. Frontiers in neuroscience, 11:682, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rueckauer%2C%20Bodo%20Hu%2C%20Yuhuang%20Lungu%2C%20Iulia-Alexandra%20Pfeiffer%2C%20Michael%20Conversion%20of%20continuous-valued%20deep%20networks%20to%20efficient%20event-driven%20networks%20for%20image%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rueckauer%2C%20Bodo%20Hu%2C%20Yuhuang%20Lungu%2C%20Iulia-Alexandra%20Pfeiffer%2C%20Michael%20Conversion%20of%20continuous-valued%20deep%20networks%20to%20efficient%20event-driven%20networks%20for%20image%20classification%202017"
        },
        {
            "id": "28",
            "entry": "[28] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by backpropagating errors. nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Williams.%20Learning%20representations%20by%20backpropagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Williams.%20Learning%20representations%20by%20backpropagating%20errors%201986"
        },
        {
            "id": "29",
            "entry": "[29] Benjamin Schrauwen and Jan Van Campenhout. BSA, a fast and accurate spike train encoding scheme. In Proceedings of the International Joint Conference on Neural Networks, volume 4, pages 2825\u20132830. IEEE Piscataway, NJ, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schrauwen%2C%20Benjamin%20BSA%2C%20Jan%20Van%20Campenhout%20a%20fast%20and%20accurate%20spike%20train%20encoding%20scheme%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schrauwen%2C%20Benjamin%20BSA%2C%20Jan%20Van%20Campenhout%20a%20fast%20and%20accurate%20spike%20train%20encoding%20scheme%202003"
        },
        {
            "id": "30",
            "entry": "[30] Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In ICDAR, volume 3, pages 958\u2013962, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simard%2C%20Patrice%20Y.%20Steinkraus%2C%20David%20Platt%2C%20John%20C.%20Best%20practices%20for%20convolutional%20neural%20networks%20applied%20to%20visual%20document%20analysis%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simard%2C%20Patrice%20Y.%20Steinkraus%2C%20David%20Platt%2C%20John%20C.%20Best%20practices%20for%20convolutional%20neural%20networks%20applied%20to%20visual%20document%20analysis%202003"
        },
        {
            "id": "31",
            "entry": "[31] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Werbos%2C%20Paul%20J.%20Backpropagation%20through%20time%3A%20what%20it%20does%20and%20how%20to%20do%20it%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Werbos%2C%20Paul%20J.%20Backpropagation%20through%20time%3A%20what%20it%20does%20and%20how%20to%20do%20it%201990"
        },
        {
            "id": "32",
            "entry": "[32] Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training high-performance spiking neural networks. arXiv preprint arXiv:1706.02609, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02609"
        },
        {
            "id": "33",
            "entry": "[33] Friedemann Zenke and Surya Ganguli. Superspike: Supervised learning in multilayer spiking neural networks. Neural computation, 30(6):1514\u20131541, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zenke%2C%20Friedemann%20Ganguli%2C%20Surya%20Superspike%3A%20Supervised%20learning%20in%20multilayer%20spiking%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zenke%2C%20Friedemann%20Ganguli%2C%20Surya%20Superspike%3A%20Supervised%20learning%20in%20multilayer%20spiking%20neural%20networks%202018"
        }
    ]
}
