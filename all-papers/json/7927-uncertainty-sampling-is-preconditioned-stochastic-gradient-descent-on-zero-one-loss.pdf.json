{
    "filename": "7927-uncertainty-sampling-is-preconditioned-stochastic-gradient-descent-on-zero-one-loss.pdf",
    "metadata": {
        "title": "Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss",
        "author": "Stephen Mussmann, Percy S. Liang",
        "date": 2006,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7927-uncertainty-sampling-is-preconditioned-stochastic-gradient-descent-on-zero-one-loss.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Uncertainty sampling, a popular active learning algorithm, is used to reduce the amount of data required to learn a classifier, but it has been observed in practice to converge to different parameters depending on the initialization and sometimes to even better parameters than standard training on all the data. In this work, we give a theoretical explanation of this phenomenon, showing that uncertainty sampling on a convex (e.g., logistic) loss can be interpreted as performing a preconditioned stochastic gradient step on the population zero-one loss. Experiments on synthetic and real datasets support this connection."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "active learning",
            "url": "https://en.wikipedia.org/wiki/active_learning"
        }
    ],
    "highlights": [
        "Active learning algorithms aim to learn parameters with less data by querying labels adaptively",
        "We show that uncertainty sampling with respect to a convex loss on all the points is performing a preconditioned 1 stochastic gradient step on the population zero-one loss",
        "We show how uncertainty sampling converges to different parameters depending on initialization, and how it can achieve lower asymptotic zero-one loss compared to minimizing the surrogate loss on all the data",
        "Note that most active learning experiments are interested in measuring the rate of convergence, whereas this paper focuses exclusively on asymptotic values and the variation that we obtain from different seed sets",
        "To gain a more quantitative understanding of all the datasets, we summarized the asymptotic zero-one loss of uncertainty sampling for various random seed set sizes",
        "We analyze active learning with offline optimization and show the connection between uncertainty sampling and one particularly important non-convex loss, the zero-one loss"
    ],
    "key_statements": [
        "Active learning algorithms aim to learn parameters with less data by querying labels adaptively",
        "One of the oddities of uncertainty sampling is that sometimes the bias is helpful: uncertainty sampling with a subset of the data can yield lower error than random sampling on all the data (<a class=\"ref-link\" id=\"cSchohn_2000_a\" href=\"#rSchohn_2000_a\">Schohn and Cohn, 2000</a>; <a class=\"ref-link\" id=\"cBordes_et+al_2005_a\" href=\"#rBordes_et+al_2005_a\">Bordes et al, 2005</a>; <a class=\"ref-link\" id=\"cChang_et+al_2017_a\" href=\"#rChang_et+al_2017_a\">Chang et al, 2017</a>)",
        "We show that uncertainty sampling with respect to a convex loss on all the points is performing a preconditioned 1 stochastic gradient step on the population zero-one loss",
        "Each uncertainty sampling iterate in expectation moves in a descent direction of the zero-one loss, unless the parameters are at an approximate stationary point",
        "We present three results shedding light on uncertainty sampling that build on each other",
        "That we have defined the set of regular parameters \u0398regular, the density at the decision boundary b(\u03b8), and Assumptions 5, 6, and 7, we are ready to formally state the expected gradient of the surrogate loss on a point chosen by uncertainty sampling",
        "If Assumptions 2, 5, 6, and 7 hold and \u03b8 \u2208 \u0398regular and b(\u03b8) = 0, if z(t) is chosen via uncertainty sampling with parameters \u03b8, lim E[\u2207",
        "We show how uncertainty sampling converges to different parameters depending on initialization, and how it can achieve lower asymptotic zero-one loss compared to minimizing the surrogate loss on all the data",
        "Note that most active learning experiments are interested in measuring the rate of convergence, whereas this paper focuses exclusively on asymptotic values and the variation that we obtain from different seed sets",
        "For most of the datasets, the behavior was more similar to the plot on the right, where uncertainty sampling has a higher mean zero-one loss than random sampling for most seed sizes",
        "To gain a more quantitative understanding of all the datasets, we summarized the asymptotic zero-one loss of uncertainty sampling for various random seed set sizes",
        "In Figure 4, we show the proportions of the runs over the datasets where uncertainty sampling converges to a lower zero-one loss than using the entire dataset",
        "In Figure 5, we show a \u201cviolin plot\u201d for the distribution of the ratio between the asymptotic zero-one loss of uncertainty sampling and the zero-one loss using the full dataset",
        "We find that uncertainty sampling updates are preconditioned SGD steps on the population zero-one loss and move in descent directions for parameters that are not approximate stationary points",
        "We analyze active learning with offline optimization and show the connection between uncertainty sampling and one particularly important non-convex loss, the zero-one loss"
    ],
    "summary": [
        "Active learning algorithms aim to learn parameters with less data by querying labels adaptively.",
        "Each uncertainty sampling iterate in expectation moves in a descent direction of the zero-one loss, unless the parameters are at an approximate stationary point.",
        "Uncertainty sampling (Algorithm 1) begins with nseed < n labeled points D drawn randomly from the pool and minimizes the regularized loss (3) to obtain initial parameters.",
        "We analyze how the sample convex surrogate loss minimizer changes with each additional point; these are the iterates of uncertainty sampling.",
        "That we have defined the set of regular parameters \u0398regular, the density at the decision boundary b(\u03b8), and Assumptions 5, 6, and 7, we are ready to formally state the expected gradient of the surrogate loss on a point chosen by uncertainty sampling.",
        "Theorem 8, states that as the minipool size goes to infinity, E[\u2207 (z(t), \u03b8)] tends in the direction of the gradient of the population zero-one loss Z(\u03b8), where the expectation is with respect to the randomness of the minipool XM and label y.",
        "If Assumptions 2, 5, 6, and 7 hold and \u03b8 \u2208 \u0398regular and b(\u03b8) = 0, if z(t) is chosen via uncertainty sampling with parameters \u03b8, lim E[\u2207",
        "We show how uncertainty sampling converges to different parameters depending on initialization, and how it can achieve lower asymptotic zero-one loss compared to minimizing the surrogate loss on all the data.",
        "For most of the datasets, the behavior was more similar to the plot on the right, where uncertainty sampling has a higher mean zero-one loss than random sampling for most seed sizes.",
        "To gain a more quantitative understanding of all the datasets, we summarized the asymptotic zero-one loss of uncertainty sampling for various random seed set sizes.",
        "In Figure 4, we show the proportions of the runs over the datasets where uncertainty sampling converges to a lower zero-one loss than using the entire dataset.",
        "Sch\u00fctze et al (2006) notes the \u201cmissed cluster effect\u201d, where active learning can ignore clusters in the data and never query points from there; corresponding to a local minimum of the zero-one loss.",
        "We find that uncertainty sampling updates are preconditioned SGD steps on the population zero-one loss and move in descent directions for parameters that are not approximate stationary points.",
        "We analyze active learning with offline optimization and show the connection between uncertainty sampling and one particularly important non-convex loss, the zero-one loss."
    ],
    "headline": "We show that uncertainty sampling with respect to a convex loss on all the points is performing a preconditioned 1 stochastic gradient step on the  population zero-one loss",
    "reference_links": [
        {
            "id": "Bach_2007_a",
            "entry": "Bach, F. R. (2007). Active learning for misspecified generalized linear models. In Advances in neural information processing systems, pages 65\u201372.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20F.R.%20Active%20learning%20for%20misspecified%20generalized%20linear%20models%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20F.R.%20Active%20learning%20for%20misspecified%20generalized%20linear%20models%202007"
        },
        {
            "id": "Balcan_et+al_2006_a",
            "entry": "Balcan, M.-F., Beygelzimer, A., and Langford, J. (2006). Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning, pages 65\u201372. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.-F.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.-F.%20Beygelzimer%2C%20A.%20Langford%2C%20J.%20Agnostic%20active%20learning%202006"
        },
        {
            "id": "Bartlett_et+al_2006_a",
            "entry": "Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. (2006). Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473), 138\u2013156.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Jordan%2C%20M.I.%20McAuliffe%2C%20J.D.%20Convexity%2C%20classification%2C%20and%20risk%20bounds%202006"
        },
        {
            "id": "Beygelzimer_et+al_2009_a",
            "entry": "Beygelzimer, A., Dasgupta, S., and Langford, J. (2009). Importance weighted active learning. In Proceedings of the 26th annual international conference on machine learning, pages 49\u201356. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beygelzimer%2C%20A.%20Dasgupta%2C%20S.%20Langford%2C%20J.%20Importance%20weighted%20active%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beygelzimer%2C%20A.%20Dasgupta%2C%20S.%20Langford%2C%20J.%20Importance%20weighted%20active%20learning%202009"
        },
        {
            "id": "Bordes_et+al_2005_a",
            "entry": "Bordes, A., Ertekin, S., Weston, J., and Bottou, L. (2005). Fast kernel classifiers with online and active learning. Journal of Machine Learning Research, 6(Sep), 1579\u20131619.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bordes%2C%20A.%20Ertekin%2C%20S.%20Weston%2C%20J.%20Bottou%2C%20L.%20Fast%20kernel%20classifiers%20with%20online%20and%20active%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bordes%2C%20A.%20Ertekin%2C%20S.%20Weston%2C%20J.%20Bottou%2C%20L.%20Fast%20kernel%20classifiers%20with%20online%20and%20active%20learning%202005"
        },
        {
            "id": "Chang_et+al_2017_a",
            "entry": "Chang, H.-S., Learned-Miller, E., and McCallum, A. (2017). Active bias: Training more accurate neural networks by emphasizing high variance samples. In Advances in Neural Information Processing Systems, pages 1003\u20131013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chang%2C%20H.-S.%20Learned-Miller%2C%20E.%20McCallum%2C%20A.%20Active%20bias%3A%20Training%20more%20accurate%20neural%20networks%20by%20emphasizing%20high%20variance%20samples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chang%2C%20H.-S.%20Learned-Miller%2C%20E.%20McCallum%2C%20A.%20Active%20bias%3A%20Training%20more%20accurate%20neural%20networks%20by%20emphasizing%20high%20variance%20samples%202017"
        },
        {
            "id": "Dasgupta_2008_a",
            "entry": "Dasgupta, S. and Hsu, D. (2008). Hierarchical sampling for active learning. In Proceedings of the 25th international conference on Machine learning, pages 208\u2013215. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20S.%20Hsu%2C%20D.%20Hierarchical%20sampling%20for%20active%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20S.%20Hsu%2C%20D.%20Hierarchical%20sampling%20for%20active%20learning%202008"
        },
        {
            "id": "Feldman_et+al_2012_a",
            "entry": "Feldman, V., Guruswami, V., Raghavendra, P., and Wu, Y. (2012). Agnostic learning of monomials by halfspaces is hard. SIAM Journal on Computing, 41(6), 1558\u20131590.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20V.%20Guruswami%2C%20V.%20Raghavendra%2C%20P.%20Wu%2C%20Y.%20Agnostic%20learning%20of%20monomials%20by%20halfspaces%20is%20hard%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20V.%20Guruswami%2C%20V.%20Raghavendra%2C%20P.%20Wu%2C%20Y.%20Agnostic%20learning%20of%20monomials%20by%20halfspaces%20is%20hard%202012"
        },
        {
            "id": "Guillory_et+al_2009_a",
            "entry": "Guillory, A., Chastain, E., and Bilmes, J. (2009). Active learning as non-convex optimization. In Artificial Intelligence and Statistics, pages 201\u2013208.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guillory%2C%20A.%20Chastain%2C%20E.%20Bilmes%2C%20J.%20Active%20learning%20as%20non-convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guillory%2C%20A.%20Chastain%2C%20E.%20Bilmes%2C%20J.%20Active%20learning%20as%20non-convex%20optimization%202009"
        },
        {
            "id": "Hanneke_2014_a",
            "entry": "Hanneke, S. et al. (2014). Theory of disagreement-based active learning. Foundations and Trends R in Machine Learning, 7(2-3), 131\u2013309.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanneke%2C%20S.%20Theory%20of%20disagreement-based%20active%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanneke%2C%20S.%20Theory%20of%20disagreement-based%20active%20learning%202014"
        },
        {
            "id": "Hoveijn_2007_a",
            "entry": "Hoveijn, I. (2007). Differentiability of the volume of a region enclosed by level sets. arXiv preprint arXiv:0712.0915.",
            "arxiv_url": "https://arxiv.org/pdf/0712.0915"
        },
        {
            "id": "Klein_et+al_2011_a",
            "entry": "Klein, S., Staring, M., Andersson, P., and Pluim, J. P. (2011). Preconditioned stochastic gradient descent optimisation for monomodal image registration. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 549\u2013556. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klein%2C%20S.%20Staring%2C%20M.%20Andersson%2C%20P.%20Pluim%2C%20J.P.%20Preconditioned%20stochastic%20gradient%20descent%20optimisation%20for%20monomodal%20image%20registration%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Klein%2C%20S.%20Staring%2C%20M.%20Andersson%2C%20P.%20Pluim%2C%20J.P.%20Preconditioned%20stochastic%20gradient%20descent%20optimisation%20for%20monomodal%20image%20registration%202011"
        },
        {
            "id": "Lewis_1994_a",
            "entry": "Lewis, D. D. and Gale, W. A. (1994). A sequential algorithm for training text classifiers. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 3\u201312. Springer-Verlag New York, Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lewis%2C%20D.D.%20Gale%2C%20W.A.%20A%20sequential%20algorithm%20for%20training%20text%20classifiers%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lewis%2C%20D.D.%20Gale%2C%20W.A.%20A%20sequential%20algorithm%20for%20training%20text%20classifiers%201994"
        },
        {
            "id": "Li_2018_a",
            "entry": "Li, X.-L. (2018). Preconditioned stochastic gradient descent. IEEE transactions on neural networks and learning systems, 29(5), 1454\u20131466.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20X.-L.%20Preconditioned%20stochastic%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20X.-L.%20Preconditioned%20stochastic%20gradient%20descent%202018"
        },
        {
            "id": "Long_2010_a",
            "entry": "Long, P. M. and Servedio, R. A. (2010). Random classification noise defeats all convex potential boosters. Machine learning, 78(3), 287\u2013304.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20P.M.%20Servedio%2C%20R.A.%20Random%20classification%20noise%20defeats%20all%20convex%20potential%20boosters%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20P.M.%20Servedio%2C%20R.A.%20Random%20classification%20noise%20defeats%20all%20convex%20potential%20boosters%202010"
        },
        {
            "id": "Micchelli_et+al_2006_a",
            "entry": "Micchelli, C. A., Xu, Y., and Zhang, H. (2006). Universal kernels. Journal of Machine Learning Research, 7(Dec), 2651\u20132667.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Micchelli%2C%20C.A.%20Xu%2C%20Y.%20Zhang%2C%20H.%20Universal%20kernels%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Micchelli%2C%20C.A.%20Xu%2C%20Y.%20Zhang%2C%20H.%20Universal%20kernels%202006"
        },
        {
            "id": "Nguyen_2013_a",
            "entry": "Nguyen, T. and Sanner, S. (2013). Algorithms for direct 0\u20131 loss optimization in binary classification. In International Conference on Machine Learning, pages 1085\u20131093.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20T.%20Sanner%2C%20S.%20Algorithms%20for%20direct%200%E2%80%931%20loss%20optimization%20in%20binary%20classification%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20T.%20Sanner%2C%20S.%20Algorithms%20for%20direct%200%E2%80%931%20loss%20optimization%20in%20binary%20classification%202013"
        },
        {
            "id": "Ramdas_2013_a",
            "entry": "Ramdas, A. and Singh, A. (2013). Algorithmic connections between active learning and stochastic convex optimization. In International Conference on Algorithmic Learning Theory, pages 339\u2013353. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ramdas%2C%20A.%20Singh%2C%20A.%20Algorithmic%20connections%20between%20active%20learning%20and%20stochastic%20convex%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ramdas%2C%20A.%20Singh%2C%20A.%20Algorithmic%20connections%20between%20active%20learning%20and%20stochastic%20convex%20optimization%202013"
        },
        {
            "id": "Schohn_2000_a",
            "entry": "Schohn, G. and Cohn, D. (2000). Less is more: Active learning with support vector machines. In ICML, pages 839\u2013846. Citeseer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schohn%2C%20G.%20Cohn%2C%20D.%20Less%20is%20more%3A%20Active%20learning%20with%20support%20vector%20machines%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schohn%2C%20G.%20Cohn%2C%20D.%20Less%20is%20more%3A%20Active%20learning%20with%20support%20vector%20machines%202000"
        },
        {
            "id": "Schuetze_et+al_2006_a",
            "entry": "Sch\u00fctze, H., Velipasaoglu, E., and Pedersen, J. O. (2006). Performance thresholding in practical text classification. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 662\u2013671. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%BCtze%2C%20H.%20Velipasaoglu%2C%20E.%20Pedersen%2C%20J.O.%20Performance%20thresholding%20in%20practical%20text%20classification%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sch%C3%BCtze%2C%20H.%20Velipasaoglu%2C%20E.%20Pedersen%2C%20J.O.%20Performance%20thresholding%20in%20practical%20text%20classification%202006"
        },
        {
            "id": "Settles_2010_a",
            "entry": "Settles, B. (2010). Active learning literature survey. Computer Sciences Technical Report, 1648.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Settles%2C%20B.%20Active%20learning%20literature%20survey%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Settles%2C%20B.%20Active%20learning%20literature%20survey%202010"
        },
        {
            "id": "Tong_2001_a",
            "entry": "Tong, S. and Koller, D. (2001). Support vector machine active learning with applications to text classification. Journal of machine learning research, 2(Nov), 45\u201366.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tong%2C%20S.%20Koller%2C%20D.%20Support%20vector%20machine%20active%20learning%20with%20applications%20to%20text%20classification%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tong%2C%20S.%20Koller%2C%20D.%20Support%20vector%20machine%20active%20learning%20with%20applications%20to%20text%20classification%202001"
        },
        {
            "id": "Wu_2007_a",
            "entry": "Wu, Y. and Liu, Y. (2007). Robust truncated hinge loss support vector machines. Journal of the",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Liu%2C%20Y.%20Robust%20truncated%20hinge%20loss%20support%20vector%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Y.%20Liu%2C%20Y.%20Robust%20truncated%20hinge%20loss%20support%20vector%20machines%202007"
        },
        {
            "id": "American_0000_a",
            "entry": "American Statistical Association, 102(479), 974\u2013983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=American%20Statistical%20Association%20102479%20974983"
        },
        {
            "id": "Yang_2016_a",
            "entry": "Yang, Y. and Loog, M. (2016). A benchmark and comparison of active learning for logistic regression.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Y.%20Loog%2C%20M.%20A%20benchmark%20and%20comparison%20of%20active%20learning%20for%20logistic%20regression%202016"
        }
    ]
}
