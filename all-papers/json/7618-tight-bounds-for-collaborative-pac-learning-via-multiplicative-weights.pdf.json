{
    "filename": "7618-tight-bounds-for-collaborative-pac-learning-via-multiplicative-weights.pdf",
    "metadata": {
        "title": "Tight Bounds for Collaborative PAC Learning via Multiplicative Weights",
        "author": "Jiecao Chen, Qin Zhang, Yuan Zhou",
        "identifiers": {
            "arxiv": "1805.09217",
            "doi": null,
            "isbn": null,
            "doc_id": null,
            "url": "https://papers.nips.cc/paper/7618-tight-bounds-for-collaborative-pac-learning-via-multiplicative-weights.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the collaborative PAC learning problem recently proposed in Blum et al.~\\cite{BHPQ17}, in which we have $k$ players and they want to learn a target function collaboratively, such that the learned function approximates the target function well on all players' distributions simultaneously. The quality of the collaborative learning algorithm is measured by the ratio between the sample complexity of the algorithm and that of the learning algorithm for a single distribution (called the overhead). We obtain a collaborative learning algorithm with overhead $O(\\ln k)$, improving the one with overhead $O(\\ln^2 k)$ in \\cite{BHPQ17}. We also show that an $\\Omega(\\ln k)$ overhead is inevitable when $k$ is polynomial bounded by the VC dimension of the hypothesis class. Finally, our experimental study has demonstrated the superiority of our algorithm compared with the one in Blum et al. on real-world datasets.",
        "date": 2018
    },
    "keywords": [
        {
            "term": "probably approximately correct",
            "url": "https://en.wikipedia.org/wiki/probably_approximately_correct"
        },
        {
            "term": "experimental study",
            "url": "https://en.wikipedia.org/wiki/experimental_study"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "target function",
            "url": "https://en.wikipedia.org/wiki/target_function"
        }
    ],
    "highlights": [
        "In this paper we study the collaborative probably approximately correct learning problem recently proposed in Blum et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "In this problem we have an instance space X , a label space Y, and an unknown target function f \u2217 : X \u2192 Y chosen from the hypothesis class F",
        "Theorem 6 In collaborative probably approximately correct learning with k players and a hypothesis class of VC-dimension d, for any , \u03b4 \u2208 (0, 0.01), there exists a hard input distribution on which any ( , \u03b4)-learning algorithm A needs \u03a9 samples in expectation, where the expectation is taken over the randomness used in obtaining the samples and the randomness used in drawing the input from the input distribution",
        "We have proved the optimal overhead ratio and sample complexity, and conducted experimental studies to show the superior performance of our proposed algorithms",
        "What is the optimal trade-off between sample complexity and balance ratio?"
    ],
    "key_statements": [
        "In this paper we study the collaborative probably approximately correct learning problem recently proposed in Blum et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "In this problem we have an instance space X , a label space Y, and an unknown target function f \u2217 : X \u2192 Y chosen from the hypothesis class F",
        "In this paper we propose an algorithm with sample complexity O(d+k) (Theorem 4), which gives an overhead ratio of O when k = O(d) and for constant \u03b4, matching the \u03a9 lower bound proved in Blum et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Proof. [of Theorem 4] While the sample complexity is easy to verify, we focus on the proof of the first property",
        "Theorem 6 In collaborative probably approximately correct learning with k players and a hypothesis class of VC-dimension d, for any , \u03b4 \u2208 (0, 0.01), there exists a hard input distribution on which any ( , \u03b4)-learning algorithm A needs \u03a9 samples in expectation, where the expectation is taken over the randomness used in obtaining the samples and the randomness used in drawing the input from the input distribution",
        "Our algorithms are based on the assumption that given a hypothesis class, we are able to compute its VC dimension d and access an oracle to compute an ( , \u03b4)-classifier with sample complexity S ,\u03b4",
        "We have proved the optimal overhead ratio and sample complexity, and conducted experimental studies to show the superior performance of our proposed algorithms",
        "What is the optimal trade-off between sample complexity and balance ratio?"
    ],
    "summary": [
        "In this paper we study the collaborative PAC learning problem recently proposed in Blum et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "In this paper we propose an algorithm with sample complexity O(d+k) (Theorem 4), which gives an overhead ratio of O when k = O(d) and for constant \u03b4, matching the \u03a9 lower bound proved in Blum et al [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "The algorithm adaptively decides the number of samples to be taken from each player distribution, and calls L to learn a function.",
        "The following lemma shows that TEST returns, with high probability, the desired set of players where g is an accurate hypothesis for its own distribution.",
        "While the 2nd and 3rd items lead to the key reduction of the sample complexity, they make it impossible to use the union bound and claim that with high probability \u201cevery call of L and TEST is successful\u201d in the analysis for Algorithm 1).",
        "Theorem 6 In collaborative PAC learning with k players and a hypothesis class of VC-dimension d, for any , \u03b4 \u2208 (0, 0.01), there exists a hard input distribution on which any ( , \u03b4)-learning algorithm A needs \u03a9 samples in expectation, where the expectation is taken over the randomness used in obtaining the samples and the randomness used in drawing the input from the input distribution.",
        "The proof of Theorem 6 is similar to that for the lower bound result in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]; we need to generalize the hard instance provided in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] in two different cases.",
        "Our algorithms are based on the assumption that given a hypothesis class, we are able to compute its VC dimension d and access an oracle to compute an ( , \u03b4)-classifier with sample complexity S ,\u03b4.",
        "For each fixed sample budget, we run the algorithm for 100 times and test whether the following happens, Pr[max i errDi",
        "We will test the collaborative learning algorithms using the following data sets.",
        "We test the algorithms for each data set using multiple values of the error threshold , and report the sample complexity for NAIVE, MWEIGHTS and CENLEARN.",
        "In Figure 1b-Figure 1f, one can observe that MWEIGHTS uses fewer samples than its competitors in almost all cases, which shows the superiority of our proposed algorithm.",
        "Our experimental results show that MWEIGHTS and CENLEARN need fewer samples than NAIVE when the input distributions D1, .",
        "We have proved the optimal overhead ratio and sample complexity, and conducted experimental studies to show the superior performance of our proposed algorithms.",
        "What is the optimal trade-off between sample complexity and balance ratio?"
    ],
    "headline": "We study the collaborative probably approximately correct learning problem recently proposed in Blum et al.~\\cite{BHPQ17}, in which we have $k$ players and they want to learn a target function collaboratively, such that the learned function approximates the target function well on all players' distributions simultaneously",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity and privacy. In COLT, pages 26.1\u201326.22, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.%20Blum%2C%20A.%20Fine%2C%20S.%20Mansour%2C%20Y.%20Distributed%20learning%2C%20communication%20complexity%20and%20privacy%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.%20Blum%2C%20A.%20Fine%2C%20S.%20Mansour%2C%20Y.%20Distributed%20learning%2C%20communication%20complexity%20and%20privacy%202012"
        },
        {
            "id": "2",
            "entry": "[2] M. Balcan, S. Ehrlich, and Y. Liang. Distributed k-means and k-median clustering on general communication topologies. In NIPS, pages 1995\u20132003, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balcan%2C%20M.%20Ehrlich%2C%20S.%20Liang%2C%20Y.%20Distributed%20k-means%20and%20k-median%20clustering%20on%20general%20communication%20topologies%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balcan%2C%20M.%20Ehrlich%2C%20S.%20Liang%2C%20Y.%20Distributed%20k-means%20and%20k-median%20clustering%20on%20general%20communication%20topologies%201995"
        },
        {
            "id": "3",
            "entry": "[3] A. Blum, N. Haghtalab, A. D. Procaccia, and M. Qiao. Collaborative PAC learning. In NIPS, pages 2389\u20132398, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Haghtalab%2C%20N.%20Procaccia%2C%20A.D.%20Qiao%2C%20M.%20Collaborative%20PAC%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20A.%20Haghtalab%2C%20N.%20Procaccia%2C%20A.D.%20Qiao%2C%20M.%20Collaborative%20PAC%20learning%202017"
        },
        {
            "id": "4",
            "entry": "[4] R. Bock, A. Chilingarian, M. Gaug, F. Hakl, T. Hengstebeck, M. Jirina, J. Klaschka, E. Kotrc, P. Savicky, S. Towers, et al. Methods for multidimensional event classification: a case study. as Internal Note in CERN, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bock%2C%20R.%20Chilingarian%2C%20A.%20Gaug%2C%20M.%20Hakl%2C%20F.%20Methods%20for%20multidimensional%20event%20classification%3A%20a%20case%20study%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bock%2C%20R.%20Chilingarian%2C%20A.%20Gaug%2C%20M.%20Hakl%2C%20F.%20Methods%20for%20multidimensional%20event%20classification%3A%20a%20case%20study%202003"
        },
        {
            "id": "5",
            "entry": "[5] P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47(4):547\u2013553, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortez%2C%20P.%20Cerdeira%2C%20A.%20Almeida%2C%20F.%20Matos%2C%20T.%20Modeling%20wine%20preferences%20by%20data%20mining%20from%20physicochemical%20properties%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortez%2C%20P.%20Cerdeira%2C%20A.%20Almeida%2C%20F.%20Matos%2C%20T.%20Modeling%20wine%20preferences%20by%20data%20mining%20from%20physicochemical%20properties%202009"
        },
        {
            "id": "6",
            "entry": "[6] A. Ehrenfeucht, D. Haussler, M. J. Kearns, and L. G. Valiant. A general lower bound on the number of examples needed for learning. Inf. Comput., 82(3):247\u2013261, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ehrenfeucht%2C%20A.%20Haussler%2C%20D.%20Kearns%2C%20M.J.%20Valiant%2C%20L.G.%20A%20general%20lower%20bound%20on%20the%20number%20of%20examples%20needed%20for%20learning%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ehrenfeucht%2C%20A.%20Haussler%2C%20D.%20Kearns%2C%20M.J.%20Valiant%2C%20L.G.%20A%20general%20lower%20bound%20on%20the%20number%20of%20examples%20needed%20for%20learning%201989"
        },
        {
            "id": "7",
            "entry": "[7] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freund%2C%20Y.%20Schapire%2C%20R.E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freund%2C%20Y.%20Schapire%2C%20R.E.%20A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting%201997"
        },
        {
            "id": "8",
            "entry": "[8] P. W. Frey and D. J. Slate. Letter recognition using holland-style adaptive classifiers. Machine Learning, 6:161\u2013182, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frey%2C%20P.W.%20Slate%2C%20D.J.%20Letter%20recognition%20using%20holland-style%20adaptive%20classifiers%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frey%2C%20P.W.%20Slate%2C%20D.J.%20Letter%20recognition%20using%20holland-style%20adaptive%20classifiers%201991"
        },
        {
            "id": "9",
            "entry": "[9] S. Guha, Y. Li, and Q. Zhang. Distributed partial clustering. In SPAA, pages 143\u2013152, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guha%2C%20S.%20Li%2C%20Y.%20Zhang%2C%20Q.%20Distributed%20partial%20clustering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guha%2C%20S.%20Li%2C%20Y.%20Zhang%2C%20Q.%20Distributed%20partial%20clustering%202017"
        },
        {
            "id": "10",
            "entry": "[10] S. Hanneke. The optimal sample complexity of pac learning. The Journal of Machine Learning Research, 17(1):1319\u20131333, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanneke%2C%20S.%20The%20optimal%20sample%20complexity%20of%20pac%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanneke%2C%20S.%20The%20optimal%20sample%20complexity%20of%20pac%20learning%202016"
        },
        {
            "id": "11",
            "entry": "[11] H. D. III, J. M. Phillips, A. Saha, and S. Venkatasubramanian. Efficient protocols for distributed classification and optimization. In ALT, pages 154\u2013168, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D.%2C%20III%2C%20H.%20Phillips%2C%20J.M.%20Saha%2C%20A.%20Venkatasubramanian%2C%20S.%20Efficient%20protocols%20for%20distributed%20classification%20and%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D.%2C%20III%2C%20H.%20Phillips%2C%20J.M.%20Saha%2C%20A.%20Venkatasubramanian%2C%20S.%20Efficient%20protocols%20for%20distributed%20classification%20and%20optimization%202012"
        },
        {
            "id": "12",
            "entry": "[12] H. D. III, J. M. Phillips, A. Saha, and S. Venkatasubramanian. Protocols for learning classifiers on distributed data. In AISTATS, pages 282\u2013290, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D.%2C%20III%2C%20H.%20Phillips%2C%20J.M.%20Saha%2C%20A.%20Venkatasubramanian%2C%20S.%20Protocols%20for%20learning%20classifiers%20on%20distributed%20data%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D.%2C%20III%2C%20H.%20Phillips%2C%20J.M.%20Saha%2C%20A.%20Venkatasubramanian%2C%20S.%20Protocols%20for%20learning%20classifiers%20on%20distributed%20data%202012"
        },
        {
            "id": "13",
            "entry": "[13] Y. Liang, M. Balcan, V. Kanchanapally, and D. P. Woodruff. Improved distributed principal component analysis. In NIPS, pages 3113\u20133121, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Y.%20Balcan%2C%20M.%20Kanchanapally%2C%20V.%20Woodruff%2C%20D.P.%20Improved%20distributed%20principal%20component%20analysis%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Y.%20Balcan%2C%20M.%20Kanchanapally%2C%20V.%20Woodruff%2C%20D.P.%20Improved%20distributed%20principal%20component%20analysis%202014"
        },
        {
            "id": "14",
            "entry": "[14] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In NIPS, pages 1041\u20131048, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mansour%2C%20Y.%20Mohri%2C%20M.%20Rostamizadeh%2C%20A.%20Domain%20adaptation%20with%20multiple%20sources%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mansour%2C%20Y.%20Mohri%2C%20M.%20Rostamizadeh%2C%20A.%20Domain%20adaptation%20with%20multiple%20sources%202008"
        },
        {
            "id": "15",
            "entry": "[15] H. L. Nguyen and L. Zakynthinou. Improved Algorithms for Collaborative PAC Learning. arXiv preprint arXiv:1805.08356, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08356"
        },
        {
            "id": "16",
            "entry": "[16] J. Wang, M. Kolar, and N. Srebro. Distributed multi-task learning. In AISTATS, pages 751\u2013760, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20J.%20Kolar%2C%20M.%20Srebro%2C%20N.%20Distributed%20multi-task%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20J.%20Kolar%2C%20M.%20Srebro%2C%20N.%20Distributed%20multi-task%20learning%202016"
        }
    ]
}
