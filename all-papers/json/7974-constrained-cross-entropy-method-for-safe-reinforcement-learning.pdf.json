{
    "filename": "7974-constrained-cross-entropy-method-for-safe-reinforcement-learning.pdf",
    "metadata": {
        "title": "Constrained Cross-Entropy Method for Safe Reinforcement Learning",
        "author": "Min Wen, Ufuk Topcu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7974-constrained-cross-entropy-method-for-safe-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study a safe reinforcement learning problem in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well-suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then we give sufficient conditions on the properties of this differential equation for the convergence of the proposed algorithm. At last, we show with simulation experiments that the proposed algorithm can effectively learn feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions."
    },
    "keywords": [
        {
            "term": "differential equation",
            "url": "https://en.wikipedia.org/wiki/differential_equation"
        },
        {
            "term": "ordinary differential equation",
            "url": "https://en.wikipedia.org/wiki/ordinary_differential_equation"
        },
        {
            "term": "cross entropy method",
            "url": "https://en.wikipedia.org/wiki/cross_entropy_method"
        },
        {
            "term": "system model",
            "url": "https://en.wikipedia.org/wiki/system_model"
        },
        {
            "term": "finite length",
            "url": "https://en.wikipedia.org/wiki/finite_length"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "Reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
        },
        {
            "term": "constraint function",
            "url": "https://en.wikipedia.org/wiki/constraint_function"
        },
        {
            "term": "optimal control",
            "url": "https://en.wikipedia.org/wiki/optimal_control"
        },
        {
            "term": "dynamical system",
            "url": "https://en.wikipedia.org/wiki/dynamical_system"
        },
        {
            "term": "Markov decision process",
            "url": "https://en.wikipedia.org/wiki/Markov_decision_process"
        },
        {
            "term": "natural exponential family",
            "url": "https://en.wikipedia.org/wiki/natural_exponential_family"
        }
    ],
    "highlights": [
        "This paper studies the following constrained optimal control problem: given a dynamical system model with continuous states and actions, a objective function and a constraint function, find a controller that maximizes the objective function while satisfying the constraint",
        "We studied a safe reinforcement learning problem with the constraints that are defined as the expected cost over finite-length trajectories",
        "We proposed a constrained cross-entropy-based method to solve this problem, analyzed its asymptotic performance using an ordinary differential equation and proved its convergence",
        "We showed with simulation experiments that our method can effectively learn feasible policies without assumptions on the feasibility of initial policies with both Markovian and non-Markovian objective functions and constraint functions",
        "Unlike gradient-based methods such as trust region policy optimization, constrained cross-entropy does not infer the performances of unseen policies from previous experience"
    ],
    "key_statements": [
        "This paper studies the following constrained optimal control problem: given a dynamical system model with continuous states and actions, a objective function and a constraint function, find a controller that maximizes the objective function while satisfying the constraint",
        "When initialized with an infeasible policy, they usually are not be able to find even a single feasible policy until their convergence. These limitations motivate the following question: Can we develop a reinforcement learning algorithm that explicitly addresses the priority of constraint satisfaction? Rather than assuming that the initial policy is feasible and that one can always find a feasible policy in the estimated gradient direction, we need to deal with cases in which the initial policy is not feasible, or we have never seen a feasible policy before",
        "Inspired by stochastic optimization methods based on the cross-entropy (CE) concept [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>], we propose a new safe reinforcement learning algorithm, which we call the constrained cross-entropy (CCE) method",
        "We present a model-free constrained Reinforcement learning algorithm that works with continuous state and action spaces",
        "We prove the convergence of Algorithm 1 by comparing the asymptotic behavior of {\u03b7l}l\u22650 with the flow induced by the following ordinary differential equation (ODE):",
        "We studied a safe reinforcement learning problem with the constraints that are defined as the expected cost over finite-length trajectories",
        "We proposed a constrained cross-entropy-based method to solve this problem, analyzed its asymptotic performance using an ordinary differential equation and proved its convergence",
        "We showed with simulation experiments that our method can effectively learn feasible policies without assumptions on the feasibility of initial policies with both Markovian and non-Markovian objective functions and constraint functions",
        "constrained cross-entropy is expected to be less sample-efficient than gradient-based methods especially for highdimensional systems",
        "Unlike gradient-based methods such as trust region policy optimization, constrained cross-entropy does not infer the performances of unseen policies from previous experience",
        "We find the constrained cross-entropy method to be particularly useful in learning hierarchical policies"
    ],
    "summary": [
        "This paper studies the following constrained optimal control problem: given a dynamical system model with continuous states and actions, a objective function and a constraint function, find a controller that maximizes the objective function while satisfying the constraint.",
        "Rather than treating the constraints as an extra term in the objective function as what policy gradient method do, we use constraint values to sort sample policies.",
        "Instead of initializing the optimization with a feasible policy, the method improves both the objective function and the constraint function with the constraint as a prioritized concern.",
        "The algorithm can be applied to any finite-horizon problem whose objective and constraint functions are defined as the average performance over some distribution of trajectories.",
        "A policy \u03c0 \u2208 \u03a0 is feasible for a constrained optimization problem with cost function Z and constraint upper bound d if HZ (\u03c0) \u2264 d.",
        "For the unconstrained CE method, the surrogate function is the conditional expectation over policies whose G-values are highly ranked with the current sampling distribution fv.",
        "Given a set \u03a0 = {\u03c0\u03b8 : \u03b8 \u2208 \u0398} of policies with parameter space \u0398, an NEF FV = {fv \u2208 D(\u0398) : v \u2208 V} of distributions over \u0398, two functions G : \u03a0 \u2192 R+ and H : \u03a0 \u2192 R, a",
        "If Assumptions 1 and 2 hold, the sequence {\u03b7l}l\u22650 in Step 11 of Algorithm 1 converges to a connected internally chain recurrent set of (8) as l \u2192 \u221e with probability 1.",
        "We compare the performance of CCE to trust region policy optimization (TRPO) [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>], a state-ofthe-art unconstrained RL algorithm, and its variant for constrained problems, i.e., CPO [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "CPO needs significantly more samples to find a single feasible policy, or converges to an infeasible policy especially if the constraint is non-Markovian.",
        "We studied a safe reinforcement learning problem with the constraints that are defined as the expected cost over finite-length trajectories.",
        "We proposed a constrained cross-entropy-based method to solve this problem, analyzed its asymptotic performance using an ODE and proved its convergence.",
        "We showed with simulation experiments that our method can effectively learn feasible policies without assumptions on the feasibility of initial policies with both Markovian and non-Markovian objective functions and constraint functions.",
        "With a high-level policy that specifies intermediate goals and reduces the state space for low-level policies, we can use CCE to train a optimal low-level policy while satisfying local constraints.",
        "As shown in the experiment of our paper, CCE converges with reasonable sample complexity and outperforms CPO on its constraint performance.",
        "This may mitigate the problem of high sample complexity as other evolutionary methods [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]"
    ],
    "headline": "We study a safe reinforcement learning problem in which the constraints are defined as the expected cost over finite-length trajectories",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J. Achiam, D. Held, A. Tamar, and P. Abbeel. Constrained policy optimization. In International Conference on Machine Learning, pages 22\u201331, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Achiam%2C%20J.%20Held%2C%20D.%20Tamar%2C%20A.%20Abbeel%2C%20P.%20Constrained%20policy%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Achiam%2C%20J.%20Held%2C%20D.%20Tamar%2C%20A.%20Abbeel%2C%20P.%20Constrained%20policy%20optimization%202017"
        },
        {
            "id": "2",
            "entry": "[2] E. Altman. Asymptotic properties of constrained markov decision processes. Mathematical Methods of Operations Research, 37(2):151\u2013170, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Altman%2C%20E.%20Asymptotic%20properties%20of%20constrained%20markov%20decision%20processes%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Altman%2C%20E.%20Asymptotic%20properties%20of%20constrained%20markov%20decision%20processes%201993"
        },
        {
            "id": "3",
            "entry": "[3] D. P. Bertsekas. Dynamic Programming and Optimal Control, Vol. II. Athena Scientific, 3rd edition, 2007. ISBN 1886529302, 9781886529304.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Dynamic%20Programming%20and%20Optimal%20Control%2C%20Vol.%20II.%20Athena%20Scientific%202007"
        },
        {
            "id": "4",
            "entry": "[4] Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with percentile risk criteria. arXiv preprint arXiv:1512.01629, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.01629"
        },
        {
            "id": "5",
            "entry": "[5] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329\u2013 1338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duan%2C%20Y.%20Chen%2C%20X.%20Houthooft%2C%20R.%20Schulman%2C%20J.%20Benchmarking%20deep%20reinforcement%20learning%20for%20continuous%20control%202016"
        },
        {
            "id": "6",
            "entry": "[6] J. Fu, I. Papusha, and U. Topcu. Sampling-based approximate optimal control under temporal logic constraints. In Proceedings of the 20th International Conference on Hybrid Systems: Computation and Control, pages 227\u2013235. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20J.%20Papusha%2C%20I.%20Topcu%2C%20U.%20Sampling-based%20approximate%20optimal%20control%20under%20temporal%20logic%20constraints%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20J.%20Papusha%2C%20I.%20Topcu%2C%20U.%20Sampling-based%20approximate%20optimal%20control%20under%20temporal%20logic%20constraints%202017"
        },
        {
            "id": "7",
            "entry": "[7] J. Garc\u0131a and F. Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Garc%C4%B1a%2C%20J.%20Fern%C3%A1ndez%2C%20F.%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Garc%C4%B1a%2C%20J.%20Fern%C3%A1ndez%2C%20F.%20A%20comprehensive%20survey%20on%20safe%20reinforcement%20learning%202015"
        },
        {
            "id": "8",
            "entry": "[8] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-learning with modelbased acceleration. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2829\u20132838, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/gu16.html.",
            "url": "http://proceedings.mlr.press/v48/gu16.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20S.%20Lillicrap%2C%20T.%20Sutskever%2C%20I.%20Levine%2C%20S.%20Continuous%20deep%20q-learning%20with%20modelbased%20acceleration%202016-06-20"
        },
        {
            "id": "9",
            "entry": "[9] J. P. Hanna, P. Stone, and S. Niekum. Bootstrapping with models: Confidence intervals for off-policy evaluation. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pages 538\u2013546. International Foundation for Autonomous Agents and Multiagent Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hanna%2C%20J.P.%20Stone%2C%20P.%20Niekum%2C%20S.%20Bootstrapping%20with%20models%3A%20Confidence%20intervals%20for%20off-policy%20evaluation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hanna%2C%20J.P.%20Stone%2C%20P.%20Niekum%2C%20S.%20Bootstrapping%20with%20models%3A%20Confidence%20intervals%20for%20off-policy%20evaluation%202017"
        },
        {
            "id": "10",
            "entry": "[10] T. Homem-de Mello. A study on the cross-entropy method for rare-event probability estimation. INFORMS Journal on Computing, 19(3):381\u2013394, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=T.%20Homem-de%20Mello.%20A%20study%20on%20the%20cross-entropy%20method%20for%20rare-event%20probability%20estimation%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=T.%20Homem-de%20Mello.%20A%20study%20on%20the%20cross-entropy%20method%20for%20rare-event%20probability%20estimation%202007"
        },
        {
            "id": "11",
            "entry": "[11] J. Hu, M. C. Fu, S. I. Marcus, et al. A model reference adaptive search method for stochastic global optimization. Communications in Information and Systems, 8(3):245\u2013276, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20J.%20Fu%2C%20M.C.%20Marcus%2C%20S.I.%20A%20model%20reference%20adaptive%20search%20method%20for%20stochastic%20global%20optimization%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20J.%20Fu%2C%20M.C.%20Marcus%2C%20S.I.%20A%20model%20reference%20adaptive%20search%20method%20for%20stochastic%20global%20optimization%202008"
        },
        {
            "id": "12",
            "entry": "[12] J. Hu, P. Hu, and H. S. Chang. A stochastic approximation framework for a class of randomized optimization algorithms. IEEE Transactions on Automatic Control, 57(1):165\u2013178, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20J.%20Hu%2C%20P.%20Chang%2C%20H.S.%20A%20stochastic%20approximation%20framework%20for%20a%20class%20of%20randomized%20optimization%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20J.%20Hu%2C%20P.%20Chang%2C%20H.S.%20A%20stochastic%20approximation%20framework%20for%20a%20class%20of%20randomized%20optimization%20algorithms%202012"
        },
        {
            "id": "13",
            "entry": "[13] N. Jiang and L. Li. Doubly robust off-policy value evaluation for reinforcement learning. In International Conference on Machine Learning, pages 652\u2013661, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiang%2C%20N.%20Li%2C%20L.%20Doubly%20robust%20off-policy%20value%20evaluation%20for%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiang%2C%20N.%20Li%2C%20L.%20Doubly%20robust%20off-policy%20value%20evaluation%20for%20reinforcement%20learning%202016"
        },
        {
            "id": "14",
            "entry": "[14] M. Kobilarov. Cross-entropy randomized motion planning. In Robotics: Science and Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kobilarov%2C%20M.%20Cross-entropy%20randomized%20motion%20planning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kobilarov%2C%20M.%20Cross-entropy%20randomized%20motion%20planning%202011"
        },
        {
            "id": "15",
            "entry": "[15] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levine%2C%20S.%20Finn%2C%20C.%20Darrell%2C%20T.%20Abbeel%2C%20P.%20End-to-end%20training%20of%20deep%20visuomotor%20policies%202016"
        },
        {
            "id": "16",
            "entry": "[16] L. Li and J. Fu. Sampling-based approximate optimal temporal logic planning. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 1328\u20131335. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20Fu%2C%20J.%20Sampling-based%20approximate%20optimal%20temporal%20logic%20planning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20Fu%2C%20J.%20Sampling-based%20approximate%20optimal%20temporal%20logic%20planning%202017"
        },
        {
            "id": "17",
            "entry": "[17] S. C. Livingston, E. M. Wolff, and R. M. Murray. Cross-entropy temporal logic motion planning. In Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control, pages 269\u2013278. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Livingston%2C%20S.C.%20Wolff%2C%20E.M.%20Murray%2C%20R.M.%20Cross-entropy%20temporal%20logic%20motion%20planning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Livingston%2C%20S.C.%20Wolff%2C%20E.M.%20Murray%2C%20R.M.%20Cross-entropy%20temporal%20logic%20motion%20planning%202015"
        },
        {
            "id": "18",
            "entry": "[18] S. Mannor, R. Rubinstein, and Y. Gat. The cross entropy method for fast policy search. In In International Conference on Machine Learning, pages 512\u2013519. Morgan Kaufmann, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mannor%2C%20S.%20Rubinstein%2C%20R.%20Gat%2C%20Y.%20The%20cross%20entropy%20method%20for%20fast%20policy%20search%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mannor%2C%20S.%20Rubinstein%2C%20R.%20Gat%2C%20Y.%20The%20cross%20entropy%20method%20for%20fast%20policy%20search%202003"
        },
        {
            "id": "19",
            "entry": "[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "20",
            "entry": "[20] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Kavukcuoglu%2C%20K.%20Silver%2C%20D.%20Rusu%2C%20A.A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "21",
            "entry": "[21] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Badia%2C%20A.P.%20Mirza%2C%20M.%20Graves%2C%20A.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "22",
            "entry": "[22] W. Montgomery, A. Ajay, C. Finn, P. Abbeel, and S. Levine. Reset-free guided policy search: Efficient deep reinforcement learning with stochastic initial states. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 3373\u20133380. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montgomery%2C%20W.%20Ajay%2C%20A.%20Finn%2C%20C.%20Abbeel%2C%20P.%20Reset-free%20guided%20policy%20search%3A%20Efficient%20deep%20reinforcement%20learning%20with%20stochastic%20initial%20states%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montgomery%2C%20W.%20Ajay%2C%20A.%20Finn%2C%20C.%20Abbeel%2C%20P.%20Reset-free%20guided%20policy%20search%3A%20Efficient%20deep%20reinforcement%20learning%20with%20stochastic%20initial%20states%202017"
        },
        {
            "id": "23",
            "entry": "[23] I. Papusha, J. Fu, U. Topcu, and R. M. Murray. Automata theory meets approximate dynamic programming: Optimal control with temporal logic constraints. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pages 434\u2013440. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papusha%2C%20I.%20Fu%2C%20J.%20Topcu%2C%20U.%20Murray%2C%20R.M.%20Automata%20theory%20meets%20approximate%20dynamic%20programming%3A%20Optimal%20control%20with%20temporal%20logic%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papusha%2C%20I.%20Fu%2C%20J.%20Topcu%2C%20U.%20Murray%2C%20R.M.%20Automata%20theory%20meets%20approximate%20dynamic%20programming%3A%20Optimal%20control%20with%20temporal%20logic%20constraints%202016"
        },
        {
            "id": "24",
            "entry": "[24] I. Popov, N. Heess, T. Lillicrap, R. Hafner, G. Barth-Maron, M. Vecerik, T. Lampe, Y. Tassa, T. Erez, and M. Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. arXiv preprint arXiv:1704.03073, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.03073"
        },
        {
            "id": "25",
            "entry": "[25] R. Y. Rubinstein and B. Melamed. Modern simulation and modeling, volume 7. Wiley New York, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R%20Y%20Rubinstein%20and%20B%20Melamed%20Modern%20simulation%20and%20modeling%20volume%207%20Wiley%20New%20York%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R%20Y%20Rubinstein%20and%20B%20Melamed%20Modern%20simulation%20and%20modeling%20volume%207%20Wiley%20New%20York%201998"
        },
        {
            "id": "26",
            "entry": "[26] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "27",
            "entry": "[27] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889\u20131897, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.%20Trust%20region%20policy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schulman%2C%20J.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Jordan%2C%20M.%20Trust%20region%20policy%20optimization%202015"
        },
        {
            "id": "28",
            "entry": "[28] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.02438"
        },
        {
            "id": "29",
            "entry": "[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "30",
            "entry": "[30] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 387\u2013395, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Lever%2C%20G.%20Heess%2C%20N.%20Degris%2C%20T.%20Deterministic%20policy%20gradient%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Lever%2C%20G.%20Heess%2C%20N.%20Degris%2C%20T.%20Deterministic%20policy%20gradient%20algorithms%202014"
        },
        {
            "id": "31",
            "entry": "[31] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Huang%2C%20A.%20Maddison%2C%20C.J.%20Guez%2C%20A.%20Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "32",
            "entry": "[32] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550 (7676):354, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20D.%20Schrittwieser%2C%20J.%20Simonyan%2C%20K.%20Antonoglou%2C%20I.%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "33",
            "entry": "[33] I. Szita and A. L\u00f6rincz. Learning tetris using the noisy cross-entropy method. Learning, 18(12), 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szita%2C%20I.%20L%C3%B6rincz%2C%20A.%20Learning%20tetris%20using%20the%20noisy%20cross-entropy%20method%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szita%2C%20I.%20L%C3%B6rincz%2C%20A.%20Learning%20tetris%20using%20the%20noisy%20cross-entropy%20method%202006"
        },
        {
            "id": "34",
            "entry": "[34] E. Uchibe and K. Doya. Constrained reinforcement learning from intrinsic and extrinsic rewards. In Development and Learning, 2007. ICDL 2007. IEEE 6th International Conference on, pages 163\u2013168. IEEE, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Uchibe%2C%20E.%20Doya%2C%20K.%20Constrained%20reinforcement%20learning%20from%20intrinsic%20and%20extrinsic%20rewards%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Uchibe%2C%20E.%20Doya%2C%20K.%20Constrained%20reinforcement%20learning%20from%20intrinsic%20and%20extrinsic%20rewards%202007"
        },
        {
            "id": "36",
            "entry": "[36] I. Zamora, N. G. Lopez, V. M. Vilches, and A. H. Cordero. Extending the openai gym for robotics: a toolkit for reinforcement learning using ros and gazebo. arXiv preprint arXiv:1608.05742, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.05742"
        },
        {
            "id": "37",
            "entry": "[37] T. Zhang, G. Kahn, S. Levine, and P. Abbeel. Learning deep control policies for autonomous aerial vehicles with mpc-guided policy search. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 528\u2013535. IEEE, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20T.%20Kahn%2C%20G.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Learning%20deep%20control%20policies%20for%20autonomous%20aerial%20vehicles%20with%20mpc-guided%20policy%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20T.%20Kahn%2C%20G.%20Levine%2C%20S.%20Abbeel%2C%20P.%20Learning%20deep%20control%20policies%20for%20autonomous%20aerial%20vehicles%20with%20mpc-guided%20policy%20search%202016"
        }
    ]
}
