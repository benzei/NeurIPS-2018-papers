{
    "filename": "7343-removing-the-feature-correlation-effect-of-multiplicative-noise.pdf",
    "metadata": {
        "title": "Removing the Feature Correlation Effect of Multiplicative Noise",
        "author": "Zijun Zhang, Yining Zhang, Zongpeng Li",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7343-removing-the-feature-correlation-effect-of-multiplicative-noise.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Multiplicative noise, including dropout, is widely used to regularize deep neural networks (DNNs), and is shown to be effective in a wide range of architectures and tasks. From an information perspective, we consider injecting multiplicative noise into a DNN as training the network to solve the task with noisy information pathways, which leads to the observation that multiplicative noise tends to increase the correlation between features, so as to increase the signal-to-noise ratio of information pathways. However, high feature correlation is undesirable, as it increases redundancy in representations. In this work, we propose non-correlating multiplicative noise (NCMN), which exploits batch normalization to remove the correlation effect in a simple yet effective way. We show that NCMN significantly improves the performance of standard multiplicative noise on image classification tasks, providing a better alternative to dropout for batch-normalized networks. Additionally, we present a unified view of NCMN and shake-shake regularization, which explains the performance gain of the latter."
    },
    "keywords": [
        {
            "term": "batch normalization",
            "url": "https://en.wikipedia.org/wiki/batch_normalization"
        },
        {
            "term": "signal-to-noise ratio",
            "url": "https://en.wikipedia.org/wiki/signal-to-noise_ratio"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "natural language processing",
            "url": "https://en.wikipedia.org/wiki/natural_language_processing"
        },
        {
            "term": "deep neural networks",
            "url": "https://en.wikipedia.org/wiki/deep_neural_networks"
        },
        {
            "term": "image recognition",
            "url": "https://en.wikipedia.org/wiki/image_recognition"
        },
        {
            "term": "multiplicative noise",
            "url": "https://en.wikipedia.org/wiki/multiplicative_noise"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "State-of-the-art deep neural networks are often over-parameterized to deliver more expressive power",
        "We demonstrate the feature correlation effect of dropout, as well as other types of multiplicative noise, through analysis and experiments",
        "In Section 3, we first propose non-correlating multiplicative noise, which we show through analysis is able to remove the feature correlation effect; we develop multiple variants of non-correlating multiplicative noise, and provide a unified view of non-correlating multiplicative noise and shake-shake regularization",
        "Since wide residual network is based on the full pre-activation variant of residual networks [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], non-correlating multiplicative noise-1, non-correlating multiplicative noise-2, and shake-shake require an extra batch normalization layer after each residual branch",
        "Our analysis suggests a side effect of dropout and other types of multiplicative noise, which increases the correlation between features, and degrades generalization performance",
        "We proposed a simple modification to the gradient of noise components, which, when combined with batch normalization, is able to effectively remove the feature correlation effect"
    ],
    "key_statements": [
        "State-of-the-art deep neural networks are often over-parameterized to deliver more expressive power",
        "We demonstrate the feature correlation effect of dropout, as well as other types of multiplicative noise, through analysis and experiments",
        "In Section 3, we first propose non-correlating multiplicative noise, which we show through analysis is able to remove the feature correlation effect; we develop multiple variants of non-correlating multiplicative noise, and provide a unified view of non-correlating multiplicative noise and shake-shake regularization",
        "non-correlating multiplicative noise-1 significantly improves the performance of standard multiplicative noise, as shown in Table 1",
        "Since wide residual network is based on the full pre-activation variant of residual networks [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], non-correlating multiplicative noise-1, non-correlating multiplicative noise-2, and shake-shake require an extra batch normalization layer after each residual branch",
        "Compared to the results of shake-shake regularization, a wide residual network-28-10 network trained with non-correlating multiplicative noise is able to achieve comparable performance in 9 times less epochs",
        "Our analysis suggests a side effect of dropout and other types of multiplicative noise, which increases the correlation between features, and degrades generalization performance",
        "We proposed a simple modification to the gradient of noise components, which, when combined with batch normalization, is able to effectively remove the feature correlation effect"
    ],
    "summary": [
        "State-of-the-art deep neural networks are often over-parameterized to deliver more expressive power.",
        "We demonstrate the feature correlation effect of dropout, as well as other types of multiplicative noise, through analysis and experiments.",
        "In Section 2, we define a general form of multiplicative noise, and identify the feature correlation effect.",
        "In Section 3, we first propose NCMN, which we show through analysis is able to remove the feature correlation effect; we develop multiple variants of NCMN, and provide a unified view of NCMN and shake-shake regularization.",
        "Dropout, and other multiplicative noise, improve generalization by preventing feature co-adaptation [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "Eq (8) implies that if we ignore the gradient of the noise component, zjn, the tendency of increasing the SNR of zj will attempt to increase the variance of the signal component, zjs, instead of increasing the feature correlation of the lower layer.",
        "If we keep the gradient of the noise component, or, equivalently, let \u03b11 = \u03b11 and \u03b12 = \u03b12, maximizing SNR will encourage large E [z1,kz2,k] and small E (z1,k \u2212 z2,k)2 , leading to highly correlated branches.",
        "If we consider the noise component as a constant, only large E [z1,kz2,k] will be encouraged, which results in much weaker correlation, and the better generalization performance observed in practice.",
        "NCMN-1 significantly improves the performance of standard multiplicative noise, as shown in Table 1.",
        "Since WRN is based on the full pre-activation variant of ResNets [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], NCMN-1, NCMN-2, and shake-shake require an extra batch normalization layer after each residual branch.",
        "10 and CIFAR-100, NCMN and shake-shake show stronger regularization effect than standard multiplicative noise, as indicated by the difference between training and testing accuracies.",
        "Compared to the results of shake-shake regularization, a WRN-28-10 network trained with NCMN is able to achieve comparable performance in 9 times less epochs.",
        "NCMN-0 is simple, fast, and can be applied to any batch-normalized neural networks, while NCMN-2 yields better generalization performance on ResNets.",
        "Our analysis suggests a side effect of dropout and other types of multiplicative noise, which increases the correlation between features, and degrades generalization performance.",
        "We proposed a simple modification to the gradient of noise components, which, when combined with batch normalization, is able to effectively remove the feature correlation effect.",
        "The resulting method, NCMN, outperforms standard multiplicative noise by a large margin, proving it to be a better alternative for batch-normalized networks."
    ],
    "headline": "We propose non-correlating multiplicative noise , which exploits batch normalization to remove the correlation effect in a simple yet effective way",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015"
        },
        {
            "id": "2",
            "entry": "[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "3",
            "entry": "[3] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "4",
            "entry": "[4] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International Conference on Machine Learning, pages 1058\u20131066, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013"
        },
        {
            "id": "5",
            "entry": "[5] Xavier Gastaldi. Shake-shake regularization of 3-branch residual networks. In Workshop of International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gastaldi%2C%20Xavier%20Shake-shake%20regularization%20of%203-branch%20residual%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gastaldi%2C%20Xavier%20Shake-shake%20regularization%20of%203-branch%20residual%20networks%202017"
        },
        {
            "id": "6",
            "entry": "[6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "7",
            "entry": "[7] Stanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. Recurrent dropout without memory loss. arXiv preprint arXiv:1603.05118, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.05118"
        },
        {
            "id": "8",
            "entry": "[8] Yoshua Bengio and James S Bergstra. Slow, decorrelated features for pretraining complex cell-like networks. In Advances in neural information processing systems, pages 99\u2013107, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Bergstra%2C%20James%20S.%20Slow%2C%20decorrelated%20features%20for%20pretraining%20complex%20cell-like%20networks%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Bergstra%2C%20James%20S.%20Slow%2C%20decorrelated%20features%20for%20pretraining%20complex%20cell-like%20networks%202009"
        },
        {
            "id": "9",
            "entry": "[9] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishkin%2C%20Dmytro%20Matas%2C%20Jiri%20All%20you%20need%20is%20a%20good%20init%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishkin%2C%20Dmytro%20Matas%2C%20Jiri%20All%20you%20need%20is%20a%20good%20init%202016"
        },
        {
            "id": "10",
            "entry": "[10] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep networks by decorrelating representations. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cogswell%2C%20Michael%20Ahmed%2C%20Faruk%20Girshick%2C%20Ross%20Zitnick%2C%20Larry%20Reducing%20overfitting%20in%20deep%20networks%20by%20decorrelating%20representations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cogswell%2C%20Michael%20Ahmed%2C%20Faruk%20Girshick%2C%20Ross%20Zitnick%2C%20Larry%20Reducing%20overfitting%20in%20deep%20networks%20by%20decorrelating%20representations%202016"
        },
        {
            "id": "11",
            "entry": "[11] Pau Rodr\u00edguez, Jordi Gonzalez, Guillem Cucurull, Josep M Gonfaus, and Xavier Roca. Regularizing cnns with locally constrained decorrelations. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodr%C3%ADguez%2C%20Pau%20Gonzalez%2C%20Jordi%20Cucurull%2C%20Guillem%20Gonfaus%2C%20Josep%20M.%20Regularizing%20cnns%20with%20locally%20constrained%20decorrelations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodr%C3%ADguez%2C%20Pau%20Gonzalez%2C%20Jordi%20Cucurull%2C%20Guillem%20Gonfaus%2C%20Josep%20M.%20Regularizing%20cnns%20with%20locally%20constrained%20decorrelations%202017"
        },
        {
            "id": "12",
            "entry": "[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "13",
            "entry": "[13] Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Efficient object localization using convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 648\u2013656, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tompson%2C%20Jonathan%20Goroshin%2C%20Ross%20Jain%2C%20Arjun%20LeCun%2C%20Yann%20Efficient%20object%20localization%20using%20convolutional%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tompson%2C%20Jonathan%20Goroshin%2C%20Ross%20Jain%2C%20Arjun%20LeCun%2C%20Yann%20Efficient%20object%20localization%20using%20convolutional%20networks%202015"
        },
        {
            "id": "14",
            "entry": "[14] Sida Wang and Christopher Manning. Fast dropout training. In International Conference on Machine Learning, pages 118\u2013126, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Sida%20Manning%2C%20Christopher%20Fast%20dropout%20training%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Sida%20Manning%2C%20Christopher%20Fast%20dropout%20training%202013"
        },
        {
            "id": "15",
            "entry": "[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pages 630\u2013645.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Identity%20mappings%20in%20deep%20residual%20networks"
        },
        {
            "id": "16",
            "entry": "[16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07146"
        },
        {
            "id": "17",
            "entry": "[17] Zijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu. Normalized direction-preserving adam. arXiv preprint arXiv:1709.04546, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.04546"
        },
        {
            "id": "18",
            "entry": "[18] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20Ba%2C%20Jimmy%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "19",
            "entry": "[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "20",
            "entry": "[20] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information Processing Systems, pages 1729\u20131739, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffer%2C%20Elad%20Hubara%2C%20Itay%20Soudry%2C%20Daniel%20Train%20longer%2C%20generalize%20better%3A%20closing%20the%20generalization%20gap%20in%20large%20batch%20training%20of%20neural%20networks%202017"
        },
        {
            "id": "21",
            "entry": "[21] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1, page 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "22",
            "entry": "[22] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20Sepp%20Schmidhuber%2C%20J%C3%BCrgen%20Long%20short-term%20memory%201997"
        },
        {
            "id": "23",
            "entry": "[23] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "24",
            "entry": "[24] Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision. Springer, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuxin%20Wu%20and%20Kaiming%20He%20Group%20normalization%20In%20European%20Conference%20on%20Computer%20Vision%20Springer%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuxin%20Wu%20and%20Kaiming%20He%20Group%20normalization%20In%20European%20Conference%20on%20Computer%20Vision%20Springer%202018"
        }
    ]
}
