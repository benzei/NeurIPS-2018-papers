{
    "filename": "7553-moonshine-distilling-with-cheap-convolutions.pdf",
    "metadata": {
        "title": "Moonshine: Distilling with Cheap Convolutions",
        "author": "Elliot J. Crowley, Gavin Gray, Amos J. Storkey",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7553-moonshine-distilling-with-cheap-convolutions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Many engineers wish to deploy modern neural networks in memory-limited settings; but the development of flexible methods for reducing memory use is in its infancy, and there is little knowledge of the resulting cost-benefit. We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used. Using attention transfer, we provide Pareto curves/tables for distillation of residual networks with four benchmark datasets, indicating the memory versus accuracy payoff. We show that substantial memory savings are possible with very little loss of accuracy, and confirm that distillation provides student network performance that is better than training that student architecture directly on data."
    },
    "keywords": [
        {
            "term": "network performance",
            "url": "https://en.wikipedia.org/wiki/network_performance"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep net",
            "url": "https://en.wikipedia.org/wiki/deep_net"
        },
        {
            "term": "image classification",
            "url": "https://en.wikipedia.org/wiki/image_classification"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "residual network",
            "url": "https://en.wikipedia.org/wiki/residual_network"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "Despite advances in deep learning for a variety of tasks (LeCun et al, 2015), deployment of deep learning into embedded devices e.g. wearable devices, digital cameras, vehicle navigation systems, has been relatively slow due to resource constraints under which these devices operate",
        "Memoryintensive neural networks do not fit on these devices, but do these networks have to be big and expensive? The dominant run-time memory cost of neural networks is the number of parameters that need to be stored",
        "We show that for a comparable number of parameters, student networks that retain the architecture of their teacher but with cheaper convolutional blocks outperform student networks with the original blocks and smaller architectures",
        "Any form of distillation to observe whether the distillation process is necessary to obtain good performance. In this way we demonstrate that the high performance comes from the distillation, and cannot be achieved by directly training the student networks using the data",
        "The bulk of an ERFNet is made up of standard residual blocks where each full convolution has been replaced by a pair of 1D alternatives: Table 3: Top 1 and Top 5 classification errors (%) on the validation set of ImageNet for models (i) trained from scratch, and those trained with attention transfer with Residual Network-34 (Res34) as a teacher",
        "We have demonstrated a model compression strategy that is fast to apply, and doesn\u2019t require any additional engineering for both image classification and semantic segmentation"
    ],
    "key_statements": [
        "Despite advances in deep learning for a variety of tasks (LeCun et al, 2015), deployment of deep learning into embedded devices e.g. wearable devices, digital cameras, vehicle navigation systems, has been relatively slow due to resource constraints under which these devices operate",
        "Memoryintensive neural networks do not fit on these devices, but do these networks have to be big and expensive? The dominant run-time memory cost of neural networks is the number of parameters that need to be stored",
        "It is possible to take a large pre-trained teacher network, and use its outputs to aid in the training of a smaller student network (<a class=\"ref-link\" id=\"cBa_2014_a\" href=\"#rBa_2014_a\">Ba & Caruana, 2014</a>) through some distillation process",
        "We show that for a comparable number of parameters, student networks that retain the architecture of their teacher but with cheaper convolutional blocks outperform student networks with the original blocks and smaller architectures",
        "In Section 4 we evaluate student networks with these blocks on CIFAR-10 and CIFAR-100 (<a class=\"ref-link\" id=\"cKrizhevsky_2009_a\" href=\"#rKrizhevsky_2009_a\">Krizhevsky, 2009</a>)",
        "In the context of this paper, we found attention transfer to be effective in our experiments, as described in Section 4.\n3 Compression with Cheap Convolutions",
        "Any form of distillation to observe whether the distillation process is necessary to obtain good performance. In this way we demonstrate that the high performance comes from the distillation, and cannot be achieved by directly training the student networks using the data",
        "We have demonstrated that the blocks outlined in Table 1 are suitable substitutes",
        "The bulk of an ERFNet is made up of standard residual blocks where each full convolution has been replaced by a pair of 1D alternatives: Table 3: Top 1 and Top 5 classification errors (%) on the validation set of ImageNet for models (i) trained from scratch, and those trained with attention transfer with Residual Network-34 (Res34) as a teacher",
        "We have demonstrated a model compression strategy that is fast to apply, and doesn\u2019t require any additional engineering for both image classification and semantic segmentation"
    ],
    "summary": [
        "Despite advances in deep learning for a variety of tasks (LeCun et al, 2015), deployment of deep learning into embedded devices e.g. wearable devices, digital cameras, vehicle navigation systems, has been relatively slow due to resource constraints under which these devices operate.",
        "We utilise and compare two different distillation methods for learning a smaller student network from a large, pre-trained teacher network: knowledge distillation (<a class=\"ref-link\" id=\"cBa_2014_a\" href=\"#rBa_2014_a\">Ba & Caruana, 2014</a>; Hinton et al, 2015) and attention transfer (<a class=\"ref-link\" id=\"cZagoruyko_2017_a\" href=\"#rZagoruyko_2017_a\">Zagoruyko & Komodakis, 2017</a>).",
        "We train and evaluate a number of student networks, each distilled from the same large teacher network.",
        "Figure 2a compares the parameter cost of each student network against the test error on CIFAR-10 obtained with attention transfer.",
        "What is fascinating is that almost every network with the same architecture as the teacher, but with cheap convolutional blocks performs better for a given parameter budget than the reduced architecture networks with standard blocks.",
        "These results support our claim that greater model compression through distillation is possible by substituting the convolutional blocks in a network, rather than by shrinking its architecture.",
        "Our experiments use a pre-trained ResNet-34 (<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016a</a>) (21.8M parameters) as a teacher and we train several networks using attention transfer (AT).",
        "That the bulk of the parameters in a ResNet are contained in four groups, as opposed to the three of a WideResNet. We train the following student networks: (i) ResNet-18, ResNet-18 with the channel widths of the last three groups halved (Res18-0.5), ResNet-34 with each convolutional block replaced by a G(N ) block2 and a G(4) block.",
        "It is intriguing that Res34-G(4) trained from scratch is on par with the original teacher despite having 13 million fewer parameters; this generalisation capability of grouped convolutions in networks has been observed previously by <a class=\"ref-link\" id=\"cIoannou_et+al_2017_a\" href=\"#rIoannou_et+al_2017_a\">Ioannou et al (2017</a>).",
        "We have shown that cheapening the convolutions of a network, coupled with a good distillation process, has allowed for a substantial reduction in the number of network parameters in return for a small drop in performance.",
        "The bulk of an ERFNet is made up of standard residual blocks where each full convolution has been replaced by a pair of 1D alternatives: Table 3: Top 1 and Top 5 classification errors (%) on the validation set of ImageNet for models (i) trained from scratch, and those trained with attention transfer with ResNet-34 (Res34) as a teacher.",
        "Implementation Details Models were trained using the same optimiser, schedule, and image scaling and augmentation as in the ERFNet paper (<a class=\"ref-link\" id=\"cRomera_et+al_2017_b\" href=\"#rRomera_et+al_2017_b\">Romera et al, 2017b</a>) with the attention transfer loss for the case of ERFNet-G(N) as a student.",
        "The optimisation algorithm of the larger model is sufficient to train the cheaper student model"
    ],
    "headline": "We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used",
    "reference_links": [
        {
            "id": "Ba_2014_a",
            "entry": "Ba, L. J. and Caruana, R. Do deep nets really need to be deep? In Advances in Neural Information Processing Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20L.J.%20Caruana%2C%20R.%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20L.J.%20Caruana%2C%20R.%20Do%20deep%20nets%20really%20need%20to%20be%20deep%3F%202014"
        },
        {
            "id": "Bucilua_et+al_2006_a",
            "entry": "Bucilua, C., Caruana, R., and Niculescu-Mizil, A. Model compression. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bucilua%2C%20C.%20Caruana%2C%20R.%20Niculescu-Mizil%2C%20A.%20Model%20compression%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bucilua%2C%20C.%20Caruana%2C%20R.%20Niculescu-Mizil%2C%20A.%20Model%20compression%202006"
        },
        {
            "id": "Chollet_2016_a",
            "entry": "Chollet, F. Xception: Deep learning with depthwise separable convolutions. arXiv:1610.02357, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.02357"
        },
        {
            "id": "Cordts_et+al_2016_a",
            "entry": "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and Schiele, B. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cordts%2C%20M.%20Omran%2C%20M.%20Ramos%2C%20S.%20Rehfeld%2C%20T.%20The%20cityscapes%20dataset%20for%20semantic%20urban%20scene%20understanding%202016"
        },
        {
            "id": "Cybenko_1989_a",
            "entry": "Cybenko, G. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303\u2013314, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cybenko%2C%20G.%20Approximation%20by%20superpositions%20of%20a%20sigmoidal%20function%201989"
        },
        {
            "id": "Denil_et+al_2013_a",
            "entry": "Denil, M., Shakibi, B., Dinh, L., Ranzato, M., and de Freitas, N. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Denil%2C%20M.%20Shakibi%2C%20B.%20Dinh%2C%20L.%20Ranzato%2C%20M.%20Predicting%20parameters%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Denil%2C%20M.%20Shakibi%2C%20B.%20Dinh%2C%20L.%20Ranzato%2C%20M.%20Predicting%20parameters%20in%20deep%20learning%202013"
        },
        {
            "id": "Garipov_et+al_2016_a",
            "entry": "Garipov, T., Podoprikhin, D., Novikov, A., and Vetrov, D. P. Ultimate tensorization: compressing convolutional and FC layers alike. arXiv:1611.03214, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03214"
        },
        {
            "id": "Han_et+al_2016_a",
            "entry": "Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20S.%20Mao%2C%20H.%20Dally%2C%20W.J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20Huffman%20coding%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20S.%20Mao%2C%20H.%20Dally%2C%20W.J.%20Deep%20compression%3A%20Compressing%20deep%20neural%20networks%20with%20pruning%2C%20trained%20quantization%20and%20Huffman%20coding%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "He_et+al_2016_b",
            "entry": "He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20mappings%20in%20deep%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20mappings%20in%20deep%20residual%20networks%202016"
        },
        {
            "id": "Hinton_et+al_0000_a",
            "entry": "Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Howard_et+al_2017_a",
            "entry": "Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "Iandola_et+al_2016_a",
            "entry": "Iandola, F. N., Moskewicz, M. W., Ashraf, K., Han, S., Dally, W. J., and Keutzer, K. SqueezeNet: AlexNet-level accuracy with 50\u00d7 fewer parameters and < 1MB model size. arXiv:1602.07360, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07360"
        },
        {
            "id": "Ioannou_et+al_2017_a",
            "entry": "Ioannou, Y., Robertson, D., Cipolla, R., and Criminisi, A. Deep roots: Improving CNN efficiency with hierarchical filter groups. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioannou%2C%20Y.%20Robertson%2C%20D.%20Cipolla%2C%20R.%20Criminisi%2C%20A.%20Deep%20roots%3A%20Improving%20CNN%20efficiency%20with%20hierarchical%20filter%20groups%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioannou%2C%20Y.%20Robertson%2C%20D.%20Cipolla%2C%20R.%20Criminisi%2C%20A.%20Deep%20roots%3A%20Improving%20CNN%20efficiency%20with%20hierarchical%20filter%20groups%202017"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jaderberg_et+al_2014_a",
            "entry": "Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up convolutional neural networks with low rank expansions. In British Machine Vision Conference, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20M.%20Vedaldi%2C%20A.%20Zisserman%2C%20A.%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20M.%20Vedaldi%2C%20A.%20Zisserman%2C%20A.%20Speeding%20up%20convolutional%20neural%20networks%20with%20low%20rank%20expansions%202014"
        },
        {
            "id": "Jin_et+al_2015_a",
            "entry": "Jin, J., Dundar, A., and Culurciello, E. Flattened convolutional neural networks for feedforward acceleration. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20J.%20Dundar%2C%20A.%20Culurciello%2C%20E.%20Flattened%20convolutional%20neural%20networks%20for%20feedforward%20acceleration%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20J.%20Dundar%2C%20A.%20Culurciello%2C%20E.%20Flattened%20convolutional%20neural%20networks%20for%20feedforward%20acceleration%202015"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "Krizhevsky, A. Learning multiple layers of features from tiny images. Master\u2019s thesis, University of Toronto, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Krizhevsky_et+al_2012_a",
            "entry": "Krizhevsky, A., Sutskever, I., and Hinton, G. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20A.%20Sutskever%2C%20I.%20Hinton%2C%20G.%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "Lecun_et+al_2015_a",
            "entry": "LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature, 521(7553):436\u2013444, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bengio%2C%20Y.%20Hinton%2C%20G.%20Deep%20learning%202015"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Li, Z., Wang, X., Lv, X., and Yang, T. SEP-Nets: Small and effective pattern networks. arXiv:1706.03912, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03912"
        },
        {
            "id": "Moczulski_et+al_2016_a",
            "entry": "Moczulski, M., Denil, M., Appleyard, J., and de Freitas, N. ACDC: a structured efficient linear layer. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moczulski%2C%20M.%20Denil%2C%20M.%20Appleyard%2C%20J.%20de%20Freitas%2C%20N.%20ACDC%3A%20a%20structured%20efficient%20linear%20layer%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moczulski%2C%20M.%20Denil%2C%20M.%20Appleyard%2C%20J.%20de%20Freitas%2C%20N.%20ACDC%3A%20a%20structured%20efficient%20linear%20layer%202016"
        },
        {
            "id": "Romera_et+al_2017_a",
            "entry": "Romera, E., \u00c1lvarez, J. M., Bergasa, L. M., and Arroyo, R. Efficient convnet for real-time semantic segmentation. IEEE Intelligent Vehicles Symposium, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romera%2C%20E.%20%C3%81lvarez%2C%20J.M.%20Bergasa%2C%20L.M.%20Arroyo%2C%20R.%20Efficient%20convnet%20for%20real-time%20semantic%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romera%2C%20E.%20%C3%81lvarez%2C%20J.M.%20Bergasa%2C%20L.M.%20Arroyo%2C%20R.%20Efficient%20convnet%20for%20real-time%20semantic%20segmentation%202017"
        },
        {
            "id": "Romera_et+al_2017_b",
            "entry": "Romera, E., \u00c1lvarez, J. M., Bergasa, L. M., and Arroyo, R. ERFNet: Efficient residual factorized convnet for real-time semantic segmentation. IEEE Transactions on Intelligent Transportation Systems, 2017b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romera%2C%20E.%20%C3%81lvarez%2C%20J.M.%20Bergasa%2C%20L.M.%20Arroyo%2C%20R.%20ERFNet%3A%20Efficient%20residual%20factorized%20convnet%20for%20real-time%20semantic%20segmentation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romera%2C%20E.%20%C3%81lvarez%2C%20J.M.%20Bergasa%2C%20L.M.%20Arroyo%2C%20R.%20ERFNet%3A%20Efficient%20residual%20factorized%20convnet%20for%20real-time%20semantic%20segmentation%202017"
        },
        {
            "id": "Romero_et+al_2015_a",
            "entry": "Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., and Bengio, Y. FitNets: Hints for thin deep nets. In International Conference on Learning Representations, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Romero%2C%20A.%20Ballas%2C%20N.%20Kahou%2C%20S.E.%20Chassang%2C%20A.%20FitNets%3A%20Hints%20for%20thin%20deep%20nets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Romero%2C%20A.%20Ballas%2C%20N.%20Kahou%2C%20S.E.%20Chassang%2C%20A.%20FitNets%3A%20Hints%20for%20thin%20deep%20nets%202015"
        },
        {
            "id": "Russakovsky_et+al_2015_a",
            "entry": "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/ s11263-015-0816-y.",
            "crossref": "https://dx.doi.org/10.1007/s11263-015-0816-y",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1007/s11263-015-0816-y"
        },
        {
            "id": "Sainath_et+al_2013_a",
            "entry": "Sainath, T. N., Kingsbury, B., Sindhwani, V., Arisoy, E., and Ramabhadran, B. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20T.N.%20Kingsbury%2C%20B.%20Sindhwani%2C%20V.%20Arisoy%2C%20E.%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20T.N.%20Kingsbury%2C%20B.%20Sindhwani%2C%20V.%20Arisoy%2C%20E.%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%202013"
        },
        {
            "id": "Sifre_2014_a",
            "entry": "Sifre, L. Rigid-Motion Scattering for Image Classification. PhD thesis, \u00c9cole Polytechnique, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sifre%2C%20L.%20Rigid-Motion%20Scattering%20for%20Image%20Classification%202014"
        },
        {
            "id": "Urban_et+al_2017_a",
            "entry": "Urban, G., Geras, K. J., Kahou, S. E., Aslan, O., Wang, S., Caruana, R., Mohamed, A., Philipose, M., and Richardson, M. Do deep convolutional nets really need to be deep and convolutional? In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Urban%2C%20G.%20Geras%2C%20K.J.%20Kahou%2C%20S.E.%20Aslan%2C%20O.%20Do%20deep%20convolutional%20nets%20really%20need%20to%20be%20deep%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Urban%2C%20G.%20Geras%2C%20K.J.%20Kahou%2C%20S.E.%20Aslan%2C%20O.%20Do%20deep%20convolutional%20nets%20really%20need%20to%20be%20deep%202017"
        },
        {
            "id": "Wang_et+al_2016_a",
            "entry": "Wang, M., Liu, B., and Foroosh, H. Design of efficient convolutional layers using single intra-channel convolution, topological subdivisioning and spatial bottleneck structure. arXiv:1608.04337, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04337"
        },
        {
            "id": "Xie_et+al_2017_a",
            "entry": "Xie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., and He, K. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20S.%20Girshick%2C%20R.%20Doll%C3%A1r%2C%20P.%20Tu%2C%20Z.%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20S.%20Girshick%2C%20R.%20Doll%C3%A1r%2C%20P.%20Tu%2C%20Z.%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "Yang_et+al_2015_a",
            "entry": "Yang, Z., Moczulski, M., Denil, M., de Freitas, N., Smola, A., Song, L., and Wang, Z. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Z.%20Moczulski%2C%20M.%20Denil%2C%20M.%20de%20Freitas%2C%20N.%20Deep%20fried%20convnets%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Z.%20Moczulski%2C%20M.%20Denil%2C%20M.%20de%20Freitas%2C%20N.%20Deep%20fried%20convnets%202015"
        },
        {
            "id": "Yu_2016_a",
            "entry": "Yu, F. and Koltun, V. Multi-scale context aggregation by dilated convolutions. In International Conference on Learning Representations, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20F.%20Koltun%2C%20V.%20Multi-scale%20context%20aggregation%20by%20dilated%20convolutions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20F.%20Koltun%2C%20V.%20Multi-scale%20context%20aggregation%20by%20dilated%20convolutions%202016"
        },
        {
            "id": "Zagoruyko_2016_a",
            "entry": "Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Conference, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Wide%20residual%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Wide%20residual%20networks%202016"
        },
        {
            "id": "Zagoruyko_2017_a",
            "entry": "Zagoruyko, S. and Komodakis, N. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Paying%20more%20attention%20to%20attention%3A%20Improving%20the%20performance%20of%20convolutional%20neural%20networks%20via%20attention%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zagoruyko%2C%20S.%20Komodakis%2C%20N.%20Paying%20more%20attention%20to%20attention%3A%20Improving%20the%20performance%20of%20convolutional%20neural%20networks%20via%20attention%20transfer%202017"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Zhang, X., Zhou, X., Lin, M., and Sun, J. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20X.%20Zhou%2C%20X.%20Lin%2C%20M.%20Sun%2C%20J.%20ShuffleNet%3A%20An%20extremely%20efficient%20convolutional%20neural%20network%20for%20mobile%20devices%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20X.%20Zhou%2C%20X.%20Lin%2C%20M.%20Sun%2C%20J.%20ShuffleNet%3A%20An%20extremely%20efficient%20convolutional%20neural%20network%20for%20mobile%20devices%202018"
        },
        {
            "id": "Zoph_et+al_2018_a",
            "entry": "Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%2C%20B.%20Vasudevan%2C%20V.%20Shlens%2C%20J.%20Le%2C%20Q.V.%20Learning%20transferable%20architectures%20for%20scalable%20image%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoph%2C%20B.%20Vasudevan%2C%20V.%20Shlens%2C%20J.%20Le%2C%20Q.V.%20Learning%20transferable%20architectures%20for%20scalable%20image%20recognition%202018"
        }
    ]
}
