{
    "filename": "7967-out-of-distribution-detection-using-multiple-semantic-label-representations.pdf",
    "metadata": {
        "title": "Out-of-Distribution Detection using Multiple Semantic Label Representations",
        "author": "Gabi Shalev, Yossi Adi, Joseph Keshet",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7967-out-of-distribution-detection-using-multiple-semantic-label-representations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep Neural Networks are powerful models that attained remarkable results on a variety of tasks. These models are shown to be extremely efficient when training and test data are drawn from the same distribution. However, it is not clear how a network will act when it is fed with an out-of-distribution example. In this work, we consider the problem of out-of-distribution detection in neural networks. We propose to use multiple semantic dense representations instead of sparse representation as the target label. Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels. We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods. Results suggest that our method compares favorably with previous work. Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples."
    },
    "keywords": [
        {
            "term": "True Positive Rate",
            "url": "https://en.wikipedia.org/wiki/True_Positive_Rate"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/Speech_Recognition"
        },
        {
            "term": "l2 norm",
            "url": "https://en.wikipedia.org/wiki/l2_norm"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "False Positive Rate",
            "url": "https://en.wikipedia.org/wiki/False_Positive_Rate"
        }
    ],
    "highlights": [
        "Deep Neural Networks (DNNs) have gained lots of success after enabling several breakthroughs in notably challenging problems such as image classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], speech recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "Similar to [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], we considered CIFAR-10, CIFAR-100 and GCommands datasets as in-distribution examples",
        "When the model is trained on CIFAR-100 and evaluated on CIFAR-10, it has 10% in-distribution examples a-priori",
        "When the model is trained on CIFAR-10 and evaluated on CIFAR-100, it has 1% in-distribution examples a-priori.\n5 Adversarial Examples",
        "We propose to use several semantic representations for each target label as supervision to the model in order to detect out-of-distribution examples, where the detection score is based on the L2-norm of the output representations",
        "We would like to further investigate the following: (i) we would like to explore better decision strategy for detecting out-of-distribution examples; we would like to rigorously analyze the notion of confidence based on the L2-norm beyond [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]; we would like to inspect the relation between detecting wrongly classified examples and adversarial examples to outof-distribution examples"
    ],
    "key_statements": [
        "Deep Neural Networks (DNNs) have gained lots of success after enabling several breakthroughs in notably challenging problems such as image classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], speech recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "Similar to [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], we considered CIFAR-10, CIFAR-100 and GCommands datasets as in-distribution examples",
        "When the model is trained on CIFAR-100 and evaluated on CIFAR-10, it has 10% in-distribution examples a-priori",
        "When the model is trained on CIFAR-10 and evaluated on CIFAR-100, it has 1% in-distribution examples a-priori.\n5 Adversarial Examples",
        "It is not clear if adversarial examples can be considered as an out-of-distribution, we found that our method is very efficient in detecting these examples",
        "Recall the embedding models were trained to minimize the cosine distance between the output vector and the word representation of the target label according to some embedding space",
        "We propose to use several semantic representations for each target label as supervision to the model in order to detect out-of-distribution examples, where the detection score is based on the L2-norm of the output representations",
        "We would like to further investigate the following: (i) we would like to explore better decision strategy for detecting out-of-distribution examples; we would like to rigorously analyze the notion of confidence based on the L2-norm beyond [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]; we would like to inspect the relation between detecting wrongly classified examples and adversarial examples to outof-distribution examples"
    ],
    "summary": [
        "Deep Neural Networks (DNNs) have gained lots of success after enabling several breakthroughs in notably challenging problems such as image classification [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>], speech recognition [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and machine translation [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>].",
        "In [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] the authors proposed a baseline method to detect out-of-distribution examples based on the models\u2019 output probabilities.",
        "Ensemble of classifiers with optional adversarial training was proposed in [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] for detecting out-ofdistribution examples.",
        "Our classifier is composed of several regression functions, each of which is trained to predict a word embedding of the target label.",
        "We propose using several different word embeddings as a supervision to gain diversity and redundancy in an ensemble model with a shared representation.",
        "The word embedding is a function e : Y \u2192 Z from the set of labels Y to an abstract vector space Z.",
        "Rather than using direct supervision, our classifier is composed of a set of regression functions, where each function is trained to predict a different word embedding.",
        "Each regression function f\u03b8kk is trained to predict a word embedding vector ek(y) corresponds to the word which represents the target label y \u2208 Y.",
        "Each example from the training set, Strain, is composed of an instance x \u2208 X and a set of K different word embeddings {e1(y), .",
        "The regression functions predict K vectors, each corresponds to a vector in a different word embedding space.",
        "We report the accuracy for our models using K = 1, 3 or 5 word embeddings, and compare it to the baseline and to the ensemble of softmax classifier.",
        "Out-of-Distribution Datasets For out-of-distribution examples, we followed a similar setting as in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] and evaluated our models on several different datasets.",
        "Figure 2 presents the distribution of the L2-norm of the proposed method using 5 word embeddings and the maximum probability of the baseline model for CIFAR-100 and SVHN",
        "Recall the embedding models were trained to minimize the cosine distance between the output vector and the word representation of the target label according to some embedding space.",
        "We propose to use several semantic representations for each target label as supervision to the model in order to detect out-of-distribution examples, where the detection score is based on the L2-norm of the output representations.",
        "We would like to further investigate the following: (i) we would like to explore better decision strategy for detecting out-of-distribution examples; we would like to rigorously analyze the notion of confidence based on the L2-norm beyond [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>]; we would like to inspect the relation between detecting wrongly classified examples and adversarial examples to outof-distribution examples."
    ],
    "headline": "We propose to use multiple semantic dense representations instead of sparse representation as the target label",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al. Deep speech 2: Endto-end speech recognition in english and mandarin. In International Conference on Machine Learning, pages 173\u2013182, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amodei%2C%20Dario%20Anubhai%2C%20Rishita%20Battenberg%2C%20Eric%20Case%2C%20Carl%20Deep%20speech%202%3A%20Endto-end%20speech%20recognition%20in%20english%20and%20mandarin%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amodei%2C%20Dario%20Anubhai%2C%20Rishita%20Battenberg%2C%20Eric%20Case%2C%20Carl%20Deep%20speech%202%3A%20Endto-end%20speech%20recognition%20in%20english%20and%20mandarin%202016"
        },
        {
            "id": "2",
            "entry": "[2] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06565"
        },
        {
            "id": "3",
            "entry": "[3] Jerone TA Andrews, Thomas Tanay, Edward J Morton, and Lewis D Griffin. Transfer representation-learning for anomaly detection. ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrews%2C%20Jerone%20T.A.%20Tanay%2C%20Thomas%20Morton%2C%20Edward%20J.%20Griffin%2C%20Lewis%20D.%20Transfer%20representation-learning%20for%20anomaly%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrews%2C%20Jerone%20T.A.%20Tanay%2C%20Thomas%20Morton%2C%20Edward%20J.%20Griffin%2C%20Lewis%20D.%20Transfer%20representation-learning%20for%20anomaly%20detection%202016"
        },
        {
            "id": "4",
            "entry": "[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "5",
            "entry": "[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.3005"
        },
        {
            "id": "6",
            "entry": "[6] Moustapha M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Fooling deep structured visual and speech recognition models with adversarial examples. In Advances in Neural Information Processing Systems, pages 6980\u20136990, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cisse%2C%20Moustapha%20M.%20Adi%2C%20Yossi%20Neverova%2C%20Natalia%20Keshet%2C%20Joseph%20Houdini%3A%20Fooling%20deep%20structured%20visual%20and%20speech%20recognition%20models%20with%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cisse%2C%20Moustapha%20M.%20Adi%2C%20Yossi%20Neverova%2C%20Natalia%20Keshet%2C%20Joseph%20Houdini%3A%20Fooling%20deep%20structured%20visual%20and%20speech%20recognition%20models%20with%20adversarial%20examples%202017"
        },
        {
            "id": "7",
            "entry": "[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009"
        },
        {
            "id": "8",
            "entry": "[8] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121\u20132129, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frome%2C%20Andrea%20Corrado%2C%20Greg%20S.%20Shlens%2C%20Jon%20Bengio%2C%20Samy%20Devise%3A%20A%20deep%20visual-semantic%20embedding%20model%202013"
        },
        {
            "id": "9",
            "entry": "[9] Tzeviya Fuchs and Joseph Keshet. Spoken term detection automatically adjusted for a given threshold. IEEE Journal of Selected Topics in Signal Processing, 11(8):1310\u20131317, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fuchs%2C%20Tzeviya%20Keshet%2C%20Joseph%20Spoken%20term%20detection%20automatically%20adjusted%20for%20a%20given%20threshold%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fuchs%2C%20Tzeviya%20Keshet%2C%20Joseph%20Spoken%20term%20detection%20automatically%20adjusted%20for%20a%20given%20threshold%202017"
        },
        {
            "id": "10",
            "entry": "[10] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u2013 1059, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20Dropout%20as%20a%20bayesian%20approximation%3A%20Representing%20model%20uncertainty%20in%20deep%20learning%202016"
        },
        {
            "id": "11",
            "entry": "[11] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "12",
            "entry": "[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "13",
            "entry": "[13] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-ofdistribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.02136"
        },
        {
            "id": "14",
            "entry": "[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "15",
            "entry": "[15] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, volume 1, page 3, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20Weinberger%2C%20Kilian%20Q.%20van%20der%20Maaten%2C%20Laurens%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "16",
            "entry": "[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "17",
            "entry": "[17] Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. Fooling end-to-end speaker verification by adversarial examples. arXiv preprint arXiv:1801.03339, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03339"
        },
        {
            "id": "18",
            "entry": "[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "19",
            "entry": "[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6405\u20136416, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lakshminarayanan%2C%20Balaji%20Pritzel%2C%20Alexander%20Blundell%2C%20Charles%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lakshminarayanan%2C%20Balaji%20Pritzel%2C%20Alexander%20Blundell%2C%20Charles%20Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles%202017"
        },
        {
            "id": "20",
            "entry": "[20] Amos Lapidoth. A foundation in digital communication. Cambridge University Press, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lapidoth%2C%20Amos%20A%20foundation%20in%20digital%20communication%202017"
        },
        {
            "id": "21",
            "entry": "[21] Claudia Leacock and Martin Chodorow. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265\u2013283, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leacock%2C%20Claudia%20Chodorow%2C%20Martin%20Combining%20local%20context%20and%20wordnet%20similarity%20for%20word%20sense%20identification.%20WordNet%3A%20An%20electronic%20lexical%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leacock%2C%20Claudia%20Chodorow%2C%20Martin%20Combining%20local%20context%20and%20wordnet%20similarity%20for%20word%20sense%20identification.%20WordNet%3A%20An%20electronic%20lexical%201998"
        },
        {
            "id": "22",
            "entry": "[22] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "23",
            "entry": "[23] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. arXiv preprint arXiv:1711.09325, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.09325"
        },
        {
            "id": "24",
            "entry": "[24] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why m heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint arXiv:1511.06314, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06314"
        },
        {
            "id": "25",
            "entry": "[25] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02690"
        },
        {
            "id": "26",
            "entry": "[26] Etai Littwin and Lior Wolf. The multiverse loss for robust transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3957\u20133966, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littwin%2C%20Etai%20Wolf%2C%20Lior%20The%20multiverse%20loss%20for%20robust%20transfer%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littwin%2C%20Etai%20Wolf%2C%20Lior%20The%20multiverse%20loss%20for%20robust%20transfer%20learning%202016"
        },
        {
            "id": "27",
            "entry": "[27] David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20David%20J.C.%20Bayesian%20methods%20for%20adaptive%20models%201992"
        },
        {
            "id": "28",
            "entry": "[28] Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. Advances in pre-training distributed word representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Grave%2C%20Edouard%20Bojanowski%2C%20Piotr%20Puhrsch%2C%20Christian%20Advances%20in%20pre-training%20distributed%20word%20representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Grave%2C%20Edouard%20Bojanowski%2C%20Piotr%20Puhrsch%2C%20Christian%20Advances%20in%20pre-training%20distributed%20word%20representations%202018"
        },
        {
            "id": "29",
            "entry": "[29] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "30",
            "entry": "[30] Einat Naaman, Yossi Adi, and Joseph Keshet. Learning similarity function for pronunciation variations. In Proc. of Interspeech, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Naaman%2C%20Einat%20Adi%2C%20Yossi%20Keshet%2C%20Joseph%20Learning%20similarity%20function%20for%20pronunciation%20variations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Naaman%2C%20Einat%20Adi%2C%20Yossi%20Keshet%2C%20Joseph%20Learning%20similarity%20function%20for%20pronunciation%20variations%202017"
        },
        {
            "id": "31",
            "entry": "[31] Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neal%2C%20Radford%20M.%20Bayesian%20learning%20for%20neural%20networks%2C%20volume%20118%202012"
        },
        {
            "id": "32",
            "entry": "[32] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Yuval%20Wang%2C%20Tao%20Coates%2C%20Adam%20Bissacco%2C%20Alessandro%20Reading%20digits%20in%20natural%20images%20with%20unsupervised%20feature%20learning%202011"
        },
        {
            "id": "33",
            "entry": "[33] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427\u2013436, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Anh%20Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Deep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%202015"
        },
        {
            "id": "34",
            "entry": "[34] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition, linguistic data consortium. Google Scholar, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parker%2C%20Robert%20Graff%2C%20David%20Kong%2C%20Junbo%20Chen%2C%20Ke%20English%20gigaword%20fifth%20edition%2C%20linguistic%20data%20consortium%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parker%2C%20Robert%20Graff%2C%20David%20Kong%2C%20Junbo%20Chen%2C%20Ke%20English%20gigaword%20fifth%20edition%2C%20linguistic%20data%20consortium%202011"
        },
        {
            "id": "35",
            "entry": "[35] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPSW%202017"
        },
        {
            "id": "36",
            "entry": "[36] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pennington%2C%20Jeffrey%20Socher%2C%20Richard%20Manning%2C%20Christopher%20Glove%3A%20Global%20vectors%20for%20word%20representation%202014"
        },
        {
            "id": "37",
            "entry": "[37] Thomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International Conference on Information Processing in Medical Imaging, pages 146\u2013157.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schlegl%2C%20Thomas%20Seeb%C3%B6ck%2C%20Philipp%20Waldstein%2C%20Sebastian%20M.%20Schmidt-Erfurth%2C%20Ursula%20Unsupervised%20anomaly%20detection%20with%20generative%20adversarial%20networks%20to%20guide%20marker%20discovery",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schlegl%2C%20Thomas%20Seeb%C3%B6ck%2C%20Philipp%20Waldstein%2C%20Sebastian%20M.%20Schmidt-Erfurth%2C%20Ursula%20Unsupervised%20anomaly%20detection%20with%20generative%20adversarial%20networks%20to%20guide%20marker%20discovery"
        },
        {
            "id": "38",
            "entry": "[38] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "39",
            "entry": "[39] Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf. Web-scale training for face identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2746\u20132754, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yaniv%20Taigman%2C%20Ming%20Yang%2C%20Marc%E2%80%99Aurelio%20Ranzato%20Wolf%2C%20Lior%20Web-scale%20training%20for%20face%20identification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yaniv%20Taigman%2C%20Ming%20Yang%2C%20Marc%E2%80%99Aurelio%20Ranzato%20Wolf%2C%20Lior%20Web-scale%20training%20for%20face%20identification%202015"
        },
        {
            "id": "40",
            "entry": "[40] Lucas Theis, A\u00e4ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20Lucas%20van%20den%20Oord%2C%20A%C3%A4ron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20Lucas%20van%20den%20Oord%2C%20A%C3%A4ron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202015"
        },
        {
            "id": "41",
            "entry": "[41] Zhibiao Wu and Martha Palmer. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133\u2013138. Association for Computational Linguistics, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Zhibiao%20Palmer%2C%20Martha%20Verbs%20semantics%20and%20lexical%20selection%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Zhibiao%20Palmer%2C%20Martha%20Verbs%20semantics%20and%20lexical%20selection%201994"
        },
        {
            "id": "42",
            "entry": "[42] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.03365"
        },
        {
            "id": "43",
            "entry": "[43] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "44",
            "entry": "[44] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1710.09412"
        }
    ]
}
