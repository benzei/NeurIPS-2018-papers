{
    "filename": "7378-metareg-towards-domain-generalization-using-meta-regularization.pdf",
    "metadata": {
        "title": "MetaReg: Towards Domain Generalization using Meta-Regularization",
        "author": "Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, we encode this notion of domain generalization using a novel regularization function. We pose the problem of finding such a regularization function in a Learning to Learn (or) metalearning framework. The objective of domain generalization is explicitly modeled by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization."
    },
    "keywords": [
        {
            "term": "domain adaptation",
            "url": "https://en.wikipedia.org/wiki/domain_adaptation"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Existing machine learning algorithms including deep neural networks achieve good performance in cases where the training and the test data are sampled from the same distribution",
        "Even strong learners such as deep neural networks are known to be sensitive to such domain shifts [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>][<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]",
        "We show how the notion of domain generalization can be explicitly encoded in a regularization function, which can be used to train models that are more robust to domain shifts",
        "We addressed the problem of domain generalization by using regularization",
        "The task of finding the desired regularizer that captures the notion of domain generalization is modeled as a meta-learning problem",
        "Experiments indicate that the learnt regularizers achieve good cross-domain generalization on the benchmark domain generalization datasets"
    ],
    "key_statements": [
        "Existing machine learning algorithms including deep neural networks achieve good performance in cases where the training and the test data are sampled from the same distribution",
        "Even strong learners such as deep neural networks are known to be sensitive to such domain shifts [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>][<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>]",
        "The primary contribution of this work is that we propose a scheme for learning regularization functions that enable domain generalization",
        "We show how the notion of domain generalization can be explicitly encoded in a regularization function, which can be used to train models that are more robust to domain shifts",
        "Our approach attempts to tackle both these problems - (1) We explicitly address the notion of domain generalization in our episodic training procedure by using a regularizer to go from a task specific representation to a task general representation at each episode",
        "We model the regularizer R as a neural network parametrized by weights \u03c6",
        "As the individual task networks are updated on their respective source domain data, the regularizer updates are derived from each point of this SGD path with the objective of cross-domain generalization",
        "We addressed the problem of domain generalization by using regularization",
        "The task of finding the desired regularizer that captures the notion of domain generalization is modeled as a meta-learning problem",
        "Experiments indicate that the learnt regularizers achieve good cross-domain generalization on the benchmark domain generalization datasets"
    ],
    "summary": [
        "Existing machine learning algorithms including deep neural networks achieve good performance in cases where the training and the test data are sampled from the same distribution.",
        "The objective of this work is to learn a regularizer that generalizes to novel distributions not present in the training data.",
        "[<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] proposed a meta learning based approach (MLDG) extending MAML to the domain generalization problem.",
        "Domain generalization involves data sampled from p source distributions and q target distributions, each containing data for performing the same task.",
        "Using the samples from the metatrain set, l steps of gradient descent is performed with the regularized loss function Lreg(\u03c8, \u03b8) on Tnew.",
        "This update ensures that l steps of gradient descent using the regularized loss on samples from domain a results in task network a performing well on domain b.",
        "Once the regularizer is learnt, the regularization parameters \u03c6 are frozen and the final task network initialized from scratch is trained on all p source domains using the regularized loss function Lreg(\u03c8, \u03b8).",
        "By using our meta-learning procedure, we select a common set of weights that achieve good cross-domain generalization across every pair of source domains (a, b).",
        "As the individual task networks are updated on their respective source domain data, the regularizer updates are derived from each point of this SGD path with the objective of cross-domain generalization.",
        "Baseline setting denotes training a neural network (Alexnet in this case) on all of the source domains without performing any domain generalization.",
        "We would like to point out that even domain adaptation methods that make use of unlabeled target data achieve similar gains in performance [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] in this dataset.",
        "For all the ablation experiments except 5.3, we use the Resnet-18 model as our neural network architecture, and Art-painting setting in PACS dataset as our experimental setting, Weighted L1 loss: R\u03c6(\u03b8) = i \u03c6i|\u03b8i|, (2) Weighted L2 loss: R\u03c6(\u03b8) = i \u03c6i\u03b8i2, and (3) Two layer neural network: R\u03c6(\u03b8) = \u03c6(2)T (ReLU (\u03c6(1)T \u03b8)).",
        "We propose the following solution: Train the feature network, task network and regularizer on the initial dataset.",
        "The task of finding the desired regularizer that captures the notion of domain generalization is modeled as a meta-learning problem.",
        "Some avenues for future work include scalable meta-learning approaches for learning regularization functions over convolutional layers while preserving the spatial dependency between the channels, and extending our approach to deep reinforcement learning problems."
    ],
    "headline": "The primary contribution of this work is that we propose a scheme for learning regularization functions that enable domain generalization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Marcin Andrychowicz, Misha Denil, Sergio G\u00f3mez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3981\u20133989. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20G%C3%B3mez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20G%C3%B3mez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "2",
            "entry": "[2] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201906, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blitzer%2C%20John%20McDonald%2C%20Ryan%20Pereira%2C%20Fernando%20Domain%20adaptation%20with%20structural%20correspondence%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blitzer%2C%20John%20McDonald%2C%20Ryan%20Pereira%2C%20Fernando%20Domain%20adaptation%20with%20structural%20correspondence%20learning%202006"
        },
        {
            "id": "3",
            "entry": "[3] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 343\u2013351. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Konstantinos%20Bousmalis%20George%20Trigeorgis%20Nathan%20Silberman%20Dilip%20Krishnan%20and%20Dumitru%20Erhan%20Domain%20separation%20networks%20In%20D%20D%20Lee%20M%20Sugiyama%20U%20V%20Luxburg%20I%20Guyon%20and%20R%20Garnett%20editors%20Advances%20in%20Neural%20Information%20Processing%20Systems%2029%20pages%20343351%20Curran%20Associates%20Inc%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Konstantinos%20Bousmalis%20George%20Trigeorgis%20Nathan%20Silberman%20Dilip%20Krishnan%20and%20Dumitru%20Erhan%20Domain%20separation%20networks%20In%20D%20D%20Lee%20M%20Sugiyama%20U%20V%20Luxburg%20I%20Guyon%20and%20R%20Garnett%20editors%20Advances%20in%20Neural%20Information%20Processing%20Systems%2029%20pages%20343351%20Curran%20Associates%20Inc%202016"
        },
        {
            "id": "4",
            "entry": "[4] Minmin Chen, Kilian Q Weinberger, and John Blitzer. Co-training for domain adaptation. In Advances in Neural Information Processing Systems 24. Curran Associates, Inc., 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Minmin%20Weinberger%2C%20Kilian%20Q.%20Blitzer%2C%20John%20Co-training%20for%20domain%20adaptation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Minmin%20Weinberger%2C%20Kilian%20Q.%20Blitzer%2C%20John%20Co-training%20for%20domain%20adaptation%202011"
        },
        {
            "id": "5",
            "entry": "[5] Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Weinberger, and Fei Sha. Marginalized denoising autoencoders for domain adaptation. In ICML. icml.cc / Omnipress, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Minmin%20Xu%2C%20Zhixiang%20Eddie%20Weinberger%2C%20Kilian%20Q.%20Sha%2C%20Fei%20Marginalized%20denoising%20autoencoders%20for%20domain%20adaptation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Minmin%20Xu%2C%20Zhixiang%20Eddie%20Weinberger%2C%20Kilian%20Q.%20Sha%2C%20Fei%20Marginalized%20denoising%20autoencoders%20for%20domain%20adaptation%202012"
        },
        {
            "id": "6",
            "entry": "[6] Hal Daume III. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, June 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daume%2C%20III%2C%20Hal%20Frustratingly%20easy%20domain%20adaptation%202007-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daume%2C%20III%2C%20Hal%20Frustratingly%20easy%20domain%20adaptation%202007-06"
        },
        {
            "id": "7",
            "entry": "[7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1126\u20131135, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks%202017-08"
        },
        {
            "id": "8",
            "entry": "[8] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. Proceedings of Machine Learning Research, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Zhang%2C%20Tianhao%20Abbeel%2C%20Pieter%20One-shot%20visual%20imitation%20learning%20via%20meta-learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Finn%2C%20Chelsea%20Yu%2C%20Tianhe%20Zhang%2C%20Tianhao%20Abbeel%2C%20Pieter%20One-shot%20visual%20imitation%20learning%20via%20meta-learning%202017"
        },
        {
            "id": "9",
            "entry": "[9] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17(1):2096\u20132030, January 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Yaroslav%20Ustinova%2C%20Evgeniya%20Ajakan%2C%20Hana%20Germain%2C%20Pascal%20Domain-adversarial%20training%20of%20neural%20networks%202016-01"
        },
        {
            "id": "10",
            "entry": "[10] Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muhammad%20Ghifary%2C%20W.Bastiaan%20Kleijn%20Zhang%2C%20Mengjie%20Balduzzi%2C%20David%20Domain%20generalization%20for%20object%20recognition%20with%20multi-task%20autoencoders%202015-12-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muhammad%20Ghifary%2C%20W.Bastiaan%20Kleijn%20Zhang%2C%20Mengjie%20Balduzzi%2C%20David%20Domain%20generalization%20for%20object%20recognition%20with%20multi-task%20autoencoders%202015-12-07"
        },
        {
            "id": "11",
            "entry": "[11] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2066\u20132073, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gong%2C%20Boqing%20Shi%2C%20Yuan%20Sha%2C%20Fei%20Grauman%2C%20Kristen%20Geodesic%20flow%20kernel%20for%20unsupervised%20domain%20adaptation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gong%2C%20Boqing%20Shi%2C%20Yuan%20Sha%2C%20Fei%20Grauman%2C%20Kristen%20Geodesic%20flow%20kernel%20for%20unsupervised%20domain%20adaptation%202012"
        },
        {
            "id": "12",
            "entry": "[12] Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Proceedings of the 2011 International Conference on Computer Vision, ICCV \u201911, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Domain%20adaptation%20for%20object%20recognition%3A%20An%20unsupervised%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gopalan%2C%20Raghuraman%20Li%2C%20Ruonan%20Chellappa%2C%20Rama%20Domain%20adaptation%20for%20object%20recognition%3A%20An%20unsupervised%20approach%202011"
        },
        {
            "id": "13",
            "entry": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.03385"
        },
        {
            "id": "14",
            "entry": "[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML\u201915, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Sergey%20Ioffe%20and%20Christian%20Szegedy.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Sergey%20Ioffe%20and%20Christian%20Szegedy.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "15",
            "entry": "[15] Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba. Undoing the damage of dataset bias. In Proceedings of the 12th European Conference on Computer Vision - Volume Part I, ECCV\u201912, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khosla%2C%20Aditya%20Zhou%2C%20Tinghui%20Malisiewicz%2C%20Tomasz%20Efros%2C%20Alexei%20A.%20Undoing%20the%20damage%20of%20dataset%20bias%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khosla%2C%20Aditya%20Zhou%2C%20Tinghui%20Malisiewicz%2C%20Tomasz%20Efros%2C%20Alexei%20A.%20Undoing%20the%20damage%20of%20dataset%20bias%202012"
        },
        {
            "id": "16",
            "entry": "[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1097\u20131105. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "17",
            "entry": "[17] Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In Advances in Neural Information Processing Systems, pages 950\u2013957, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krogh%2C%20Anders%20Hertz%2C%20John%20A.%20A%20simple%20weight%20decay%20can%20improve%20generalization%201992"
        },
        {
            "id": "18",
            "entry": "[18] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 5543\u20135551, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Da%20Yang%2C%20Yongxin%20Song%2C%20Yi-Zhe%20Hospedales%2C%20Timothy%20M.%20Deeper%2C%20broader%20and%20artier%20domain%20generalization%202017-10-22",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Da%20Yang%2C%20Yongxin%20Song%2C%20Yi-Zhe%20Hospedales%2C%20Timothy%20M.%20Deeper%2C%20broader%20and%20artier%20domain%20generalization%202017-10-22"
        },
        {
            "id": "19",
            "entry": "[19] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for domain generalization. CoRR, abs/1710.03463, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03463"
        },
        {
            "id": "20",
            "entry": "[20] Ke Li and Jitendra Malik. Learning to optimize neural nets. CoRR, abs/1703.00441, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00441"
        },
        {
            "id": "21",
            "entry": "[21] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, pages 97\u2013105, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Mingsheng%20Cao%2C%20Yue%20Wang%2C%20Jianmin%20Jordan%2C%20Michael%20I.%20Learning%20transferable%20features%20with%20deep%20adaptation%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Mingsheng%20Cao%2C%20Yue%20Wang%2C%20Jianmin%20Jordan%2C%20Michael%20I.%20Learning%20transferable%20features%20with%20deep%20adaptation%20networks%202015"
        },
        {
            "id": "22",
            "entry": "[22] K. Muandet, D. Balduzzi, and B. Sch\u00f6lkopf. Domain generalization via invariant feature representation. In Proceedings of the 30th International Conference on Machine Learning, W&CP 28(1), pages 10\u201318. JMLR, 2013. Volume 28, number 1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muandet%2C%20K.%20Balduzzi%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Domain%20generalization%20via%20invariant%20feature%20representation%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muandet%2C%20K.%20Balduzzi%2C%20D.%20Sch%C3%B6lkopf%2C%20B.%20Domain%20generalization%20via%20invariant%20feature%20representation%202013"
        },
        {
            "id": "23",
            "entry": "[23] Jie Ni, Qiang Qiu, and Rama Chellappa. Subspace interpolation via dictionary learning for unsupervised domain adaptation. In 2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA, June 23-28, 2013, pages 692\u2013699, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ni%2C%20Jie%20Qiu%2C%20Qiang%20Chellappa%2C%20Rama%20Subspace%20interpolation%20via%20dictionary%20learning%20for%20unsupervised%20domain%20adaptation%202013-06-23",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ni%2C%20Jie%20Qiu%2C%20Qiang%20Chellappa%2C%20Rama%20Subspace%20interpolation%20via%20dictionary%20learning%20for%20unsupervised%20domain%20adaptation%202013-06-23"
        },
        {
            "id": "24",
            "entry": "[24] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ravi%2C%20Sachin%20Larochelle%2C%20Hugo%20Optimization%20as%20a%20model%20for%20few-shot%20learning%202017"
        },
        {
            "id": "25",
            "entry": "[25] Swami Sankaranarayanan, Yogesh Balaji, Carlos D. Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sankaranarayanan%2C%20Swami%20Balaji%2C%20Yogesh%20Castillo%2C%20Carlos%20D.%20Chellappa%2C%20Rama%20Generate%20to%20adapt%3A%20Aligning%20domains%20using%20generative%20adversarial%20networks%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sankaranarayanan%2C%20Swami%20Balaji%2C%20Yogesh%20Castillo%2C%20Carlos%20D.%20Chellappa%2C%20Rama%20Generate%20to%20adapt%3A%20Aligning%20domains%20using%20generative%20adversarial%20networks%202018-06"
        },
        {
            "id": "26",
            "entry": "[26] Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama Chellappa. Learning from synthetic data: Addressing domain shift for semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sankaranarayanan%2C%20Swami%20Balaji%2C%20Yogesh%20Jain%2C%20Arpit%20Lim%2C%20Ser%20Nam%20Learning%20from%20synthetic%20data%3A%20Addressing%20domain%20shift%20for%20semantic%20segmentation%202018-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sankaranarayanan%2C%20Swami%20Balaji%2C%20Yogesh%20Jain%2C%20Arpit%20Lim%2C%20Ser%20Nam%20Learning%20from%20synthetic%20data%3A%20Addressing%20domain%20shift%20for%20semantic%20segmentation%202018-06"
        },
        {
            "id": "27",
            "entry": "[27] J\u00fcrgen Schmidhuber. On learning how to learn learning strategies. Technical report, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidhuber%2C%20J%C3%BCrgen%20On%20learning%20how%20to%20learn%20learning%20strategies%201995"
        },
        {
            "id": "28",
            "entry": "[28] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1), January 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014-01"
        },
        {
            "id": "29",
            "entry": "[29] Damien Teney and Anton van den Hengel. Visual question answering as a meta learning task. CoRR, abs/1711.08105, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.08105"
        },
        {
            "id": "30",
            "entry": "[30] Sebastian Thrun and Lorien Pratt, editors. Learning to Learn. Kluwer Academic Publishers, Norwell, MA, USA, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Sebastian%20Thrun%20and%20Lorien%20Pratt%2C%20editors.%20Learning%20to%20Learn%201998"
        },
        {
            "id": "31",
            "entry": "[31] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning, pages 1058\u20131066, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wan%2C%20Li%20Zeiler%2C%20Matthew%20Zhang%2C%20Sixin%20Cun%2C%20Yann%20Le%20Regularization%20of%20neural%20networks%20using%20dropconnect%202013"
        },
        {
            "id": "32",
            "entry": "[32] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chiyuan%20Bengio%2C%20Samy%20Hardt%2C%20Moritz%20Recht%2C%20Benjamin%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "33",
            "entry": "[33] Yang Zhang, Philip David, and Boqing Gong. Curriculum domain adaptation for semantic segmentation of urban scenes. In The IEEE International Conference on Computer Vision (ICCV), volume 2, page 6, Oct 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yang%20David%2C%20Philip%20Gong%2C%20Boqing%20Curriculum%20domain%20adaptation%20for%20semantic%20segmentation%20of%20urban%20scenes%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yang%20David%2C%20Philip%20Gong%2C%20Boqing%20Curriculum%20domain%20adaptation%20for%20semantic%20segmentation%20of%20urban%20scenes%202017-10"
        },
        {
            "id": "34",
            "entry": "[34] Fengwei Zhou, Bin Wu, and Zhenguo Li. Deep meta-learning: Learning to learn in the concept space. CoRR, abs/1802.03596, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1802.03596"
        }
    ]
}
