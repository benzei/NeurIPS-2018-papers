{
    "filename": "7767-causal-inference-and-mechanism-clustering-of-a-mixture-of-additive-noise-models.pdf",
    "metadata": {
        "title": "Causal Inference and Mechanism Clustering of A Mixture of Additive Noise Models",
        "author": "Shoubo Hu, Zhitang Chen, Vahid Partovi Nia, Laiwan CHAN, Yanhui Geng",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7767-causal-inference-and-mechanism-clustering-of-a-mixture-of-additive-noise-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The inference of the causal relationship between a pair of observed variables is a fundamental problem in science, and most existing approaches are based on one single causal model. In practice, however, observations are often collected from multiple sources with heterogeneous causal models due to certain uncontrollable factors, which renders causal analysis results obtained by a single model skeptical. In this paper, we generalize the Additive Noise Model (ANM) to a mixture model, which consists of a finite number of ANMs, and provide the condition of its causal identifiability. To conduct model estimation, we propose Gaussian Process Partially Observable Model (GPPOM), and incorporate independence enforcement into it to learn latent parameter associated with each observation. Causal inference and clustering according to the underlying generating mechanisms of the mixture model are addressed in this work. Experiments on synthetic and real data demonstrate the effectiveness of our proposed approach."
    },
    "keywords": [
        {
            "term": "finite number",
            "url": "https://en.wikipedia.org/wiki/finite_number"
        },
        {
            "term": "causal inference",
            "url": "https://en.wikipedia.org/wiki/causal_inference"
        },
        {
            "term": "spectral clustering",
            "url": "https://en.wikipedia.org/wiki/spectral_clustering"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        },
        {
            "term": "causal model",
            "url": "https://en.wikipedia.org/wiki/causal_model"
        }
    ],
    "highlights": [
        "Understanding the data-generating mechanism (g.m.) has been a main theme of causal inference",
        "Similar to additive noise model, most causal inference approaches based on functional models, such as LiNGAM [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], PNL [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], and IGCI [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], assume a single causal model for all observations",
        "We focus on analyzing observations generated by a mixture of additive noise model of two r.v.s and try to answer two questions: 1) causal inference: how can we infer the causal direction between the two r.v.s? 2) mechanism clustering: how can we cluster the observations generated from the same g.m. together? To answer these questions, first as the main result of this paper, we show that the causal direction of the mixture of additive noise model is identifiable in most cases, and we propose a variant of GP-LVM [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] named Gaussian Process Partially Observable Model (GPPOM) for model estimation, based on which we further develop the algorithms for causal inference and mechanism clustering",
        "We propose Gaussian process partially observable model (GPPOM) and incorporate Hilbert-Schmidt independence criterion (HSIC) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] enforcement into Gaussian process partially observable model to estimate the model parameter \u03b8",
        "Like GP-LVM, Xis mapped to Y by the same set of Gaussian processes in Gaussian process partially observable model so the differences in the g.m.s is captured by \u03b8n, the latent representations associated with each observation.\n3.3",
        "We extend the additive noise model to a more general model (ANM-MM) in which there are a finite number of additive noise model of the same function form and differ only in parameter values"
    ],
    "key_statements": [
        "Understanding the data-generating mechanism (g.m.) has been a main theme of causal inference",
        "Similar to additive noise model, most causal inference approaches based on functional models, such as LiNGAM [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], PNL [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], and IGCI [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], assume a single causal model for all observations",
        "We focus on analyzing observations generated by a mixture of additive noise model of two r.v.s and try to answer two questions: 1) causal inference: how can we infer the causal direction between the two r.v.s? 2) mechanism clustering: how can we cluster the observations generated from the same g.m. together? To answer these questions, first as the main result of this paper, we show that the causal direction of the mixture of additive noise model is identifiable in most cases, and we propose a variant of GP-LVM [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] named Gaussian Process Partially Observable Model (GPPOM) for model estimation, based on which we further develop the algorithms for causal inference and mechanism clustering",
        "We propose Gaussian process partially observable model (GPPOM) and incorporate Hilbert-Schmidt independence criterion (HSIC) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] enforcement into Gaussian process partially observable model to estimate the model parameter \u03b8",
        "Like GP-LVM, Xis mapped to Y by the same set of Gaussian processes in Gaussian process partially observable model so the differences in the g.m.s is captured by \u03b8n, the latent representations associated with each observation.\n3.3",
        "Approaches based on a single causal model are sensitive to the change in a1 whereas ECP and additive noise model-MM are more robust and outperform others",
        "We evaluate the causal inference performance of additive noise model-MM on real world benchmark cause-effect pairs3 [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>]",
        "We extend the additive noise model to a more general model (ANM-MM) in which there are a finite number of additive noise model of the same function form and differ only in parameter values"
    ],
    "summary": [
        "Understanding the data-generating mechanism (g.m.) has been a main theme of causal inference.",
        "Similar to ANM, most causal inference approaches based on functional models, such as LiNGAM [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], PNL [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], and IGCI [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], assume a single causal model for all observations.",
        "An approach was proposed for inferring the causal direction of mixtures of ANMs with discrete variables [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>].",
        "First as the main result of this paper, we show that the causal direction of the mixture of ANMs is identifiable in most cases, and we propose a variant of GP-LVM [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] named Gaussian Process Partially Observable Model (GPPOM) for model estimation, based on which we further develop the algorithms for causal inference and mechanism clustering.",
        "The differences between causal models in an ANM-MM stem only from different values of function parameter \u03b8.",
        "In ANM-MM, all observations are generated by a set of g.m.s, which share the same function form (f ) but differ in parameter values (\u03b8).",
        "If there exists a backward ANM in the anti-causal direction, i.e. X = g(Y ) + \u0303, the cause distribution, the noise distribution (p ), the nonlinear function (f ) and its parameter distribution should jointly fulfill the following ordinary differential equation (ODE)",
        "The causal direction in ANM-MM can be inferred by investigating the independence between the hypothetical cause and the corresponding function parameter.",
        "We summarize algorithms for causal inference and mechanism clustering of ANM-MM.",
        "LVM, the dual PPCA with observable X and input :D = {}nN=1 - the set of latent \u03b8 can be generalized to nonlinobservations of two r.v.s; ear cases.",
        "Like GP-LVM, Xis mapped to Y by the same set of Gaussian processes in GPPOM so the differences in the g.m.s is captured by \u03b8n, the latent representations associated with each observation.",
        "Approaches based on a single causal model are sensitive to the change in a1 whereas ECP and ANM-MM are more robust and outperform others.",
        "We evaluate the causal inference performance of ANM-MM on real world benchmark cause-effect pairs3 [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>].",
        "Different inference approaches are applied on 90 points randomly sampled from raw data without replacement.",
        "We extend the ANM to a more general model (ANM-MM) in which there are a finite number of ANMs of the same function form and differ only in parameter values.",
        "To estimate ANM-MM, we adopt the GP-LVM framework and propose a variant of it called GPPOM to find the optimized latent representations and further conduct causal inference and mechanism clustering.",
        "Results on both synthetic and real world data verify the effectiveness of our proposed approach"
    ],
    "headline": "We propose Gaussian Process Partially Observable Model , and incorporate independence enforcement into it to learn latent parameter associated with each observation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Daniusis, P., Janzing, D., Mooij, J., Zscheischler, J., Steudel, B., Zhang, K., and Sch\u00f6lkopf, B. (2012). Inferring deterministic causal relations. arXiv preprint arXiv:1203.3475.",
            "arxiv_url": "https://arxiv.org/pdf/1203.3475"
        },
        {
            "id": "2",
            "entry": "[2] Ester, M., Kriegel, H.-P., Sander, J., and Xu, X. (1996). A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD\u201996, pages 226\u2013231. AAAI Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ester%2C%20M.%20Kriegel%2C%20H.-P.%20Sander%2C%20J.%20Xu%2C%20X.%20A%20density-based%20algorithm%20for%20discovering%20clusters%20a%20density-based%20algorithm%20for%20discovering%20clusters%20in%20large%20spatial%20databases%20with%20noise%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ester%2C%20M.%20Kriegel%2C%20H.-P.%20Sander%2C%20J.%20Xu%2C%20X.%20A%20density-based%20algorithm%20for%20discovering%20clusters%20a%20density-based%20algorithm%20for%20discovering%20clusters%20in%20large%20spatial%20databases%20with%20noise%201996"
        },
        {
            "id": "3",
            "entry": "[3] Fukumizu, K., Bach, F. R., and Jordan, M. I. (2004). Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):73\u2013 99.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fukumizu%2C%20K.%20Bach%2C%20F.R.%20Jordan%2C%20M.I.%20Dimensionality%20reduction%20for%20supervised%20learning%20with%20reproducing%20kernel%20hilbert%20spaces%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fukumizu%2C%20K.%20Bach%2C%20F.R.%20Jordan%2C%20M.I.%20Dimensionality%20reduction%20for%20supervised%20learning%20with%20reproducing%20kernel%20hilbert%20spaces%202004"
        },
        {
            "id": "4",
            "entry": "[4] Gretton, A., Bousquet, O., Smola, A., and Sch\u00f6lkopf, B. (2005a). Measuring statistical dependence with hilbert-schmidt norms. In International conference on algorithmic learning theory, pages 63\u201377. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Measuring%20statistical%20dependence%20with%20hilbert-schmidt%20norms%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Measuring%20statistical%20dependence%20with%20hilbert-schmidt%20norms%202005"
        },
        {
            "id": "5",
            "entry": "[5] Gretton, A., Smola, A. J., Bousquet, O., Herbrich, R., Belitski, A., Augath, M., Murayama, Y., Pauls, J., Sch\u00f6lkopf, B., and Logothetis, N. K. (2005b). Kernel constrained covariance for dependence measurement. In AISTATS, volume 10, pages 112\u2013119.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gretton%2C%20A.%20Smola%2C%20A.J.%20Bousquet%2C%20O.%20Herbrich%2C%20R.%20Kernel%20constrained%20covariance%20for%20dependence%20measurement%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gretton%2C%20A.%20Smola%2C%20A.J.%20Bousquet%2C%20O.%20Herbrich%2C%20R.%20Kernel%20constrained%20covariance%20for%20dependence%20measurement%202005"
        },
        {
            "id": "6",
            "entry": "[6] Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., and Sch\u00f6lkopf, B. (2009). Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pages 689\u2013696.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nonlinear%20causal%20discovery%20with%20additive%20noise%20models%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nonlinear%20causal%20discovery%20with%20additive%20noise%20models%202009"
        },
        {
            "id": "7",
            "entry": "[7] Hubert, L. and Arabie, P. (1985). Comparing partitions. Journal of classification, 2(1):193\u2013218.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hubert%2C%20L.%20Arabie%2C%20P.%20Comparing%20partitions%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hubert%2C%20L.%20Arabie%2C%20P.%20Comparing%20partitions%201985"
        },
        {
            "id": "8",
            "entry": "[8] Janzing, D., Mooij, J., Zhang, K., Lemeire, J., Zscheischler, J., Daniu\u0161is, P., Steudel, B., and Sch\u00f6lkopf, B. (2012). Information-geometric approach to inferring causal directions. Artificial Intelligence, 182:1\u201331.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Information-geometric%20approach%20to%20inferring%20causal%20directions%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Information-geometric%20approach%20to%20inferring%20causal%20directions%202012"
        },
        {
            "id": "9",
            "entry": "[9] Janzing, D. and Scholkopf, B. (2010). Causal inference using the algorithmic markov condition. IEEE Transactions on Information Theory, 56(10):5168\u20135194.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Janzing%2C%20D.%20Scholkopf%2C%20B.%20Causal%20inference%20using%20the%20algorithmic%20markov%20condition%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Janzing%2C%20D.%20Scholkopf%2C%20B.%20Causal%20inference%20using%20the%20algorithmic%20markov%20condition%202010"
        },
        {
            "id": "10",
            "entry": "[10] Lawrence, N. (2005). Probabilistic non-linear principal component analysis with gaussian process latent variable models. Journal of machine learning research, 6(Nov):1783\u20131816.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lawrence%2C%20N.%20Probabilistic%20non-linear%20principal%20component%20analysis%20with%20gaussian%20process%20latent%20variable%20models%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lawrence%2C%20N.%20Probabilistic%20non-linear%20principal%20component%20analysis%20with%20gaussian%20process%20latent%20variable%20models%202005"
        },
        {
            "id": "11",
            "entry": "[11] Lawrence, N. D. (2004). Gaussian process latent variable models for visualisation of high dimensional data. In Advances in neural information processing systems, pages 329\u2013336.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lawrence%2C%20N.D.%20Gaussian%20process%20latent%20variable%20models%20for%20visualisation%20of%20high%20dimensional%20data%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lawrence%2C%20N.D.%20Gaussian%20process%20latent%20variable%20models%20for%20visualisation%20of%20high%20dimensional%20data%202004"
        },
        {
            "id": "12",
            "entry": "[12] Liu, F. and Chan, L. (2016). Causal discovery on discrete data with extensions to mixture model. ACM Transactions on Intelligent Systems and Technology (TIST), 7(2):21.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20F.%20Chan%2C%20L.%20Causal%20discovery%20on%20discrete%20data%20with%20extensions%20to%20mixture%20model%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20F.%20Chan%2C%20L.%20Causal%20discovery%20on%20discrete%20data%20with%20extensions%20to%20mixture%20model%202016"
        },
        {
            "id": "13",
            "entry": "[13] MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics, pages 281\u2013297, Berkeley, Calif. University of California Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacQueen%2C%20J.%20Some%20methods%20for%20classification%20and%20analysis%20of%20multivariate%20observations%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacQueen%2C%20J.%20Some%20methods%20for%20classification%20and%20analysis%20of%20multivariate%20observations%201967"
        },
        {
            "id": "14",
            "entry": "[14] M\u00f8ller, M. F. (1993). A scaled conjugate gradient algorithm for fast supervised learning. Neural networks, 6(4):525\u2013533.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%B8ller%2C%20M.F.%20A%20scaled%20conjugate%20gradient%20algorithm%20for%20fast%20supervised%20learning%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%B8ller%2C%20M.F.%20A%20scaled%20conjugate%20gradient%20algorithm%20for%20fast%20supervised%20learning%201993"
        },
        {
            "id": "15",
            "entry": "[15] Mooij, J. M., Peters, J., Janzing, D., Zscheischler, J., and Sch\u00f6lkopf, B. (2016). Distinguishing cause from effect using observational data: methods and benchmarks. The Journal of Machine Learning Research, 17(1):1103\u20131204.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Distinguishing%20cause%20from%20effect%20using%20observational%20data%3A%20methods%20and%20benchmarks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Distinguishing%20cause%20from%20effect%20using%20observational%20data%3A%20methods%20and%20benchmarks%202016"
        },
        {
            "id": "16",
            "entry": "[16] Rasmussen, C. E. (2000). The infinite gaussian mixture model. In Advances in neural information processing systems, pages 554\u2013560.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20The%20infinite%20gaussian%20mixture%20model%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20C.E.%20The%20infinite%20gaussian%20mixture%20model%202000"
        },
        {
            "id": "17",
            "entry": "[17] Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888\u2013905.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20J.%20Malik%2C%20J.%20Normalized%20cuts%20and%20image%20segmentation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20J.%20Malik%2C%20J.%20Normalized%20cuts%20and%20image%20segmentation%202000"
        },
        {
            "id": "18",
            "entry": "[18] Shimizu, S., Hoyer, P. O., Hyv\u00e4rinen, A., and Kerminen, A. (2006). A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003\u20132030.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shimizu%2C%20S.%20Hoyer%2C%20P.O.%20Hyv%C3%A4rinen%2C%20A.%20Kerminen%2C%20A.%20A%20linear%20non-gaussian%20acyclic%20model%20for%20causal%20discovery%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shimizu%2C%20S.%20Hoyer%2C%20P.O.%20Hyv%C3%A4rinen%2C%20A.%20Kerminen%2C%20A.%20A%20linear%20non-gaussian%20acyclic%20model%20for%20causal%20discovery%202006"
        },
        {
            "id": "19",
            "entry": "[19] Williams, C. K. (1998). Prediction with gaussian processes: From linear regression to linear prediction and beyond. In Learning in graphical models, pages 599\u2013621. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20C.K.%20Prediction%20with%20gaussian%20processes%3A%20From%20linear%20regression%20to%20linear%20prediction%20and%20beyond.%20In%20Learning%20in%20graphical%20models%201998"
        },
        {
            "id": "20",
            "entry": "[20] Zhang, K., Huang, B., Zhang, J., Sch\u00f6lkopf, B., and Glymour, C. (2015). Discovery and visualization of nonstationary causal models. arXiv preprint arXiv:1509.08056.",
            "arxiv_url": "https://arxiv.org/pdf/1509.08056"
        },
        {
            "id": "21",
            "entry": "[21] Zhang, K. and Hyv\u00e4rinen, A. (2009). On the identifiability of the post-nonlinear causal model. In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence, pages 647\u2013655. AUAI Press. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20K.%20Hyv%C3%A4rinen%2C%20A.%20On%20the%20identifiability%20of%20the%20post-nonlinear%20causal%20model%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20K.%20Hyv%C3%A4rinen%2C%20A.%20On%20the%20identifiability%20of%20the%20post-nonlinear%20causal%20model%202009"
        }
    ]
}
