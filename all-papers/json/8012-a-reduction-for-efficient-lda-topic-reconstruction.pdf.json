{
    "filename": "8012-a-reduction-for-efficient-lda-topic-reconstruction.pdf",
    "metadata": {
        "title": "A Reduction for Efficient LDA Topic Reconstruction",
        "author": "Matteo Almanza, Flavio Chierichetti, Alessandro Panconesi, Andrea Vattani",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8012-a-reduction-for-efficient-lda-topic-reconstruction.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruction. The main technical idea is to show that the distribution over the documents generated by LDA can be transformed into a distribution for a much simpler generative model in which documents are generated from the same set of topics but have a much simpler structure: documents are single topic and topics are chosen uniformly at random. Furthermore, this reduction is approximation preserving, in the sense that approximate distributions \u2014 the only ones we can hope to compute in practice \u2014 are mapped into approximate distribution in the simplified world. This opens up the possibility of efficiently reconstructing LDA topics in a roundabout way. Compute an approximate document distribution from the given corpus, transform it into an approximate distribution for the single-topic world, and run a reconstruction algorithm in the uniform, single-topic world \u2014 a much simpler task than direct LDA reconstruction. We show the viability of the approach by giving very simple algorithms for a generalization of two notable cases that have been studied in the literature, p-separability and matrix-like topics."
    },
    "keywords": [
        {
            "term": "probability distribution",
            "url": "https://en.wikipedia.org/wiki/probability_distribution"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        }
    ],
    "highlights": [
        "Latent Dirichlet Allocation is a well-known paradigm for topic reconstruction (<a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\"><a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\">Blei et al , 2003</a></a>)",
        "LDA is a generative model according to which documents are generated from a given set of unknown topics, where each topic is modelled as a probability distribution over the words"
    ],
    "key_statements": [
        "Latent Dirichlet Allocation is a well-known paradigm for topic reconstruction (<a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\"><a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\">Blei et al , 2003</a></a>)",
        "LDA is a generative model according to which documents are generated from a given set of unknown topics, where each topic is modelled as a probability distribution over the words"
    ],
    "summary": [
        "Latent Dirichlet Allocation is a well-known paradigm for topic reconstruction (<a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\"><a class=\"ref-link\" id=\"cBlei_et+al_2003_a\" href=\"#rBlei_et+al_2003_a\">Blei et al , 2003</a></a>).",
        "LDA is a generative model according to which documents are generated from a given set of unknown topics, where each topic is modelled as a probability distribution over the words.",
        "Given a set T of K topics and a Dirichlet parameter \u03b1, let D = D be the distribution that they induce via LDA over the documents of a given length .",
        "Let S = S denote the distribution induced by STA over the documents of the same length when the same set of topics T is used.",
        "Starting from a document corpus generated by LDA from a set of hidden topics T that we wish to reconstruct, compute D, an approximation of the true document distribution D.",
        "The main result of <a class=\"ref-link\" id=\"cArora_et+al_2012_a\" href=\"#rArora_et+al_2012_a\">Arora et al (2012</a>) states that there is an algorithm such that if a set of LDA topics are p-separable they are identifiable within additive error \u03b4 in the \u221e -norm, provided that the corpus contains",
        "In this paper we give an algorithm for LDA topic reconstruction-separability) that, starting from a random LDA corpus over a vocabulary of m words consisting of",
        "In \u00a7 3 we give the reduction from LDA to STA, followed by \u00a7 4 in which a robust algorithm for STA topic reconstruction is presented with which we solve the (p, t)-separable case for t = 1, 2, which subsumes both pseparability and matrix-like topics.",
        "The lemma tells us how to compute a good approximation D of the true document distribution D induced by LDA starting from a corpus.",
        "Require: K, p > 0, \u03b4, corpus C of documents, \u03b1 parameter of the symmetric LDA mixture, 1: Let W be the set of words w whose empirical fraction in C is at least p/2K.",
        "A third family, SYNTHETIC, was generated by sampling from a uniform Dirichlet distribution with parameter \u03b2 = 1/k and, to enforce p-separability, anchor words were added.",
        "Our algorithm implements the following pipeline, C \u2212(1\u2192) L \u2212(2\u2192) S \u2212(3\u2192) T , where the first step, starting from the corpus C, computes the approximation to the distribution induced over the documents by LDA; the second step implements the reduction from the latter to the STA-induced distribution, and, lastly, the third step is Algorithm 1.",
        "The two plots on the left of Figure 1 compare the running times of the algorithms with corpora of documents of length = 2, 3, with 10 topics."
    ],
    "headline": "We present a novel approach for LDA  topic reconstruction",
    "reference_links": [
        {
            "id": "Alvarez-Melis_2016_a",
            "entry": "Alvarez-Melis, David, & Saveski, Martin. 2016. Topic Modeling in Twitter: Aggregating Tweets by Conversations. In: ICWSM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alvarez-Melis%2C%20David%20Saveski%2C%20Martin%20Topic%20Modeling%20in%20Twitter%3A%20Aggregating%20Tweets%20by%20Conversations%202016"
        },
        {
            "id": "Anandkumar_et+al_2013_a",
            "entry": "Anandkumar, Anima, Hsu, Daniel J., Janzamin, Majid, & Kakade, Sham M. 2013. When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity. Pages 1986\u20131994 of: Burges, Christopher J. C., Bottou, L\u00e9on, Ghahramani, Zoubin, & Weinberger, Kilian Q. (eds), NIPS.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Anima%20Hsu%2C%20Daniel%20J.%20Janzamin%2C%20Majid%20Kakade%2C%20Sham%20M.%20When%20are%20Overcomplete%20Topic%20Models%20Identifiable%3F%20Uniqueness%20of%20Tensor%20Tucker%20Decompositions%20with%20Structured%20Sparsity%202013"
        },
        {
            "id": "Anandkumar_et+al_2014_a",
            "entry": "Anandkumar, Animashree, Ge, Rong, Hsu, Daniel, Kakade, Sham M., & Telgarsky, Matus. 2014. Tensor Decompositions for Learning Latent Variable Models. Journal of Machine Learning Research, 15, 2773\u20132832.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20Decompositions%20for%20Learning%20Latent%20Variable%20Models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20Decompositions%20for%20Learning%20Latent%20Variable%20Models%202014"
        },
        {
            "id": "Arora_et+al_2012_a",
            "entry": "Arora, Sanjeev, Ge, Rong, & Moitra, Ankur. 2012. Learning Topic Models \u2013 Going Beyond SVD. Pages 1\u201310 of: Proceedings of the 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science. FOCS \u201912. Washington, DC, USA: IEEE Computer Society.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Moitra%2C%20Ankur%20Learning%20Topic%20Models%20%E2%80%93%20Going%20Beyond%20SVD.%20Pages%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Moitra%2C%20Ankur%20Learning%20Topic%20Models%20%E2%80%93%20Going%20Beyond%20SVD.%20Pages%202012"
        },
        {
            "id": "Arora_et+al_2013_a",
            "entry": "Arora, Sanjeev, Ge, Rong, Halpern, Yonatan, Mimno, David M., Moitra, Ankur, Sontag, David, Wu, Yichen, & Zhu, Michael. 2013. A Practical Algorithm for Topic Modeling with Provable Guarantees. Pages 280\u2013288 of: ICML (2). JMLR Workshop and Conference Proceedings, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Halpern%2C%20Yonatan%20Mimno%2C%20David%20M.%20A%20Practical%20Algorithm%20for%20Topic%20Modeling%20with%20Provable%20Guarantees.%20Pages%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Sanjeev%20Ge%2C%20Rong%20Halpern%2C%20Yonatan%20Mimno%2C%20David%20M.%20A%20Practical%20Algorithm%20for%20Topic%20Modeling%20with%20Provable%20Guarantees.%20Pages%202013"
        },
        {
            "id": "Bansal_et+al_2014_a",
            "entry": "Bansal, Trapit, Bhattacharyya, Chiranjib, & Kannan, Ravindran. 2014. A provable SVD-based algorithm for learning topics in dominant admixture corpus. Pages 1997\u20132005 of: Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bansal%20Trapit%20Bhattacharyya%20Chiranjib%20%20Kannan%20Ravindran%202014%20A%20provable%20SVDbased%20algorithm%20for%20learning%20topics%20in%20dominant%20admixture%20corpus%20Pages%2019972005%20of%20Advances%20in%20Neural%20Information%20Processing%20Systems%2027%20Annual%20Conference%20on%20Neural%20Information%20Processing%20Systems%202014%20December%20813%202014%20Montreal%20Quebec%20Canada",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bansal%20Trapit%20Bhattacharyya%20Chiranjib%20%20Kannan%20Ravindran%202014%20A%20provable%20SVDbased%20algorithm%20for%20learning%20topics%20in%20dominant%20admixture%20corpus%20Pages%2019972005%20of%20Advances%20in%20Neural%20Information%20Processing%20Systems%2027%20Annual%20Conference%20on%20Neural%20Information%20Processing%20Systems%202014%20December%20813%202014%20Montreal%20Quebec%20Canada"
        },
        {
            "id": "Blei_et+al_2003_a",
            "entry": "Blei, David M., Ng, Andrew Y., & Jordan, Michael I. 2003. Latent Dirichlet Allocation. J. Mach. Learn. Res., 3(Mar.), 993\u20131022.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20Dirichlet%20Allocation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20Dirichlet%20Allocation%202003"
        },
        {
            "id": "Chierichetti_et+al_2018_a",
            "entry": "Chierichetti, Flavio, Panconesi, Alessandro, & Vattani, Andrea. 2018. The equivalence of SingleTopic and LDA topic reconstruction. Zenodo, 10.5281/zenodo.1470295.",
            "crossref": "https://dx.doi.org/10.5281/zenodo.1470295",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.5281/zenodo.1470295"
        },
        {
            "id": "Griffiths_2004_a",
            "entry": "Griffiths, T. L., & Steyvers, M. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1), 5228\u20135235.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griffiths%2C%20T.L.%20Steyvers%2C%20M.%20Finding%20scientific%20topics%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Griffiths%2C%20T.L.%20Steyvers%2C%20M.%20Finding%20scientific%20topics%202004"
        },
        {
            "id": "Hajjem_2017_a",
            "entry": "Hajjem, Malek, & Latiri, Chiraz. 2017. Combining IR and LDA Topic Modeling for Filtering Microblogs. Pages 761\u2013770 of: Zanni-Merk, Cecilia, Frydman, Claudia S., Toro, Carlos, Hicks, Yulia, Howlett, Robert J., & Jain, Lakhmi C. (eds), KES. Procedia Computer Science, vol.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hajjem%2C%20Malek%20Latiri%2C%20Chiraz%20Combining%20IR%20and%20LDA%20Topic%20Modeling%20for%20Filtering%20Microblogs.%20Pages%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hajjem%2C%20Malek%20Latiri%2C%20Chiraz%20Combining%20IR%20and%20LDA%20Topic%20Modeling%20for%20Filtering%20Microblogs.%20Pages%202017"
        },
        {
            "id": "Hong_2010_a",
            "entry": "Hong, Liangjie, & Davison, Brian D. 2010. Empirical Study of Topic Modeling in Twitter. Pages 80\u201388 of: Proceedings of the First Workshop on Social Media Analytics. SOMA \u201910. New York, NY, USA: ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hong%2C%20Liangjie%20Davison%2C%20Brian%20D.%20Empirical%20Study%20of%20Topic%20Modeling%20in%20Twitter.%20Pages%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hong%2C%20Liangjie%20Davison%2C%20Brian%20D.%20Empirical%20Study%20of%20Topic%20Modeling%20in%20Twitter.%20Pages%202010"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "Li, Chenliang, Wang, Haoran, Zhang, Zhiqian, Sun, Aixin, & Ma, Zongyang. 2016. Topic Modeling for Short Texts with Auxiliary Word Embeddings. Pages 165\u2013174 of: Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR \u201916. New York, NY, USA: ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Chenliang%20Wang%2C%20Haoran%20Zhang%2C%20Zhiqian%20Sun%2C%20Aixin%20Topic%20Modeling%20for%20Short%20Texts%20with%20Auxiliary%20Word%20Embeddings.%20Pages%202016"
        },
        {
            "id": "Mccallum_2002_a",
            "entry": "McCallum, A.K. 2002. A machine learning for language toolkit.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCallum%2C%20A.K.%20A%20machine%20learning%20for%20language%20toolkit%202002"
        },
        {
            "id": "Newman_2008_a",
            "entry": "Newman, David. 2008. NIPS Dataset.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newman%20David%202008%20NIPS%20Dataset"
        },
        {
            "id": "Nigam_et+al_2000_a",
            "entry": "Nigam, Kamal, McCallum, Andrew, Thrun, Sebastian, & Mitchell, Tom M. 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning, 39, 103\u2013134.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nigam%2C%20Kamal%20McCallum%2C%20Andrew%20Thrun%2C%20Sebastian%20Mitchell%2C%20Tom%20M.%20Text%20Classification%20from%20Labeled%20and%20Unlabeled%20Documents%20using%20EM%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nigam%2C%20Kamal%20McCallum%2C%20Andrew%20Thrun%2C%20Sebastian%20Mitchell%2C%20Tom%20M.%20Text%20Classification%20from%20Labeled%20and%20Unlabeled%20Documents%20using%20EM%202000"
        },
        {
            "id": "Sridhar_2015_a",
            "entry": "Sridhar, Vivek Kumar Rangarajan. 2015. Unsupervised Topic Modeling for Short Texts Using Distributed Representations of Words. Pages 192\u2013200 of: Blunsom, Phil, Cohen, Shay B., Dhillon, Paramveer S., & Liang, Percy (eds), VS@HLT-NAACL. The Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sridhar%2C%20Vivek%20Kumar%20Rangarajan%20Unsupervised%20Topic%20Modeling%20for%20Short%20Texts%20Using%20Distributed%20Representations%20of%20Words.%20Pages%20192%E2%80%93200%20of%3A%20Blunsom%202015"
        },
        {
            "id": "Weng_et+al_2010_a",
            "entry": "Weng, Jianshu, Lim, Ee-Peng, Jiang, Jing, & He, Qi. 2010. TwitterRank: Finding Topic-sensitive Influential Twitterers. Pages 261\u2013270 of: Proceedings of the Third ACM International Conference on Web Search and Data Mining. WSDM \u201910. New York, NY, USA: ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weng%2C%20Jianshu%20Lim%2C%20Ee-Peng%20Jiang%2C%20Jing%20He%2C%20Qi%20TwitterRank%3A%20Finding%20Topic-sensitive%20Influential%20Twitterers.%20Pages%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weng%2C%20Jianshu%20Lim%2C%20Ee-Peng%20Jiang%2C%20Jing%20He%2C%20Qi%20TwitterRank%3A%20Finding%20Topic-sensitive%20Influential%20Twitterers.%20Pages%202010"
        },
        {
            "id": "Yan_et+al_2013_a",
            "entry": "Yan, Xiaohui, Guo, Jiafeng, Lan, Yanyan, & Cheng, Xueqi. 2013. A biterm topic model for short texts. In: WWW.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20Xiaohui%20Guo%2C%20Jiafeng%20Lan%2C%20Yanyan%20Cheng%2C%20Xueqi%20A%20biterm%20topic%20model%20for%20short%20texts%202013"
        },
        {
            "id": "Zhao_et+al_2011_a",
            "entry": "Zhao, Wayne Xin, Jiang, Jing, Weng, Jianshu, He, Jing, Lim, Ee-Peng, Yan, Hongfei, & Li, Xiaoming. 2011. Comparing Twitter and Traditional Media Using Topic Models. Pages 338\u2013349 of: Proceedings of the 33rd European Conference on Advances in Information Retrieval. ECIR\u201911. Berlin, Heidelberg: Springer-Verlag.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhao%2C%20Wayne%20Xin%20Jiang%2C%20Jing%20Weng%2C%20Jianshu%20He%2C%20Jing%20Comparing%20Twitter%20and%20Traditional%20Media%20Using%20Topic%20Models.%20Pages%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhao%2C%20Wayne%20Xin%20Jiang%2C%20Jing%20Weng%2C%20Jianshu%20He%2C%20Jing%20Comparing%20Twitter%20and%20Traditional%20Media%20Using%20Topic%20Models.%20Pages%202011"
        }
    ]
}
