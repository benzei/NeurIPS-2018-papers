{
    "filename": "8265-benefits-of-over-parameterization-with-em.pdf",
    "metadata": {
        "title": "Benefits of over-parameterization with EM",
        "author": "Ji Xu, Daniel J. Hsu, Arian Maleki",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8265-benefits-of-over-parameterization-with-em.pdf"
        },
        "abstract": "Expectation Maximization (EM) is among the most popular algorithms for maximum likelihood estimation, but it is generally only guaranteed to find its stationary points of the log-likelihood objective. The goal of this article is to present theoretical and empirical evidence that over-parameterization can help EM avoid spurious local optima in the log-likelihood. We consider the problem of estimating the mean vectors of a Gaussian mixture model in a scenario where the mixing weights are known. Our study shows that the global behavior of EM, when one uses an over-parameterized model in which the mixing weights are treated as unknown, is better than that when one uses the (correct) model with the mixing weights fixed to the known values. For symmetric Gaussians mixtures with two components, we prove that introducing the (statistically redundant) weight parameters enables EM to find the global maximizer of the log-likelihood starting from almost any initial mean parameters, whereas EM without this over-parameterization may very often fail. For other Gaussian mixtures, we provide empirical evidence that shows similar behavior. Our results corroborate the value of over-parameterization in solving non-convex optimization problems, previously observed in other domains."
    },
    "keywords": [
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "empirical evidence",
            "url": "https://en.wikipedia.org/wiki/empirical_evidence"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "expectation maximization",
            "url": "https://en.wikipedia.org/wiki/expectation_maximization"
        },
        {
            "term": "stationary point",
            "url": "https://en.wikipedia.org/wiki/stationary_point"
        },
        {
            "term": "maximum likelihood estimate",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood_estimate"
        },
        {
            "term": "matrix completion",
            "url": "https://en.wikipedia.org/wiki/matrix_completion"
        },
        {
            "term": "em algorithm",
            "url": "https://en.wikipedia.org/wiki/em_algorithm"
        },
        {
            "term": "gaussian mixture model",
            "url": "https://en.wikipedia.org/wiki/gaussian_mixture_model"
        },
        {
            "term": "local maxima",
            "url": "https://en.wikipedia.org/wiki/local_maxima"
        }
    ],
    "highlights": [
        "Model 2 converges to the global maximizer of the log-likelihood objective with almost any initialization of the mean parameters, while Expectation Maximization for Model 1 will fail to do so for many choices of w\u21e4, w\u21e4",
        "Parameteris ofi theiGMM are often estimated using the Expectation Maximization (EM) algorithm, which aims to find the maximizer of the log-likelihood objective",
        "We present an empirical study to show that for more general mixtures of Gaussians, with a variety of model parameters and sample sizes, Expectation Maximization for Model 2 has higher probability to find the maximum likelihood estimate than Model 1 under random initializations",
        "We study idealized executions of the Expectation Maximization algorithms in the large sample limit, where the algorithms are modified to be computed over an infinitely large i.i.d. sample drawn from the mixture distribution in (2)",
        "When we compare the landscapes of the log-likelihood objective for Population Expectation Maximization and Population Expectation Maximization , we find that overparameterization eliminates spurious local maxima that were obstacles for Population Expectation Maximization",
        "Our goal is to analyze the effect of the sample size, mixing weights, and the number of mixture components on the success of the two Expectation Maximization algorithms described in Section 2.1"
    ],
    "key_statements": [
        "Model 2 converges to the global maximizer of the log-likelihood objective with almost any initialization of the mean parameters, while Expectation Maximization for Model 1 will fail to do so for many choices of w\u21e4, w\u21e4",
        "Parameteris ofi theiGMM are often estimated using the Expectation Maximization (EM) algorithm, which aims to find the maximizer of the log-likelihood objective",
        "We present an empirical study to show that for more general mixtures of Gaussians, with a variety of model parameters and sample sizes, Expectation Maximization for Model 2 has higher probability to find the maximum likelihood estimate than Model 1 under random initializations",
        "For Gaussian mixture model, <a class=\"ref-link\" id=\"cXu_et+al_2016_a\" href=\"#rXu_et+al_2016_a\">Xu et al [2016</a>] and [<a class=\"ref-link\" id=\"cDaskalakis_et+al_2017_a\" href=\"#rDaskalakis_et+al_2017_a\">Daskalakis et al, 2017</a>] study mixtures of two Gaussians with equal weights and show that the log-likelihood objective has only two global maxima and one saddle point; and if Expectation Maximization is randomly initialized in a natural way, the probability that Expectation Maximization converges to this saddle point is zero. (Our Theorem 2 generalizes these results.) It is known that for mixtures of three or more Gaussians, global convergence is not generally possible [<a class=\"ref-link\" id=\"cJin_et+al_2016_a\" href=\"#rJin_et+al_2016_a\">Jin et al, 2016</a>]",
        "We study idealized executions of the Expectation Maximization algorithms in the large sample limit, where the algorithms are modified to be computed over an infinitely large i.i.d. sample drawn from the mixture distribution in (2)",
        "When we compare the landscapes of the log-likelihood objective for Population Expectation Maximization and Population Expectation Maximization , we find that overparameterization eliminates spurious local maxima that were obstacles for Population Expectation Maximization",
        "Our goal is to analyze the effect of the sample size, mixing weights, and the number of mixture components on the success of the two Expectation Maximization algorithms described in Section 2.1"
    ],
    "summary": [
        "Model 2 converges to the global maximizer of the log-likelihood objective with almost any initialization of the mean parameters, while EM for Model 1 will fail to do so for many choices of w\u21e4, w\u21e4 .",
        "3. We present an empirical study to show that for more general mixtures of Gaussians, with a variety of model parameters and sample sizes, EM for Model 2 has higher probability to find the MLE than Model 1 under random initializations.",
        "For GMMs, Xu et al [2016] and [<a class=\"ref-link\" id=\"cDaskalakis_et+al_2017_a\" href=\"#rDaskalakis_et+al_2017_a\">Daskalakis et al, 2017</a>] study mixtures of two Gaussians with equal weights and show that the log-likelihood objective has only two global maxima and one saddle point; and if EM is randomly initialized in a natural way, the probability that EM converges to this saddle point is zero.",
        "This is the EM algorithm for a different Gaussian mixture model in which the weights w\u21e4 and w\u21e4 are not fixed, and must be estimated.",
        "Our goal is to study the global convergence properties of the above two EM algorithms on data from the first model, where the mixing weights are, known.",
        "This theorem, which is proved in Appendix A, implies that if we use random initialization, Population EM may converge to the wrong fixed point with constant probability.",
        "Theorem 2 implies that if we use random initialization for \u2713h0i, with probability one, the Population EM estimates converge to the true parameters.",
        "Estimates of Population EM converge \u2713, w \u2713\u21e4, w\u21e4 , G \u2713, w \u27132\u21e4, w\u21e4 , as to the fixed points defined in (6) and of the mapping M : (7).",
        "Because of this nice property, as we will confirm in our simulations, when the mixture components are well-separated, EM for Model 2 performs well for most of the initializations, while EM for Model 1 fails in many cases.",
        "To complete the analysis of EM for the mixtures of two Gaussians, we present the following result that applies to Sample-based EM .",
        "To prove that { \u2713hti, whti } converges to the fixed point \u2713 , w , we",
        "We can prove the convergence of the iterates by showing certain geometric relationships between the curves of fixed points of the two functions.",
        "Our goal is to analyze the effect of the sample size, mixing weights, and the number of mixture components on the success of the two EM algorithms described in Section 2.1."
    ],
    "headline": "For symmetric Gaussians mixtures with two components, we prove that introducing the  weight parameters enables Expectation Maximization to find the global maximizer of the log-likelihood starting from almost any initial mean parameters, whereas Expectation Maximization without this over-parameterization may very often fail",
    "reference_links": [
        {
            "id": "Balakrishnan_et+al_2017_a",
            "entry": "Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the em algorithm: From population to sample-based analysis. Ann. Statist., 45(1):77\u2013120, 02 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balakrishnan%2C%20Sivaraman%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Statistical%20guarantees%20for%20the%20em%20algorithm%3A%20From%20population%20to%20sample-based%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balakrishnan%2C%20Sivaraman%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Statistical%20guarantees%20for%20the%20em%20algorithm%3A%20From%20population%20to%20sample-based%20analysis%202017"
        },
        {
            "id": "Cand_2009_a",
            "entry": "Emmanuel J Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20Emmanuel%20J.%20Recht%2C%20Benjamin%20Exact%20matrix%20completion%20via%20convex%20optimization%202009"
        },
        {
            "id": "Cand_2010_a",
            "entry": "Emmanuel J Cand\u00e8s and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053\u20132080, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20The%20power%20of%20convex%20relaxation%3A%20Near-optimal%20matrix%20completion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20Emmanuel%20J.%20Tao%2C%20Terence%20The%20power%20of%20convex%20relaxation%3A%20Near-optimal%20matrix%20completion%202010"
        },
        {
            "id": "Chr_2008_a",
            "entry": "St\u00e9phane Chr\u00e9tien and Alfred O Hero. On EM algorithms and their proximal generalizations. ESAIM: Probability and Statistics, 12:308\u2013326, May 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chr%C3%A9tien%2C%20St%C3%A9phane%20Hero%2C%20Alfred%20O.%20On%20EM%20algorithms%20and%20their%20proximal%20generalizations%202008-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chr%C3%A9tien%2C%20St%C3%A9phane%20Hero%2C%20Alfred%20O.%20On%20EM%20algorithms%20and%20their%20proximal%20generalizations%202008-05"
        },
        {
            "id": "Daskalakis_et+al_2017_a",
            "entry": "Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for mixtures of two gaussians. In Proceedings of the 2017 Conference on Learning Theory, pages 704\u2013710, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20Constantinos%20Tzamos%2C%20Christos%20Zampetakis%2C%20Manolis%20Ten%20steps%20of%20em%20suffice%20for%20mixtures%20of%20two%20gaussians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20Constantinos%20Tzamos%2C%20Christos%20Zampetakis%2C%20Manolis%20Ten%20steps%20of%20em%20suffice%20for%20mixtures%20of%20two%20gaussians%202017"
        },
        {
            "id": "D_et+al_2005_a",
            "entry": "Alexandre d\u2019Aspremont, Laurent E Ghaoui, Michael I Jordan, and Gert R Lanckriet. A direct formulation for sparse pca using semidefinite programming. In Advances in neural information processing systems, pages 41\u201348, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=d%E2%80%99Aspremont%2C%20Alexandre%20Ghaoui%2C%20Laurent%20E.%20Jordan%2C%20Michael%20I.%20Lanckriet%2C%20Gert%20R.%20A%20direct%20formulation%20for%20sparse%20pca%20using%20semidefinite%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=d%E2%80%99Aspremont%2C%20Alexandre%20Ghaoui%2C%20Laurent%20E.%20Jordan%2C%20Michael%20I.%20Lanckriet%2C%20Gert%20R.%20A%20direct%20formulation%20for%20sparse%20pca%20using%20semidefinite%20programming%202005"
        },
        {
            "id": "Dempster_et+al_1977_a",
            "entry": "A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statist. Soc. Ser. B, 39:1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum-likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20A.P.%20Laird%2C%20N.M.%20Rubin%2C%20D.B.%20Maximum-likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm%201977"
        },
        {
            "id": "Donoho_2006_a",
            "entry": "David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Compressed%20sensing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Compressed%20sensing%202006"
        },
        {
            "id": "Du_2018_a",
            "entry": "Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation. In Proceedings of the 35th International Conference on Machine Learning, pages 1329\u20131338, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Simon%20Lee%2C%20Jason%20On%20the%20power%20of%20over-parametrization%20in%20neural%20networks%20with%20quadratic%20activation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Simon%20Lee%2C%20Jason%20On%20the%20power%20of%20over-parametrization%20in%20neural%20networks%20with%20quadratic%20activation%202018"
        },
        {
            "id": "Haeffele_2015_a",
            "entry": "Benjamin D Haeffele and Ren\u00e9 Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.07540"
        },
        {
            "id": "Jin_et+al_2016_a",
            "entry": "Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J Wainwright, and Michael I Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences. In Advances in Neural Information Processing Systems, pages 4116\u20134124, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20Chi%20Zhang%2C%20Yuchen%20Balakrishnan%2C%20Sivaraman%20Wainwright%2C%20Martin%20J.%20Local%20maxima%20in%20the%20likelihood%20of%20gaussian%20mixture%20models%3A%20Structural%20results%20and%20algorithmic%20consequences%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20Chi%20Zhang%2C%20Yuchen%20Balakrishnan%2C%20Sivaraman%20Wainwright%2C%20Martin%20J.%20Local%20maxima%20in%20the%20likelihood%20of%20gaussian%20mixture%20models%3A%20Structural%20results%20and%20algorithmic%20consequences%202016"
        },
        {
            "id": "Klusowski_2016_a",
            "entry": "J. M. Klusowski and W. D. Brinda. Statistical Guarantees for Estimating the Centers of a Twocomponent Gaussian Mixture by EM. ArXiv e-prints, August 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Klusowski%2C%20J.M.%20Brinda%2C%20W.D.%20Statistical%20Guarantees%20for%20Estimating%20the%20Centers%20of%20a%20Twocomponent%20Gaussian%20Mixture%20by%20EM.%20ArXiv%20e-prints%202016-08"
        },
        {
            "id": "Koltchinskii_2011_a",
            "entry": "V. Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems. In \u00c9cole d0\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour XXXVIII, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koltchinskii%2C%20V.%20Oracle%20inequalities%20in%20empirical%20risk%20minimization%20and%20sparse%20recovery%20problems.%20In%20%C3%89cole%20d0%C3%A9t%C3%A9%20de%20probabilit%C3%A9s%20de%20Saint-Flour%20XXXVIII%202011"
        },
        {
            "id": "Livni_et+al_2014_a",
            "entry": "Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pages 855\u2013863, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Livni%2C%20Roi%20Shalev-Shwartz%2C%20Shai%20Shamir%2C%20Ohad%20On%20the%20computational%20efficiency%20of%20training%20neural%20networks%202014"
        },
        {
            "id": "Nguyen_2017_a",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings of the 34th International Conference on Machine Learning, pages 2603\u20132612, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20The%20loss%20surface%20of%20deep%20and%20wide%20neural%20networks%202017"
        },
        {
            "id": "Nguyen_2018_a",
            "entry": "Quynh Nguyen and Matthias Hein. Optimization landscape and expressivity of deep CNNs. In Proceedings of the 35th International Conference on Machine Learning, pages 3730\u20133739, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20CNNs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Quynh%20Hein%2C%20Matthias%20Optimization%20landscape%20and%20expressivity%20of%20deep%20CNNs%202018"
        },
        {
            "id": "Redner_1984_a",
            "entry": "R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM algorithm. SIAM Review, 26(2):195\u2013239, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Redner%2C%20R.A.%20Walker%2C%20H.F.%20Mixture%20densities%2C%20maximum%20likelihood%20and%20the%20EM%20algorithm%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Redner%2C%20R.A.%20Walker%2C%20H.F.%20Mixture%20densities%2C%20maximum%20likelihood%20and%20the%20EM%20algorithm%201984"
        },
        {
            "id": "Safran_2017_a",
            "entry": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08968"
        },
        {
            "id": "Mohammadreza_2018_a",
            "entry": "Mohammadreza Soltani and Chinmay Hegde. Towards provable learning of polynomial neural networks using low-rank matrix estimation. In International Conference on Artificial Intelligence and Statistics, pages 1417\u20131426, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Mohammadreza%20Soltani%20and%20Chinmay%20Hegde.%20Towards%20provable%20learning%20of%20polynomial%20neural%20networks%20using%20low-rank%20matrix%20estimation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Mohammadreza%20Soltani%20and%20Chinmay%20Hegde.%20Towards%20provable%20learning%20of%20polynomial%20neural%20networks%20using%20low-rank%20matrix%20estimation%202018"
        },
        {
            "id": "Tseng_2004_a",
            "entry": "Paul Tseng. An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of Operations Research, 29(1):27\u201344, Feb 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tseng%2C%20Paul%20An%20analysis%20of%20the%20EM%20algorithm%20and%20entropy-like%20proximal%20point%20methods%202004-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tseng%2C%20Paul%20An%20analysis%20of%20the%20EM%20algorithm%20and%20entropy-like%20proximal%20point%20methods%202004-02"
        },
        {
            "id": "Wu_1983_a",
            "entry": "C. F. Jeff Wu. On the convergence properties of the EM algorithm. The Annals of Statistics, 11(1): 95\u2013103, Mar 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20C.F.Jeff%20On%20the%20convergence%20properties%20of%20the%20EM%20algorithm%201983-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20C.F.Jeff%20On%20the%20convergence%20properties%20of%20the%20EM%20algorithm%201983-03"
        },
        {
            "id": "Xu_et+al_2016_a",
            "entry": "Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. In Advances in Neural Information Processing Systems, pages 2676\u20132684, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Ji%20Hsu%2C%20Daniel%20J.%20Maleki%2C%20Arian%20Global%20analysis%20of%20expectation%20maximization%20for%20mixtures%20of%20two%20gaussians%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Ji%20Hsu%2C%20Daniel%20J.%20Maleki%2C%20Arian%20Global%20analysis%20of%20expectation%20maximization%20for%20mixtures%20of%20two%20gaussians%202016"
        },
        {
            "id": "Xu_1996_a",
            "entry": "L. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures. Neural Computation, 8:129\u2013151, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20L.%20Jordan%2C%20M.I.%20On%20convergence%20properties%20of%20the%20EM%20algorithm%20for%20Gaussian%20mixtures%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20L.%20Jordan%2C%20M.I.%20On%20convergence%20properties%20of%20the%20EM%20algorithm%20for%20Gaussian%20mixtures%201996"
        },
        {
            "id": "Yan_et+al_2017_a",
            "entry": "Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence of gradient em on multi-component mixture of gaussians. In Advances in Neural Information Processing Systems, pages 6956\u20136966, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20Bowei%20Yin%2C%20Mingzhang%20Sarkar%2C%20Purnamrita%20Convergence%20of%20gradient%20em%20on%20multi-component%20mixture%20of%20gaussians%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20Bowei%20Yin%2C%20Mingzhang%20Sarkar%2C%20Purnamrita%20Convergence%20of%20gradient%20em%20on%20multi-component%20mixture%20of%20gaussians%202017"
        }
    ]
}
