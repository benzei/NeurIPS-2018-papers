{
    "filename": "7806-learning-others-intentional-models-in-multi-agent-settings-using-interactive-pomdps.pdf",
    "metadata": {
        "title": "Learning Others' Intentional Models in Multi-Agent Settings Using Interactive POMDPs",
        "author": "Yanlin Han, Piotr Gmytrasiewicz",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7806-learning-others-intentional-models-in-multi-agent-settings-using-interactive-pomdps.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable, stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents\u2019 actions using I-POMDPs, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others\u2019 intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Our approach serves as a generalized Bayesian learning algorithm that learns other agents\u2019 beliefs, strategy levels, and transition, observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy."
    },
    "keywords": [
        {
            "term": "the left",
            "url": "https://en.wikipedia.org/wiki/the_left"
        },
        {
            "term": "bayesian inference",
            "url": "https://en.wikipedia.org/wiki/bayesian_inference"
        },
        {
            "term": "the right",
            "url": "https://en.wikipedia.org/wiki/the_right"
        },
        {
            "term": "markov chain monte carlo",
            "url": "https://en.wikipedia.org/wiki/markov_chain_monte_carlo"
        },
        {
            "term": "markov decision process",
            "url": "https://en.wikipedia.org/wiki/markov_decision_process"
        }
    ],
    "highlights": [
        "Observable Markov decision processes (POMDPs) [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] is a general decision-theoretic framework for planning under uncertainty in a partially observable, stochastic environment",
        "We firstly fix the modeled agent j to be a level-2 I-Partially observable Markov decision processes agent and experiment with different modeling approaches for agent i in order to compare the performance in terms of average reward",
        "We compare level-3, level-2, level-1 intentional I-Partially observable Markov decision processes models with a subintentional model, in which agent j is assumed to choose his action according to a fixed but unknown distribution and is called a frequency-based model [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We show the correctness of our theoretical framework using the multi-agent tiger and UAV problems in which it accurately learns others\u2019 models over the entire intentional model space and can be generalized to problems of larger scale in a straightforward manner",
        "The applications presented in this paper can be extended to more complex problems by leveraging emerging deep reinforcement learning (DRL) methods, which already solves Partially observable Markov decision processes in an neural analogy [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "deep reinforcement learning should be capable of approximating key functions in I-Partially observable Markov decision processes, has the potential to serve as an efficient computational tool for I-Partially observable Markov decision processes"
    ],
    "key_statements": [
        "Observable Markov decision processes (POMDPs) [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] is a general decision-theoretic framework for planning under uncertainty in a partially observable, stochastic environment",
        "We firstly fix the modeled agent j to be a level-2 I-Partially observable Markov decision processes agent and experiment with different modeling approaches for agent i in order to compare the performance in terms of average reward",
        "We compare level-3, level-2, level-1 intentional I-Partially observable Markov decision processes models with a subintentional model, in which agent j is assumed to choose his action according to a fixed but unknown distribution and is called a frequency-based model [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "In Figure 2, we see that the intentional I-Partially observable Markov decision processes approaches has significantly higher reward as agent i perceives more observations, and level-2 I-Partially observable Markov decision processes performs slightly better than level-1 while level-3 has high variance but at least competes with level-2",
        "The subintentional approach has certain learning ability but is not sophisticated enough to model a rational agent, its performance is worse than all I-Partially observable Markov decision processes models",
        "We have described a novel approach to learn other agents\u2019 intentional models by making the interactive belief update using Bayesian inference and Monte Carlo sampling methods",
        "We show the correctness of our theoretical framework using the multi-agent tiger and UAV problems in which it accurately learns others\u2019 models over the entire intentional model space and can be generalized to problems of larger scale in a straightforward manner",
        "The applications presented in this paper can be extended to more complex problems by leveraging emerging deep reinforcement learning (DRL) methods, which already solves Partially observable Markov decision processes in an neural analogy [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "deep reinforcement learning should be capable of approximating key functions in I-Partially observable Markov decision processes, has the potential to serve as an efficient computational tool for I-Partially observable Markov decision processes"
    ],
    "summary": [
        "Observable Markov decision processes (POMDPs) [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] is a general decision-theoretic framework for planning under uncertainty in a partially observable, stochastic environment.",
        "We propose a Bayesian learning method that utilizes customized sequential Monte Carlo sampling [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] to obtain approximate solutions to I-POMDPs. We assume that the modeling agent maintains beliefs over intentional models of other agents and make sequential Bayesian updates using observations from the environment.",
        "While in multi-agent settings, others agents\u2019 models other than their beliefs are usually assumed to be known, in our assumption the modeling agent does not know any information about others\u2019 beliefs, strategy levels, and transition, observation, and reward functions.",
        "Our approach successfully recovers other agents\u2019 models over the intentional model space which contains their beliefs, strategy levels, and transition, observation and reward functions.",
        "It extends I-POMDP\u2019s belief update to larger model space, and it serves as a generalized Bayesian learning method for multi-agent systems in which other agents\u2019 beliefs, transition, observation and reward functions are unknown.",
        "Our interactive belief update described in Algorithm 1 and 2, generalizes I-POMDP\u2019s belief update to larger intentional model space which contains other agents\u2019 beliefs, and transition, observation and reward functions.",
        "The Algorithm 1 requires inputs of the modeling agent\u2019s prior belief, \u0303btk\u2212,l1, which is represented as a set of n samples isk(n),t\u22121, along with the action, atk\u22121, the observation, okt , and the belief nesting level, l > 0.",
        "The 0-level belief update, described in Algorithm 2, takes agent model, \u03b8kt\u2212,01, action, akt\u22121, and observation, okt , as input arguments and returns the belief about the physical state, btk,0.",
        "For each possible action a\u2212t\u2212k1, it computes the actual state transition and observation function by marginalizing over others\u2019 actions, and returns the normalized belief bkt ,0.",
        "In order to update the belief over this intentional model space of other agents, their initial belief, transition function, observation function and reward function in their frames are all unknown and become samples.",
        "The transition and observation functions of the level-0 agent, in line 7 and 10 of Algorithm 2, are passed in as input arguments which correspond to each model sample.",
        "We want to recover the possible initial belief b0j about the physical state, the transition, Tj, the observation, Oj and the reward, Rj. the main idea of our experiment is to do Bayesian parametric learning with the help of our sampling algorithm.",
        "We have described a novel approach to learn other agents\u2019 intentional models by making the interactive belief update using Bayesian inference and Monte Carlo sampling methods.",
        "DRL should be capable of approximating key functions in I-POMDPs, has the potential to serve as an efficient computational tool for I-POMDPs"
    ],
    "headline": "In order to predict other agents\u2019 actions using I-Partially observable Markov decision processes, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others\u2019 intentional models which ascribe to them beliefs, preferences and rationality in action selection",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Pierre Del Moral. Non-linear filtering: interacting particle resolution. Markov processes and related fields, 2(4):555\u2013581, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moral%2C%20Pierre%20Del%20Non-linear%20filtering%3A%20interacting%20particle%20resolution.%20Markov%20processes%20and%20related%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moral%2C%20Pierre%20Del%20Non-linear%20filtering%3A%20interacting%20particle%20resolution.%20Markov%20processes%20and%20related%201996"
        },
        {
            "id": "2",
            "entry": "[2] Prashant Doshi and Piotr J Gmytrasiewicz. Monte Carlo sampling methods for approximating interactive POMDPs. Journal of Artificial Intelligence Research, 34:297\u2013337, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doshi%2C%20Prashant%20Gmytrasiewicz%2C%20Piotr%20J.%20Monte%20Carlo%20sampling%20methods%20for%20approximating%20interactive%20POMDPs%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doshi%2C%20Prashant%20Gmytrasiewicz%2C%20Piotr%20J.%20Monte%20Carlo%20sampling%20methods%20for%20approximating%20interactive%20POMDPs%202009"
        },
        {
            "id": "3",
            "entry": "[3] Finale Doshi-Velez, David Pfau, Frank Wood, and Nicholas Roy. Bayesian nonparametric methods for partially-observable reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 37(2):394\u2013407, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doshi-Velez%2C%20Finale%20Pfau%2C%20David%20Wood%2C%20Frank%20Roy%2C%20Nicholas%20Bayesian%20nonparametric%20methods%20for%20partially-observable%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doshi-Velez%2C%20Finale%20Pfau%2C%20David%20Wood%2C%20Frank%20Roy%2C%20Nicholas%20Bayesian%20nonparametric%20methods%20for%20partially-observable%20reinforcement%20learning%202015"
        },
        {
            "id": "4",
            "entry": "[4] A Douced, Nando de Freitas, and Neil Gordon. An introduction to sequential Monte Carlo methods. Sequential Monte Carlo Methods in Practice, pages 3\u201313, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Douced%2C%20A.%20de%20Freitas%2C%20Nando%20Gordon%2C%20Neil%20An%20introduction%20to%20sequential%20Monte%20Carlo%20methods.%20Sequential%20Monte%20Carlo%20Methods%20in%20Practice%202001"
        },
        {
            "id": "5",
            "entry": "[5] Drew Fudenberg and David K Levine. The theory of learning in games, volume 2. MIT press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fudenberg%2C%20Drew%20Levine%2C%20David%20K.%20The%20theory%20of%20learning%20in%20games%2C%20volume%202%201998"
        },
        {
            "id": "6",
            "entry": "[6] Walter R Gilks, Sylvia Richardson, and David J Spiegelhalter. Introducing Markov chain Monte Carlo. Markov chain Monte Carlo in practice, 1:19, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilks%2C%20Walter%20R.%20Richardson%2C%20Sylvia%20Spiegelhalter%2C%20David%20J.%20Introducing%20Markov%20chain%20Monte%20Carlo.%20Markov%20chain%20Monte%20Carlo%20in%20practice%2C%201%3A%2019%201996"
        },
        {
            "id": "7",
            "entry": "[7] Piotr J Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multi-agent settings. J. Artif. Intell. Res.(JAIR), 24:49\u201379, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gmytrasiewicz%2C%20Piotr%20J.%20Doshi%2C%20Prashant%20A%20framework%20for%20sequential%20planning%20in%20multi-agent%20settings%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gmytrasiewicz%2C%20Piotr%20J.%20Doshi%2C%20Prashant%20A%20framework%20for%20sequential%20planning%20in%20multi-agent%20settings%202005"
        },
        {
            "id": "8",
            "entry": "[8] Piotr J Gmytrasiewicz and Edmund H Durfee. Rational coordination in multi-agent environments. Autonomous Agents and Multi-Agent Systems, 3(4):319\u2013350, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20Rational%20coordination%20in%20multi-agent%20environments%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gmytrasiewicz%2C%20Piotr%20J.%20Durfee%2C%20Edmund%20H.%20Rational%20coordination%20in%20multi-agent%20environments%202000"
        },
        {
            "id": "9",
            "entry": "[9] Neil J Gordon, David J Salmond, and Adrian FM Smith. Novel approach to nonlinear/non-Gaussian Bayesian state estimation. In IEE Proceedings F (Radar and Signal Processing), volume 140, pages 107\u2013113. IET, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20FM%20Smith.%20Novel%20approach%20to%20nonlinear/non-Gaussian%20Bayesian%20state%20estimation%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20FM%20Smith.%20Novel%20approach%20to%20nonlinear/non-Gaussian%20Bayesian%20state%20estimation%201993"
        },
        {
            "id": "10",
            "entry": "[10] John C Harsanyi. Games with incomplete information played by \u201cBayesian\u201d players, i\u2013iii part i. the basic model. Management science, 14(3):159\u2013182, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harsanyi%2C%20John%20C.%20Games%20with%20incomplete%20information%20played%20by%20%E2%80%9CBayesian%E2%80%9D%20players%2C%20i%E2%80%93iii%20part%20i.%20the%20basic%20model%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harsanyi%2C%20John%20C.%20Games%20with%20incomplete%20information%20played%20by%20%E2%80%9CBayesian%E2%80%9D%20players%2C%20i%E2%80%93iii%20part%20i.%20the%20basic%20model%201967"
        },
        {
            "id": "11",
            "entry": "[11] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1):99\u2013134, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaelbling%2C%20Leslie%20Pack%20Littman%2C%20Michael%20L.%20Cassandra%2C%20Anthony%20R.%20Planning%20and%20acting%20in%20partially%20observable%20stochastic%20domains%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaelbling%2C%20Leslie%20Pack%20Littman%2C%20Michael%20L.%20Cassandra%2C%20Anthony%20R.%20Planning%20and%20acting%20in%20partially%20observable%20stochastic%20domains%201998"
        },
        {
            "id": "12",
            "entry": "[12] Peter Karkus, David Hsu, and Wee Sun Lee. Qmdp-net: Deep learning for planning under partial observability. In Advances in Neural Information Processing Systems, pages 4697\u20134707, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karkus%2C%20Peter%20Hsu%2C%20David%20Lee%2C%20Wee%20Sun%20Qmdp-net%3A%20Deep%20learning%20for%20planning%20under%20partial%20observability%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karkus%2C%20Peter%20Hsu%2C%20David%20Lee%2C%20Wee%20Sun%20Qmdp-net%3A%20Deep%20learning%20for%20planning%20under%20partial%20observability%202017"
        },
        {
            "id": "13",
            "entry": "[13] Miao Liu, Xuejun Liao, and Lawrence Carin. The infinite regionalized policy representation. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 769\u2013776, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Miao%20Liao%2C%20Xuejun%20Carin%2C%20Lawrence%20The%20infinite%20regionalized%20policy%20representation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Miao%20Liao%2C%20Xuejun%20Carin%2C%20Lawrence%20The%20infinite%20regionalized%20policy%20representation%202011"
        },
        {
            "id": "14",
            "entry": "[14] Andrew Kachites McCallum and Dana Ballard. Reinforcement learning with selective perception and hidden state. PhD thesis, University of Rochester. Dept. of Computer Science, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCallum%2C%20Andrew%20Kachites%20Ballard%2C%20Dana%20Reinforcement%20learning%20with%20selective%20perception%20and%20hidden%20state%201996"
        },
        {
            "id": "15",
            "entry": "[15] Alessandro Panella and Piotr J Gmytrasiewicz. Bayesian learning of other agents\u2019 finite controllers for interactive POMDPs. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 2530\u20132536, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panella%2C%20Alessandro%20Gmytrasiewicz%2C%20Piotr%20J.%20Bayesian%20learning%20of%20other%20agents%E2%80%99%20finite%20controllers%20for%20interactive%20POMDPs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panella%2C%20Alessandro%20Gmytrasiewicz%2C%20Piotr%20J.%20Bayesian%20learning%20of%20other%20agents%E2%80%99%20finite%20controllers%20for%20interactive%20POMDPs%202016"
        },
        {
            "id": "16",
            "entry": "[16] Christos H Papadimitriou and John N Tsitsiklis. The complexity of Markov decision processes. Mathematics of operations research, 12(3):441\u2013450, 1987. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papadimitriou%2C%20Christos%20H.%20Tsitsiklis%2C%20John%20N.%20The%20complexity%20of%20Markov%20decision%20processes%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papadimitriou%2C%20Christos%20H.%20Tsitsiklis%2C%20John%20N.%20The%20complexity%20of%20Markov%20decision%20processes%201987"
        }
    ]
}
