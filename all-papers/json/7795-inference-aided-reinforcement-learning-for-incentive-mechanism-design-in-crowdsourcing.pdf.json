{
    "filename": "7795-inference-aided-reinforcement-learning-for-incentive-mechanism-design-in-crowdsourcing.pdf",
    "metadata": {
        "title": "Inference Aided Reinforcement Learning for Incentive Mechanism Design in Crowdsourcing",
        "author": "Zehong Hu, Yitao Liang, Jie Zhang, Zhao Li, Yang Liu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7795-inference-aided-reinforcement-learning-for-incentive-mechanism-design-in-crowdsourcing.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs of exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that learns to incentivize high-quality data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers\u2019 labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to the existing one-shot mechanisms."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "temporal difference",
            "url": "https://en.wikipedia.org/wiki/temporal_difference"
        },
        {
            "term": "ground truth",
            "url": "https://en.wikipedia.org/wiki/ground_truth"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "incentive compatible",
            "url": "https://en.wikipedia.org/wiki/incentive_compatible"
        },
        {
            "term": "National Research Foundation",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation"
        }
    ],
    "highlights": [
        "The ability to quickly collect large-scale and high-quality labeled datasets is crucial for Machine Learning (ML)",
        "(3) We prove that our Bayesian inference algorithm and reinforcement incentive learning algorithm are incentive compatible (IC) at each step and in the long run, respectively",
        "Our mechanism mainly consists of three components: the payment rule, Bayesian inference and reinforcement incentive learning (RIL); see Figure 1 for an overview, where estimated values are denoted with tildes",
        "To reduce the estimation bias, we develop a Bayesian inference algorithm by introducing soft Dirichlet priors to both the distribution of true labels \u03c4 = [\u03c4\u22121, \u03c4+1] \u223c Dir(\u03b2\u22121, \u03b2+1), where \u03c4\u22121 and \u03c4+1 denote that of label \u22121 and +1, respectively, and workers\u2019 probability of being correct [Pi, 1 \u2212 Pi] \u223c Dir(\u03b11, \u03b12)",
        "The aggregated label accuracy estimated from our Bayesian inference algorithm serves as a major component of the state representation and reward function to reinforcement incentive learning, and critically affects the performance of our mechanism",
        "We provide a simple description of them as follows, whereas the detailed version is deferred to Appendix H. (i) Rational workers alway take the utility-maximizing strategies. QR workers [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] follow strategies corresponding to an utility-dependent distribution"
    ],
    "key_statements": [
        "The ability to quickly collect large-scale and high-quality labeled datasets is crucial for Machine Learning (ML)",
        "We address the above challenges and make the following contributions: (1) We propose a Gibbs sampling augmented Bayesian inference algorithm, which estimates workers\u2019 labeling strategies and the aggregated label accuracy, as done in most existing inference algorithms, but significantly lowers the estimation bias of labeling accuracy",
        "(3) We prove that our Bayesian inference algorithm and reinforcement incentive learning algorithm are incentive compatible (IC) at each step and in the long run, respectively",
        "Our mechanism mainly consists of three components: the payment rule, Bayesian inference and reinforcement incentive learning (RIL); see Figure 1 for an overview, where estimated values are denoted with tildes",
        "To reduce the estimation bias, we develop a Bayesian inference algorithm by introducing soft Dirichlet priors to both the distribution of true labels \u03c4 = [\u03c4\u22121, \u03c4+1] \u223c Dir(\u03b2\u22121, \u03b2+1), where \u03c4\u22121 and \u03c4+1 denote that of label \u22121 and +1, respectively, and workers\u2019 probability of being correct [Pi, 1 \u2212 Pi] \u223c Dir(\u03b11, \u03b12)",
        "We formally introduce our reinforcement incentive learning (RIL) algorithm, which adjusts the scaling factor at to maximize the data requesters\u2019 utility accumulated in the long run",
        "The aggregated label accuracy estimated from our Bayesian inference algorithm serves as a major component of the state representation and reward function to reinforcement incentive learning, and critically affects the performance of our mechanism",
        "In our Bayesian inference algorithm, we introduce a soft Dirichlet prior for true labels, acting as a regularization term, to alleviate this bias",
        "We provide a simple description of them as follows, whereas the detailed version is deferred to Appendix H. (i) Rational workers alway take the utility-maximizing strategies. QR workers [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] follow strategies corresponding to an utility-dependent distribution"
    ],
    "summary": [
        "The ability to quickly collect large-scale and high-quality labeled datasets is crucial for Machine Learning (ML).",
        "Our mechanism not only incentivizesrational workers to provide high-quality labels but dynamically adjusts the payments according to workers\u2019 responses to maximize the data requester\u2019s cumulative utility.",
        "We use Pi,H and Pi,L to denote worker i\u2019s probability of observing the true label when exerting high and low efforts, respectively.",
        "Our mechanism mainly consists of three components: the payment rule, Bayesian inference and reinforcement incentive learning (RIL); see Figure 1 for an overview, where estimated values are denoted with tildes.",
        "The Bayesian inference algorithm is responsible for estimating the true labels, workers\u2019 PoBCs and the aggregated label accuracy at each time step, preparing the necessary inputs to RIL.",
        "All these convergence guarantees enable us to use the estimates computed by Bayesian inference to construct the state and reward signal in our reinforcement learning algorithm RIL.",
        "We formally introduce our reinforcement incentive learning (RIL) algorithm, which adjusts the scaling factor at to maximize the data requesters\u2019 utility accumulated in the long run.",
        "Where the noise t follows a Gaussian process, and \u03c0 = P(a|s) denotes Algorithm 2 Reinforcement Incentive Learning (RIL)",
        "In Appendix E, we prove that when at > ci,H /(Pi,H \u2212 0.5), if Pit \u2248 Pti, any worker i\u2019s utility-maximizing strategy would be reporting truthfully and exerting high efforts.",
        "We demonstrate that our RIL algorithm consistently manages to learn a good incentive policy, under above three rationality models of the workers.",
        "We show as a bonus benefit of our mechanism that, leveraging Bayesian inference to fully exploit the information contained in the collected labels leads to more robust and lower-variance payments at each step.",
        "The aggregated label accuracy estimated from our Bayesian inference algorithm serves as a major component of the state representation and reward function to RIL, and critically affects the performance of our mechanism.",
        "For all the experiments in this subsection, we set the environment parameters as follows: N = 10, PH = 0.9, b = 0, cH = 0.02; the set of the scaling factors is A = {0.1, 1.0, 5.0, 10}; F (A) = A10 and \u03b7 = 0.001 as in the utility function (Eqn (3)); the number of time steps for an episode is set to be 28.",
        "Our reinforcement incentive learning (RIL) algorithm ensures our mechanism to perform consistently well under different worker models."
    ],
    "headline": "We propose a novel inference aided reinforcement mechanism that learns to incentivize high-quality data sequentially and requires no such prior assumptions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Xi Chen, Qihang Lin, and Dengyong Zhou. Statistical decision making for optimal budget allocation in crowd labeling. Journal of Machine Learning Research, 16:1\u201346, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Xi%20Lin%2C%20Qihang%20Zhou%2C%20Dengyong%20Statistical%20decision%20making%20for%20optimal%20budget%20allocation%20in%20crowd%20labeling%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Xi%20Lin%2C%20Qihang%20Zhou%2C%20Dengyong%20Statistical%20decision%20making%20for%20optimal%20budget%20allocation%20in%20crowd%20labeling%202015"
        },
        {
            "id": "2",
            "entry": "[2] Anirban Dasgupta and Arpita Ghosh. Crowdsourced judgement elicitation with endogenous proficiency. In Proc. of WWW, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dasgupta%2C%20Anirban%20Ghosh%2C%20Arpita%20Crowdsourced%20judgement%20elicitation%20with%20endogenous%20proficiency%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dasgupta%2C%20Anirban%20Ghosh%2C%20Arpita%20Crowdsourced%20judgement%20elicitation%20with%20endogenous%20proficiency%202013"
        },
        {
            "id": "3",
            "entry": "[3] Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Applied statistics, pages 20\u201328, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dawid%2C%20Alexander%20Philip%20Skene%2C%20Allan%20M.%20Maximum%20likelihood%20estimation%20of%20observer%20error-rates%20using%20the%20em%20algorithm%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dawid%2C%20Alexander%20Philip%20Skene%2C%20Allan%20M.%20Maximum%20likelihood%20estimation%20of%20observer%20error-rates%20using%20the%20em%20algorithm%201979"
        },
        {
            "id": "4",
            "entry": "[4] Djellel Eddine Difallah, Michele Catasta, Gianluca Demartini, Panagiotis G Ipeirotis, and Philippe Cudr\u00e9-Mauroux. The dynamics of micro-task crowdsourcing: The case of amazon mturk. In Proc. of WWW, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Difallah%2C%20Djellel%20Eddine%20Catasta%2C%20Michele%20Demartini%2C%20Gianluca%20Ipeirotis%2C%20Panagiotis%20G.%20The%20dynamics%20of%20micro-task%20crowdsourcing%3A%20The%20case%20of%20amazon%20mturk%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Difallah%2C%20Djellel%20Eddine%20Catasta%2C%20Michele%20Demartini%2C%20Gianluca%20Ipeirotis%2C%20Panagiotis%20G.%20The%20dynamics%20of%20micro-task%20crowdsourcing%3A%20The%20case%20of%20amazon%20mturk%202015"
        },
        {
            "id": "5",
            "entry": "[5] Yaakov Engel, Shie Mannor, and Ron Meir. Reinforcement learning with gaussian processes. In Proc. of ICML, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Engel%2C%20Yaakov%20Mannor%2C%20Shie%20Meir%2C%20Ron%20Reinforcement%20learning%20with%20gaussian%20processes%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Engel%2C%20Yaakov%20Mannor%2C%20Shie%20Meir%2C%20Ron%20Reinforcement%20learning%20with%20gaussian%20processes%202005"
        },
        {
            "id": "6",
            "entry": "[6] Milica Gasic and Steve Young. Gaussian processes for pomdp-based dialogue manager optimization. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(1):28\u201340, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gasic%2C%20Milica%20Young%2C%20Steve%20Gaussian%20processes%20for%20pomdp-based%20dialogue%20manager%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gasic%2C%20Milica%20Young%2C%20Steve%20Gaussian%20processes%20for%20pomdp-based%20dialogue%20manager%20optimization%202014"
        },
        {
            "id": "7",
            "entry": "[7] Jeff Howe. The rise of crowdsourcing. Wired Magazine, 14(6), 06 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Howe%2C%20Jeff%20The%20rise%20of%20crowdsourcing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Howe%2C%20Jeff%20The%20rise%20of%20crowdsourcing%202006"
        },
        {
            "id": "8",
            "entry": "[8] Radu Jurca, Boi Faltings, et al. Mechanisms for making crowds truthful. Journal of Artificial Intelligence Research, 34(1):209, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jurca%2C%20Radu%20Faltings%2C%20Boi%20Mechanisms%20for%20making%20crowds%20truthful%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jurca%2C%20Radu%20Faltings%2C%20Boi%20Mechanisms%20for%20making%20crowds%20truthful%202009"
        },
        {
            "id": "9",
            "entry": "[9] Yitao Liang, Marlos C. Machado, Erik Talvitie, and Michael Bowling. State of the art control of atari games using shallow reinforcement learning. In Proc. of AAMAS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liang%2C%20Yitao%20Machado%2C%20Marlos%20C.%20Talvitie%2C%20Erik%20Bowling%2C%20Michael%20State%20of%20the%20art%20control%20of%20atari%20games%20using%20shallow%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liang%2C%20Yitao%20Machado%2C%20Marlos%20C.%20Talvitie%2C%20Erik%20Bowling%2C%20Michael%20State%20of%20the%20art%20control%20of%20atari%20games%20using%20shallow%20reinforcement%20learning%202016"
        },
        {
            "id": "10",
            "entry": "[10] Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and computation, 108(2):212\u2013261, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littlestone%2C%20Nick%20Warmuth%2C%20Manfred%20K.%20The%20weighted%20majority%20algorithm%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littlestone%2C%20Nick%20Warmuth%2C%20Manfred%20K.%20The%20weighted%20majority%20algorithm%201994"
        },
        {
            "id": "11",
            "entry": "[11] Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In Proc. of NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Qiang%20Peng%2C%20Jian%20Ihler%2C%20Alexander%20T.%20Variational%20inference%20for%20crowdsourcing%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Qiang%20Peng%2C%20Jian%20Ihler%2C%20Alexander%20T.%20Variational%20inference%20for%20crowdsourcing%202012"
        },
        {
            "id": "12",
            "entry": "[12] Yang Liu and Yiling Chen. Sequential peer prediction: Learning to elicit effort using posted prices. In Proc. of AAAI, pages 607\u2013613, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Yang%20Chen%2C%20Yiling%20Sequential%20peer%20prediction%3A%20Learning%20to%20elicit%20effort%20using%20posted%20prices%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Yang%20Chen%2C%20Yiling%20Sequential%20peer%20prediction%3A%20Learning%20to%20elicit%20effort%20using%20posted%20prices%202017"
        },
        {
            "id": "13",
            "entry": "[13] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form games. Games and economic behavior, 10(1):6\u201338, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McKelvey%2C%20Richard%20D.%20Palfrey%2C%20Thomas%20R.%20Quantal%20response%20equilibria%20for%20normal%20form%20games%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McKelvey%2C%20Richard%20D.%20Palfrey%2C%20Thomas%20R.%20Quantal%20response%20equilibria%20for%20normal%20form%20games%201995"
        },
        {
            "id": "14",
            "entry": "[14] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. Nature, 518(7540):529\u2013533, 02 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20Control%20through%20Deep%20Reinforcement%20Learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20Control%20through%20Deep%20Reinforcement%20Learning%202015"
        },
        {
            "id": "15",
            "entry": "[15] Dra\u017een Prelec. A bayesian truth serum for subjective data. Science, 306(5695):462\u2013466, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Prelec%2C%20Dra%C5%BEen%20A%20bayesian%20truth%20serum%20for%20subjective%20data%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Prelec%2C%20Dra%C5%BEen%20A%20bayesian%20truth%20serum%20for%20subjective%20data%202004"
        },
        {
            "id": "16",
            "entry": "[16] Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. Journal of Machine Learning Research, 11(Apr):1297\u20131322, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raykar%2C%20Vikas%20C.%20Yu%2C%20Shipeng%20Zhao%2C%20Linda%20H.%20Valadez%2C%20Gerardo%20Hermosillo%20Learning%20from%20crowds%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raykar%2C%20Vikas%20C.%20Yu%2C%20Shipeng%20Zhao%2C%20Linda%20H.%20Valadez%2C%20Gerardo%20Hermosillo%20Learning%20from%20crowds%202010"
        },
        {
            "id": "17",
            "entry": "[17] Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proc. of SIGKDD, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sheng%2C%20Victor%20S.%20Provost%2C%20Foster%20Ipeirotis%2C%20Panagiotis%20G.%20Get%20another%20label%3F%20improving%20data%20quality%20and%20data%20mining%20using%20multiple%2C%20noisy%20labelers%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sheng%2C%20Victor%20S.%20Provost%2C%20Foster%20Ipeirotis%2C%20Panagiotis%20G.%20Get%20another%20label%3F%20improving%20data%20quality%20and%20data%20mining%20using%20multiple%2C%20noisy%20labelers%202008"
        },
        {
            "id": "18",
            "entry": "[18] Edwin D Simpson, Matteo Venanzi, Steven Reece, Pushmeet Kohli, John Guiver, Stephen J Roberts, and Nicholas R Jennings. Language understanding in the wild: Combining crowdsourcing and machine learning. In Proc. of WWW, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simpson%2C%20Edwin%20D.%20Venanzi%2C%20Matteo%20Reece%2C%20Steven%20Kohli%2C%20Pushmeet%20Language%20understanding%20in%20the%20wild%3A%20Combining%20crowdsourcing%20and%20machine%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simpson%2C%20Edwin%20D.%20Venanzi%2C%20Matteo%20Reece%2C%20Steven%20Kohli%2C%20Pushmeet%20Language%20understanding%20in%20the%20wild%3A%20Combining%20crowdsourcing%20and%20machine%20learning%202015"
        },
        {
            "id": "19",
            "entry": "[19] Aleksandrs Slivkins and Jennifer Wortman Vaughan. Online decision making in crowdsourcing markets: Theoretical challenges. ACM SIGecom Exchanges, 12(2):4\u201323, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Slivkins%2C%20Aleksandrs%20Vaughan%2C%20Jennifer%20Wortman%20Online%20decision%20making%20in%20crowdsourcing%20markets%3A%20Theoretical%20challenges%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Slivkins%2C%20Aleksandrs%20Vaughan%2C%20Jennifer%20Wortman%20Online%20decision%20making%20in%20crowdsourcing%20markets%3A%20Theoretical%20challenges%202014"
        },
        {
            "id": "20",
            "entry": "[20] Rion Snow, Brendan O\u2019Connor, Daniel Jurafsky, and Andrew Y Ng. Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks. In Proc. of EMNLP, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snow%2C%20Rion%20O%E2%80%99Connor%2C%20Brendan%20Jurafsky%2C%20Daniel%20Ng%2C%20Andrew%20Y.%20Cheap%20and%20fast%E2%80%94but%20is%20it%20good%3F%3A%20evaluating%20non-expert%20annotations%20for%20natural%20language%20tasks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snow%2C%20Rion%20O%E2%80%99Connor%2C%20Brendan%20Jurafsky%2C%20Daniel%20Ng%2C%20Andrew%20Y.%20Cheap%20and%20fast%E2%80%94but%20is%20it%20good%3F%3A%20evaluating%20non-expert%20annotations%20for%20natural%20language%20tasks%202008"
        },
        {
            "id": "21",
            "entry": "[21] Jens Witkowski and David C Parkes. Peer prediction without a common prior. In Proc. of ACM EC, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Witkowski%2C%20Jens%20Parkes%2C%20David%20C.%20Peer%20prediction%20without%20a%20common%20prior%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Witkowski%2C%20Jens%20Parkes%2C%20David%20C.%20Peer%20prediction%20without%20a%20common%20prior%202012"
        },
        {
            "id": "22",
            "entry": "[22] Yuchen Zhang, Xi Chen, Denny Zhou, and Michael I Jordan. Spectral methods meet em: A provably optimal algorithm for crowdsourcing. In Proc. of NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Chen%2C%20Xi%20Zhou%2C%20Denny%20Jordan%2C%20Michael%20I.%20Spectral%20methods%20meet%20em%3A%20A%20provably%20optimal%20algorithm%20for%20crowdsourcing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Chen%2C%20Xi%20Zhou%2C%20Denny%20Jordan%2C%20Michael%20I.%20Spectral%20methods%20meet%20em%3A%20A%20provably%20optimal%20algorithm%20for%20crowdsourcing%202014"
        },
        {
            "id": "23",
            "entry": "[23] Yudian Zheng, Guoliang Li, Yuanbing Li, Caihua Shan, and Reynold Cheng. Truth inference in crowdsourcing: is the problem solved? Proc. of the VLDB Endowment, 10(5):541\u2013552, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Yudian%20Li%2C%20Guoliang%20Li%2C%20Yuanbing%20Shan%2C%20Caihua%20Truth%20inference%20in%20crowdsourcing%3A%20is%20the%20problem%20solved%3F%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Yudian%20Li%2C%20Guoliang%20Li%2C%20Yuanbing%20Shan%2C%20Caihua%20Truth%20inference%20in%20crowdsourcing%3A%20is%20the%20problem%20solved%3F%202017"
        },
        {
            "id": "24",
            "entry": "[24] Dengyong Zhou, Qiang Liu, John Platt, and Christopher Meek. Aggregating ordinal labels from crowds by minimax conditional entropy. In Proc. of ICML, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Dengyong%20Liu%2C%20Qiang%20Platt%2C%20John%20Meek%2C%20Christopher%20Aggregating%20ordinal%20labels%20from%20crowds%20by%20minimax%20conditional%20entropy%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Dengyong%20Liu%2C%20Qiang%20Platt%2C%20John%20Meek%2C%20Christopher%20Aggregating%20ordinal%20labels%20from%20crowds%20by%20minimax%20conditional%20entropy%202014"
        }
    ]
}
