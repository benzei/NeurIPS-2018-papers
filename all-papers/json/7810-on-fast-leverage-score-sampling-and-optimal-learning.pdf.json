{
    "filename": "7810-on-fast-leverage-score-sampling-and-optimal-learning.pdf",
    "metadata": {
        "title": "On Fast Leverage Score Sampling and Optimal Learning",
        "author": "Alessandro Rudi, Daniele Calandriello, Luigi Carratino, Lorenzo Rosasco",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7810-on-fast-leverage-score-sampling-and-optimal-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Leverage score sampling provides an appealing way to perform approximate computations for large matrices. Indeed, it allows to derive faithful approximations with a complexity adapted to the problem at hand. Yet, performing leverage scores sampling is a challenge in its own right requiring further approximations. In this paper, we study the problem of leverage score sampling for positive definite matrices defined by a kernel. Our contribution is twofold. First we provide a novel algorithm for leverage score sampling and second, we exploit the proposed method in statistical learning by deriving a novel solver for kernel ridge regression. Our main technical contribution is showing that the proposed algorithms are currently the most efficient and accurate for these problems."
    },
    "keywords": [
        {
            "term": "statistical learning",
            "url": "https://en.wikipedia.org/wiki/statistical_learning"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "BLESS",
            "url": "https://en.wikipedia.org/wiki/Bless"
        }
    ],
    "highlights": [
        "A variety of machine learning problems require manipulating and performing computations with large matrices that often do not fit memory",
        "This approach is fast to compute, but the number of columns needed for a prescribed approximation accuracy does not take advantage of the possible low rank structure of the matrix at hand",
        "Performing leverage score sampling provides a challenge in its own right, since it has complexity in the same order of an eigendecomposition of the original matrix",
        "Unlike most previous results on leverage scores sampling in this context [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], we consider the setting of statistical learning, where the challenge is that inputs, as well as the outputs, are random",
        "We describe the derivation of the considered algorithm starting from kernel ridge regression (KRR)",
        "In this paper we presented two algorithms Bottom-up Leverage Scores Sampling and Bottom-up Leverage Scores Sampling-R to efficiently compute a small set of columns from a large symmetric positive semidefinite matrix K, useful for approximating the matrix or to compute leverage scores with a given precision"
    ],
    "key_statements": [
        "A variety of machine learning problems require manipulating and performing computations with large matrices that often do not fit memory",
        "This approach is fast to compute, but the number of columns needed for a prescribed approximation accuracy does not take advantage of the possible low rank structure of the matrix at hand",
        "Performing leverage score sampling provides a challenge in its own right, since it has complexity in the same order of an eigendecomposition of the original matrix",
        "We propose and study Bottom-up Leverage Scores Sampling, a novel algorithm for approximate leverage scores sampling",
        "We extend the approach in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] for efficient kernel ridge regression based on combining fast optimization [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] showed that algorithms with uniform optimal learning bounds can be achieved with a complexity wsahmicphliinsgO.eRs in in time and Oe(n) space",
        "We study the impact of replacing uniform with leverage score sampling",
        "), Xn deff( ) = `(i, ), i=1 for > 0, it is easy to see that deff( )\n1/ for all , and previous results show that the number of columns required for accurate approximation are d1 for uniform sampling and deff for leverage score sampling [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "The procedure we propose, dubbed Bottom-up Leverage Scores Sampling, has similarities to the one proposed in [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]), but some important differences",
        "Since Bottom-up Leverage Scores Sampling computes leverage scores for sets of size at most 1/ , this allows to perform leverage scores sampling on matrices with millions of rows/columns, as shown in the experiments",
        "Unlike most previous results on leverage scores sampling in this context [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], we consider the setting of statistical learning, where the challenge is that inputs, as well as the outputs, are random",
        "The algorithm we propose, called FALKON-Bottom-up Leverage Scores Sampling, combines Bottom-up Leverage Scores Sampling with FALKON [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] a state of the art algorithm to solve the least squares problem presented above",
        "We describe the derivation of the considered algorithm starting from kernel ridge regression (KRR)",
        "Statistics.The above theorem provides statistical guarantees in terms of finite sample bounds on the excess risk of FALKON-Bottom-up Leverage Scores Sampling, A first bound depends of the number of examples n, the regularization parameter and the population effective dimension deff\u21e4( )",
        "Oe2) time and reducing the time complexity of FALKON to Oe). In this case the complexity of leverage scores sampling dominates, and our results provide Bottom-up Leverage Scores Sampling as a fix",
        "We study the performance of FALKON-Bottom-up Leverage Scores Sampling and compare it with the original FALKON [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] where an equal number of Nystr\u00f6m centres are sampled uniformly at random (FALKON-UNI)",
        "In this paper we presented two algorithms Bottom-up Leverage Scores Sampling and Bottom-up Leverage Scores Sampling-R to efficiently compute a small set of columns from a large symmetric positive semidefinite matrix K, useful for approximating the matrix or to compute leverage scores with a given precision"
    ],
    "summary": [
        "A variety of machine learning problems require manipulating and performing computations with large matrices that often do not fit memory.",
        "We propose and study BLESS, a novel algorithm for approximate leverage scores sampling.",
        "1/ for all , and previous results show that the number of columns required for accurate approximation are d1 for uniform sampling and deff for leverage score sampling [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>].",
        "We note that all the above procedures require specifying the number of iteration to be performed, the weights matrix to compute the leverage scores at each iteration, and a strategy to select the subsets (Uh)h.",
        "Algorithm 1 Bottom-up Leverage Scores Sampling (BLESS)",
        "Algorithm 2 Bottom-up Leverage Scores Sampling without Replacement (BLESS-R)",
        "Since BLESS computes leverage scores for sets of size at most 1/ , this allows to perform leverage scores sampling on matrices with millions of rows/columns, as shown in the experiments.",
        "Unlike most previous results on leverage scores sampling in this context [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], we consider the setting of statistical learning, where the challenge is that inputs, as well as the outputs, are random.",
        "Statistics.The above theorem provides statistical guarantees in terms of finite sample bounds on the excess risk of FALKON-BLESS, A first bound depends of the number of examples n, the regularization parameter and the population effective dimension deff\u21e4( ).",
        "Computations.To discuss computational implications, we recall a result from [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] showing that the population version of the effective dimension deff\u21e4( ) and the effective dimension deff( ) associated to the empirical kernel matrix converge up to constants.",
        "For the parameter choices leading to optimal learning rates, FALKON-BLESS has complexity Oe), in time and Oe2) in space, ignoring log terms.",
        "In this case the complexity of leverage scores sampling dominates, and our results provide BLESS as a fix.",
        "We begin by uniformly sampling a subsets of n = 7 \u21e5 104 points from the SUSY dataset [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>], and computing the exact leverage scores(i, ) using a Gaussian Kernel with = 4 and = 10 5, which is at the limit of our computational feasibility.",
        "In this paper we presented two algorithms BLESS and BLESS-R to efficiently compute a small set of columns from a large symmetric positive semidefinite matrix K, useful for approximating the matrix or to compute leverage scores with a given precision.",
        "We can extend the proposed work in several ways: (a) combine BLESS with fast stochastic gradient algorithms [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] and other approximation schemes, to further reduce the computational complexity for optimal rates, (b) consider the impact of BLESS in the context of multi-tasking [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] or structured prediction [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]."
    ],
    "headline": "We study the problem of leverage score sampling for positive definite matrices defined by a kernel",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Raman Arora, Andrew Cotter, Karen Livescu, and Nathan Srebro. Stochastic optimization for pca and pls. In Communication, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on, pages 861\u2013868. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20Raman%20Cotter%2C%20Andrew%20Livescu%2C%20Karen%20Srebro%2C%20Nathan%20Stochastic%20optimization%20for%20pca%20and%20pls%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20Raman%20Cotter%2C%20Andrew%20Livescu%2C%20Karen%20Srebro%2C%20Nathan%20Stochastic%20optimization%20for%20pca%20and%20pls%202012"
        },
        {
            "id": "2",
            "entry": "[2] David P. Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.4357"
        },
        {
            "id": "3",
            "entry": "[3] Joel A Tropp. User-friendly tools for random matrices: An introduction. Technical report, CALIFORNIA INST OF TECH PASADENA DIV OF ENGINEERING AND APPLIED SCIENCE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tropp%2C%20Joel%20A.%20User-friendly%20tools%20for%20random%20matrices%3A%20An%20introduction.%20Technical%20report%2C%20CALIFORNIA%20INST%20OF%20TECH%20PASADENA%20DIV%20OF%20ENGINEERING%20AND%20APPLIED%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tropp%2C%20Joel%20A.%20User-friendly%20tools%20for%20random%20matrices%3A%20An%20introduction.%20Technical%20report%2C%20CALIFORNIA%20INST%20OF%20TECH%20PASADENA%20DIV%20OF%20ENGINEERING%20AND%20APPLIED%202012"
        },
        {
            "id": "4",
            "entry": "[4] Christopher Williams and Matthias Seeger. Using the Nystrom method to speed up kernel machines. In Neural Information Processing Systems, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20Seeger%2C%20Matthias%20Using%20the%20Nystrom%20method%20to%20speed%20up%20kernel%20machines%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Christopher%20Seeger%2C%20Matthias%20Using%20the%20Nystrom%20method%20to%20speed%20up%20kernel%20machines%202001"
        },
        {
            "id": "5",
            "entry": "[5] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on Learning Theory, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Sharp%20analysis%20of%20low-rank%20kernel%20matrix%20approximations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Sharp%20analysis%20of%20low-rank%20kernel%20matrix%20approximations%202013"
        },
        {
            "id": "6",
            "entry": "[6] Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel methods with statistical guarantees. In Neural Information Processing Systems, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alaoui%2C%20Ahmed%20El%20Mahoney%2C%20Michael%20W.%20Fast%20randomized%20kernel%20methods%20with%20statistical%20guarantees%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alaoui%2C%20Ahmed%20El%20Mahoney%2C%20Michael%20W.%20Fast%20randomized%20kernel%20methods%20with%20statistical%20guarantees%202015"
        },
        {
            "id": "7",
            "entry": "[7] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approximation of matrix coherence and statistical leverage. The Journal of Machine Learning Research, 13(1):3475\u20133506, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Magdon-Ismail%2C%20Malik%20Mahoney%2C%20Michael%20W.%20Woodruff%2C%20David%20P.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Magdon-Ismail%2C%20Malik%20Mahoney%2C%20Michael%20W.%20Woodruff%2C%20David%20P.%20Fast%20approximation%20of%20matrix%20coherence%20and%20statistical%20leverage%202012"
        },
        {
            "id": "8",
            "entry": "[8] Daniele Calandriello, Alessandro Lazaric, and Michal Valko. Distributed sequential sampling for kernel matrix approximation. In AISTATS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Calandriello%2C%20Daniele%20Lazaric%2C%20Alessandro%20Valko%2C%20Michal%20Distributed%20sequential%20sampling%20for%20kernel%20matrix%20approximation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Calandriello%2C%20Daniele%20Lazaric%2C%20Alessandro%20Valko%2C%20Michal%20Distributed%20sequential%20sampling%20for%20kernel%20matrix%20approximation%202017"
        },
        {
            "id": "9",
            "entry": "[9] Cameron Musco and Christopher Musco. Recursive Sampling for the Nystr\u00f6m Method. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Recursive%20Sampling%20for%20the%20Nystr%C3%B6m%20Method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Musco%2C%20Cameron%20Musco%2C%20Christopher%20Recursive%20Sampling%20for%20the%20Nystr%C3%B6m%20Method%202017"
        },
        {
            "id": "10",
            "entry": "[10] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. Adaptive computation and machine learning. MIT Press, Cambridge, Mass, 2006. OCLC: ocm61285753.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20Carl%20Edward%20Williams%2C%20Christopher%20K.I.%20Gaussian%20processes%20for%20machine%20learning.%20Adaptive%20computation%20and%20machine%20learning%202006"
        },
        {
            "id": "11",
            "entry": "[11] Bernhard Sch\u00f6lkopf, Alexander J Smola, et al. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20Bernhard%20Smola%2C%20Alexander%20J.%20Learning%20with%20kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond%202002"
        },
        {
            "id": "12",
            "entry": "[12] Alex J Smola and Bernhard Sch\u00f6lkopf. Sparse greedy matrix approximation for machine learning. 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20Alex%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20Sparse%20greedy%20matrix%20approximation%20for%20machine%20learning%202000"
        },
        {
            "id": "13",
            "entry": "[13] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel method. In Advances in Neural Information Processing Systems, pages 3891\u20133901, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017"
        },
        {
            "id": "14",
            "entry": "[14] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystr\u00f6m computational regularization. In Advances in Neural Information Processing Systems, pages 1657\u20131665, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015"
        },
        {
            "id": "15",
            "entry": "[15] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007"
        },
        {
            "id": "16",
            "entry": "[16] Ingo Steinwart, Don R Hush, Clint Scovel, et al. Optimal rates for regularized least squares regression. In COLT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression%202009"
        },
        {
            "id": "17",
            "entry": "[17] Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral algorithms with least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018"
        },
        {
            "id": "18",
            "entry": "[18] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature communications, 5:4308, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20Pierre%20Sadowski%2C%20Peter%20Whiteson%2C%20Daniel%20Searching%20for%20exotic%20particles%20in%20high-energy%20physics%20with%20deep%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20Pierre%20Sadowski%2C%20Peter%20Whiteson%2C%20Daniel%20Searching%20for%20exotic%20particles%20in%20high-energy%20physics%20with%20deep%20learning%202014"
        },
        {
            "id": "19",
            "entry": "[19] Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential convergence _rate for finite training sets. In Advances in neural information processing systems, pages 2663\u20132671, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20_rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20_rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "20",
            "entry": "[20] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177\u20131184, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "21",
            "entry": "[21] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pages 3215\u20133225, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "22",
            "entry": "[22] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243\u2013272, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Argyriou%2C%20Andreas%20Evgeniou%2C%20Theodoros%20Pontil%2C%20Massimiliano%20Convex%20multi-task%20feature%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Argyriou%2C%20Andreas%20Evgeniou%2C%20Theodoros%20Pontil%2C%20Massimiliano%20Convex%20multi-task%20feature%20learning%202008"
        },
        {
            "id": "23",
            "entry": "[23] Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, and Massimiliano Pontil. Consistent multitask learning with nonlinear output relations. In Advances in Neural Information Processing Systems, pages 1986\u20131996, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986"
        },
        {
            "id": "24",
            "entry": "[24] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A consistent regularization approach for structured prediction. Advances in Neural Information Processing Systems 29 (NIPS), pages 4412\u20134420, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016"
        },
        {
            "id": "25",
            "entry": "[25] Anna Korba, Alexandre Garcia, and Florence d\u2019Alch\u00e9 Buc. A structured prediction approach for label ranking. arXiv preprint arXiv:1807.02374, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.02374"
        },
        {
            "id": "26",
            "entry": "[26] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337\u2013404, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950"
        },
        {
            "id": "27",
            "entry": "[27] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20vector%20machines%202008"
        }
    ]
}
