{
    "filename": "7578-the-nearest-neighbor-information-estimator-is-adaptively-near-minimax-rate-optimal.pdf",
    "metadata": {
        "title": "The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal",
        "author": "Jiantao Jiao, Weihao Gao, Yanjun Han",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7578-the-nearest-neighbor-information-estimator-is-adaptively-near-minimax-rate-optimal.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We analyze the Kozachenko\u2013Leonenko (KL) fixed k-nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance for any fixed k over Holder balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a recent minimax lower bound over the Holder ball, we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the Holder ball for s \u2208 (0, 2] and arbitrary dimension d, rendering it the first estimator that provably satisfies this property."
    },
    "keywords": [
        {
            "term": "entropy estimation",
            "url": "https://en.wikipedia.org/wiki/entropy_estimation"
        },
        {
            "term": "nonparametric estimation",
            "url": "https://en.wikipedia.org/wiki/nonparametric_estimation"
        },
        {
            "term": "mutual information",
            "url": "https://en.wikipedia.org/wiki/mutual_information"
        },
        {
            "term": "feature selection",
            "url": "https://en.wikipedia.org/wiki/feature_selection"
        },
        {
            "term": "random variable",
            "url": "https://en.wikipedia.org/wiki/random_variable"
        },
        {
            "term": "differential entropy",
            "url": "https://en.wikipedia.org/wiki/differential_entropy"
        }
    ],
    "highlights": [
        "Information theoretic measures such as entropy, Kullback-Leibler divergence and mutual information quantify the amount of information among random variables",
        "Estimating information theoretic measures from data is a crucial sub-routine in the aforementioned applications and has attracted much interest in statistics community",
        "We study the problem of estimating Shannon differential entropy, which is the basis of estimating other information theoretic measures for continuous random variables",
        "The KL differential entropy estimator is defined as hn,k (X)\n1 n n i=1 ln n k \u03bb(B",
        "We introduce a new random variable X \u223c f independent of"
    ],
    "key_statements": [
        "Information theoretic measures such as entropy, Kullback-Leibler divergence and mutual information quantify the amount of information among random variables",
        "Estimating information theoretic measures from data is a crucial sub-routine in the aforementioned applications and has attracted much interest in statistics community",
        "We study the problem of estimating Shannon differential entropy, which is the basis of estimating other information theoretic measures for continuous random variables",
        "The KL differential entropy estimator is defined as hn,k (X)\n1 n n i=1 ln n k \u03bb(B",
        "We introduce a new random variable X \u223c f independent of"
    ],
    "summary": [
        "Information theoretic measures such as entropy, Kullback-Leibler divergence and mutual information quantify the amount of information among random variables.",
        ", Xn} drawn from density function f where Xi \u2208 Rd. We consider the problem of estimating the differential entropy h(f ) = \u2212 f (x) ln f (x)dx , (1)",
        "Let B(x, \u03c1) denote the closed 2 ball centered at x of radius \u03c1 and \u03bb be the Lebesgue measure on Rd. The KL differential entropy estimator is defined as hn,k (X)",
        "The nearest neighbor density estimator f(Xi) follows from the \u201cintuition\u201d 1that f(Xi)\u03bb(B(Xi, Ri,k))",
        "It was shown in [<a class=\"ref-link\" id=\"c42\" href=\"#r42\">42</a>] that the bias of the nearest neighbor density estimator does not vanish even",
        "The worst case risk of the fixed k-nearest neighbor differential entropy estimator over Hds(L; [0, 1]d) is controlled by the following theorem.",
        "For 0 < s \u2264 2, the fixed k-nearest neighbor KL differential entropy estimator hn,k in (3) satisfies sup",
        "Holder ball has a too small width, the density itself is bounded away from zero, which makes the differential entropy a smooth functional, with minimax rates n\u2212",
        "We obtain the first uniform upper bound on the performance of the fixed k-nearest neighbor KL differential entropy estimator over Holder balls without assuming how close the density could be from zero.",
        "If the density f is assumed to satisfy f (x) \u2265 c for some constant c > 0, the differential entropy becomes a smooth functional and the general technique for estimating smooth nonparametric functionals [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>, <a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>, <a class=\"ref-link\" id=\"c43\" href=\"#r43\">43</a>] can be directly applied here to achieve the minimax rates n\u2212",
        "We show that, for any fixed k, the k-nearest neighbor KL entropy estimator nearly achieves the minimax rates without knowing the smoothness parameter s.",
        "Recent works [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c52\" href=\"#r52\">52</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] analyzed the performance of the KL estimator under various assumptions on how close the density could be to zero, with no matching lower bound up to logarithmic factors in general.",
        "We have proved that C5 \u2264 C51 + C52 s,L,d n\u2212s/(s+d) ln(n + 1), which completes the proof of the upper bound on E",
        "+n\u22121/2 due to a more careful analysis of the bias, since Hardy\u2013Littlewood maximal inequalities apply to arbitrary measurable functions but we have assumed regularity properties of the underlying density.",
        "It is interesting to analyze k-nearest neighbor based mutual information estimators, such as the KSG estimator [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>], and show that they are \u201cnear\u201d-optimal and adaptive to both the smoothness and the dimension of the distributions."
    ],
    "headline": "Accompanying a recent minimax lower bound over the Holder ball, we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the Holder ball for s \u2208 (0, 2] and arbitrary dimension d, rendering it the first estimator that provably satisfies this property",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] R. Battiti. Using mutual information for selecting features in supervised neural net learning. Neural Networks, IEEE Transactions on, 5(4):537\u2013550, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Battiti%2C%20R.%20Using%20mutual%20information%20for%20selecting%20features%20in%20supervised%20neural%20net%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Battiti%2C%20R.%20Using%20mutual%20information%20for%20selecting%20features%20in%20supervised%20neural%20net%20learning%201994"
        },
        {
            "id": "2",
            "entry": "[2] Jan Beirlant, Edward J Dudewicz, Laszlo Gyorfi, and Edward C Van der Meulen. Nonparametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 6(1):17\u201339, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beirlant%2C%20Jan%20Dudewicz%2C%20Edward%20J.%20Gyorfi%2C%20Laszlo%20der%20Meulen%2C%20Edward%20C.Van%20Nonparametric%20entropy%20estimation%3A%20An%20overview%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beirlant%2C%20Jan%20Dudewicz%2C%20Edward%20J.%20Gyorfi%2C%20Laszlo%20der%20Meulen%2C%20Edward%20C.Van%20Nonparametric%20entropy%20estimation%3A%20An%20overview%201997"
        },
        {
            "id": "3",
            "entry": "[3] Thomas B Berrett, Richard J Samworth, and Ming Yuan. Efficient multivariate entropy estimation via k-nearest neighbour distances. arXiv preprint arXiv:1606.00304, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.00304"
        },
        {
            "id": "4",
            "entry": "[4] Gerard Biau and Luc Devroye. Lectures on the nearest neighbor method. Springer, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biau%2C%20Gerard%20Devroye%2C%20Luc%20Lectures%20on%20the%20nearest%20neighbor%20method%202015"
        },
        {
            "id": "5",
            "entry": "[5] Peter J Bickel and Ya\u2019acov Ritov. Estimating integrated squared density derivatives: sharp best order of convergence estimates. Sankhya: The Indian Journal of Statistics, Series A, pages 381\u2013393, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bickel%2C%20Peter%20J.%20Ritov%2C%20Ya%E2%80%99acov%20Estimating%20integrated%20squared%20density%20derivatives%3A%20sharp%20best%20order%20of%20convergence%20estimates%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bickel%2C%20Peter%20J.%20Ritov%2C%20Ya%E2%80%99acov%20Estimating%20integrated%20squared%20density%20derivatives%3A%20sharp%20best%20order%20of%20convergence%20estimates%201988"
        },
        {
            "id": "6",
            "entry": "[6] Lucien Birgeand Pascal Massart. Estimation of integral functionals of a density. The Annals of Statistics, pages 11\u201329, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Massart%2C%20Lucien%20Birgeand%20Pascal%20Estimation%20of%20integral%20functionals%20of%20a%20density%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Massart%2C%20Lucien%20Birgeand%20Pascal%20Estimation%20of%20integral%20functionals%20of%20a%20density%201995"
        },
        {
            "id": "7",
            "entry": "[7] Yuheng Bu, Shaofeng Zou, Yingbin Liang, and Venugopal V Veeravalli. Estimation of KL divergence between large-alphabet distributions. In 2016 IEEE International Symposium on Information Theory (ISIT), pages 1118\u20131122. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bu%2C%20Yuheng%20Zou%2C%20Shaofeng%20Liang%2C%20Yingbin%20Veeravalli%2C%20Venugopal%20V.%20Estimation%20of%20KL%20divergence%20between%20large-alphabet%20distributions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bu%2C%20Yuheng%20Zou%2C%20Shaofeng%20Liang%2C%20Yingbin%20Veeravalli%2C%20Venugopal%20V.%20Estimation%20of%20KL%20divergence%20between%20large-alphabet%20distributions%202016"
        },
        {
            "id": "8",
            "entry": "[8] T Tony Cai and Mark G Low. A note on nonparametric estimation of linear functionals. Annals of statistics, pages 1140\u20131153, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20T.Tony%20Low%2C%20Mark%20G.%20A%20note%20on%20nonparametric%20estimation%20of%20linear%20functionals%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20T.Tony%20Low%2C%20Mark%20G.%20A%20note%20on%20nonparametric%20estimation%20of%20linear%20functionals%202003"
        },
        {
            "id": "9",
            "entry": "[9] T Tony Cai and Mark G Low. Nonquadratic estimators of a quadratic functional. The Annals of Statistics, pages 2930\u20132956, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20T.Tony%20Low%2C%20Mark%20G.%20Nonquadratic%20estimators%20of%20a%20quadratic%20functional%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20T.Tony%20Low%2C%20Mark%20G.%20Nonquadratic%20estimators%20of%20a%20quadratic%20functional%202005"
        },
        {
            "id": "10",
            "entry": "[10] C. Chan, A. Al-Bashabsheh, J. B. Ebrahimi, T. Kaced, and T. Liu. Multivariate mutual information inspired by secret-key agreement. Proceedings of the IEEE, 103(10):1883\u20131913, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20C.%20Al-Bashabsheh%2C%20A.%20Ebrahimi%2C%20J.B.%20Kaced%2C%20T.%20Multivariate%20mutual%20information%20inspired%20by%20secret-key%20agreement%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20C.%20Al-Bashabsheh%2C%20A.%20Ebrahimi%2C%20J.B.%20Kaced%2C%20T.%20Multivariate%20mutual%20information%20inspired%20by%20secret-key%20agreement%202015"
        },
        {
            "id": "11",
            "entry": "[11] Sylvain Delattre and Nicolas Fournier. On the kozachenko\u2013leonenko entropy estimator. Journal of Statistical Planning and Inference, 185:69\u201393, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Delattre%2C%20Sylvain%20Fournier%2C%20Nicolas%20On%20the%20kozachenko%E2%80%93leonenko%20entropy%20estimator%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Delattre%2C%20Sylvain%20Fournier%2C%20Nicolas%20On%20the%20kozachenko%E2%80%93leonenko%20entropy%20estimator%202017"
        },
        {
            "id": "12",
            "entry": "[12] David L Donoho and Michael Nussbaum. Minimax quadratic estimation of a quadratic functional. Journal of Complexity, 6(3):290\u2013323, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Nussbaum%2C%20Michael%20Minimax%20quadratic%20estimation%20of%20a%20quadratic%20functional%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Nussbaum%2C%20Michael%20Minimax%20quadratic%20estimation%20of%20a%20quadratic%20functional%201990"
        },
        {
            "id": "13",
            "entry": "[13] Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics, pages 586\u2013596, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Efron%2C%20Bradley%20Stein%2C%20Charles%20The%20jackknife%20estimate%20of%20variance%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Efron%2C%20Bradley%20Stein%2C%20Charles%20The%20jackknife%20estimate%20of%20variance%201981"
        },
        {
            "id": "14",
            "entry": "[14] Fidah El Haje Hussein and Yu Golubev. On entropy estimation by m-spacing method. Journal of Mathematical Sciences, 163(3):290\u2013309, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hussein%2C%20Fidah%20El%20Haje%20Golubev%2C%20Yu%20On%20entropy%20estimation%20by%20m-spacing%20method%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hussein%2C%20Fidah%20El%20Haje%20Golubev%2C%20Yu%20On%20entropy%20estimation%20by%20m-spacing%20method%202009"
        },
        {
            "id": "15",
            "entry": "[15] Lawrence Craig Evans and Ronald F Gariepy. Measure theory and fine properties of functions. CRC press, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evans%2C%20Lawrence%20Craig%20Gariepy%2C%20Ronald%20F.%20Measure%20theory%20and%20fine%20properties%20of%20functions%202015"
        },
        {
            "id": "16",
            "entry": "[16] Jianqing Fan. On the estimation of quadratic functionals. The Annals of Statistics, pages 1273\u20131294, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Jianqing%20On%20the%20estimation%20of%20quadratic%20functionals%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Jianqing%20On%20the%20estimation%20of%20quadratic%20functionals%201991"
        },
        {
            "id": "17",
            "entry": "[17] F. Fleuret. Fast binary feature selection with conditional mutual information. The Journal of Machine Learning Research, 5:1531\u20131555, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fleuret%2C%20F.%20Fast%20binary%20feature%20selection%20with%20conditional%20mutual%20information%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fleuret%2C%20F.%20Fast%20binary%20feature%20selection%20with%20conditional%20mutual%20information%202004"
        },
        {
            "id": "18",
            "entry": "[18] Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath. Conditional dependence via shannon capacity: Axioms, estimators and applications. In International Conference on Machine Learning, pages 2780\u20132789, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Weihao%20Kannan%2C%20Sreeram%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Conditional%20dependence%20via%20shannon%20capacity%3A%20Axioms%2C%20estimators%20and%20applications%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Weihao%20Kannan%2C%20Sreeram%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Conditional%20dependence%20via%20shannon%20capacity%3A%20Axioms%2C%20estimators%20and%20applications%202016"
        },
        {
            "id": "19",
            "entry": "[19] Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath. Estimating mutual information for discrete-continuous mixtures. In Advances in Neural Information Processing Systems, pages 5988\u20135999, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Weihao%20Kannan%2C%20Sreeram%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Estimating%20mutual%20information%20for%20discrete-continuous%20mixtures%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Weihao%20Kannan%2C%20Sreeram%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Estimating%20mutual%20information%20for%20discrete-continuous%20mixtures%202017"
        },
        {
            "id": "20",
            "entry": "[20] Weihao Gao, Sewoong Oh, and Pramod Viswanath. Breaking the bandwidth barrier: Geometrical adaptive entropy estimation. In Advances in Neural Information Processing Systems, pages 2460\u20132468, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Weihao%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Breaking%20the%20bandwidth%20barrier%3A%20Geometrical%20adaptive%20entropy%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Weihao%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Breaking%20the%20bandwidth%20barrier%3A%20Geometrical%20adaptive%20entropy%20estimation%202016"
        },
        {
            "id": "21",
            "entry": "[21] Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying fixed k-nearest neighbor information estimators. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages 1267\u20131271. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20Weihao%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Demystifying%20fixed%20k-nearest%20neighbor%20information%20estimators%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20Weihao%20Oh%2C%20Sewoong%20Viswanath%2C%20Pramod%20Demystifying%20fixed%20k-nearest%20neighbor%20information%20estimators%202017"
        },
        {
            "id": "22",
            "entry": "[22] Evarist Gineand Richard Nickl. A simple adaptive estimator of the integrated square of a density. Bernoulli, pages 47\u201361, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickl%2C%20Evarist%20Gineand%20Richard%20A%20simple%20adaptive%20estimator%20of%20the%20integrated%20square%20of%20a%20density%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickl%2C%20Evarist%20Gineand%20Richard%20A%20simple%20adaptive%20estimator%20of%20the%20integrated%20square%20of%20a%20density%202008"
        },
        {
            "id": "23",
            "entry": "[23] Peter Hall. Limit theorems for sums of general functions of m-spacings. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 96, pages 517\u2013532. Cambridge University Press, 1984.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hall%2C%20Peter%20Limit%20theorems%20for%20sums%20of%20general%20functions%20of%20m-spacings%201984",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hall%2C%20Peter%20Limit%20theorems%20for%20sums%20of%20general%20functions%20of%20m-spacings%201984"
        },
        {
            "id": "24",
            "entry": "[24] Peter Hall and James Stephen Marron. Estimation of integrated squared density derivatives. Statistics & Probability Letters, 6(2):109\u2013115, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hall%2C%20Peter%20Marron%2C%20James%20Stephen%20Estimation%20of%20integrated%20squared%20density%20derivatives%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hall%2C%20Peter%20Marron%2C%20James%20Stephen%20Estimation%20of%20integrated%20squared%20density%20derivatives%201987"
        },
        {
            "id": "25",
            "entry": "[25] Peter Hall and Sally C Morton. On the estimation of entropy. Annals of the Institute of Statistical Mathematics, 45(1):69\u201388, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hall%2C%20Peter%20Morton%2C%20Sally%20C.%20On%20the%20estimation%20of%20entropy%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hall%2C%20Peter%20Morton%2C%20Sally%20C.%20On%20the%20estimation%20of%20entropy%201993"
        },
        {
            "id": "26",
            "entry": "[26] Yanjun Han, Jiantao Jiao, , Tsachy Weissman, and Yihong Wu. Optimal rates of entropy estimation over lipschitz balls. arXiv preprint arXiv:1711.02141, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.02141"
        },
        {
            "id": "27",
            "entry": "[27] Yanjun Han, Jiantao Jiao, Rajarshi Mukherjee, and Tsachy Weissman. On estimation of lrnorms in gaussian white noise models. arXiv preprint arXiv:1710.03863, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.03863"
        },
        {
            "id": "28",
            "entry": "[28] Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax rate-optimal estimation of divergences between discrete distributions. arXiv preprint arXiv:1605.09124, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.09124"
        },
        {
            "id": "29",
            "entry": "[29] Harry Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the Institute of Statistical Mathematics, 41(4):683\u2013697, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Joe%2C%20Harry%20Estimation%20of%20entropy%20and%20other%20functionals%20of%20a%20multivariate%20density%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Joe%2C%20Harry%20Estimation%20of%20entropy%20and%20other%20functionals%20of%20a%20multivariate%20density%201989"
        },
        {
            "id": "30",
            "entry": "[30] Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, et al. Nonparametric von Mises estimators for entropies, divergences and mutual informations. In Advances in Neural Information Processing Systems, pages 397\u2013405, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kandasamy%2C%20Kirthevasan%20Krishnamurthy%2C%20Akshay%20Poczos%2C%20Barnabas%20Wasserman%2C%20Larry%20Nonparametric%20von%20Mises%20estimators%20for%20entropies%2C%20divergences%20and%20mutual%20informations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kandasamy%2C%20Kirthevasan%20Krishnamurthy%2C%20Akshay%20Poczos%2C%20Barnabas%20Wasserman%2C%20Larry%20Nonparametric%20von%20Mises%20estimators%20for%20entropies%2C%20divergences%20and%20mutual%20informations%202015"
        },
        {
            "id": "31",
            "entry": "[31] Rhoana J Karunamuni and Tom Alberts. On boundary correction in kernel density estimation. Statistical Methodology, 2(3):191\u2013212, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karunamuni%2C%20Rhoana%20J.%20Alberts%2C%20Tom%20On%20boundary%20correction%20in%20kernel%20density%20estimation%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karunamuni%2C%20Rhoana%20J.%20Alberts%2C%20Tom%20On%20boundary%20correction%20in%20kernel%20density%20estimation%202005"
        },
        {
            "id": "32",
            "entry": "[32] Gerard Kerkyacharian and Dominique Picard. Estimating nonquadratic functionals of a density using haar wavelets. The Annals of Statistics, 24(2):485\u2013507, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kerkyacharian%2C%20Gerard%20Picard%2C%20Dominique%20Estimating%20nonquadratic%20functionals%20of%20a%20density%20using%20haar%20wavelets%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kerkyacharian%2C%20Gerard%20Picard%2C%20Dominique%20Estimating%20nonquadratic%20functionals%20of%20a%20density%20using%20haar%20wavelets%201996"
        },
        {
            "id": "33",
            "entry": "[33] LF Kozachenko and Nikolai N Leonenko. Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2):9\u201316, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kozachenko%2C%20L.F.%20Leonenko%2C%20Nikolai%20N.%20Sample%20estimate%20of%20the%20entropy%20of%20a%20random%20vector%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kozachenko%2C%20L.F.%20Leonenko%2C%20Nikolai%20N.%20Sample%20estimate%20of%20the%20entropy%20of%20a%20random%20vector%201987"
        },
        {
            "id": "34",
            "entry": "[34] Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Physical Review E, 69(6):066138, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kraskov%2C%20Alexander%20Stogbauer%2C%20Harald%20Grassberger%2C%20Peter%20Estimating%20mutual%20information%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kraskov%2C%20Alexander%20Stogbauer%2C%20Harald%20Grassberger%2C%20Peter%20Estimating%20mutual%20information%202004"
        },
        {
            "id": "35",
            "entry": "[35] Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, and Larry Wasserman. Nonparametric estimation of Renyi divergence and friends. In International Conference on Machine Learning, pages 919\u2013927, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnamurthy%2C%20Akshay%20Kandasamy%2C%20Kirthevasan%20Poczos%2C%20Barnabas%20Wasserman%2C%20Larry%20Nonparametric%20estimation%20of%20Renyi%20divergence%20and%20friends%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishnamurthy%2C%20Akshay%20Kandasamy%2C%20Kirthevasan%20Poczos%2C%20Barnabas%20Wasserman%2C%20Larry%20Nonparametric%20estimation%20of%20Renyi%20divergence%20and%20friends%202014"
        },
        {
            "id": "36",
            "entry": "[36] Smita Krishnaswamy, Matthew H Spitzer, Michael Mingueneau, Sean C Bendall, Oren Litvin, Erica Stone, Dana Pe\u2019er, and Garry P Nolan. Conditional density-based analysis of t cell signaling in single-cell data. Science, 346(6213):1250689, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conditional%20density-based%20analysis%20of%20t%20cell%20signaling%20in%20single-cell%20data%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conditional%20density-based%20analysis%20of%20t%20cell%20signaling%20in%20single-cell%20data%202014"
        },
        {
            "id": "37",
            "entry": "[37] Beatrice Laurent. Efficient estimation of integral functionals of a density. The Annals of Statistics, 24(2):659\u2013681, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Laurent%2C%20Beatrice%20Efficient%20estimation%20of%20integral%20functionals%20of%20a%20density%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Laurent%2C%20Beatrice%20Efficient%20estimation%20of%20integral%20functionals%20of%20a%20density%201996"
        },
        {
            "id": "38",
            "entry": "[38] Oleg Lepski, Arkady Nemirovski, and Vladimir Spokoiny. On estimation of the Lr norm of a regression function. Probability theory and related fields, 113(2):221\u2013253, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lepski%2C%20Oleg%20Nemirovski%2C%20Arkady%20Spokoiny%2C%20Vladimir%20On%20estimation%20of%20the%20Lr%20norm%20of%20a%20regression%20function%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lepski%2C%20Oleg%20Nemirovski%2C%20Arkady%20Spokoiny%2C%20Vladimir%20On%20estimation%20of%20the%20Lr%20norm%20of%20a%20regression%20function%201999"
        },
        {
            "id": "39",
            "entry": "[39] Oleg V Lepski. On problems of adaptive estimation in white gaussian noise. Topics in nonparametric estimation, 12:87\u2013106, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lepski%2C%20Oleg%20V.%20On%20problems%20of%20adaptive%20estimation%20in%20white%20gaussian%20noise%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lepski%2C%20Oleg%20V.%20On%20problems%20of%20adaptive%20estimation%20in%20white%20gaussian%20noise%201992"
        },
        {
            "id": "40",
            "entry": "[40] Boris Ya Levit. Asymptotically efficient estimation of nonlinear functionals. Problemy Peredachi Informatsii, 14(3):65\u201372, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levit%2C%20Boris%20Ya%20Asymptotically%20efficient%20estimation%20of%20nonlinear%20functionals%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levit%2C%20Boris%20Ya%20Asymptotically%20efficient%20estimation%20of%20nonlinear%20functionals%201978"
        },
        {
            "id": "41",
            "entry": "[41] Pan Li and Olgica Milenkovic. Inhomogoenous hypergraph clustering with applications. arXiv preprint arXiv:1709.01249, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01249"
        },
        {
            "id": "42",
            "entry": "[42] YP Mack and Murray Rosenblatt. Multivariate k-nearest neighbor density estimates. Journal of Multivariate Analysis, 9(1):1\u201315, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mack%2C%20Y.P.%20Rosenblatt%2C%20Murray%20Multivariate%20k-nearest%20neighbor%20density%20estimates%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mack%2C%20Y.P.%20Rosenblatt%2C%20Murray%20Multivariate%20k-nearest%20neighbor%20density%20estimates%201979"
        },
        {
            "id": "43",
            "entry": "[43] Rajarshi Mukherjee, Whitney K Newey, and James M Robins. Semiparametric efficient empirical higher order influence function estimators. arXiv preprint arXiv:1705.07577, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07577"
        },
        {
            "id": "44",
            "entry": "[44] Rajarshi Mukherjee, Eric Tchetgen Tchetgen, and James Robins. On adaptive estimation of nonparametric functionals. arXiv preprint arXiv:1608.01364, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.01364"
        },
        {
            "id": "45",
            "entry": "[45] Rajarshi Mukherjee, Eric Tchetgen Tchetgen, and James Robins. Lepski\u2019s method and adaptive estimation of nonlinear integral functionals of density. arXiv preprint arXiv:1508.00249, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.00249"
        },
        {
            "id": "46",
            "entry": "[46] A. C. Muller, S. Nowozin, and C. H. Lampert. Information theoretic clustering using minimum spanning trees. Springer, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muller%2C%20A.C.%20Nowozin%2C%20S.%20Lampert%2C%20C.H.%20Information%20theoretic%20clustering%20using%20minimum%20spanning%20trees%202012"
        },
        {
            "id": "47",
            "entry": "[47] Arkadi Nemirovski. Topics in non-parametric. Ecole d\u2019Etede Probabilites de Saint-Flour, 28:85, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovski%2C%20Arkadi%20Topics%20in%20non-parametric.%20Ecole%20d%E2%80%99Etede%20Probabilites%20de%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nemirovski%2C%20Arkadi%20Topics%20in%20non-parametric.%20Ecole%20d%E2%80%99Etede%20Probabilites%20de%202000"
        },
        {
            "id": "48",
            "entry": "[48] H. Peng, F. Long, and C. Ding. Feature selection based on mutual information criteria of maxdependency, max-relevance, and min-redundancy. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27(8):1226\u20131238, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peng%2C%20H.%20Long%2C%20F.%20Ding%2C%20C.%20Feature%20selection%20based%20on%20mutual%20information%20criteria%20of%20maxdependency%2C%20max-relevance%2C%20and%20min-redundancy%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peng%2C%20H.%20Long%2C%20F.%20Ding%2C%20C.%20Feature%20selection%20based%20on%20mutual%20information%20criteria%20of%20maxdependency%2C%20max-relevance%2C%20and%20min-redundancy%202005"
        },
        {
            "id": "49",
            "entry": "[49] David N Reshef, Yakir A Reshef, Hilary K Finucane, Sharon R Grossman, Gilean McVean, Peter J Turnbaugh, Eric S Lander, Michael Mitzenmacher, and Pardis C Sabeti. Detecting novel associations in large data sets. science, 334(6062):1518\u20131524, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reshef%2C%20David%20N.%20Reshef%2C%20Yakir%20A.%20Finucane%2C%20Hilary%20K.%20Grossman%2C%20Sharon%20R.%20Eric%20S%20Lander%2C%20Michael%20Mitzenmacher%2C%20and%20Pardis%20C%20Sabeti.%20Detecting%20novel%20associations%20in%20large%20data%20sets%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reshef%2C%20David%20N.%20Reshef%2C%20Yakir%20A.%20Finucane%2C%20Hilary%20K.%20Grossman%2C%20Sharon%20R.%20Eric%20S%20Lander%2C%20Michael%20Mitzenmacher%2C%20and%20Pardis%20C%20Sabeti.%20Detecting%20novel%20associations%20in%20large%20data%20sets%202011"
        },
        {
            "id": "50",
            "entry": "[50] James Robins, Lingling Li, Rajarshi Mukherjee, Eric Tchetgen Tchetgen, and Aad van der Vaart. Higher order estimating equations for high-dimensional models. The Annals of Statistics (To Appear), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robins%2C%20James%20Li%2C%20Lingling%20Mukherjee%2C%20Rajarshi%20Tchetgen%2C%20Eric%20Tchetgen%20Higher%20order%20estimating%20equations%20for%20high-dimensional%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robins%2C%20James%20Li%2C%20Lingling%20Mukherjee%2C%20Rajarshi%20Tchetgen%2C%20Eric%20Tchetgen%20Higher%20order%20estimating%20equations%20for%20high-dimensional%20models%202016"
        },
        {
            "id": "51",
            "entry": "[51] James Robins, Lingling Li, Eric Tchetgen, and Aad van der Vaart. Higher order influence functions and minimax estimation of nonlinear functionals. In Probability and Statistics: Essays in Honor of David A. Freedman, pages 335\u2013421. Institute of Mathematical Statistics, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robins%2C%20James%20Li%2C%20Lingling%20Tchetgen%2C%20Eric%20van%20der%20Vaart%2C%20Aad%20Higher%20order%20influence%20functions%20and%20minimax%20estimation%20of%20nonlinear%20functionals%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robins%2C%20James%20Li%2C%20Lingling%20Tchetgen%2C%20Eric%20van%20der%20Vaart%2C%20Aad%20Higher%20order%20influence%20functions%20and%20minimax%20estimation%20of%20nonlinear%20functionals%202008"
        },
        {
            "id": "52",
            "entry": "[52] Shashank Singh and Barnabas Poczos. Finite-sample analysis of fixed-k nearest neighbor density functional estimators. In Advances in Neural Information Processing Systems, pages 1217\u20131225, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Shashank%20Poczos%2C%20Barnabas%20Finite-sample%20analysis%20of%20fixed-k%20nearest%20neighbor%20density%20functional%20estimators%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Shashank%20Poczos%2C%20Barnabas%20Finite-sample%20analysis%20of%20fixed-k%20nearest%20neighbor%20density%20functional%20estimators%202016"
        },
        {
            "id": "53",
            "entry": "[53] Kumar Sricharan, Raviv Raich, and Alfred O Hero. Estimation of nonlinear functionals of densities with confidence. IEEE Transactions on Information Theory, 58(7):4135\u20134159, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sricharan%2C%20Kumar%20Raich%2C%20Raviv%20Hero%2C%20Alfred%20O.%20Estimation%20of%20nonlinear%20functionals%20of%20densities%20with%20confidence%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sricharan%2C%20Kumar%20Raich%2C%20Raviv%20Hero%2C%20Alfred%20O.%20Estimation%20of%20nonlinear%20functionals%20of%20densities%20with%20confidence%202012"
        },
        {
            "id": "54",
            "entry": "[54] Eric Tchetgen, Lingling Li, James Robins, and Aad van der Vaart. Minimax estimation of the integral of a power of a density. Statistics & Probability Letters, 78(18):3307\u20133311, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tchetgen%2C%20Eric%20Li%2C%20Lingling%20Robins%2C%20James%20van%20der%20Vaart%2C%20Aad%20Minimax%20estimation%20of%20the%20integral%20of%20a%20power%20of%20a%20density%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tchetgen%2C%20Eric%20Li%2C%20Lingling%20Robins%2C%20James%20van%20der%20Vaart%2C%20Aad%20Minimax%20estimation%20of%20the%20integral%20of%20a%20power%20of%20a%20density%202008"
        },
        {
            "id": "55",
            "entry": "[55] A. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20A.%20Introduction%20to%20Nonparametric%20Estimation%202008"
        },
        {
            "id": "56",
            "entry": "[56] Alexandre B Tsybakov and EC Van der Meulen. Root-n consistent estimators of entropy for densities with unbounded support. Scandinavian Journal of Statistics, pages 75\u201383, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsybakov%2C%20Alexandre%20B.%20der%20Meulen%2C%20E.C.Van%20Root-n%20consistent%20estimators%20of%20entropy%20for%20densities%20with%20unbounded%20support%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsybakov%2C%20Alexandre%20B.%20der%20Meulen%2C%20E.C.Van%20Root-n%20consistent%20estimators%20of%20entropy%20for%20densities%20with%20unbounded%20support%201996"
        },
        {
            "id": "57",
            "entry": "[57] Bert Van Es. Estimating functionals related to a density by a class of statistics based on spacings. Scandinavian Journal of Statistics, pages 61\u201372, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Es%2C%20Bert%20Van%20Estimating%20functionals%20related%20to%20a%20density%20by%20a%20class%20of%20statistics%20based%20on%20spacings%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Es%2C%20Bert%20Van%20Estimating%20functionals%20related%20to%20a%20density%20by%20a%20class%20of%20statistics%20based%20on%20spacings%201992"
        },
        {
            "id": "58",
            "entry": "[58] G. Ver Steeg and A. Galstyan. Maximally informative hierarchical representations of highdimensional data. stat, 1050:27, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steeg%2C%20G.Ver%20Galstyan%2C%20A.%20Maximally%20informative%20hierarchical%20representations%20of%20highdimensional%20data%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steeg%2C%20G.Ver%20Galstyan%2C%20A.%20Maximally%20informative%20hierarchical%20representations%20of%20highdimensional%20data%202014"
        },
        {
            "id": "59",
            "entry": "[59] Qing Wang, Sanjeev R Kulkarni, and Sergio Verdu. Divergence estimation for multidimensional densities via k-nearest-neighbor distances. Information Theory, IEEE Transactions on, 55(5):2392\u20132405, 2009. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Qing%20Kulkarni%2C%20Sanjeev%20R.%20Verdu%2C%20Sergio%20Divergence%20estimation%20for%20multidimensional%20densities%20via%20k-nearest-neighbor%20distances%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Qing%20Kulkarni%2C%20Sanjeev%20R.%20Verdu%2C%20Sergio%20Divergence%20estimation%20for%20multidimensional%20densities%20via%20k-nearest-neighbor%20distances%202009"
        }
    ]
}
