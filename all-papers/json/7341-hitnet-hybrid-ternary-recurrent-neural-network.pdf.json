{
    "filename": "7341-hitnet-hybrid-ternary-recurrent-neural-network.pdf",
    "metadata": {
        "title": "HitNet: Hybrid Ternary Recurrent Neural Network",
        "author": "Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang, Yuan Xie",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7341-hitnet-hybrid-ternary-recurrent-neural-network.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Quantization is a promising technique to reduce the model size, memory footprint, and computational cost of neural networks for the employment on embedded devices with limited resources. Although quantization has achieved impressive success in convolutional neural networks (CNNs), it still suffers from large accuracy degradation on recurrent neural networks (RNNs), especially in the extremely lowbit cases. In this paper, we first investigate the accuracy degradation of RNNs under different quantization schemes and visualize the distribution of tensor values in the full precision models. Our observation reveals that due to the different distributions of weights and activations, different quantization methods should be used for each part. Accordingly, we propose HitNet, a hybrid ternary RNN, which bridges the accuracy gap between the full precision model and the quantized model with ternary weights and activations. In HitNet, we develop a hybrid quantization method to quantize weights and activations. Moreover, we introduce a sloping factor into the activation functions to address the error-sensitive problem, further closing the mentioned accuracy gap. We test our method on typical RNN models, such as Long-Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). Overall, HitNet can quantize RNN models into ternary values of {-1, 0, 1} and significantly outperform the state-of-the-art methods towards extremely quantized RNNs. Specifically, we improve the perplexity per word (PPW) of a ternary LSTM on Penn Tree Bank (PTB) corpus from 126 to 110.3 and a ternary GRU from 142 to 113.5."
    },
    "keywords": [
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "long short term memory",
            "url": "https://en.wikipedia.org/wiki/long_short_term_memory"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "National Science Foundations",
            "url": "https://en.wikipedia.org/wiki/National_Science_Foundation"
        }
    ],
    "highlights": [
        "Recurrent Neural Networks (RNNs) yield great results across many natural language processing applications, including speech recognition, machine translation, language modeling, and question answering [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "In order to observe how uniform quantization affects the accuracy of Recurrent Neural Networks, we evaluate a 2-bit quantized Long-Short-Term Memory on Penn Tree Bank and the results are shown in Table 1(a)",
        "We evaluate the effectiveness of the proposed HitNet on language modeling with two typical Recurrent Neural Networks models (LSTM [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] and Gated Recurrent Units [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>])",
        "We introduce HitNet, a hybrid ternary quantization method on Recurrent Neural Networks to significantly bridge the accuracy gap between the full-precision model and the quantized model, outperforming existing quantization methods",
        "Our observation reveals that the distributions of weights and activations are completely different",
        "Our experiments on both Long-Short-Term Memory and Gated Recurrent Units models over Penn Tree Bank, Wikidata-2, and Text8 datasets demonstrate that HitNet can achieve significantly better accuracy compared to previous work, and resulting in >16x memory saving via only 3 data states and extreme compute simplification via only logic operations"
    ],
    "key_statements": [
        "Recurrent Neural Networks (RNNs) yield great results across many natural language processing applications, including speech recognition, machine translation, language modeling, and question answering [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]",
        "We propose a hybrid ternary quantization method called HitNet to significantly bridge the accuracy gap",
        "Motivated by the above analysis, we propose a hybrid ternary quantization method, termed as HitNet, which adopts different quantization methods to quantize weights and activations according to their statistical characteristics",
        "In order to observe how uniform quantization affects the accuracy of Recurrent Neural Networks, we evaluate a 2-bit quantized Long-Short-Term Memory on Penn Tree Bank and the results are shown in Table 1(a)",
        "We evaluate the effectiveness of the proposed HitNet on language modeling with two typical Recurrent Neural Networks models (LSTM [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] and Gated Recurrent Units [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>])",
        "For all Recurrent Neural Networks models in our experiments, a word embedding layer is used on the input side",
        "We introduce HitNet, a hybrid ternary quantization method on Recurrent Neural Networks to significantly bridge the accuracy gap between the full-precision model and the quantized model, outperforming existing quantization methods",
        "Our observation reveals that the distributions of weights and activations are completely different",
        "A sloping factor is introduced into the activation functions, which guides the activation distribution more bipolar and further reduces the ternarization error",
        "Our experiments on both Long-Short-Term Memory and Gated Recurrent Units models over Penn Tree Bank, Wikidata-2, and Text8 datasets demonstrate that HitNet can achieve significantly better accuracy compared to previous work, and resulting in >16x memory saving via only 3 data states and extreme compute simplification via only logic operations"
    ],
    "summary": [
        "Recurrent Neural Networks (RNNs) yield great results across many natural language processing applications, including speech recognition, machine translation, language modeling, and question answering [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>].",
        "Our HitNet addresses this problem by quantizing both weights and activations of RNNs into ternary states with impressive accuracy improvement compared to previous work.",
        "We quantize both weights and activations in RNNs to ternary values using TTQ and the evaluation results are shown in Table 1(b).",
        "In order to understand the accuracy gap between the original full precision and quantized low-bit precision with different quantization schemes, we first visualize the distributions of weights (i.e. Wxo and Who) and activations in Figure 2.",
        "Because the activations present not only bipolar distribution but with many middle data, none of the prior methods can avoid significant accuracy loss in an extreme low-bits quantization.",
        "Prior work [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] proposed a temperature factor for the sigmoid(\u00b7) function in deep belief networks to control the activation distribution for better accuracy.",
        "Based on previous analysis, we propose a hybrid ternary quantization method that uses TTQ for weight quantization and BTQ for activation quantization.",
        "As prior work[<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] pointed out that a properly small \u03bb (\u03bb < 1) can scale the propagated gradients solve the gradient vanish problem to some extent to improve the accuracy; whereas, too small \u03bb will lead to extreme bipolar activation distribution that harms the model expressive power.",
        "Our hybrid ternary quantization method using TTQ for weights and BTQ for activations can improve the accuracy of the quantized model even without introducing the sloping factor (i.e. \u03bb=1) compared to previous schemes.",
        "Compared to these existing studies, HitNet achieves much better accuracy even using one less state for quantizing both the weights and activations.",
        "HitNet adopts hybrid quantization methods for different data types to achieve better accuracy, providing a good match between the data distribution and the quantization scheme.",
        "We introduce HitNet, a hybrid ternary quantization method on RNNs to significantly bridge the accuracy gap between the full-precision model and the quantized model, outperforming existing quantization methods.",
        "In HitNet, we quantize RNN models into ternary values {-1, 0, 1} by using TTQ for weight quantization and BTQ (Bernoulli ternary quantization) for activation quantization.",
        "Our experiments on both LSTM and GRU models over PTB, Wikidata-2, and Text8 datasets demonstrate that HitNet can achieve significantly better accuracy compared to previous work, and resulting in >16x memory saving via only 3 data states and extreme compute simplification via only logic operations."
    ],
    "headline": "We propose HitNet, a hybrid ternary Recurrent Neural Networks, which bridges the accuracy gap between the full precision model and the quantized model with ternary weights and activations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp. 6645\u20136649, IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20A.%20Mohamed%2C%20A.-r%20Hinton%2C%20G.%20%E2%80%9CSpeech%20recognition%20with%20deep%20recurrent%20neural%20networks%2C%E2%80%9D%20in%20Acoustics%2C%20speech%20and%20signal%20processing%202013"
        },
        {
            "id": "2",
            "entry": "[2] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "3",
            "entry": "[3] T. Mikolov, M. Karafi\u00e1t, L. Burget, J. Cernocky, and S. Khudanpur, \u201cRecurrent neural network based language model,\u201d in Eleventh Annual Conference of the International Speech Communication Association, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernocky%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20T.%20Karafi%C3%A1t%2C%20M.%20Burget%2C%20L.%20Cernocky%2C%20J.%20Recurrent%20neural%20network%20based%20language%20model%2C%202010"
        },
        {
            "id": "4",
            "entry": "[4] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Fifteenth annual conference of the international speech communication association, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sak%2C%20H.%20Senior%2C%20A.%20Beaufays%2C%20F.%20Long%20short-term%20memory%20recurrent%20neural%20network%20architectures%20for%20large%20scale%20acoustic%20modeling%2C%202014"
        },
        {
            "id": "5",
            "entry": "[5] M. Iyyer, J. Boyd-Graber, L. Claudino, R. Socher, and H. Daum\u00e9 III, \u201cA neural network for factoid question answering over paragraphs,\u201d in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 633\u2013644, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyyer%2C%20M.%20Boyd-Graber%2C%20J.%20Claudino%2C%20L.%20Socher%2C%20R.%20Daum%C3%A9%20III%2C%20%E2%80%9CA%20neural%20network%20for%20factoid%20question%20answering%20over%20paragraphs%2C%E2%80%9D%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iyyer%2C%20M.%20Boyd-Graber%2C%20J.%20Claudino%2C%20L.%20Socher%2C%20R.%20Daum%C3%A9%20III%2C%20%E2%80%9CA%20neural%20network%20for%20factoid%20question%20answering%20over%20paragraphs%2C%E2%80%9D%202014"
        },
        {
            "id": "6",
            "entry": "[6] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%2C%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%2C%201997"
        },
        {
            "id": "7",
            "entry": "[7] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1406.1078"
        },
        {
            "id": "8",
            "entry": "[8] T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran, \u201cLow-rank matrix factorization for deep neural network training with high-dimensional output targets,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 6655\u20136659, IEEE, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sainath%2C%20T.N.%20Kingsbury%2C%20B.%20Sindhwani%2C%20V.%20Arisoy%2C%20E.%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sainath%2C%20T.N.%20Kingsbury%2C%20B.%20Sindhwani%2C%20V.%20Arisoy%2C%20E.%20Low-rank%20matrix%20factorization%20for%20deep%20neural%20network%20training%20with%20high-dimensional%20output%20targets%2C%202013"
        },
        {
            "id": "9",
            "entry": "[9] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally, \u201cEie: efficient inference engine on compressed deep neural network,\u201d in Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on, pp. 243\u2013254, IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20S.%20Liu%2C%20X.%20Mao%2C%20H.%20Pu%2C%20J.%20Eie%3A%20efficient%20inference%20engine%20on%20compressed%20deep%20neural%20network%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20S.%20Liu%2C%20X.%20Mao%2C%20H.%20Pu%2C%20J.%20Eie%3A%20efficient%20inference%20engine%20on%20compressed%20deep%20neural%20network%2C%202016"
        },
        {
            "id": "10",
            "entry": "[10] S. Han, J. Kang, H. Mao, Y. Hu, X. Li, Y. Li, D. Xie, H. Luo, S. Yao, Y. Wang, et al., \u201cEse: Efficient speech recognition engine with sparse lstm on fpga,\u201d in Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pp. 75\u201384, ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20S.%20Kang%2C%20J.%20Mao%2C%20H.%20Hu%2C%20Y.%20Ese%3A%20Efficient%20speech%20recognition%20engine%20with%20sparse%20lstm%20on%20fpga%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20S.%20Kang%2C%20J.%20Mao%2C%20H.%20Hu%2C%20Y.%20Ese%3A%20Efficient%20speech%20recognition%20engine%20with%20sparse%20lstm%20on%20fpga%2C%202017"
        },
        {
            "id": "11",
            "entry": "[11] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, \u201cLearning structured sparsity in deep neural networks,\u201d in Advances in Neural Information Processing Systems, pp. 2074\u20132082, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20W.%20Wu%2C%20C.%20Wang%2C%20Y.%20Chen%2C%20Y.%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20W.%20Wu%2C%20C.%20Wang%2C%20Y.%20Chen%2C%20Y.%20Learning%20structured%20sparsity%20in%20deep%20neural%20networks%2C%202016"
        },
        {
            "id": "12",
            "entry": "[12] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio, \u201cBinarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1,\u201d arXiv preprint arXiv:1602.02830, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.02830"
        },
        {
            "id": "13",
            "entry": "[13] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, \u201cQuantized neural networks: Training neural networks with low precision weights and activations,\u201d arXiv preprint arXiv:1609.07061, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07061"
        },
        {
            "id": "14",
            "entry": "[14] J. Ott, Z. Lin, Y. Zhang, S.-C. Liu, and Y. Bengio, \u201cRecurrent neural networks with limited numerical precision,\u201d arXiv preprint arXiv:1608.06902, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06902"
        },
        {
            "id": "15",
            "entry": "[15] C. Z. Tang and H. K. Kwan, \u201cMultilayer feedforward neural networks with single powers-of-two weights,\u201d IEEE Transactions on Signal Processing, vol. 41, no. 8, pp. 2724\u20132727, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20C.Z.%20Kwan%2C%20H.K.%20Multilayer%20feedforward%20neural%20networks%20with%20single%20powers-of-two%20weights%2C%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20C.Z.%20Kwan%2C%20H.K.%20Multilayer%20feedforward%20neural%20networks%20with%20single%20powers-of-two%20weights%2C%201993"
        },
        {
            "id": "16",
            "entry": "[16] S. Han, H. Mao, and W. J. Dally, \u201cDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding,\u201d arXiv preprint arXiv:1510.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.00149"
        },
        {
            "id": "17",
            "entry": "[17] Y. van de Burgt, E. Lubberman, E. J. Fuller, S. T. Keene, G. C. Faria, S. Agarwal, M. J. Marinella, A. A. Talin, and A. Salleo, \u201cA non-volatile organic electrochemical device as a low-voltage artificial synapse for neuromorphic computing,\u201d Nature materials, vol. 16, no. 4, p. 414, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20de%20Burgt%2C%20Y.%20Lubberman%2C%20E.%20Fuller%2C%20E.J.%20Keene%2C%20S.T.%20A%20non-volatile%20organic%20electrochemical%20device%20as%20a%20low-voltage%20artificial%20synapse%20for%20neuromorphic%20computing%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20de%20Burgt%2C%20Y.%20Lubberman%2C%20E.%20Fuller%2C%20E.J.%20Keene%2C%20S.T.%20A%20non-volatile%20organic%20electrochemical%20device%20as%20a%20low-voltage%20artificial%20synapse%20for%20neuromorphic%20computing%2C%202017"
        },
        {
            "id": "18",
            "entry": "[18] M. Horowitz, \u201c1.1 computing\u2019s energy problem (and what we can do about it),\u201d in Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 2014 IEEE International, pp. 10\u201314, IEEE, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Horowitz%2011%20computings%20energy%20problem%20and%20what%20we%20can%20do%20about%20it%20in%20SolidState%20Circuits%20Conference%20Digest%20of%20Technical%20Papers%20ISSCC%202014%20IEEE%20International%20pp%201014%20IEEE%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Horowitz%2011%20computings%20energy%20problem%20and%20what%20we%20can%20do%20about%20it%20in%20SolidState%20Circuits%20Conference%20Digest%20of%20Technical%20Papers%20ISSCC%202014%20IEEE%20International%20pp%201014%20IEEE%202014"
        },
        {
            "id": "19",
            "entry": "[19] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, \u201cDorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients,\u201d arXiv preprint arXiv:1606.06160, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.06160"
        },
        {
            "id": "20",
            "entry": "[20] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, \u201cXnor-net: Imagenet classification using binary convolutional neural networks,\u201d in European Conference on Computer Vision, pp. 525\u2013542, Springer, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20M.%20Ordonez%2C%20V.%20Redmon%2C%20J.%20Farhadi%2C%20A.%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rastegari%2C%20M.%20Ordonez%2C%20V.%20Redmon%2C%20J.%20Farhadi%2C%20A.%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%2C%202016"
        },
        {
            "id": "21",
            "entry": "[21] F. Li, B. Zhang, and B. Liu, \u201cTernary weight networks,\u201d arXiv preprint arXiv:1605.04711, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.04711"
        },
        {
            "id": "22",
            "entry": "[22] C. Zhu, S. Han, H. Mao, and W. J. Dally, \u201cTrained ternary quantization,\u201d arXiv preprint arXiv:1612.01064, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.01064"
        },
        {
            "id": "23",
            "entry": "[23] L. Deng, P. Jiao, J. Pei, Z. Wu, and G. Li, \u201cGxnor-net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework,\u201d Neural Networks, vol. 100, pp. 49\u201358, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20L.%20Jiao%2C%20P.%20Pei%2C%20J.%20Wu%2C%20Z.%20Gxnor-net%3A%20Training%20deep%20neural%20networks%20with%20ternary%20weights%20and%20activations%20without%20full-precision%20memory%20under%20a%20unified%20discretization%20framework%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20L.%20Jiao%2C%20P.%20Pei%2C%20J.%20Wu%2C%20Z.%20Gxnor-net%3A%20Training%20deep%20neural%20networks%20with%20ternary%20weights%20and%20activations%20without%20full-precision%20memory%20under%20a%20unified%20discretization%20framework%2C%202018"
        },
        {
            "id": "24",
            "entry": "[24] Q. He, H. Wen, S. Zhou, Y. Wu, C. Yao, X. Zhou, and Y. Zou, \u201cEffective quantization methods for recurrent neural networks,\u201d arXiv preprint arXiv:1611.10176, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.10176"
        },
        {
            "id": "25",
            "entry": "[25] C. Xu, J. Yao, Z. Lin, W. Ou, Y. Cao, Z. Wang, and H. Zha, \u201cAlternating multi-bit quantization for recurrent neural networks,\u201d arXiv preprint arXiv:1802.00150, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.00150"
        },
        {
            "id": "26",
            "entry": "[26] S.-C. Zhou, Y.-Z. Wang, H. Wen, Q.-Y. He, and Y.-H. Zou, \u201cBalanced quantization: An effective and efficient approach to quantized neural networks,\u201d Journal of Computer Science and Technology, vol. 32, no. 4, pp. 667\u2013682, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20S.-C.%20Wang%2C%20Y.-Z.%20Wen%2C%20H.%20He%2C%20Q.-Y.%20Balanced%20quantization%3A%20An%20effective%20and%20efficient%20approach%20to%20quantized%20neural%20networks%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20S.-C.%20Wang%2C%20Y.-Z.%20Wen%2C%20H.%20He%2C%20Q.-Y.%20Balanced%20quantization%3A%20An%20effective%20and%20efficient%20approach%20to%20quantized%20neural%20networks%2C%202017"
        },
        {
            "id": "27",
            "entry": "[27] Y. Guo, A. Yao, H. Zhao, and Y. Chen, \u201cNetwork sketching: Exploiting binary structure in deep cnns,\u201d arXiv preprint arXiv:1706.02021, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02021"
        },
        {
            "id": "28",
            "entry": "[28] S. Kapur, A. Mishra, and D. Marr, \u201cLow precision rnns: Quantizing rnns without losing accuracy,\u201d arXiv preprint arXiv:1710.07706, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.07706"
        },
        {
            "id": "29",
            "entry": "[29] Y. Bengio, N. L\u00e9onard, and A. Courville, \u201cEstimating or propagating gradients through stochastic neurons for conditional computation,\u201d arXiv preprint arXiv:1308.3432, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "30",
            "entry": "[30] A. Taylor, M. Marcus, and B. Santorini, \u201cThe penn treebank: an overview,\u201d in Treebanks, pp. 5\u201322, Springer, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%2C%20A.%20Marcus%2C%20M.%20Santorini%2C%20B.%20%E2%80%9CThe%20penn%20treebank%3A%20an%20overview%2C%E2%80%9D%20in%20Treebanks%202003"
        },
        {
            "id": "31",
            "entry": "[31] R. M. Gray and D. L. Neuhoff, \u201cQuantization,\u201d IEEE transactions on information theory, vol. 44, no. 6, pp. 2325\u20132383, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gray%2C%20R.M.%20Neuhoff%2C%20D.L.%20Quantization%2C%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gray%2C%20R.M.%20Neuhoff%2C%20D.L.%20Quantization%2C%201998"
        },
        {
            "id": "32",
            "entry": "[32] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, \u201cDeep learning with limited numerical precision,\u201d in International Conference on Machine Learning, pp. 1737\u20131746, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gupta%2C%20S.%20Agrawal%2C%20A.%20Gopalakrishnan%2C%20K.%20Narayanan%2C%20P.%20Deep%20learning%20with%20limited%20numerical%20precision%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gupta%2C%20S.%20Agrawal%2C%20A.%20Gopalakrishnan%2C%20K.%20Narayanan%2C%20P.%20Deep%20learning%20with%20limited%20numerical%20precision%2C%202015"
        },
        {
            "id": "33",
            "entry": "[33] G. Li, L. Deng, Y. Xu, C. Wen, W. Wang, J. Pei, and L. Shi, \u201cTemperature based restricted boltzmann machines,\u201d Scientific reports, vol. 6, p. 19133, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20G.%20Deng%2C%20L.%20Xu%2C%20Y.%20Wen%2C%20C.%20Temperature%20based%20restricted%20boltzmann%20machines%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20G.%20Deng%2C%20L.%20Xu%2C%20Y.%20Wen%2C%20C.%20Temperature%20based%20restricted%20boltzmann%20machines%2C%202016"
        },
        {
            "id": "34",
            "entry": "[34] T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. Ranzato, \u201cLearning longer memory in recurrent neural networks,\u201d arXiv preprint arXiv:1412.7753, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.7753"
        },
        {
            "id": "35",
            "entry": "[35] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and A. Moshovos, \u201cCnvlutin: Ineffectualneuron-free deep neural network computing,\u201d in ACM SIGARCH Computer Architecture News, vol. 44, pp. 1\u201313, IEEE Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Albericio%2C%20J.%20Judd%2C%20P.%20Hetherington%2C%20T.%20Aamodt%2C%20T.%20%E2%80%9CCnvlutin%3A%20Ineffectualneuron-free%20deep%20neural%20network%20computing%2C%E2%80%9D%20in%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Albericio%2C%20J.%20Judd%2C%20P.%20Hetherington%2C%20T.%20Aamodt%2C%20T.%20%E2%80%9CCnvlutin%3A%20Ineffectualneuron-free%20deep%20neural%20network%20computing%2C%E2%80%9D%20in%202016"
        },
        {
            "id": "36",
            "entry": "[36] S. Zhang, Z. Du, L. Zhang, H. Lan, S. Liu, L. Li, Q. Guo, T. Chen, and Y. Chen, \u201cCambricon-x: An accelerator for sparse neural networks,\u201d in Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on, pp. 1\u201312, IEEE, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20S.%20Du%2C%20Z.%20Zhang%2C%20L.%20Lan%2C%20H.%20Cambricon-x%3A%20An%20accelerator%20for%20sparse%20neural%20networks%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20S.%20Du%2C%20Z.%20Zhang%2C%20L.%20Lan%2C%20H.%20Cambricon-x%3A%20An%20accelerator%20for%20sparse%20neural%20networks%2C%202016"
        }
    ]
}
