{
    "filename": "7633-convergence-of-cubic-regularization-for-nonconvex-optimization-under-kl-property.pdf",
    "metadata": {
        "title": "Convergence of Cubic Regularization for Nonconvex Optimization under KL Property",
        "author": "Yi Zhou, Zhe Wang, Yingbin Liang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7633-convergence-of-cubic-regularization-for-nonconvex-optimization-under-kl-property.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Cubic-regularized Newton\u2019s method (CR) is a popular algorithm that guarantees to produce a second-order stationary solution for solving nonconvex optimization problems. However, existing understandings of the convergence rate of CR are conditioned on special types of geometrical properties of the objective function. In this paper, we explore the asymptotic convergence rate of CR by exploiting the ubiquitous Kurdyka-\u0141ojasiewicz (K\u0141) property of nonconvex objective functions. In specific, we characterize the asymptotic convergence rate of various types of optimality measures for CR including function value gap, variable distance gap, gradient norm and least eigenvalue of the Hessian matrix. Our results fully characterize the diverse convergence behaviors of these optimality measures in the full parameter regime of the K\u0141 property. Moreover, we show that the obtained asymptotic convergence rates of CR are order-wise faster than those of first-order gradient descent algorithms under the K\u0141 property."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "linear convergence",
            "url": "https://en.wikipedia.org/wiki/linear_convergence"
        },
        {
            "term": "phase retrieval",
            "url": "https://en.wikipedia.org/wiki/phase_retrieval"
        },
        {
            "term": "second order",
            "url": "https://en.wikipedia.org/wiki/second_order"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        }
    ],
    "highlights": [
        "A majority of machine learning applications are naturally formulated as nonconvex optimization due to the complex mechanism of the underlying model",
        "Cubic regularization: cubic regularization algorithm was first proposed in <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak (2006</a>), in which the authors analyzed the convergence of cubic regularization to second-order stationary points for nonconvex optimization",
        "We provide a detailed comparison between the two algorithmic dynamics in Table 1 for illustration, where Lgrad corresponds to the Lipschitz parameter of \u2207f for gradient descent, L is the Lipschitz parameter for Hessian and we choose M = L for cubic regularization for simple illustration",
        "We explore the convergence rates of the gradient norm and the least eigenvalue of the Hessian along the iterates generated by cubic regularization under the K\u0141 property",
        "We explore the asymptotic convergence rates of the cubic regularization algorithm under the K\u0141 property of the nonconvex objective function, and establish the convergence rates of function value gap, iterate distance and second-order stationary gap for cubic regularization",
        "Our results show that the convergence behavior of cubic regularization ranges from sub-linear convergence to super-linear convergence depending on the parameter of the underlying K\u0141 geometry, and the obtained convergence rates are order-wise improved compared to those of first-order algorithms under the K\u0141 property"
    ],
    "key_statements": [
        "A majority of machine learning applications are naturally formulated as nonconvex optimization due to the complex mechanism of the underlying model",
        "The K\u0141 error bound further leads to characterization of the convergence rate of distance between the variable and the solution set, which ranges from sub-linearly to super-linearly depending on the parameter \u03b8 associated with the K\u0141 property",
        "Cubic regularization: cubic regularization algorithm was first proposed in <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak (2006</a>), in which the authors analyzed the convergence of cubic regularization to second-order stationary points for nonconvex optimization",
        "We provide a detailed comparison between the two algorithmic dynamics in Table 1 for illustration, where Lgrad corresponds to the Lipschitz parameter of \u2207f for gradient descent, L is the Lipschitz parameter for Hessian and we choose M = L for cubic regularization for simple illustration",
        "We explore the convergence rates of the gradient norm and the least eigenvalue of the Hessian along the iterates generated by cubic regularization under the K\u0141 property",
        "We explore the asymptotic convergence rates of the cubic regularization algorithm under the K\u0141 property of the nonconvex objective function, and establish the convergence rates of function value gap, iterate distance and second-order stationary gap for cubic regularization",
        "Our results show that the convergence behavior of cubic regularization ranges from sub-linear convergence to super-linear convergence depending on the parameter of the underlying K\u0141 geometry, and the obtained convergence rates are order-wise improved compared to those of first-order algorithms under the K\u0141 property"
    ],
    "summary": [
        "A majority of machine learning applications are naturally formulated as nonconvex optimization due to the complex mechanism of the underlying model.",
        "The pioneering work <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\"><a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\"><a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak (2006</a></a></a>) first proposed the CR algorithm and showed that the variable sequence {xk}k generated by CR has second-order stationary limit points and converges sub-linearly.",
        "We characterize the convergence rate of CR locally towards second-stationary points for nonconvex optimization under the K\u0141 property of the objective function.",
        "The K\u0141 error bound further leads to characterization of the convergence rate of distance between the variable and the solution set, which ranges from sub-linearly to super-linearly depending on the parameter \u03b8 associated with the K\u0141 property.",
        "Cubic regularization: CR algorithm was first proposed in <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\"><a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\"><a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak (2006</a></a></a>), in which the authors analyzed the convergence of CR to second-order stationary points for nonconvex optimization.",
        "It has been proved in (<a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak, 2006</a>, Theorem 2) that the function value sequence {f}k generated by CR decreases to a finite limit f, which corresponds to the function value evaluated at a certain second-order stationary point.",
        "The corresponding convergence rate has been developed in <a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\"><a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\"><a class=\"ref-link\" id=\"cNesterov_2006_a\" href=\"#rNesterov_2006_a\">Nesterov and Polyak (2006</a></a></a>) under certain gradient dominance condition of the objective function.",
        "From Theorem 2, it can be seen that the function value sequence generated by CR has diverse asymptotic convergence rates in different regimes of the K\u0141 parameter \u03b8.",
        "We can further compare the function value convergence rates of CR with those of gradient descent method <a class=\"ref-link\" id=\"cFrankel_et+al_2015_a\" href=\"#rFrankel_et+al_2015_a\">Frankel et al (2015</a>) under the K\u0141 property.",
        "We note that the convergence of {xk}k to a second-order stationary point is established for CR in <a class=\"ref-link\" id=\"cYue_et+al_2018_a\" href=\"#rYue_et+al_2018_a\"><a class=\"ref-link\" id=\"cYue_et+al_2018_a\" href=\"#rYue_et+al_2018_a\">Yue et al (2018</a></a>), but under the special error bound condition, whereas we establish the convergence of {xk}k under the K\u0141 property that holds for general nonconvex functions.",
        "As the K\u0141 property generalizes the gradient dominance condition, it implies a much more general spectrum of the geometry that includes the error bound in <a class=\"ref-link\" id=\"cYue_et+al_2018_a\" href=\"#rYue_et+al_2018_a\"><a class=\"ref-link\" id=\"cYue_et+al_2018_a\" href=\"#rYue_et+al_2018_a\">Yue et al (2018</a></a>) as a special case.",
        "We explore the asymptotic convergence rates of the CR algorithm under the K\u0141 property of the nonconvex objective function, and establish the convergence rates of function value gap, iterate distance and second-order stationary gap for CR.",
        "Our results show that the convergence behavior of CR ranges from sub-linear convergence to super-linear convergence depending on the parameter of the underlying K\u0141 geometry, and the obtained convergence rates are order-wise improved compared to those of first-order algorithms under the K\u0141 property.",
        "It is interesting to study the convergence of other computationally efficient variants of the CR algorithm such as stochastic variance-reduced CR under the K\u0141 property in nonconvex optimization"
    ],
    "headline": "We explore the asymptotic convergence rate of cubic regularization by exploiting the ubiquitous Kurdyka-\u0141ojasiewicz  property of nonconvex objective functions",
    "reference_links": [
        {
            "id": "Agarwal_et+al_2017_a",
            "entry": "Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., and Ma, T. (2017). Finding approximate local minima faster than gradient descent. In Proc. 49th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1195\u20131199.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20N.%20Allen-Zhu%2C%20Z.%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017"
        },
        {
            "id": "Allen-Zhu_2017_a",
            "entry": "Allen-Zhu, Z. (2017). Natasha 2: Faster Non-Convex Optimization Than SGD. ArXiv:1708.08694v3.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694v3"
        },
        {
            "id": "Attouch_2009_a",
            "entry": "Attouch, H. and Bolte, J. (2009). On the convergence of the proximal algorithm for nonsmooth functions involving analytic features. Mathematical Programming, 116(1-2):5\u201316.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Attouch%2C%20H.%20Bolte%2C%20J.%20On%20the%20convergence%20of%20the%20proximal%20algorithm%20for%20nonsmooth%20functions%20involving%20analytic%20features%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Attouch%2C%20H.%20Bolte%2C%20J.%20On%20the%20convergence%20of%20the%20proximal%20algorithm%20for%20nonsmooth%20functions%20involving%20analytic%20features%202009"
        },
        {
            "id": "Baldi_1989_a",
            "entry": "Baldi, P. and Hornik, K. (1989). Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2(1):53 \u2013 58.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baldi%2C%20P.%20Hornik%2C%20K.%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baldi%2C%20P.%20Hornik%2C%20K.%20Neural%20networks%20and%20principal%20component%20analysis%3A%20Learning%20from%20examples%20without%20local%20minima%201989"
        },
        {
            "id": "Bhojanapalli_et+al_2016_a",
            "entry": "Bhojanapalli, S., Neyshabur, B., and Srebro, N. (2016). Global optimality of local search for low rank matrix recovery. In Proc. Advances in Neural Information Processing Systems (NIPS).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhojanapalli%2C%20S.%20Neyshabur%2C%20B.%20Srebro%2C%20N.%20Global%20optimality%20of%20local%20search%20for%20low%20rank%20matrix%20recovery%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bhojanapalli%2C%20S.%20Neyshabur%2C%20B.%20Srebro%2C%20N.%20Global%20optimality%20of%20local%20search%20for%20low%20rank%20matrix%20recovery%202016"
        },
        {
            "id": "Bolte_et+al_2007_a",
            "entry": "Bolte, J., Daniilidis, A., and Lewis, A. (2007). The \u0141ojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems. SIAM Journal on Optimization, 17:1205\u20131223.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bolte%2C%20J.%20Daniilidis%2C%20A.%20Lewis%2C%20A.%20The%20%C5%81ojasiewicz%20inequality%20for%20nonsmooth%20subanalytic%20functions%20with%20applications%20to%20subgradient%20dynamical%20systems%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bolte%2C%20J.%20Daniilidis%2C%20A.%20Lewis%2C%20A.%20The%20%C5%81ojasiewicz%20inequality%20for%20nonsmooth%20subanalytic%20functions%20with%20applications%20to%20subgradient%20dynamical%20systems%202007"
        },
        {
            "id": "Bolte_et+al_2014_a",
            "entry": "Bolte, J., Sabach, S., and Teboulle, M. (2014). Proximal alternating linearized minimization for nonconvex and nonsmooth problems. Mathematical Programming, 146(1-2):459\u2013494.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bolte%2C%20J.%20Sabach%2C%20S.%20Teboulle%2C%20M.%20Proximal%20alternating%20linearized%20minimization%20for%20nonconvex%20and%20nonsmooth%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bolte%2C%20J.%20Sabach%2C%20S.%20Teboulle%2C%20M.%20Proximal%20alternating%20linearized%20minimization%20for%20nonconvex%20and%20nonsmooth%20problems%202014"
        },
        {
            "id": "Cand_et+al_2015_a",
            "entry": "Cand\u00e8s, E. J., Li, X., and Soltanolkotabi, M. (2015). Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985\u20132007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20E.J.%20Li%2C%20X.%20Soltanolkotabi%2C%20M.%20Phase%20retrieval%20via%20wirtinger%20flow%3A%20Theory%20and%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20E.J.%20Li%2C%20X.%20Soltanolkotabi%2C%20M.%20Phase%20retrieval%20via%20wirtinger%20flow%3A%20Theory%20and%20algorithms%202015"
        },
        {
            "id": "Carmon_2016_a",
            "entry": "Carmon, Y. and Duchi, J. C. (2016). Gradient descent efficiently finds the cubic-regularized nonconvex Newton step. ArXiv: 1612.00547.",
            "arxiv_url": "https://arxiv.org/pdf/1612.00547"
        },
        {
            "id": "Cartis_et+al_2011_a",
            "entry": "Cartis, C., Gould, N. I. M., and Toint, P. (2011a). Adaptive cubic regularization methods for unconstrained optimization. part ii: worst-case functionand derivative-evaluation complexity. Mathematical Programming, 130(2):295\u2013319.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.I.M.%20Toint%2C%20P.%20Adaptive%20cubic%20regularization%20methods%20for%20unconstrained%20optimization.%20part%20ii%3A%20worst-case%20functionand%20derivative-evaluation%20complexity%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.I.M.%20Toint%2C%20P.%20Adaptive%20cubic%20regularization%20methods%20for%20unconstrained%20optimization.%20part%20ii%3A%20worst-case%20functionand%20derivative-evaluation%20complexity%202011"
        },
        {
            "id": "Cartis_et+al_2011_b",
            "entry": "Cartis, C., Gould, N. I. M., and Toint, P. L. (2011b). Adaptive cubic regularization methods for unconstrained optimization. part i : motivation, convergence and numerical results. Mathematical Programming.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Adaptive%20cubic%20regularization%20methods%20for%20unconstrained%20optimization.%20part%20i%20%3A%20motivation%2C%20convergence%20and%20numerical%20results.%20Mathematical%20Programming%202011"
        },
        {
            "id": "Frankel_et+al_2015_a",
            "entry": "Frankel, P., Garrigos, G., and Peypouquet, J. (2015). Splitting methods with variable metric for Kurdyka\u2013\u0141ojasiewicz functions and general convergence rates. Journal of Optimization Theory and Applications, 165(3):874\u2013900.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frankel%2C%20P.%20Garrigos%2C%20G.%20Peypouquet%2C%20J.%20Splitting%20methods%20with%20variable%20metric%20for%20Kurdyka%E2%80%93%C5%81ojasiewicz%20functions%20and%20general%20convergence%20rates%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frankel%2C%20P.%20Garrigos%2C%20G.%20Peypouquet%2C%20J.%20Splitting%20methods%20with%20variable%20metric%20for%20Kurdyka%E2%80%93%C5%81ojasiewicz%20functions%20and%20general%20convergence%20rates%202015"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "Ge, R., Huang, F., Jin, C., and Yuan, Y. (2015). Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In Proc. 28th Conference on Learning Theory (COLT), volume 40, pages 797\u2013842.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "Ge_et+al_2016_a",
            "entry": "Ge, R., Lee, J., and Ma, T. (2016). Matrix completion has no spurious local minimum. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 2973\u20132981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Lee%2C%20J.%20Ma%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Lee%2C%20J.%20Ma%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016"
        },
        {
            "id": "Ghadimi_et+al_2017_a",
            "entry": "Ghadimi, S., Liu, H., and Zhang, T. (2017). Second-order methods with cubic regularization under inexact information. ArXiv: 1710.05782.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05782"
        },
        {
            "id": "Goodfellow_et+al_2016_a",
            "entry": "Goodfellow, I., Y, B., and Courville, A. (2016). Deep Learning. MIT Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Y%2C%20B.%20Courville%2C%20A.%20Deep%20Learning%202016"
        },
        {
            "id": "Hartman_2002_a",
            "entry": "Hartman, P. (2002). Ordinary Differential Equations. Society for Industrial and Applied Mathematics, second edition.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartman%2C%20P.%20Ordinary%20Differential%20Equations.%20Society%20for%20Industrial%20and%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hartman%2C%20P.%20Ordinary%20Differential%20Equations.%20Society%20for%20Industrial%20and%202002"
        },
        {
            "id": "Jiang_et+al_2017_a",
            "entry": "Jiang, B., Lin, T., and Zhang, S. (2017). A unified scheme to accelerate adaptive cubic regularization and gradient methods for convex optimization. ArXiv:1710.04788.",
            "arxiv_url": "https://arxiv.org/pdf/1710.04788"
        },
        {
            "id": "Karimi_et+al_2016_a",
            "entry": "Karimi, H., Nutini, J., and Schmidt, M. (2016). Linear Convergence of Gradient and ProximalGradient Methods Under the Polyak-\u0141ojasiewicz Condition, pages 795\u2013811.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20H.%20Nutini%2C%20J.%20Schmidt%2C%20M.%20Linear%20Convergence%20of%20Gradient%20and%20ProximalGradient%20Methods%20Under%20the%20Polyak-%C5%81ojasiewicz%20Condition%202016"
        },
        {
            "id": "Kohler_2017_a",
            "entry": "Kohler, J. M. and Lucchi, A. (2017). Sub-sampled cubic regularization for non-convex optimization. In Proc. 34th International Conference on Machine Learning (ICML), volume 70, pages 1895\u2013 1904.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017"
        },
        {
            "id": "Li_et+al_2017_a",
            "entry": "Li, Q., Zhou, Y., Liang, Y., and Varshney, P. K. (2017). Convergence analysis of proximal gradient with momentum for nonconvex optimization. In Proc. 34th International Conference on Machine Learning (ICML, volume 70, pages 2111\u20132119.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Q.%20Zhou%2C%20Y.%20Liang%2C%20Y.%20Varshney%2C%20P.K.%20Convergence%20analysis%20of%20proximal%20gradient%20with%20momentum%20for%20nonconvex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Q.%20Zhou%2C%20Y.%20Liang%2C%20Y.%20Varshney%2C%20P.K.%20Convergence%20analysis%20of%20proximal%20gradient%20with%20momentum%20for%20nonconvex%20optimization%202017"
        },
        {
            "id": "Liu_2017_a",
            "entry": "Liu, M. and Yang, T. (2017). On Noisy Negative Curvature Descent: Competing with Gradient Descent for Faster Non-convex Optimization. ArXiv:1709.08571v2.",
            "arxiv_url": "https://arxiv.org/pdf/1709.08571v2"
        },
        {
            "id": "Ojasiewicz_1963_a",
            "entry": "\u0141ojasiewicz, S. (1963). A topological property of real analytic subsets. Coll. du CNRS, Les equations aux derivees partielles, page 87\u201389.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C5%81ojasiewicz%2C%20S.%20A%20topological%20property%20of%20real%20analytic%20subsets.%20Coll.%20du%20CNRS%2C%20Les%20equations%20aux%20derivees%201963"
        },
        {
            "id": "Ojasiewicz_1965_a",
            "entry": "\u0141ojasiewicz, S. (1965). Ensembles semi-analytiques. Institut des Hautes Etudes Scientifiques.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=%C5%81ojasiewicz%2C%20S.%20Ensembles%20semi-analytiques.%20Institut%20des%20Hautes%20Etudes%20Scientifiques%201965"
        },
        {
            "id": "Nesterov_2008_a",
            "entry": "Nesterov, Y. (2008). Accelerating the cubic regularization of newton\u2019s method on convex problems. Mathematical Programming, 112(1):159\u2013181.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Accelerating%20the%20cubic%20regularization%20of%20newton%E2%80%99s%20method%20on%20convex%20problems%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Accelerating%20the%20cubic%20regularization%20of%20newton%E2%80%99s%20method%20on%20convex%20problems%202008"
        },
        {
            "id": "Nesterov_2006_a",
            "entry": "Nesterov, Y. and Polyak, B. (2006). Cubic regularization of newton\u2019s method and its global performance. Mathematical Programming.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Polyak%2C%20B.%20Cubic%20regularization%20of%20newton%E2%80%99s%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Polyak%2C%20B.%20Cubic%20regularization%20of%20newton%E2%80%99s%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "Noll_2013_a",
            "entry": "Noll, D. and Rondepierre, A. (2013). Convergence of linesearch and trust-region methods using the Kurdyka\u2013\u0141ojasiewicz inequality. In Proc. Computational and Analytical Mathematics, pages 593\u2013611.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noll%2C%20D.%20Rondepierre%2C%20A.%20Convergence%20of%20linesearch%20and%20trust-region%20methods%20using%20the%20Kurdyka%E2%80%93%C5%81ojasiewicz%20inequality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noll%2C%20D.%20Rondepierre%2C%20A.%20Convergence%20of%20linesearch%20and%20trust-region%20methods%20using%20the%20Kurdyka%E2%80%93%C5%81ojasiewicz%20inequality%202013"
        },
        {
            "id": "Sun_et+al_2015_a",
            "entry": "Sun, J., Qu, Q., and Wright, J. (2015). When Are Nonconvex Problems Not Scary? ArXiv:1510.06096v2.",
            "arxiv_url": "https://arxiv.org/pdf/1510.06096v2"
        },
        {
            "id": "Sun_et+al_2017_a",
            "entry": "Sun, J., Qu, Q., and Wright, J. (2017). A geometrical analysis of phase retrieval. Foundations of Computational Mathematics, pages 1\u201368.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20A%20geometrical%20analysis%20of%20phase%20retrieval%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20A%20geometrical%20analysis%20of%20phase%20retrieval%202017"
        },
        {
            "id": "Tripuraneni_et+al_2017_a",
            "entry": "Tripuraneni, N., Stern, M., Jin, C., Regier, J., and Jordan, M. I. (2017). Stochastic Cubic Regularization for Fast Nonconvex Optimization. ArXiv: 711.02838.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tripuraneni%2C%20N.%20Stern%2C%20M.%20Jin%2C%20C.%20Regier%2C%20J.%20Stochastic%20Cubic%20Regularization%20for%20Fast%20Nonconvex%20Optimization%202017"
        },
        {
            "id": "Wang_et+al_2018_a",
            "entry": "Wang, Z., Zhou, Y., Liang, Y., and Lan, G. (2018). Sample Complexity of Stochastic VarianceReduced Cubic Regularization for Nonconvex Optimization. ArXiv:1802.07372v1.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07372v1"
        },
        {
            "id": "Xu_et+al_2017_a",
            "entry": "Xu, P., Roosta-Khorasani, F., and Mahoney, M. W. (2017). Newton-type methods for non-convex optimization under inexact hessian information. ArXiv: 1708.07164.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07164"
        },
        {
            "id": "Yue_et+al_2018_a",
            "entry": "Yue, M., Zhou, Z., and So, M. (2018). On the Quadratic Convergence of the Cubic Regularization Method under a Local Error Bound Condition. ArXiv:1801.09387v1.",
            "arxiv_url": "https://arxiv.org/pdf/1801.09387v1"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "Zhang, H., Liang, Y., and Chi, Y. (2017). A nonconvex approach for phase retrieval: Reshaped wirtinger flow and incremental algorithms. Journal of Machine Learning Research, 18(141):1\u201335.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Liang%2C%20Y.%20Chi%2C%20Y.%20A%20nonconvex%20approach%20for%20phase%20retrieval%3A%20Reshaped%20wirtinger%20flow%20and%20incremental%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Liang%2C%20Y.%20Chi%2C%20Y.%20A%20nonconvex%20approach%20for%20phase%20retrieval%3A%20Reshaped%20wirtinger%20flow%20and%20incremental%20algorithms%202017"
        },
        {
            "id": "Zhou_et+al_2018_a",
            "entry": "Zhou, D., Xu, P., and Gu, Q. (2018). Stochastic Variance-Reduced Cubic Regularized Newton Method. ArXiv:1802.04796v1.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04796v1"
        },
        {
            "id": "Zhou_2017_a",
            "entry": "Zhou, Y. and Liang, Y. (2017). Characterization of Gradient Dominance and Regularity Conditions for Neural Networks. ArXiv:1710.06910v2.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06910v2"
        },
        {
            "id": "Zhou_et+al_2018_b",
            "entry": "Zhou, Y., Yu, Y., Dai, W., Liang, Y., and Xing, E. P. (2018). Distributed Proximal Gradient Algorithm for Partially Asynchronous Computer Clusters. Journal of Machine Learning Research (JMLR), 19(19):1\u201332.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Y.%20Yu%2C%20Y.%20Dai%2C%20W.%20Liang%2C%20Y.%20Distributed%20Proximal%20Gradient%20Algorithm%20for%20Partially%20Asynchronous%20Computer%20Clusters%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Y.%20Yu%2C%20Y.%20Dai%2C%20W.%20Liang%2C%20Y.%20Distributed%20Proximal%20Gradient%20Algorithm%20for%20Partially%20Asynchronous%20Computer%20Clusters%202018"
        },
        {
            "id": "Zhou_et+al_2016_a",
            "entry": "Zhou, Y., Yu, Y., Dai, W., Liang, Y., and Xing, P. (2016a). On convergence of model parallel proximal gradient algorithm for stale synchronous parallel system. In Proc 19th International Conference on Artificial Intelligence and Statistics (AISTATS, volume 51, pages 713\u2013722.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Y.%20Yu%2C%20Y.%20Dai%2C%20W.%20Liang%2C%20Y.%20On%20convergence%20of%20model%20parallel%20proximal%20gradient%20algorithm%20for%20stale%20synchronous%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Y.%20Yu%2C%20Y.%20Dai%2C%20W.%20Liang%2C%20Y.%20On%20convergence%20of%20model%20parallel%20proximal%20gradient%20algorithm%20for%20stale%20synchronous%202016"
        },
        {
            "id": "Zhou_et+al_2016_b",
            "entry": "Zhou, Y., Zhang, H., and Liang, Y. (2016b). Geometrical properties and accelerated gradient solvers of non-convex phase retrieval. In Proc. 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 331\u2013335.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20Y.%20Zhang%2C%20H.%20Liang%2C%20Y.%20Geometrical%20properties%20and%20accelerated%20gradient%20solvers%20of%20non-convex%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20Y.%20Zhang%2C%20H.%20Liang%2C%20Y.%20Geometrical%20properties%20and%20accelerated%20gradient%20solvers%20of%20non-convex%20phase%20retrieval%202016"
        }
    ]
}
