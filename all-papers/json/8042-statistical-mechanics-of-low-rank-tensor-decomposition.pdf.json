{
    "filename": "8042-statistical-mechanics-of-low-rank-tensor-decomposition.pdf",
    "metadata": {
        "title": "Statistical mechanics of low-rank tensor decomposition",
        "author": "Jonathan Kadmon, Surya Ganguli",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8042-statistical-mechanics-of-low-rank-tensor-decomposition.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Often, large, high dimensional datasets collected across multiple modalities can be organized as a higher order tensor. Low-rank tensor decomposition then arises as a powerful and widely used tool to discover simple low dimensional structures underlying such data. However, we currently lack a theoretical understanding of the algorithmic behavior of low-rank tensor decompositions. We derive Bayesian approximate message passing (AMP) algorithms for recovering arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic mean field theory to precisely characterize their performance. Our theory reveals the existence of phase transitions between easy, hard and impossible inference regimes, and displays an excellent match with simulations. Moreover it reveals several qualitative surprises compared to the behavior of symmetric, cubic tensor decomposition. Finally, we compare our AMP algorithm to the most commonly used algorithm, alternating least squares (ALS), and demonstrate that AMP significantly outperforms ALS in the presence of noise."
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "phase transition",
            "url": "https://en.wikipedia.org/wiki/phase_transition"
        },
        {
            "term": "tensor decomposition",
            "url": "https://en.wikipedia.org/wiki/tensor_decomposition"
        },
        {
            "term": "signal-to-noise ratio",
            "url": "https://en.wikipedia.org/wiki/signal-to-noise_ratio"
        }
    ],
    "highlights": [
        "The ability to take noisy, complex data structures and decompose them into smaller, interpretable components in an unsupervised manner is essential to many fields, from machine learning and signal processing [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to neuroscience [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We derive and analyze an approximate message passing (AMP) algorithm for optimal Bayesian recovery of arbitrarily shaped, high-order low-rank tensors buried in noise",
        "We find that the low-rank decomposition problem admits two phase transitions separating three qualitatively different inference regimes: (1) the easy regime at low noise where approximate message passing works, (2) the hard regime at intermediate noise where approximate message passing fails but the ground truth tensor is still possible to recover, if not in a computationally tractable manner, and (3) the impossible regime at high noise where it is believed no algorithm can recover the ground-truth low rank tensor",
        "Our work partially bridges the gap between theory and practice by creating new approximate message passing algorithms that can flexibly assign different priors to different modes of a high-order tensor, thereby enabling approximate message passing to handle arbitrarily shaped high order tensors that occur in the wild",
        "We hope the superior performance of our flexible approximate message passing algorithms relative to alternating least squares will promote the adoption of approximate message passing in the wild",
        "Code to reproduce all simulations presented in this paper is available at https://github.com/ganguli-lab/tensorAMP"
    ],
    "key_statements": [
        "The ability to take noisy, complex data structures and decompose them into smaller, interpretable components in an unsupervised manner is essential to many fields, from machine learning and signal processing [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to neuroscience [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "In datasets that can be organized as an order 2 data matrix, many popular unsupervised structure discovery algorithms, like PCA, ICA, SVD or other spectral methods, can be unified under rubric of low rank matrix decomposition",
        "We derive and analyze an approximate message passing (AMP) algorithm for optimal Bayesian recovery of arbitrarily shaped, high-order low-rank tensors buried in noise",
        "We find that the low-rank decomposition problem admits two phase transitions separating three qualitatively different inference regimes: (1) the easy regime at low noise where approximate message passing works, (2) the hard regime at intermediate noise where approximate message passing fails but the ground truth tensor is still possible to recover, if not in a computationally tractable manner, and (3) the impossible regime at high noise where it is believed no algorithm can recover the ground-truth low rank tensor",
        "We find that in the space of all possible tensor shapes, the hard regime has the largest width along the noise axis when the shape is cubic, thereby indicating that tensor shape can have a strong effect on inference performance, and that cubic tensors have highly non-generic properties in the space of all possible tensor shapes",
        "In low-rank tensor decomposition, the statistical independence of incoming messages originates from weak pairwise interactions that scale as w \u223c N \u2212(p\u22121)/2",
        "We study the case where x\u03b1i are sampled from normal distributions with mode-dependent mean and variance P\u03b1(x) \u223c N",
        "If the initial values x\u03b10 i have sufficiently high overlap with the true factors x\u03b1i, the approximate message passing dynamics will converge to the low error fixed point; we refer to this as the informative initialization, as it requires prior knowledge about the true structure",
        "Performs well at low noise levels, but here we explore how well it compares to approximate message passing at high noise levels, in the scaling regime defined by defined by (1) and (2), where inference can be non-trivial",
        "Our work partially bridges the gap between theory and practice by creating new approximate message passing algorithms that can flexibly assign different priors to different modes of a high-order tensor, thereby enabling approximate message passing to handle arbitrarily shaped high order tensors that occur in the wild",
        "We hope the superior performance of our flexible approximate message passing algorithms relative to alternating least squares will promote the adoption of approximate message passing in the wild",
        "Code to reproduce all simulations presented in this paper is available at https://github.com/ganguli-lab/tensorAMP"
    ],
    "summary": [
        "The ability to take noisy, complex data structures and decompose them into smaller, interpretable components in an unsupervised manner is essential to many fields, from machine learning and signal processing [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to neuroscience [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "We derive and analyze an approximate message passing (AMP) algorithm for optimal Bayesian recovery of arbitrarily shaped, high-order low-rank tensors buried in noise.",
        "Given that tensors in the wild are almost never cubic, nor symmetric, to bridge the gap between theory and experiment, we go beyond prior work to derive and analyze Bayes optimal AMP algorithms for arbitrarily shaped high order and low rank tensor decomposition with different priors for different tensor modes, reflecting their different measurement types.",
        "For symmetric tensors, it was shown that the easy inference regime cannot exist, unless the prior over the low rank factor has non-zero mean.",
        "In the following we define the low-rank tensor decomposition problem and present a derivation of AMP algorithms designed to solve this problem, as well as a dynamical mean field theory analysis of their performance.",
        "In low-rank tensor decomposition, the statistical independence of incoming messages originates from weak pairwise interactions that scale as w \u223c N \u2212(p\u22121)/2.",
        "Further analytic analysis, which is the focus of this current work, and the derivation of the dynamic mean-field theory which we present below is applicable in the Bayes-optimal regime, were there is no replica-symmetry breaking, and the estimators are self-averaging.",
        "To study the performance of the algorithm defined by eq (11)-(14), we use another mean-field approximation that estimates the evolution of the inference error.",
        "If the initial values x\u03b10 i have sufficiently high overlap with the true factors x\u03b1i, the AMP dynamics will converge to the low error fixed point; we refer to this as the informative initialization, as it requires prior knowledge about the true structure.",
        "With noise levels \u2206 above \u2206dyn, the dynamic mean field equations will always converge to a high error fixed point.",
        "This difficulty was previously noted for the low-rank decomposition of symmetric tensors [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], and it was further shown there that the prior must be non-zero for the existence of an easy inference regime.",
        "The general tensor decomposition case is qualitatively different than the symmetric case in that an easy regime can exist even when a tensor mode has zero mean.",
        "Moving from cubic to non-cubic tensors lowers the minimum noise level \u2206alg at which the uninformative solution is stable, thereby extending the hard regime to the left in Fig. 2.D. 4 Bayesian AMP compared to maximum a-posteriori (MAP) methods",
        "Code to reproduce all simulations presented in this paper is available at https://github.com/ganguli-lab/tensorAMP"
    ],
    "headline": "We find that the low-rank decomposition problem admits two phase transitions separating three qualitatively different inference regimes:  the easy regime at low noise where approximate message passing works,  the hard regime at intermediate noise where approximate message passing fails but the ground truth tensor is still possible to recover, if not in a computationally tractable manner, and  the impossible regime at high noise where it is believed no algorithm can recover the ground-truth low rank tensor",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):2773\u20132832, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20decompositions%20for%20learning%20latent%20variable%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Anandkumar%2C%20Animashree%20Ge%2C%20Rong%20Hsu%2C%20Daniel%20Kakade%2C%20Sham%20M.%20Tensor%20decompositions%20for%20learning%20latent%20variable%20models%202014"
        },
        {
            "id": "2",
            "entry": "[2] Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis, and Christos Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal Processing, 65(13):3551\u20133582, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sidiropoulos%2C%20Nicholas%20D.%20Lathauwer%2C%20Lieven%20De%20Fu%2C%20Xiao%20Huang%2C%20Kejun%20Tensor%20decomposition%20for%20signal%20processing%20and%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sidiropoulos%2C%20Nicholas%20D.%20Lathauwer%2C%20Lieven%20De%20Fu%2C%20Xiao%20Huang%2C%20Kejun%20Tensor%20decomposition%20for%20signal%20processing%20and%20machine%20learning%202017"
        },
        {
            "id": "3",
            "entry": "[3] Alex H. Williams, Tony Hyun Kim, Forea Wang, Saurabh Vyas, Stephen I. Ryu, Krishna V. Shenoy, Mark Schnitzer, Tamara G. Kolda, and Surya Ganguli. Unsupervised discovery of demixed, low-dimensional neural dynamics across multiple timescales through tensor component analysis. Neuron, 98(6):1099 \u2013 1115.e8, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Alex%20H.%20Kim%2C%20Tony%20Hyun%20Wang%2C%20Forea%20Vyas%2C%20Saurabh%20Unsupervised%20discovery%20of%20demixed%2C%20low-dimensional%20neural%20dynamics%20across%20multiple%20timescales%20through%20tensor%20component%20analysis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Alex%20H.%20Kim%2C%20Tony%20Hyun%20Wang%2C%20Forea%20Vyas%2C%20Saurabh%20Unsupervised%20discovery%20of%20demixed%2C%20low-dimensional%20neural%20dynamics%20across%20multiple%20timescales%20through%20tensor%20component%20analysis%202018"
        },
        {
            "id": "4",
            "entry": "[4] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):455\u2013500, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20decompositions%20and%20applications%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20Tamara%20G.%20Bader%2C%20Brett%20W.%20Tensor%20decompositions%20and%20applications%202009"
        },
        {
            "id": "5",
            "entry": "[5] J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of \u201ceckart-young\u201d decomposition. Psychometrika, 35(3):283\u2013 319, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carroll%2C%20J.Douglas%20Chang%2C%20Jih-Jie%20Analysis%20of%20individual%20differences%20in%20multidimensional%20scaling%20via%20an%20n-way%20generalization%20of%20%E2%80%9Ceckart-young%E2%80%9D%20decomposition%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carroll%2C%20J.Douglas%20Chang%2C%20Jih-Jie%20Analysis%20of%20individual%20differences%20in%20multidimensional%20scaling%20via%20an%20n-way%20generalization%20of%20%E2%80%9Ceckart-young%E2%80%9D%20decomposition%201970"
        },
        {
            "id": "6",
            "entry": "[6] Richard A Harshman. Foundations of the parafac procedure: Models and conditions for an\" explanatory\" multimodal factor analysis. UCLA Working Papers in Phonetics, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harshman%2C%20Richard%20A.%20Foundations%20of%20the%20parafac%20procedure%3A%20Models%20and%20conditions%20for%20an%22%20explanatory%22%20multimodal%20factor%20analysis%201970",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Harshman%2C%20Richard%20A.%20Foundations%20of%20the%20parafac%20procedure%3A%20Models%20and%20conditions%20for%20an%22%20explanatory%22%20multimodal%20factor%20analysis%201970"
        },
        {
            "id": "7",
            "entry": "[7] Marc Mezard. The space of interactions in neural networks: Gardner\u2019s computation with the cavity method. Journal of Physics A: Mathematical and General, 22(12):2181, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mezard%2C%20Marc%20The%20space%20of%20interactions%20in%20neural%20networks%3A%20Gardner%E2%80%99s%20computation%20with%20the%20cavity%20method%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mezard%2C%20Marc%20The%20space%20of%20interactions%20in%20neural%20networks%3A%20Gardner%E2%80%99s%20computation%20with%20the%20cavity%20method%201989"
        },
        {
            "id": "8",
            "entry": "[8] Yoshiyuki Kabashima and Shinsuke Uda. A bp-based algorithm for performing bayesian inference in large perceptron-type networks. In International Conference on Algorithmic Learning Theory, pages 479\u2013493.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kabashima%2C%20Yoshiyuki%20Uda%2C%20Shinsuke%20A%20bp-based%20algorithm%20for%20performing%20bayesian%20inference%20in%20large%20perceptron-type%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kabashima%2C%20Yoshiyuki%20Uda%2C%20Shinsuke%20A%20bp-based%20algorithm%20for%20performing%20bayesian%20inference%20in%20large%20perceptron-type%20networks"
        },
        {
            "id": "9",
            "entry": "[9] Madhu Advani, Subhaneil Lahiri, and Surya Ganguli. Statistical mechanics of complex neural systems and high dimensional data. Journal of Statistical Mechanics: Theory and Experiment, 2013(03):P03014, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Advani%2C%20Madhu%20Lahiri%2C%20Subhaneil%20Ganguli%2C%20Surya%20Statistical%20mechanics%20of%20complex%20neural%20systems%20and%20high%20dimensional%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Advani%2C%20Madhu%20Lahiri%2C%20Subhaneil%20Ganguli%2C%20Surya%20Statistical%20mechanics%20of%20complex%20neural%20systems%20and%20high%20dimensional%20data%202013"
        },
        {
            "id": "10",
            "entry": "[10] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914\u201318919, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donoho%2C%20David%20L.%20Maleki%2C%20Arian%20Montanari%2C%20Andrea%20Message-passing%20algorithms%20for%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donoho%2C%20David%20L.%20Maleki%2C%20Arian%20Montanari%2C%20Andrea%20Message-passing%20algorithms%20for%20compressed%20sensing%202009"
        },
        {
            "id": "11",
            "entry": "[11] Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka. A typical reconstruction limit for compressed sensing based on lp-norm minimization. Journal of Statistical Mechanics: Theory and Experiment, 2009(09):L09003, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kabashima%2C%20Yoshiyuki%20Wadayama%2C%20Tadashi%20Tanaka%2C%20Toshiyuki%20A%20typical%20reconstruction%20limit%20for%20compressed%20sensing%20based%20on%20lp-norm%20minimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kabashima%2C%20Yoshiyuki%20Wadayama%2C%20Tadashi%20Tanaka%2C%20Toshiyuki%20A%20typical%20reconstruction%20limit%20for%20compressed%20sensing%20based%20on%20lp-norm%20minimization%202009"
        },
        {
            "id": "12",
            "entry": "[12] Sundeep Rangan, Vivek Goyal, and Alyson K Fletcher. Asymptotic analysis of map estimation via the replica method and compressed sensing. In Advances in Neural Information Processing Systems, pages 1545\u20131553, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rangan%2C%20Sundeep%20Goyal%2C%20Vivek%20Fletcher%2C%20Alyson%20K.%20Asymptotic%20analysis%20of%20map%20estimation%20via%20the%20replica%20method%20and%20compressed%20sensing%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rangan%2C%20Sundeep%20Goyal%2C%20Vivek%20Fletcher%2C%20Alyson%20K.%20Asymptotic%20analysis%20of%20map%20estimation%20via%20the%20replica%20method%20and%20compressed%20sensing%202009"
        },
        {
            "id": "13",
            "entry": "[13] Surya Ganguli and Haim Sompolinsky. Statistical mechanics of compressed sensing. Physical review letters, 104(18):188701, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganguli%2C%20Surya%20Sompolinsky%2C%20Haim%20Statistical%20mechanics%20of%20compressed%20sensing%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganguli%2C%20Surya%20Sompolinsky%2C%20Haim%20Statistical%20mechanics%20of%20compressed%20sensing%202010"
        },
        {
            "id": "14",
            "entry": "[14] Surya Ganguli and Haim Sompolinsky. Short-term memory in neuronal networks through dynamical compressed sensing. In Advances in neural information processing systems, pages 667\u2013675, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganguli%2C%20Surya%20Sompolinsky%2C%20Haim%20Short-term%20memory%20in%20neuronal%20networks%20through%20dynamical%20compressed%20sensing%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganguli%2C%20Surya%20Sompolinsky%2C%20Haim%20Short-term%20memory%20in%20neuronal%20networks%20through%20dynamical%20compressed%20sensing%202010"
        },
        {
            "id": "15",
            "entry": "[15] Madhu Advani and Surya Ganguli. Statistical mechanics of optimal convex inference in high dimensions. Physical Review X, 6(3):031034, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Advani%2C%20Madhu%20Ganguli%2C%20Surya%20Statistical%20mechanics%20of%20optimal%20convex%20inference%20in%20high%20dimensions%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Advani%2C%20Madhu%20Ganguli%2C%20Surya%20Statistical%20mechanics%20of%20optimal%20convex%20inference%20in%20high%20dimensions%202016"
        },
        {
            "id": "16",
            "entry": "[16] Madhu Advani and Surya Ganguli. An equivalence between high dimensional bayes optimal inference and m-estimation. In Advances in Neural Information Processing Systems, pages 3378\u20133386, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Advani%2C%20Madhu%20Ganguli%2C%20Surya%20An%20equivalence%20between%20high%20dimensional%20bayes%20optimal%20inference%20and%20m-estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Advani%2C%20Madhu%20Ganguli%2C%20Surya%20An%20equivalence%20between%20high%20dimensional%20bayes%20optimal%20inference%20and%20m-estimation%202016"
        },
        {
            "id": "17",
            "entry": "[17] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bayati%2C%20Mohsen%20Montanari%2C%20Andrea%20The%20dynamics%20of%20message%20passing%20on%20dense%20graphs%2C%20with%20applications%20to%20compressed%20sensing%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bayati%2C%20Mohsen%20Montanari%2C%20Andrea%20The%20dynamics%20of%20message%20passing%20on%20dense%20graphs%2C%20with%20applications%20to%20compressed%20sensing%202011"
        },
        {
            "id": "18",
            "entry": "[18] Sundeep Rangan and Alyson K Fletcher. Iterative estimation of constrained rank-one matrices in noise. In Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on, pages 1246\u20131250. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rangan%2C%20Sundeep%20Fletcher%2C%20Alyson%20K.%20Iterative%20estimation%20of%20constrained%20rank-one%20matrices%20in%20noise%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rangan%2C%20Sundeep%20Fletcher%2C%20Alyson%20K.%20Iterative%20estimation%20of%20constrained%20rank-one%20matrices%20in%20noise%202012"
        },
        {
            "id": "19",
            "entry": "[19] Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00e1. Mmse of probabilistic low-rank matrix estimation: Universality with respect to the output channel. In Communication, Control, and Computing (Allerton), 2015 53rd Annual Allerton Conference on, pages 680\u2013687. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lesieur%2C%20Thibault%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Mmse%20of%20probabilistic%20low-rank%20matrix%20estimation%3A%20Universality%20with%20respect%20to%20the%20output%20channel%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lesieur%2C%20Thibault%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Mmse%20of%20probabilistic%20low-rank%20matrix%20estimation%3A%20Universality%20with%20respect%20to%20the%20output%20channel%202015"
        },
        {
            "id": "20",
            "entry": "[20] Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00e1. Constrained low-rank matrix estimation: Phase transitions, approximate message passing and applications. Journal of Statistical Mechanics: Theory and Experiment, 2017(7):073403, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lesieur%2C%20Thibault%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Constrained%20low-rank%20matrix%20estimation%3A%20Phase%20transitions%2C%20approximate%20message%20passing%20and%20applications%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lesieur%2C%20Thibault%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Constrained%20low-rank%20matrix%20estimation%3A%20Phase%20transitions%2C%20approximate%20message%20passing%20and%20applications%202017"
        },
        {
            "id": "21",
            "entry": "[21] Thibault Lesieur, L\u00e9o Miolane, Marc Lelarge, Florent Krzakala, and Lenka Zdeborov\u00e1. Statistical and computational phase transitions in spiked tensor estimation. In Information Theory (ISIT), 2017 IEEE International Symposium on, pages 511\u2013515. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lesieur%2C%20Thibault%20Miolane%2C%20L%C3%A9o%20Lelarge%2C%20Marc%20Krzakala%2C%20Florent%20Statistical%20and%20computational%20phase%20transitions%20in%20spiked%20tensor%20estimation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lesieur%2C%20Thibault%20Miolane%2C%20L%C3%A9o%20Lelarge%2C%20Marc%20Krzakala%2C%20Florent%20Statistical%20and%20computational%20phase%20transitions%20in%20spiked%20tensor%20estimation%202017"
        },
        {
            "id": "22",
            "entry": "[22] Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov\u00e1. Phase transitions in sparse pca. In Information Theory (ISIT), 2015 IEEE International Symposium on, pages 1635\u20131639. IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lesieur%2C%20Thibault%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Phase%20transitions%20in%20sparse%20pca%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lesieur%2C%20Thibault%20Krzakala%2C%20Florent%20Zdeborov%C3%A1%2C%20Lenka%20Phase%20transitions%20in%20sparse%20pca%202015"
        },
        {
            "id": "23",
            "entry": "[23] Evrim Acar, Canan Aykut-Bingol, Haluk Bingol, Rasmus Bro, and B\u00fclent Yener. Multiway analysis of epilepsy tensors. Bioinformatics, 23(13):i10\u2013i18, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Multiway%20analysis%20of%20epilepsy%20tensors%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Multiway%20analysis%20of%20epilepsy%20tensors%202007"
        },
        {
            "id": "24",
            "entry": "[24] Borb\u00e1la Hunyadi, Patrick Dupont, Wim Van Paesschen, and Sabine Van Huffel. Tensor decompositions and data fusion in epileptic electroencephalography and functional magnetic resonance imaging data. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 7(1), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hunyadi%2C%20Borb%C3%A1la%20Dupont%2C%20Patrick%20Paesschen%2C%20Wim%20Van%20Huffel%2C%20Sabine%20Van%20Tensor%20decompositions%20and%20data%20fusion%20in%20epileptic%20electroencephalography%20and%20functional%20magnetic%20resonance%20imaging%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hunyadi%2C%20Borb%C3%A1la%20Dupont%2C%20Patrick%20Paesschen%2C%20Wim%20Van%20Huffel%2C%20Sabine%20Van%20Tensor%20decompositions%20and%20data%20fusion%20in%20epileptic%20electroencephalography%20and%20functional%20magnetic%20resonance%20imaging%20data%202017"
        },
        {
            "id": "25",
            "entry": "[25] Jeffrey S Seely, Matthew T Kaufman, Stephen I Ryu, Krishna V Shenoy, John P Cunningham, and Mark M Churchland. Tensor analysis reveals distinct population structure that parallels the different computational roles of areas m1 and v1. PLoS computational biology, 12(11):e1005164, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seely%2C%20Jeffrey%20S.%20Kaufman%2C%20Matthew%20T.%20Ryu%2C%20Stephen%20I.%20Shenoy%2C%20Krishna%20V.%20Tensor%20analysis%20reveals%20distinct%20population%20structure%20that%20parallels%20the%20different%20computational%20roles%20of%20areas%20m1%20and%20v1%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seely%2C%20Jeffrey%20S.%20Kaufman%2C%20Matthew%20T.%20Ryu%2C%20Stephen%20I.%20Shenoy%2C%20Krishna%20V.%20Tensor%20analysis%20reveals%20distinct%20population%20structure%20that%20parallels%20the%20different%20computational%20roles%20of%20areas%20m1%20and%20v1%202016"
        },
        {
            "id": "26",
            "entry": "[26] Neil C Rabinowitz, Robbe L Goris, Marlene Cohen, and Eero P Simoncelli. Attention stabilizes the shared gain of v4 populations. Elife, 4, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rabinowitz%2C%20Neil%20C.%20Goris%2C%20Robbe%20L.%20Cohen%2C%20Marlene%20Simoncelli%2C%20Eero%20P.%20Attention%20stabilizes%20the%20shared%20gain%20of%20v4%20populations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rabinowitz%2C%20Neil%20C.%20Goris%2C%20Robbe%20L.%20Cohen%2C%20Marlene%20Simoncelli%2C%20Eero%20P.%20Attention%20stabilizes%20the%20shared%20gain%20of%20v4%20populations%202015"
        },
        {
            "id": "27",
            "entry": "[27] David J Thouless, Philip W Anderson, and Robert G Palmer. Solution of\u2019solvable model of a spin glass\u2019. Philosophical Magazine, 35(3):593\u2013601, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thouless%2C%20David%20J.%20Anderson%2C%20Philip%20W.%20Palmer%2C%20Robert%20G.%20Solution%20of%E2%80%99solvable%20model%20of%20a%20spin%20glass%E2%80%99%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thouless%2C%20David%20J.%20Anderson%2C%20Philip%20W.%20Palmer%2C%20Robert%20G.%20Solution%20of%E2%80%99solvable%20model%20of%20a%20spin%20glass%E2%80%99%201977"
        },
        {
            "id": "28",
            "entry": "[28] A Crisanti and H-J Sommers. Thouless-anderson-palmer approach to the spherical p-spin spin glass model. Journal de Physique I, 5(7):805\u2013813, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20A%20Crisanti%20and%20H-J%20Sommers.%20Thouless-anderson-palmer%20approach%20to%20the%20spherical%20p-spin%20spin%20glass%20model%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20A%20Crisanti%20and%20H-J%20Sommers.%20Thouless-anderson-palmer%20approach%20to%20the%20spherical%20p-spin%20spin%20glass%20model%201995"
        },
        {
            "id": "29",
            "entry": "[29] Andrea Crisanti and H-J Sommers. The spherical p-spin interaction spin glass model: the statics. Zeitschrift f\u00fcr Physik B Condensed Matter, 87(3):341\u2013354, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crisanti%2C%20Andrea%20Sommers%2C%20H.-J.%20The%20spherical%20p-spin%20interaction%20spin%20glass%20model%3A%20the%20statics%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crisanti%2C%20Andrea%20Sommers%2C%20H.-J.%20The%20spherical%20p-spin%20interaction%20spin%20glass%20model%3A%20the%20statics%201992"
        },
        {
            "id": "30",
            "entry": "[30] Hidetoshi Nishimori. Statistical physics of spin glasses and information processing: an introduction, volume 111. Clarendon Press, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nishimori%2C%20Hidetoshi%20Statistical%20physics%20of%20spin%20glasses%20and%20information%20processing%3A%20an%20introduction%2C%20volume%20111%202001"
        },
        {
            "id": "31",
            "entry": "[31] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 1\u201338, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dempster%2C%20Arthur%20P.%20Laird%2C%20Nan%20M.%20Rubin%2C%20Donald%20B.%20Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20em%20algorithm%201977"
        },
        {
            "id": "32",
            "entry": "[32] Lenka Zdeborov\u00e1 and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances in Physics, 65(5):453\u2013552, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zdeborov%C3%A1%2C%20Lenka%20Krzakala%2C%20Florent%20Statistical%20physics%20of%20inference%3A%20Thresholds%20and%20algorithms%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zdeborov%C3%A1%2C%20Lenka%20Krzakala%2C%20Florent%20Statistical%20physics%20of%20inference%3A%20Thresholds%20and%20algorithms%202016"
        },
        {
            "id": "33",
            "entry": "[33] Emile Richard and Andrea Montanari. A statistical model for tensor pca. In Advances in Neural Information Processing Systems, pages 2897\u20132905, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richard%2C%20Emile%20Montanari%2C%20Andrea%20A%20statistical%20model%20for%20tensor%20pca%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Richard%2C%20Emile%20Montanari%2C%20Andrea%20A%20statistical%20model%20for%20tensor%20pca%202014"
        },
        {
            "id": "34",
            "entry": "[34] Jonathan S Yedidia, William T Freeman, and Yair Weiss. Bethe free energy, kikuchi approximations, and belief propagation algorithms. Advances in neural information processing systems, 13, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yedidia%2C%20Jonathan%20S.%20Freeman%2C%20William%20T.%20Weiss%2C%20Yair%20Bethe%20free%20energy%2C%20kikuchi%20approximations%2C%20and%20belief%20propagation%20algorithms.%20Advances%20in%20neural%20information%20processing%20systems%202001"
        },
        {
            "id": "35",
            "entry": "[35] Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mezard%2C%20Marc%20Montanari%2C%20Andrea%20Information%2C%20physics%2C%20and%20computation%202009"
        },
        {
            "id": "36",
            "entry": "[36] Hans A Bethe. Statistical theory of superlattices. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, 150(871):552\u2013575, 1935.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Hans%20A%20Bethe.%20Statistical%20theory%20of%20superlattices%201935",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Hans%20A%20Bethe.%20Statistical%20theory%20of%20superlattices%201935"
        },
        {
            "id": "37",
            "entry": "[37] Marc M\u00e9zard and Giorgio Parisi. Replicas and optimization. Journal de Physique Lettres, 46(17):771\u2013778, 1985. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%A9zard%2C%20Marc%20Parisi%2C%20Giorgio%20Replicas%20and%20optimization%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%A9zard%2C%20Marc%20Parisi%2C%20Giorgio%20Replicas%20and%20optimization%201985"
        }
    ]
}
