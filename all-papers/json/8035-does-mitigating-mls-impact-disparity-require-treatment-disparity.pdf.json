{
    "filename": "8035-does-mitigating-mls-impact-disparity-require-treatment-disparity.pdf",
    "metadata": {
        "title": "Does mitigating ML's impact disparity require treatment disparity?",
        "author": "Zachary Lipton, Julian McAuley, Alexandra Chouldechova",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8035-does-mitigating-mls-impact-disparity-require-treatment-disparity.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Following precedent in employment discrimination law, two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here, the sensitive feature is used during training but a group-blind classifier is produced. In this paper, we show that: (i) when sensitive and (nominally) nonsensitive features are correlated, DLPs will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) in general, DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs."
    },
    "keywords": [
        {
            "term": "trade off",
            "url": "https://en.wikipedia.org/wiki/trade_off"
        },
        {
            "term": "disparate impact",
            "url": "https://en.wikipedia.org/wiki/disparate_impact"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "decision making",
            "url": "https://en.wikipedia.org/wiki/decision_making"
        },
        {
            "term": "disparate treatment",
            "url": "https://en.wikipedia.org/wiki/disparate_treatment"
        },
        {
            "term": "intentional discrimination",
            "url": "https://en.wikipedia.org/wiki/intentional_discrimination"
        }
    ],
    "highlights": [
        "Effective decision-making requires choosing among options given the available information",
        "Interpretation of this law has led to two notions of discrimination: disparate treatment and disparate impact",
        "Others incorporate the protected characteristic as either a regularizer, a constraint, or to preprocess the training data [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>\u2013<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]. These approaches are grounded in the premise that disparate learning processes are acceptable in cases where using a protected characteristic as a direct input to the model would constitute disparate treatment and be impermissible",
        "When protected characteristics are redundantly encoded in the other features, sufficiently powerful disparate learning processes can implement any form of treatment disparity.\n2",
        "We present a set of simple theoretical results that demonstrate the optimality of treatment disparity, and highlight some properties of disparate learning processes",
        "We find that several women who would have been hired under historical practices, owing to their 12+ years of work experience, would not be hired under the disparate learning processes due to their short hair"
    ],
    "key_statements": [
        "Effective decision-making requires choosing among options given the available information",
        "Interpretation of this law has led to two notions of discrimination: disparate treatment and disparate impact",
        "Others incorporate the protected characteristic as either a regularizer, a constraint, or to preprocess the training data [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>\u2013<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]. These approaches are grounded in the premise that disparate learning processes are acceptable in cases where using a protected characteristic as a direct input to the model would constitute disparate treatment and be impermissible",
        "When protected characteristics are redundantly encoded in the other features, sufficiently powerful disparate learning processes can implement any form of treatment disparity.\n2",
        "We present a set of simple theoretical results that demonstrate the optimality of treatment disparity, and highlight some properties of disparate learning processes",
        "We find that several women who would have been hired under historical practices, owing to their 12+ years of work experience, would not be hired under the disparate learning processes due to their short hair"
    ],
    "summary": [
        "Effective decision-making requires choosing among options given the available information.",
        "These approaches are grounded in the premise that DLPs are acceptable in cases where using a protected characteristic as a direct input to the model would constitute disparate treatment and be impermissible.",
        "When protected characteristics are redundantly encoded in the other features, sufficiently powerful DLPs can implement any form of treatment disparity.",
        "We consider binary protected features that divide the set of examples into two groups a and b.",
        "Many papers in discrimination-aware ML propose to optimize accuracy subject to constraints on the resulting level of impact parity as assessed by some metric [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>\u2013<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "Direct treatment disparity on the basis of z is the optimal strategy for maximizing classification accuracy1 subject to CV and p-% constraints.",
        "Treatment disparity is optimal Absent impact parity constraints, the Bayes-optimal decision rule for minimizing expected 0 \u2212 1 loss is given by du\u2217ncon(x, z) = \u03b4 \u2265 0.5), where \u03b4() is an indicator function.",
        "The authors characterize the optimal decision rule d = d(x, z) that maximizes the immediate utility u(d, c) = E[Y d(X, Z) \u2212 cd(X, Z)] for (0 < c < 1), under different exact parity criteria.",
        "We note that the results are related to the recent independent work of [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], who derive Bayes-optimal decision rules under the same parity constraints we consider here, working instead with the cost-sensitive risk, CS(d; c) = \u03c0(1 \u2212 c)FNR(d) + (1 \u2212 \u03c0)cFPR(d), where \u03c0 = P(Y = 1).",
        "The optimal decision rules d\u2217 under various parity constraints have the following form and are unique up to a set of probability zero: 1.",
        "Suppose that the optimal solution under the CV or p-% rule constraint classifies proportions qa and qb of the advantaged and disadvantaged groups, respectively, to the positive class.",
        "Optimality: As demonstrated for CV score and for p-% rule, intervention via per-group thresholds maximizes accuracy subject to an impact parity constraint.",
        "Figure 1 shows the test set results of applying a DLP to the available historical data to equalize hiring rates between men and women.",
        "We compare applying our simple thresholding scheme against the fairness constraint of [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], considering a binary outcome and a single protected feature.",
        "Treatment disparity approaches have three advantages over DLPs: they optimally trade accuracy for representativeness, preserve rankings among members of each group, and do no harm to members of the disadvantaged group."
    ],
    "headline": "We show that:  when sensitive and  nonsensitive features are correlated, disparate learning processes will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address;  when group membership is partly revealed by other features, disparate learning processes induce within-class discrimination; and  in general, disparate learning processes provide suboptimal trade-offs between accuracy and impact parity",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Civil rights act of 1964, 1964. Accessed on September 11th, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Civil%20rights%20act%20of%201964%201964%20Accessed%20on%20September%2011th%202017"
        },
        {
            "id": "2",
            "entry": "[2] Anemona Hartocollis and Stepanie Saul. Affirmative action battle has a new focus: Asian americans. 2017. URL https://www.nytimes.com/2017/08/02/us/affirmative-action-battle-has-a-new-focus-asian-americans.html?mcubz=1.",
            "url": "https://www.nytimes.com/2017/08/02/us/affirmative-action-battle-has-a-new-focus-asian-americans.html?mcubz=1"
        },
        {
            "id": "3",
            "entry": "[3] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In KDD, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedreshi%2C%20Dino%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20Discrimination-aware%20data%20mining%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedreshi%2C%20Dino%20Ruggieri%2C%20Salvatore%20Turini%2C%20Franco%20Discrimination-aware%20data%20mining%202008"
        },
        {
            "id": "4",
            "entry": "[4] Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization approach. In ICDM Workshops, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamishima%2C%20Toshihiro%20Akaho%2C%20Shotaro%20Sakuma%2C%20Jun%20Fairness-aware%20learning%20through%20regularization%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamishima%2C%20Toshihiro%20Akaho%2C%20Shotaro%20Sakuma%2C%20Jun%20Fairness-aware%20learning%20through%20regularization%20approach%202011"
        },
        {
            "id": "5",
            "entry": "[5] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. In AISTATS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20Muhammad%20Bilal%20Valera%2C%20Isabel%20Rodriguez%2C%20Manuel%20Gomez%20Gummadi%2C%20Krishna%20P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017"
        },
        {
            "id": "6",
            "entry": "[6] Faisal Kamiran and Toon Calders. Classifying without discriminating. In Computer, Control and Communication, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Classifying%20without%202009"
        },
        {
            "id": "7",
            "entry": "[7] Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree learning. In ICDM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Pechenizkiy%2C%20Mykola%20Discrimination%20aware%20decision%20tree%20learning%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Pechenizkiy%2C%20Mykola%20Discrimination%20aware%20decision%20tree%20learning%202010"
        },
        {
            "id": "8",
            "entry": "[8] Pauline Kim. Auditing algorithms for discrimination. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Pauline%20Auditing%20algorithms%20for%20discrimination%202017"
        },
        {
            "id": "9",
            "entry": "[9] Pauline T Kim. Data-driven discrimination at work. William & Mary Law Review, 58(3), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Pauline%20T.%20Data-driven%20discrimination%20at%20work.%20William%20%26%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Pauline%20T.%20Data-driven%20discrimination%20at%20work.%20William%20%26%202017"
        },
        {
            "id": "10",
            "entry": "[10] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. arXiv preprint arXiv:1701.08230, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08230"
        },
        {
            "id": "11",
            "entry": "[11] Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Max Leiserson. Decoupled classifiers for fair and efficient machine learning. arXiv preprint arXiv:1707.06613, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06613"
        },
        {
            "id": "12",
            "entry": "[12] Yahav Bechavod and Katrina Ligett. Learning fair classifiers: A regularization-inspired approach. arXiv preprint arXiv:1707.00044, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00044"
        },
        {
            "id": "13",
            "entry": "[13] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hardt%2C%20Moritz%20Price%2C%20Eric%20Srebro%2C%20Nati%20Equality%20of%20opportunity%20in%20supervised%20learning%202016"
        },
        {
            "id": "14",
            "entry": "[14] Ya\u2019acov Ritov, Yuekai Sun, and Ruofei Zhao. On conditional parity as a notion of nondiscrimination in machine learning. arXiv preprint arXiv:1706.08519, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.08519"
        },
        {
            "id": "15",
            "entry": "[15] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Innovations in Theoretical Computer Science Conference, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20Cynthia%20Hardt%2C%20Moritz%20Pitassi%2C%20Toniann%20Reingold%2C%20Omer%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "16",
            "entry": "[16] Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1\u201333, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kamiran%2C%20Faisal%20Calders%2C%20Toon%20Data%20preprocessing%20techniques%20for%20classification%20without%20discrimination%202012"
        },
        {
            "id": "17",
            "entry": "[17] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In KDD, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20A.%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feldman%2C%20Michael%20Friedler%2C%20Sorelle%20A.%20Moeller%2C%20John%20Scheidegger%2C%20Carlos%20Certifying%20and%20removing%20disparate%20impact%202015"
        },
        {
            "id": "18",
            "entry": "[18] Philip Adler, Casey Falk, Sorelle A Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. Auditing black-box models by obscuring features. arXiv preprint arXiv:1602.07043, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.07043"
        },
        {
            "id": "19",
            "entry": "[19] James E Johndrow and Kristian Lum. An algorithm for removing sensitive information: application to race-independent recidivism prediction. arXiv preprint arXiv:1703.04957, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04957"
        },
        {
            "id": "20",
            "entry": "[20] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In ICML, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zemel%2C%20Rich%20Wu%2C%20Yu%20Swersky%2C%20Kevin%20Pitassi%2C%20Toni%20Learning%20fair%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20Rich%20Wu%2C%20Yu%20Swersky%2C%20Kevin%20Pitassi%2C%20Toni%20Learning%20fair%20representations%202013"
        },
        {
            "id": "21",
            "entry": "[21] Aditya Menon and Robert Williamson. The cost of fairness in binary classification. In Fairness, Accountability and Transparency, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Menon%2C%20Aditya%20Williamson%2C%20Robert%20The%20cost%20of%20fairness%20in%20binary%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Menon%2C%20Aditya%20Williamson%2C%20Robert%20The%20cost%20of%20fairness%20in%20binary%20classification%202018"
        },
        {
            "id": "22",
            "entry": "[22] Ron Kohavi. Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid. In KDD, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohavi%2C%20Ron%20Scaling%20up%20the%20accuracy%20of%20naive-bayes%20classifiers%3A%20a%20decision-tree%20hybrid%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohavi%2C%20Ron%20Scaling%20up%20the%20accuracy%20of%20naive-bayes%20classifiers%3A%20a%20decision-tree%20hybrid%201996"
        },
        {
            "id": "23",
            "entry": "[23] S. Moro, P. Cortez, and P. Rita. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moro%2C%20S.%20Cortez%2C%20P.%20Rita%2C%20P.%20A%20data-driven%20approach%20to%20predict%20the%20success%20of%20bank%20telemarketing.%20Decision%20Support%20Systems%202014"
        },
        {
            "id": "24",
            "entry": "[24] I. C. Yeh and C. H. Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yeh%2C%20I.C.%20Lien%2C%20C.H.%20The%20comparisons%20of%20data%20mining%20techniques%20for%20the%20predictive%20accuracy%20of%20probability%20of%20default%20of%20credit%20card%20clients.%20Expert%20Systems%20with%20Applications%202009"
        },
        {
            "id": "25",
            "entry": "[25] IBM Watson analytics blog. watson-analytics-blog/. https://www.ibm.com/communities/analytics/",
            "url": "https://www.ibm.com/communities/analytics/"
        },
        {
            "id": "26",
            "entry": "[26] Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Rawlsian fairness for machine learning. arXiv preprint arXiv:1610.09559, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09559"
        },
        {
            "id": "27",
            "entry": "[27] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.05807"
        },
        {
            "id": "28",
            "entry": "[28] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chouldechova%2C%20Alexandra%20Fair%20prediction%20with%20disparate%20impact%3A%20A%20study%20of%20bias%20in%20recidivism%20prediction%20instruments%202017"
        },
        {
            "id": "29",
            "entry": "[29] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: The state of the art. arXiv preprint arXiv:1703.09207, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.09207"
        },
        {
            "id": "30",
            "entry": "[30] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P Gummadi, and Adrian Weller. From parity to preference-based notions of fairness in classification. arXiv preprint arXiv:1707.00010, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.00010"
        },
        {
            "id": "31",
            "entry": "[31] Anupam Datta, Matt Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen. Proxy nondiscrimination in data-driven systems. arXiv preprint arXiv:1707.08120, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08120"
        },
        {
            "id": "32",
            "entry": "[32] Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. arXiv preprint arXiv:1705.10378, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.10378"
        },
        {
            "id": "33",
            "entry": "[33] Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Avoiding discrimination through causal reasoning. arXiv preprint arXiv:1706.02744, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02744"
        },
        {
            "id": "34",
            "entry": "[34] Matt J Kusner, Joshua R Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. arXiv preprint arXiv:1703.06856, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1703.06856"
        }
    ]
}
