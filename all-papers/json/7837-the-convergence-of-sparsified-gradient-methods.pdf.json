{
    "filename": "7837-the-convergence-of-sparsified-gradient-methods.pdf",
    "metadata": {
        "title": "The Convergence of Sparsified Gradient Methods",
        "author": "Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, Cedric Renggli",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7837-the-convergence-of-sparsified-gradient-methods.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Stochastic Gradient Descent (SGD) has become the standard tool for distributed training of massive machine learning models, in particular deep neural networks. Several families of communication-reduction methods, such as quantization, largebatch methods, and gradient sparsification, have been proposed to reduce the overheads of distribution. To date, gradient sparsification methods\u2013where each node sorts gradients by magnitude, and only communicates a subset of the components, accumulating the rest locally\u2013are known to yield some of the largest practical gains. Such methods can reduce the amount of communication per step by up to three orders of magnitude, while preserving model accuracy. Yet, this family of methods currently has no theoretical justification."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Please recall our modeling of the basic stochastic gradient descent process in Equation (1).<br/><br/>Fix n to be the dimension of the problems we consider; unless otherwise stated \u00b7 will denote the 2-norm",
        "Under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel stochastic gradient descent",
        "Under analytic assumptions, gradient sparsification methods provide convergence guarantees for stochastic gradient descent",
        "One way of doing lossless communication-reduction is through factorization [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], which is effective in deep neural networks with large fully-connected layers, whose gradients can be decomposed as outer vector products",
        "We will consider a variant of distributed stochastic gradient descent where, in each iteration t, each node computes a local gradient based on its current view of the model, which we denote by vt, which is consistent across nodes",
        "We provide a theoretical foundation for empirical results shown with large-scale experiments on recurrent neural networks on production-scale speech, neural machine translation, as well as image classification tasks [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]"
    ],
    "key_statements": [
        "Please recall our modeling of the basic stochastic gradient descent process in Equation (1).<br/><br/>Fix n to be the dimension of the problems we consider; unless otherwise stated \u00b7 will denote the 2-norm",
        "Under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel stochastic gradient descent",
        "Under analytic assumptions, gradient sparsification methods provide convergence guarantees for stochastic gradient descent",
        "There has been a recent surge of interest in distributed machine learning, e.g., [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>]; due to space limits, we focus on communication-reduction techniques that are closely related",
        "One way of doing lossless communication-reduction is through factorization [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>], which is effective in deep neural networks with large fully-connected layers, whose gradients can be decomposed as outer vector products",
        "There exists a Wt where, if the algorithm has not succeeded by timestep t, Wt = 2\u03b1c \u2212 \u03b12M 2 log e xt \u2212 x\u2217 2 \u22121 + t, where Mis a bound on the second moment of the stochastic gradients for the sequential stochastic gradient descent process",
        "We will consider a variant of distributed stochastic gradient descent where, in each iteration t, each node computes a local gradient based on its current view of the model, which we denote by vt, which is consistent across nodes",
        "We apply this result with the martingale Wt for the sequential stochastic gradient descent process that uses the average of P stochastic gradients as an update",
        "When K is a constant fraction of n, the convergence of the TopK algorithm is essentially dictated by the Lipschitz constant of the supermartingale W , and by the second-moment bound M , and will be similar to sequential stochastic gradient descent",
        "We provide a theoretical foundation for empirical results shown with large-scale experiments on recurrent neural networks on production-scale speech, neural machine translation, as well as image classification tasks [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]"
    ],
    "summary": [
        "Please recall our modeling of the basic SGD process in Equation (1).<br/><br/>Fix n to be the dimension of the problems we consider; unless otherwise stated \u00b7 will denote the 2-norm.",
        "Under analytic assumptions, gradient sparsification methods provide convergence guarantees for SGD.",
        "Given standard data-parallel SGD, in each iteration t, each node computes a local gradient G, based on its current view of the model.",
        "The distinctions are: 1) the algorithm we analyze is different; 2) we do not assume the existence of a bound \u03c4 on the delay with which a component may be applied; 3) we do not make sparsity assumptions on the original stochastic gradients.",
        "There exists a Wt where, if the algorithm has not succeeded by timestep t, Wt = 2\u03b1c \u2212 \u03b12M 2 log e xt \u2212 x\u2217 2 \u22121 + t, where Mis a bound on the second moment of the stochastic gradients for the sequential SGD process.",
        "We will consider a variant of distributed SGD where, in each iteration t, each node computes a local gradient based on its current view of the model, which we denote by vt, which is consistent across nodes.",
        "We will assume that such overlaps can only cause the algorithm to lose a small amount of information at each step, with respect to the norm of \u201ctrue\u201d gradient Gt. Specifically: Assumption 1.",
        "We apply this result with the martingale Wt for the sequential SGD process that uses the average of P stochastic gradients as an update.",
        "Such analysis will rely on a bound on the variance of the local gradients), which is a strong assumption that ignores the effect of averaging over the P nodes.",
        "Assumption 1 allows for a more elegant analysis that provides better convergence rates, which are due to the averaging of the local gradients at every step of the TopK algorithm.",
        "The Impact of the Parameter K and Gradient \u201cShape.\u201d In the convex case, the dependence on the convergence with respect to K and n is encapsulated by the parameter C = O(n/K) assuming P is constant.",
        "When K is a constant fraction of n, the convergence of the TopK algorithm is essentially dictated by the Lipschitz constant of the supermartingale W , and by the second-moment bound M , and will be similar to sequential SGD.",
        "TopK had superior convergence rate when compared to stochastic quantization / sparsification [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] given the same communication budget per node.",
        "We provide a theoretical foundation for empirical results shown with large-scale experiments on recurrent neural networks on production-scale speech, neural machine translation, as well as image classification tasks [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]"
    ],
    "headline": "This is the question we address in this paper",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. arXiv preprint arXiv:1704.05021, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05021"
        },
        {
            "id": "3",
            "entry": "[3] Dan Alistarh, Christopher De Sa, and Nikola Konstantinov. The convergence of stochastic gradient descent in asynchronous shared memory. arXiv preprint arXiv:1803.08841, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.08841"
        },
        {
            "id": "4",
            "entry": "[4] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Randomized quantization for communication-efficient stochastic gradient descent. In Proceedings of NIPS 2017, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alistarh%2C%20Dan%20Grubic%2C%20Demjan%20Li%2C%20Jerry%20Tomioka%2C%20Ryota%20QSGD%3A%20Randomized%20quantization%20for%20communication-efficient%20stochastic%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alistarh%2C%20Dan%20Grubic%2C%20Demjan%20Li%2C%20Jerry%20Tomioka%2C%20Ryota%20QSGD%3A%20Randomized%20quantization%20for%20communication-efficient%20stochastic%20gradient%20descent%202017"
        },
        {
            "id": "5",
            "entry": "[5] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1512.01274"
        },
        {
            "id": "6",
            "entry": "[6] Trishul M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam: Building an efficient and scalable deep learning training system. In OSDI, volume 14, pages 571\u2013582, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chilimbi%2C%20Trishul%20M.%20Suzue%2C%20Yutaka%20Apacible%2C%20Johnson%20Kalyanaraman%2C%20Karthik%20Project%20adam%3A%20Building%20an%20efficient%20and%20scalable%20deep%20learning%20training%20system%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chilimbi%2C%20Trishul%20M.%20Suzue%2C%20Yutaka%20Apacible%2C%20Johnson%20Kalyanaraman%2C%20Karthik%20Project%20adam%3A%20Building%20an%20efficient%20and%20scalable%20deep%20learning%20training%20system%202014"
        },
        {
            "id": "7",
            "entry": "[7] Christopher De Sa, Ce Zhang, Kunle Olukotun, and Christopher R\u00e9. Taming the wild: A unified analysis of Hogwild. Style Algorithms. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20Christopher%20De%20Zhang%2C%20Ce%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20Hogwild.%20Style%20Algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20Christopher%20De%20Zhang%2C%20Ce%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20Hogwild.%20Style%20Algorithms%202015"
        },
        {
            "id": "8",
            "entry": "[8] Nikoli Dryden, Sam Ade Jacobs, Tim Moon, and Brian Van Essen. Communication quantization for data-parallel training of deep neural networks. In Proceedings of the Workshop on Machine Learning in High Performance Computing Environments, pages 1\u20138. IEEE Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dryden%2C%20Nikoli%20Jacobs%2C%20Sam%20Ade%20Moon%2C%20Tim%20Essen%2C%20Brian%20Van%20Communication%20quantization%20for%20data-parallel%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dryden%2C%20Nikoli%20Jacobs%2C%20Sam%20Ade%20Moon%2C%20Tim%20Essen%2C%20Brian%20Van%20Communication%20quantization%20for%20data-parallel%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "9",
            "entry": "[9] John C Duchi, Sorathan Chaturapruek, and Christopher R\u00e9. Asynchronous stochastic convex optimization. arXiv preprint arXiv:1508.00882, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.00882"
        },
        {
            "id": "10",
            "entry": "[10] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.02677"
        },
        {
            "id": "11",
            "entry": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "12",
            "entry": "[12] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "13",
            "entry": "[13] David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for text categorization research. Journal of machine learning research, 5(Apr):361\u2013397, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lewis%2C%20David%20D.%20Yang%2C%20Yiming%20Rose%2C%20Tony%20G.%20Li%2C%20Fan%20Rcv1%3A%20A%20new%20benchmark%20collection%20for%20text%20categorization%20research%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lewis%2C%20David%20D.%20Yang%2C%20Yiming%20Rose%2C%20Tony%20G.%20Li%2C%20Fan%20Rcv1%3A%20A%20new%20benchmark%20collection%20for%20text%20categorization%20research%202004"
        },
        {
            "id": "14",
            "entry": "[14] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737\u20132745, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lian%2C%20Xiangru%20Huang%2C%20Yijun%20Li%2C%20Yuncheng%20Liu%2C%20Ji%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lian%2C%20Xiangru%20Huang%2C%20Yijun%20Li%2C%20Yuncheng%20Liu%2C%20Ji%20Asynchronous%20parallel%20stochastic%20gradient%20for%20nonconvex%20optimization%202015"
        },
        {
            "id": "15",
            "entry": "[15] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01887"
        },
        {
            "id": "16",
            "entry": "[16] Ji Liu and Stephen J Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIAM Journal on Optimization, 25(1):351\u2013376, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20Asynchronous%20stochastic%20coordinate%20descent%3A%20Parallelism%20and%20convergence%20properties%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20Asynchronous%20stochastic%20coordinate%20descent%3A%20Parallelism%20and%20convergence%20properties%202015"
        },
        {
            "id": "17",
            "entry": "[17] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rastegari%2C%20M.%20Ordonez%2C%20V.%20Redmon%2C%20J.%20Farhadi%2C%20A.%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rastegari%2C%20M.%20Ordonez%2C%20V.%20Redmon%2C%20J.%20Farhadi%2C%20A.%20Xnor-net%3A%20Imagenet%20classification%20using%20binary%20convolutional%20neural%20networks%202016"
        },
        {
            "id": "18",
            "entry": "[18] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693\u2013701, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lock-free%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "19",
            "entry": "[19] C\u00e8dric Renggli, Dan Alistarh, and Torsten Hoefler. Sparcml: High-performance sparse communication for machine learning. arXiv preprint arXiv:1802.08021, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08021"
        },
        {
            "id": "20",
            "entry": "[20] F. Seide, H. Fu, L. G. Jasha, and D. Yu. 1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns. Interspeech, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seide%2C%20F.%20Fu%2C%20H.%20Jasha%2C%20L.G.%20Yu%2C%20D.%201-bit%20stochastic%20gradient%20descent%20and%20application%20to%20data-parallel%20distributed%20training%20of%20speech%20dnns%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seide%2C%20F.%20Fu%2C%20H.%20Jasha%2C%20L.G.%20Yu%2C%20D.%201-bit%20stochastic%20gradient%20descent%20and%20application%20to%20data-parallel%20distributed%20training%20of%20speech%20dnns%202014"
        },
        {
            "id": "21",
            "entry": "[21] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit Stochastic Gradient Descent and its Application to Data-parallel Distributed Training of Speech DNNs. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seide%2C%20Frank%20Fu%2C%20Hao%20Droppo%2C%20Jasha%20Li%2C%20Gang%201-bit%20Stochastic%20Gradient%20Descent%20and%20its%20Application%20to%20Data-parallel%20Distributed%20Training%20of%20Speech%20DNNs%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seide%2C%20Frank%20Fu%2C%20Hao%20Droppo%2C%20Jasha%20Li%2C%20Gang%201-bit%20Stochastic%20Gradient%20Descent%20and%20its%20Application%20to%20Data-parallel%20Distributed%20Training%20of%20Speech%20DNNs%202014"
        },
        {
            "id": "22",
            "entry": "[22] Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1310\u20131321. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shokri%2C%20Reza%20Shmatikov%2C%20Vitaly%20Privacy-preserving%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shokri%2C%20Reza%20Shmatikov%2C%20Vitaly%20Privacy-preserving%20deep%20learning%202015"
        },
        {
            "id": "23",
            "entry": "[23] Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In Sixteenth Annual Conference of the International Speech Communication Association, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strom%2C%20Nikko%20Scalable%20distributed%20dnn%20training%20using%20commodity%20gpu%20cloud%20computing%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strom%2C%20Nikko%20Scalable%20distributed%20dnn%20training%20using%20commodity%20gpu%20cloud%20computing%202015"
        },
        {
            "id": "24",
            "entry": "[24] Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang. meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting. arXiv preprint arXiv:1706.06197, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06197"
        },
        {
            "id": "25",
            "entry": "[25] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In AAAI, pages 4278\u20134284, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inception-resnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inception-resnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017"
        },
        {
            "id": "26",
            "entry": "[26] Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-efficient distributed optimization. arXiv preprint arXiv:1710.09854, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09854"
        },
        {
            "id": "27",
            "entry": "[27] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1508\u20131518, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017"
        },
        {
            "id": "28",
            "entry": "[28] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1508\u20131518, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wen%2C%20Wei%20Xu%2C%20Cong%20Yan%2C%20Feng%20Wu%2C%20Chunpeng%20Terngrad%3A%20Ternary%20gradients%20to%20reduce%20communication%20in%20distributed%20deep%20learning%202017"
        },
        {
            "id": "29",
            "entry": "[29] Eric P Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu. Petuum: A new platform for distributed machine learning on big data. IEEE Transactions on Big Data, 1(2):49\u201367, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xing%2C%20Eric%20P.%20Ho%2C%20Qirong%20Dai%2C%20Wei%20Kim%2C%20Jin%20Kyu%20Petuum%3A%20A%20new%20platform%20for%20distributed%20machine%20learning%20on%20big%20data%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xing%2C%20Eric%20P.%20Ho%2C%20Qirong%20Dai%2C%20Wei%20Kim%2C%20Jin%20Kyu%20Petuum%3A%20A%20new%20platform%20for%20distributed%20machine%20learning%20on%20big%20data%202015"
        },
        {
            "id": "30",
            "entry": "[30] Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training. arXiv preprint arXiv:1708.03888, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03888"
        },
        {
            "id": "31",
            "entry": "[31] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, et al. An introduction to computational networks and the computational network toolkit. Microsoft Technical Report MSR-TR-2014\u2013112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Dong%20Eversole%2C%20Adam%20Seltzer%2C%20Mike%20Yao%2C%20Kaisheng%20An%20introduction%20to%20computational%20networks%20and%20the%20computational%20network%20toolkit%202014"
        },
        {
            "id": "32",
            "entry": "[32] Jian Zhang, Ioannis Mitliagkas, and Christopher R\u00e9. Yellowfin and the art of momentum tuning. arXiv preprint arXiv:1706.03471, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1706.03471"
        }
    ]
}
