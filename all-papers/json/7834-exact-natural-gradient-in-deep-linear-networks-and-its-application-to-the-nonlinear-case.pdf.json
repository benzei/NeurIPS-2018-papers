{
    "filename": "7834-exact-natural-gradient-in-deep-linear-networks-and-its-application-to-the-nonlinear-case.pdf",
    "metadata": {
        "title": "Exact natural gradient in deep linear networks and its application to the nonlinear case",
        "author": "Alberto Bernacchia, Mate Lengyel, Guillaume Hennequin",
        "date": 2010,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7834-exact-natural-gradient-in-deep-linear-networks-and-its-application-to-the-nonlinear-case.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Stochastic gradient descent (SGD) remains the method of choice for deep learning, despite the limitations arising for ill-behaved objective functions. In cases where it could be estimated, the natural gradient has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function, but little is known theoretically about its convergence properties, and it has yet to find a practical implementation that would scale to very deep and large networks. Here, we derive an exact expression for the natural gradient in deep linear networks, which exhibit pathological curvature similar to the nonlinear case. We provide for the first time an analytical solution for its convergence rate, showing that the loss decreases exponentially to the global minimum in parameter space. Our expression for the natural gradient is surprisingly simple, computationally tractable, and explains why some approximations proposed previously work well in practice. This opens new avenues for approximating the natural gradient in the nonlinear case, and we show in preliminary experiments that our online natural gradient descent outperforms SGD on MNIST autoencoding while sharing its computational simplicity."
    },
    "keywords": [
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "fisher matrix",
            "url": "https://en.wikipedia.org/wiki/fisher_matrix"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "Stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Stochastic gradient descent (SGD) is used ubiquitously to train deep neural networks, due to its low computational cost and ease of implementation",
        "Even in the simple linear case, the exact inverse Fisher matrix is not block-diagonal and the contributions of the off-diagonal blocks to the natural gradient have the same order of magnitude as the on-diagonal blocks",
        "We provide a simplified derivation of the exponential decrease of the loss function under the the natural gradient updates given by Eq 10, which are based on a particular form of the generalized inverse of F",
        "A more general derivation of the exponential decrease of the loss function is given in the Supplementary Material, where we show that the same exponential decay of the loss holds for all possible generalized inverses",
        "We found that natural gradient updates afford an unexpectedly cheap form, with similar computational requirements as plain Stochastic gradient descent",
        "While we presented numerical evidence showing that a discrete-time implementation of NGD performs well, and it shows the exponential decrease of the loss function predicted by our theory, further work is necessary in order to derive principled methods for discretizing the parameter updates [<a class=\"ref-link\" id=\"cMartens_2014_a\" href=\"#rMartens_2014_a\">Martens, 2014</a>]"
    ],
    "key_statements": [
        "Stochastic gradient descent (SGD) is used ubiquitously to train deep neural networks, due to its low computational cost and ease of implementation",
        "Even in the simple linear case, the exact inverse Fisher matrix is not block-diagonal and the contributions of the off-diagonal blocks to the natural gradient have the same order of magnitude as the on-diagonal blocks",
        "We provide a simplified derivation of the exponential decrease of the loss function under the the natural gradient updates given by Eq 10, which are based on a particular form of the generalized inverse of F",
        "A more general derivation of the exponential decrease of the loss function is given in the Supplementary Material, where we show that the same exponential decay of the loss holds for all possible generalized inverses",
        "Block-diagonal approximations In order to obtain an expression for the natural gradient that would be computationally cheap and feasible for practical applications, previous studies suggested a block-diagonal approximation to the inverse Fisher information matrix, in the nonlinear case (K-FAC, [<a class=\"ref-link\" id=\"cMartens_2015_a\" href=\"#rMartens_2015_a\">Martens and Grosse, 2015</a>; <a class=\"ref-link\" id=\"cGrosse_2016_a\" href=\"#rGrosse_2016_a\">Grosse and Martens, 2016</a>; <a class=\"ref-link\" id=\"cBa_et+al_2016_a\" href=\"#rBa_et+al_2016_a\">Ba et al, 2016</a>], see [<a class=\"ref-link\" id=\"cHeskes_2000_a\" href=\"#rHeskes_2000_a\">Heskes, 2000</a>; <a class=\"ref-link\" id=\"cPovey_et+al_2014_a\" href=\"#rPovey_et+al_2014_a\">Povey et al, 2014</a>; <a class=\"ref-link\" id=\"cDesjardins_et+al_2015_a\" href=\"#rDesjardins_et+al_2015_a\">Desjardins et al, 2015</a>])",
        "We found that natural gradient updates afford an unexpectedly cheap form, with similar computational requirements as plain Stochastic gradient descent",
        "While we presented numerical evidence showing that a discrete-time implementation of NGD performs well, and it shows the exponential decrease of the loss function predicted by our theory, further work is necessary in order to derive principled methods for discretizing the parameter updates [<a class=\"ref-link\" id=\"cMartens_2014_a\" href=\"#rMartens_2014_a\">Martens, 2014</a>]"
    ],
    "summary": [
        "Stochastic gradient descent (SGD) is used ubiquitously to train deep neural networks, due to its low computational cost and ease of implementation.",
        "This expression is equal to the standard gradient, multiplied by the inverse covariance of both the backward error ei and forward activation xi\u22121.",
        "We provide a simplified derivation of the exponential decrease of the loss function under the the natural gradient updates given by Eq 10, which are based on a particular form of the generalized inverse of F .",
        "Under natural gradient descent in continuous time, the total weight matrix obeys first order dynamics, and converges exponentially fast towards yxT \u03a3\u22121, which is the least squares solution to the linear regression problem [<a class=\"ref-link\" id=\"cBishop_2016_a\" href=\"#rBishop_2016_a\">Bishop, 2016</a>].",
        "As predicted by our theory, natural gradient descent caused the test error to decrease exponentially and reach the minimum achievable loss after only a few passes through the training set (Fig. 1A-C, top).",
        "Owing to the size of the input layer, our implementation of NGD via direct inversion of the relevant covariance matrices outperformed SGD only modestly in wall-clock time (Fig. 1D, bottom).",
        "Block-diagonal approximations In order to obtain an expression for the natural gradient that would be computationally cheap and feasible for practical applications, previous studies suggested a block-diagonal approximation to the inverse Fisher information matrix, in the nonlinear case (K-FAC, [<a class=\"ref-link\" id=\"cMartens_2015_a\" href=\"#rMartens_2015_a\">Martens and Grosse, 2015</a>; <a class=\"ref-link\" id=\"cGrosse_2016_a\" href=\"#rGrosse_2016_a\">Grosse and Martens, 2016</a>; <a class=\"ref-link\" id=\"cBa_et+al_2016_a\" href=\"#rBa_et+al_2016_a\">Ba et al, 2016</a>], see [<a class=\"ref-link\" id=\"cHeskes_2000_a\" href=\"#rHeskes_2000_a\">Heskes, 2000</a>; <a class=\"ref-link\" id=\"cPovey_et+al_2014_a\" href=\"#rPovey_et+al_2014_a\">Povey et al, 2014</a>; <a class=\"ref-link\" id=\"cDesjardins_et+al_2015_a\" href=\"#rDesjardins_et+al_2015_a\"><a class=\"ref-link\" id=\"cDesjardins_et+al_2015_a\" href=\"#rDesjardins_et+al_2015_a\">Desjardins et al, 2015</a></a>]).",
        "In our deep linear network model, we show in the Supplementary Material that the (i, j)-block of the exact Fisher information matrix, is equal to",
        "Whitening and biological algorithms Our expression for the natural gradient offers post-hoc justifications for some recently proposed modifications of the standard gradient, whereby the forward activation and backward errors are whitened prior to being multiplied to obtain the gradient at each layer [<a class=\"ref-link\" id=\"cDesjardins_et+al_2015_a\" href=\"#rDesjardins_et+al_2015_a\"><a class=\"ref-link\" id=\"cDesjardins_et+al_2015_a\" href=\"#rDesjardins_et+al_2015_a\">Desjardins et al, 2015</a></a>; <a class=\"ref-link\" id=\"cFujimoto_2018_a\" href=\"#rFujimoto_2018_a\">Fujimoto and Ohira, 2018</a>].",
        "We found that the loss function has the same convergence properties for all possible natural gradients, i.e. as obtained by any generalized inverse of the Fisher matrix.",
        "One of our main results is the first exact solution for the convergence rate of the loss function under natural gradient descent, for a linear multilayer network: exponential decrease towards the minimum loss.",
        "While we presented numerical evidence showing that a discrete-time implementation of NGD performs well, and it shows the exponential decrease of the loss function predicted by our theory, further work is necessary in order to derive principled methods for discretizing the parameter updates [<a class=\"ref-link\" id=\"cMartens_2014_a\" href=\"#rMartens_2014_a\">Martens, 2014</a>].",
        "We found that natural gradient updates afford an unexpectedly cheap form, with similar computational requirements as plain SGD"
    ],
    "headline": "We provide for the first time an analytical solution for its convergence rate, showing that the loss decreases exponentially to the global minimum in parameter space",
    "reference_links": [
        {
            "id": "Amari_1998_a",
            "entry": "Amari, S.-I. (1998). Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "Amari_et+al_2000_a",
            "entry": "Amari, S.-I., Park, H., and Fukumizu, K. (2000). Adaptive method of realizing natural gradient learning for multilayer perceptrons. Neural Computation, 12(6):1399\u20131409.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.-I.%20Park%2C%20H.%20Fukumizu%2C%20K.%20Adaptive%20method%20of%20realizing%20natural%20gradient%20learning%20for%20multilayer%20perceptrons%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.-I.%20Park%2C%20H.%20Fukumizu%2C%20K.%20Adaptive%20method%20of%20realizing%20natural%20gradient%20learning%20for%20multilayer%20perceptrons%202000"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "Ba, J., Grosse, R., and Martens, J. (2016). Distributed second-order optimization using Kronecker-factored approximations. In Proc. 5rd Int. Conf. Learn. Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ba%2C%20J.%20Grosse%2C%20R.%20Martens%2C%20J.%20Distributed%20second-order%20optimization%20using%20Kronecker-factored%20approximations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ba%2C%20J.%20Grosse%2C%20R.%20Martens%2C%20J.%20Distributed%20second-order%20optimization%20using%20Kronecker-factored%20approximations%202016"
        },
        {
            "id": "Bishop_2016_a",
            "entry": "Bishop, C. M. (2016). Pattern recognition and machine learning. Springer-Verlag New York.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bishop%2C%20C.M.%20Pattern%20recognition%20and%20machine%20learning%202016"
        },
        {
            "id": "Dauphin_et+al_2014_a",
            "entry": "Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pages 2933\u20132941.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "Desjardins_et+al_2015_a",
            "entry": "Desjardins, G., Simonyan, K., Pascanu, R., et al. (2015). Natural neural networks. In Advances in Neural Information Processing Systems, pages 2071\u20132079.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Desjardins%2C%20G.%20Simonyan%2C%20K.%20Pascanu%2C%20R.%20Natural%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Desjardins%2C%20G.%20Simonyan%2C%20K.%20Pascanu%2C%20R.%20Natural%20neural%20networks%202015"
        },
        {
            "id": "Duchi_et+al_2011_a",
            "entry": "Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "Fujimoto_2018_a",
            "entry": "Fujimoto, Y. and Ohira, T. (2018). A neural network model with bidirectional whitening. In International Conference on Artificial Intelligence and Soft Computing, pages 47\u201357. Springer.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fujimoto%2C%20Y.%20Ohira%2C%20T.%20A%20neural%20network%20model%20with%20bidirectional%20whitening%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fujimoto%2C%20Y.%20Ohira%2C%20T.%20A%20neural%20network%20model%20with%20bidirectional%20whitening%202018"
        },
        {
            "id": "Grosse_2016_a",
            "entry": "Grosse, R. and Martens, J. (2016). A Kronecker-factored approximate fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573\u2013582.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grosse%2C%20R.%20Martens%2C%20J.%20A%20Kronecker-factored%20approximate%20fisher%20matrix%20for%20convolution%20layers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grosse%2C%20R.%20Martens%2C%20J.%20A%20Kronecker-factored%20approximate%20fisher%20matrix%20for%20convolution%20layers%202016"
        },
        {
            "id": "Halko_et+al_2011_a",
            "entry": "Halko, N., Martinsson, P.-G., and Tropp, J. A. (2011). Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217\u2013288.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Halko%2C%20N.%20Martinsson%2C%20P.-G.%20Tropp%2C%20J.A.%20Finding%20structure%20with%20randomness%3A%20Probabilistic%20algorithms%20for%20constructing%20approximate%20matrix%20decompositions%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Halko%2C%20N.%20Martinsson%2C%20P.-G.%20Tropp%2C%20J.A.%20Finding%20structure%20with%20randomness%3A%20Probabilistic%20algorithms%20for%20constructing%20approximate%20matrix%20decompositions%202011"
        },
        {
            "id": "Heskes_2000_a",
            "entry": "Heskes, T. (2000). On natural learning and pruning in multilayered perceptrons. Neural Computation, 12(4):881\u2013 901.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heskes%2C%20T.%20On%20natural%20learning%20and%20pruning%20in%20multilayered%20perceptrons%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heskes%2C%20T.%20On%20natural%20learning%20and%20pruning%20in%20multilayered%20perceptrons%202000"
        },
        {
            "id": "Kingma_2014_a",
            "entry": "Kingma, D. P. and Ba, J. L. (2014). Adam: A method for stochastic optimization. In Proc. 3rd Int. Conf. Learn. Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.L.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202014"
        },
        {
            "id": "Le_et+al_2008_a",
            "entry": "Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural gradient algorithm. In Advances in neural information processing systems, pages 849\u2013856.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%20Roux%2C%20N.%20Manzagol%2C%20P.-A.%20Bengio%2C%20Y.%20Topmoumoute%20online%20natural%20gradient%20algorithm%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%20Roux%2C%20N.%20Manzagol%2C%20P.-A.%20Bengio%2C%20Y.%20Topmoumoute%20online%20natural%20gradient%20algorithm%202008"
        },
        {
            "id": "Lillicrap_et+al_2016_a",
            "entry": "Lillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. (2016). Random synaptic feedback weights support error backpropagation for deep learning. Nature communications, 7:13276.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20T.P.%20Cownden%2C%20D.%20Tweed%2C%20D.B.%20Akerman%2C%20C.J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lillicrap%2C%20T.P.%20Cownden%2C%20D.%20Tweed%2C%20D.B.%20Akerman%2C%20C.J.%20Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning%202016"
        },
        {
            "id": "Luo_et+al_2017_a",
            "entry": "Luo, H., Fu, J., and Glass, J. (2017). Bidirectional backpropagation: Towards biologically plausible error signal transmission in neural networks. arXiv preprint arXiv:1702.07097.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07097"
        },
        {
            "id": "Mandt_et+al_2017_a",
            "entry": "Mandt, S., Hoffman, M. D., and Blei, D. M. (2017). Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873\u20134907.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mandt%2C%20S.%20Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Stochastic%20gradient%20descent%20as%20approximate%20bayesian%20inference%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mandt%2C%20S.%20Hoffman%2C%20M.D.%20Blei%2C%20D.M.%20Stochastic%20gradient%20descent%20as%20approximate%20bayesian%20inference%202017"
        },
        {
            "id": "Martens_2010_a",
            "entry": "Martens, J. (2010). Deep learning via hessian-free optimization. In International conference on machine learning, volume 27, pages 735\u2013742.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Deep%20learning%20via%20hessian-free%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20J.%20Deep%20learning%20via%20hessian-free%20optimization%202010"
        },
        {
            "id": "Martens_2014_a",
            "entry": "Martens, J. (2014). New insights and perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193.",
            "arxiv_url": "https://arxiv.org/pdf/1412.1193"
        },
        {
            "id": "Martens_2015_a",
            "entry": "Martens, J. and Grosse, R. (2015). Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 2408\u20132417.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20neural%20networks%20with%20Kronecker-factored%20approximate%20curvature%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20neural%20networks%20with%20Kronecker-factored%20approximate%20curvature%202015"
        },
        {
            "id": "Ollivier_2015_a",
            "entry": "Ollivier, Y. (2015). Riemannian metrics for neural networks I: feedforward networks. Information and Inference: A Journal of the IMA, 4(2):108\u2013153.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ollivier%2C%20Y.%20Riemannian%20metrics%20for%20neural%20networks%20I%3A%20feedforward%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ollivier%2C%20Y.%20Riemannian%20metrics%20for%20neural%20networks%20I%3A%20feedforward%20networks%202015"
        },
        {
            "id": "Park_et+al_2000_a",
            "entry": "Park, H., Amari, S.-I., and Fukumizu, K. (2000). Adaptive natural gradient learning algorithms for various stochastic models. Neural Networks, 13(7):755\u2013764.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Park%2C%20H.%20Amari%2C%20S.-I.%20Fukumizu%2C%20K.%20Adaptive%20natural%20gradient%20learning%20algorithms%20for%20various%20stochastic%20models%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Park%2C%20H.%20Amari%2C%20S.-I.%20Fukumizu%2C%20K.%20Adaptive%20natural%20gradient%20learning%20algorithms%20for%20various%20stochastic%20models%202000"
        },
        {
            "id": "Pascanu_2013_a",
            "entry": "Pascanu, R. and Bengio, Y. (2013). Revisiting natural gradient for deep networks. In Proc. 2rd Int. Conf. Learn. Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20R.%20Bengio%2C%20Y.%20Revisiting%20natural%20gradient%20for%20deep%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20R.%20Bengio%2C%20Y.%20Revisiting%20natural%20gradient%20for%20deep%20networks%202013"
        },
        {
            "id": "Povey_et+al_2014_a",
            "entry": "Povey, D., Zhang, X., and Khudanpur, S. (2014). Parallel training of dnns with natural gradient and parameter averaging. arXiv preprint arXiv:1410.7455.",
            "arxiv_url": "https://arxiv.org/pdf/1410.7455"
        },
        {
            "id": "Saxe_et+al_2013_a",
            "entry": "Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In Proc. 2rd Int. Conf. Learn. Representations.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20A.M.%20McClelland%2C%20J.L.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20A.M.%20McClelland%2C%20J.L.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202013"
        },
        {
            "id": "Vinyals_2012_a",
            "entry": "Vinyals, O. and Povey, D. (2012). Krylov subspace descent for deep learning. In Artificial Intelligence and Statistics, pages 1261\u20131268.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vinyals%2C%20O.%20Povey%2C%20D.%20Krylov%20subspace%20descent%20for%20deep%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vinyals%2C%20O.%20Povey%2C%20D.%20Krylov%20subspace%20descent%20for%20deep%20learning%202012"
        },
        {
            "id": "Yang_1998_a",
            "entry": "Yang, H. H. and Amari, S.-i. (1998). Complexity issues in natural gradient descent method for training multilayer perceptrons. Neural Computation, 10(8):2137\u20132157.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20H.H.%20Amari%2C%20S.-i%20Complexity%20issues%20in%20natural%20gradient%20descent%20method%20for%20training%20multilayer%20perceptrons%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20H.H.%20Amari%2C%20S.-i%20Complexity%20issues%20in%20natural%20gradient%20descent%20method%20for%20training%20multilayer%20perceptrons%201998"
        },
        {
            "id": "Zeiler_2012_a",
            "entry": "Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.",
            "arxiv_url": "https://arxiv.org/pdf/1212.5701"
        }
    ]
}
