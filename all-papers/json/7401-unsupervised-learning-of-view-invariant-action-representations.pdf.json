{
    "filename": "7401-unsupervised-learning-of-view-invariant-action-representations.pdf",
    "metadata": {
        "title": "Unsupervised Learning of View-invariant Action Representations",
        "author": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7401-unsupervised-learning-of-view-invariant-action-representations.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets."
    },
    "keywords": [
        {
            "term": "action recognition",
            "url": "https://en.wikipedia.org/wiki/action_recognition"
        },
        {
            "term": "generative adversarial network",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_network"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "Convolutional Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Convolutional_Neural_Network"
        },
        {
            "term": "LSTM",
            "url": "https://en.wikipedia.org/wiki/LSTM"
        },
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        }
    ],
    "highlights": [
        "Recognizing human action in videos is a long-standing research problem in computer vision",
        "We propose an unsupervised learning framework, where the task is to construct the 3D motions for multiple target views using the video representation from a source view",
        "We argue that in order for the network to infer cross-view motion dynamics, the learned representations should reside in a view-invariant discriminative space for action recognition",
        "We propose an unsupervised learning framework that leverages unlabeled video data from multiple views to learn view-invariant video representations that capture motion dynamics",
        "We learn the video representations by using the representations for a source view to predict the 3D flows for multiple target views",
        "We propose a view-adversarial training to enhance view-invariance of the learned representations"
    ],
    "key_statements": [
        "Recognizing human action in videos is a long-standing research problem in computer vision",
        "We propose an unsupervised learning framework, where the task is to construct the 3D motions for multiple target views using the video representation from a source view",
        "We argue that in order for the network to infer cross-view motion dynamics, the learned representations should reside in a view-invariant discriminative space for action recognition",
        "We propose a view-adversarial training that encourages the encoder to learn video representations invariant to view changes",
        "We propose an unsupervised learning framework that leverages unlabeled video data from multiple views to learn view-invariant video representations that capture motion dynamics",
        "We learn the video representations by using the representations for a source view to predict the 3D flows for multiple target views",
        "We propose a view-adversarial training to enhance view-invariance of the learned representations"
    ],
    "summary": [
        "Recognizing human action in videos is a long-standing research problem in computer vision.",
        "We propose an unsupervised learning framework, where the task is to construct the 3D motions for multiple target views using the video representation from a source view.",
        "We argue that in order for the network to infer cross-view motion dynamics, the learned representations should reside in a view-invariant discriminative space for action recognition.",
        "View-invariant representation learning for cross-view action recognition has been widely studied [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>, <a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c59\" href=\"#r59\">59</a>, <a class=\"ref-link\" id=\"c64\" href=\"#r64\">64</a>].",
        "We propose an unsupervised framework to effectively learn view-invariant video representation that can predict motion sequences for multiple views.",
        "We demonstrate the effectiveness of our learned representation on cross-subject and cross-view action recognition tasks.",
        "While the above representation learning mostly capture semantic features, Luo et al [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>] proposed an unsupervised learning framework that predicts future 3D motions from a pair of consecutive frames.",
        "The encoder can learn to generate view-invariant representations that capture motion dynamics.",
        "We achieve this by training a model that uses the representation to predict sequences of motion for multiple views.",
        "Learning flow reconstruction helps the encoder to extract basic motions, and when used together with cross-view decoders, enhances learning of view-invariant motion dynamics.",
        "We propose a view-adversarial training that encourages the encoder to learn video representations invariant to view changes.",
        "For Conv in encoder and depth CNN in cross-view decoder, we employ the ResNet-18 architecture [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] up until the final convolution layer, and add a 1\u00d71\u00d764 convolutional layer to reduce the feature size.",
        "Flow reconstruction and view adversarial training both can improve the cross-view flow prediction performance.",
        "Table 2 shows the classification accuracy for both cross-subject and cross-view action recognition with three input modalities.",
        "The increase in accuracy is more significant for cross-view recognition, which shows that the learned representation is invariant to viewpoint changes.",
        "We perform transfer learning tasks, where we use the unsupervised learned representations for action recognition on two other datasets in new domains.",
        "We propose an unsupervised learning framework that leverages unlabeled video data from multiple views to learn view-invariant video representations that capture motion dynamics.",
        "We train our unsupervised framework on NTU RGB+D dataset, and demonstrate the effectiveness of the learned representations on both cross-subject and cross-view action recognition tasks across multiple datasets.",
        "We intend to extend our framework for view-invariant representation learning in other tasks such as gesture recognition and person re-identification."
    ],
    "headline": "We propose an unsupervised learning framework, which exploits unlabeled data to learn video representations",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In NIPS, pages 153\u2013160, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Lamblin%2C%20P.%20Popovici%2C%20D.%20Larochelle%2C%20H.%20Greedy%20layer-wise%20training%20of%20deep%20networks%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Lamblin%2C%20P.%20Popovici%2C%20D.%20Larochelle%2C%20H.%20Greedy%20layer-wise%20training%20of%20deep%20networks%202006"
        },
        {
            "id": "2",
            "entry": "[2] Y. Bengio, E. Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop. In ICML, pages 226\u2013234, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Y.%20Laufer%2C%20E.%20Alain%2C%20G.%20Yosinski%2C%20J.%20Deep%20generative%20stochastic%20networks%20trainable%20by%20backprop%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Y.%20Laufer%2C%20E.%20Alain%2C%20G.%20Yosinski%2C%20J.%20Deep%20generative%20stochastic%20networks%20trainable%20by%20backprop%202014"
        },
        {
            "id": "3",
            "entry": "[3] J. Carreira and A. Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724\u20134733, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira%2C%20J.%20Zisserman%2C%20A.%20Quo%20vadis%2C%20action%20recognition%3F%20A%20new%20model%20and%20the%20kinetics%20dataset%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira%2C%20J.%20Zisserman%2C%20A.%20Quo%20vadis%2C%20action%20recognition%3F%20A%20new%20model%20and%20the%20kinetics%20dataset%202017"
        },
        {
            "id": "4",
            "entry": "[4] G. Cheng, Y. Wan, A. N. Saudagar, K. Namuduri, and B. P. Buckles. Advances in human action recognition: A survey. arXiv preprint arXiv:1501.05964, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1501.05964"
        },
        {
            "id": "5",
            "entry": "[5] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction. In ICCV, pages 1422\u20131430, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Doersch%2C%20C.%20Gupta%2C%20A.%20Efros%2C%20A.A.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Doersch%2C%20C.%20Gupta%2C%20A.%20Efros%2C%20A.A.%20Unsupervised%20visual%20representation%20learning%20by%20context%20prediction%202015"
        },
        {
            "id": "6",
            "entry": "[6] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, T. Darrell, and K. Saenko. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, pages 2625\u20132634, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donahue%2C%20J.%20Hendricks%2C%20L.A.%20Guadarrama%2C%20S.%20Rohrbach%2C%20M.%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donahue%2C%20J.%20Hendricks%2C%20L.A.%20Guadarrama%2C%20S.%20Rohrbach%2C%20M.%20Long-term%20recurrent%20convolutional%20networks%20for%20visual%20recognition%20and%20description%202015"
        },
        {
            "id": "7",
            "entry": "[7] Y. Du, W. Wang, and L. Wang. Hierarchical recurrent neural network for skeleton based action recognition. In CVPR, pages 1110\u20131118, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%2C%20Y.%20Wang%2C%20W.%20Wang%2C%20L.%20Hierarchical%20recurrent%20neural%20network%20for%20skeleton%20based%20action%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%2C%20Y.%20Wang%2C%20W.%20Wang%2C%20L.%20Hierarchical%20recurrent%20neural%20network%20for%20skeleton%20based%20action%20recognition%202015"
        },
        {
            "id": "8",
            "entry": "[8] B. Fernando, H. Bilen, E. Gavves, and S. Gould. Self-supervised video representation learning with odd-one-out networks. In CVPR, pages 5729\u20135738, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fernando%2C%20B.%20Bilen%2C%20H.%20Gavves%2C%20E.%20Gould%2C%20S.%20Self-supervised%20video%20representation%20learning%20with%20odd-one-out%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fernando%2C%20B.%20Bilen%2C%20H.%20Gavves%2C%20E.%20Gould%2C%20S.%20Self-supervised%20video%20representation%20learning%20with%20odd-one-out%20networks%202017"
        },
        {
            "id": "9",
            "entry": "[9] Y. Ganin and V. S. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, pages 1180\u20131189, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Y.%20Lempitsky%2C%20V.S.%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Y.%20Lempitsky%2C%20V.S.%20Unsupervised%20domain%20adaptation%20by%20backpropagation%202015"
        },
        {
            "id": "10",
            "entry": "[10] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096\u20132030, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganin%2C%20Y.%20Ustinova%2C%20E.%20Ajakan%2C%20H.%20Germain%2C%20P.%20Domain-adversarial%20training%20of%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganin%2C%20Y.%20Ustinova%2C%20E.%20Ajakan%2C%20H.%20Germain%2C%20P.%20Domain-adversarial%20training%20of%20neural%20networks%202016"
        },
        {
            "id": "11",
            "entry": "[11] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gidaris%2C%20S.%20Singh%2C%20P.%20Komodakis%2C%20N.%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gidaris%2C%20S.%20Singh%2C%20P.%20Komodakis%2C%20N.%20Unsupervised%20representation%20learning%20by%20predicting%20image%20rotations%202018"
        },
        {
            "id": "12",
            "entry": "[12] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.J.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.J.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "13",
            "entry": "[13] A. Haque, B. Peng, Z. Luo, A. Alahi, S. Yeung, and F. Li. Towards viewpoint invariant 3d human pose estimation. In ECCV, pages 160\u2013177, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haque%2C%20A.%20Peng%2C%20B.%20Luo%2C%20Z.%20Alahi%2C%20A.%20Towards%20viewpoint%20invariant%203d%20human%20pose%20estimation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haque%2C%20A.%20Peng%2C%20B.%20Luo%2C%20Z.%20Alahi%2C%20A.%20Towards%20viewpoint%20invariant%203d%20human%20pose%20estimation%202016"
        },
        {
            "id": "14",
            "entry": "[14] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, pages 1026\u20131034, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Delving%20deep%20into%20rectifiers%3A%20Surpassing%20human-level%20performance%20on%20imagenet%20classification%202015"
        },
        {
            "id": "15",
            "entry": "[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "16",
            "entry": "[16] J. Hu, W. Zheng, J. Lai, and J. Zhang. Jointly learning heterogeneous features for RGB-D activity recognition. In CVPR, pages 5344\u20135352, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20J.%20Zheng%2C%20W.%20Lai%2C%20J.%20Zhang%2C%20J.%20Jointly%20learning%20heterogeneous%20features%20for%20RGB-D%20activity%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20J.%20Zheng%2C%20W.%20Lai%2C%20J.%20Zhang%2C%20J.%20Jointly%20learning%20heterogeneous%20features%20for%20RGB-D%20activity%20recognition%202015"
        },
        {
            "id": "17",
            "entry": "[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "18",
            "entry": "[18] L. Isik, A. Tacchetti, and T. Poggio. A fast, invariant representation for human action in the visual system. Journal of neurophysiology, 119(2):631\u2013640, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isik%2C%20L.%20Tacchetti%2C%20A.%20Poggio%2C%20T.%20A%20fast%2C%20invariant%20representation%20for%20human%20action%20in%20the%20visual%20system%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isik%2C%20L.%20Tacchetti%2C%20A.%20Poggio%2C%20T.%20A%20fast%2C%20invariant%20representation%20for%20human%20action%20in%20the%20visual%20system%202017"
        },
        {
            "id": "19",
            "entry": "[19] M. Jaimez, M. Souiai, J. G. Jim\u00e9nez, and D. Cremers. A primal-dual framework for real-time dense RGB-D scene flow. In ICRA, pages 98\u2013104, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaimez%2C%20M.%20Souiai%2C%20M.%20Jim%C3%A9nez%2C%20J.G.%20Cremers%2C%20D.%20A%20primal-dual%20framework%20for%20real-time%20dense%20RGB-D%20scene%20flow%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaimez%2C%20M.%20Souiai%2C%20M.%20Jim%C3%A9nez%2C%20J.G.%20Cremers%2C%20D.%20A%20primal-dual%20framework%20for%20real-time%20dense%20RGB-D%20scene%20flow%202015"
        },
        {
            "id": "20",
            "entry": "[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20D.P.%20Ba%2C%20J.%20Adam%3A%20A%20method%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "21",
            "entry": "[21] Y. Kong, Z. Ding, J. Li, and Y. Fu. Deeply learned view-invariant features for cross-view action recognition. IEEE Trans. Image Processing, 26(6):3028\u20133037, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kong%2C%20Y.%20Ding%2C%20Z.%20Li%2C%20J.%20Fu%2C%20Y.%20Deeply%20learned%20view-invariant%20features%20for%20cross-view%20action%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kong%2C%20Y.%20Ding%2C%20Z.%20Li%2C%20J.%20Fu%2C%20Y.%20Deeply%20learned%20view-invariant%20features%20for%20cross-view%20action%20recognition%202017"
        },
        {
            "id": "22",
            "entry": "[22] Q. V. Le. Building high-level features using large scale unsupervised learning. In ICASSP, pages 8595\u2013 8598, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Q.V.%20Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Q.V.%20Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning%202013"
        },
        {
            "id": "23",
            "entry": "[23] H. Lee, J. Huang, M. Singh, and M. Yang. Unsupervised representation learning by sorting sequences. In ICCV, pages 667\u2013676, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20H.%20Huang%2C%20J.%20Singh%2C%20M.%20Yang%2C%20M.%20Unsupervised%20representation%20learning%20by%20sorting%20sequences%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20H.%20Huang%2C%20J.%20Singh%2C%20M.%20Yang%2C%20M.%20Unsupervised%20representation%20learning%20by%20sorting%20sequences%202017"
        },
        {
            "id": "24",
            "entry": "[24] I. Lee, D. Kim, S. Kang, and S. Lee. Ensemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks. In ICCV, pages 1012\u20131020, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20I.%20Kim%2C%20D.%20Kang%2C%20S.%20Lee%2C%20S.%20Ensemble%20deep%20learning%20for%20skeleton-based%20action%20recognition%20using%20temporal%20sliding%20LSTM%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20I.%20Kim%2C%20D.%20Kang%2C%20S.%20Lee%2C%20S.%20Ensemble%20deep%20learning%20for%20skeleton-based%20action%20recognition%20using%20temporal%20sliding%20LSTM%20networks%202017"
        },
        {
            "id": "25",
            "entry": "[25] B. Li, O. I. Camps, and M. Sznaier. Cross-view activity recognition using hankelets. In CVPR, pages 1362\u20131369, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20B.%20Camps%2C%20O.I.%20Sznaier%2C%20M.%20Cross-view%20activity%20recognition%20using%20hankelets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20B.%20Camps%2C%20O.I.%20Sznaier%2C%20M.%20Cross-view%20activity%20recognition%20using%20hankelets%202012"
        },
        {
            "id": "26",
            "entry": "[26] J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli. Attention transfer from web images for video recognition. In ACM Multimedia, pages 1\u20139, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Wong%2C%20Y.%20Zhao%2C%20Q.%20Kankanhalli%2C%20M.S.%20Attention%20transfer%20from%20web%20images%20for%20video%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Wong%2C%20Y.%20Zhao%2C%20Q.%20Kankanhalli%2C%20M.S.%20Attention%20transfer%20from%20web%20images%20for%20video%20recognition%202017"
        },
        {
            "id": "27",
            "entry": "[27] J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli. Dual-glance model for deciphering social relationships. In ICCV, pages 2669\u20132678, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20J.%20Wong%2C%20Y.%20Zhao%2C%20Q.%20Kankanhalli%2C%20M.S.%20Dual-glance%20model%20for%20deciphering%20social%20relationships%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20J.%20Wong%2C%20Y.%20Zhao%2C%20Q.%20Kankanhalli%2C%20M.S.%20Dual-glance%20model%20for%20deciphering%20social%20relationships%202017"
        },
        {
            "id": "28",
            "entry": "[28] R. Li and T. Zickler. Discriminative virtual views for cross-view action recognition. In CVPR, pages 2855\u20132862, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20R.%20Zickler%2C%20T.%20Discriminative%20virtual%20views%20for%20cross-view%20action%20recognition%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20R.%20Zickler%2C%20T.%20Discriminative%20virtual%20views%20for%20cross-view%20action%20recognition%202012"
        },
        {
            "id": "29",
            "entry": "[29] W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. In CVPR, pages 9\u201314, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20W.%20Zhang%2C%20Z.%20Liu%2C%20Z.%20Action%20recognition%20based%20on%20a%20bag%20of%203d%20points%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20W.%20Zhang%2C%20Z.%20Liu%2C%20Z.%20Action%20recognition%20based%20on%20a%20bag%20of%203d%20points%202010"
        },
        {
            "id": "30",
            "entry": "[30] J. Liu, A. Shahroudy, D. Xu, and G. Wang. Spatio-temporal LSTM with trust gates for 3D human action recognition. In ECCV, pages 816\u2013833, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20J.%20Shahroudy%2C%20A.%20Xu%2C%20D.%20Wang%2C%20G.%20Spatio-temporal%20LSTM%20with%20trust%20gates%20for%203D%20human%20action%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20J.%20Shahroudy%2C%20A.%20Xu%2C%20D.%20Wang%2C%20G.%20Spatio-temporal%20LSTM%20with%20trust%20gates%20for%203D%20human%20action%20recognition%202016"
        },
        {
            "id": "31",
            "entry": "[31] J. Liu, G. Wang, P. Hu, L. Duan, and A. C. Kot. Global context-aware attention LSTM networks for 3d action recognition. In CVPR, pages 3671\u20133680, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20J.%20Wang%2C%20G.%20Hu%2C%20P.%20Duan%2C%20L.%20Global%20context-aware%20attention%20LSTM%20networks%20for%203d%20action%20recognition%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20J.%20Wang%2C%20G.%20Hu%2C%20P.%20Duan%2C%20L.%20Global%20context-aware%20attention%20LSTM%20networks%20for%203d%20action%20recognition%202017"
        },
        {
            "id": "32",
            "entry": "[32] C. Lu, J. Jia, and C. Tang. Range-sample depth feature for action recognition. In CVPR, pages 772\u2013779, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20C.%20Jia%2C%20J.%20Tang%2C%20C.%20Range-sample%20depth%20feature%20for%20action%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20C.%20Jia%2C%20J.%20Tang%2C%20C.%20Range-sample%20depth%20feature%20for%20action%20recognition%202014"
        },
        {
            "id": "33",
            "entry": "[33] Z. Luo, B. Peng, D. Huang, A. Alahi, and L. Fei-Fei. Unsupervised learning of long-term motion dynamics for videos. In CVPR, pages 7101\u20137110, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luo%2C%20Z.%20Peng%2C%20B.%20Huang%2C%20D.%20Alahi%2C%20A.%20Unsupervised%20learning%20of%20long-term%20motion%20dynamics%20for%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luo%2C%20Z.%20Peng%2C%20B.%20Huang%2C%20D.%20Alahi%2C%20A.%20Unsupervised%20learning%20of%20long-term%20motion%20dynamics%20for%20videos%202017"
        },
        {
            "id": "34",
            "entry": "[34] I. Misra, C. L. Zitnick, and M. Hebert. Shuffle and learn: Unsupervised learning using temporal order verification. In ECCV, pages 527\u2013544, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20I.%20Zitnick%2C%20C.L.%20Hebert%2C%20M.%20Shuffle%20and%20learn%3A%20Unsupervised%20learning%20using%20temporal%20order%20verification%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20I.%20Zitnick%2C%20C.L.%20Hebert%2C%20M.%20Shuffle%20and%20learn%3A%20Unsupervised%20learning%20using%20temporal%20order%20verification%202016"
        },
        {
            "id": "35",
            "entry": "[35] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici. Beyond short snippets: Deep networks for video classification. In CVPR, pages 4694\u20134702, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ng%2C%20J.Y.%20Hausknecht%2C%20M.J.%20Vijayanarasimhan%2C%20S.%20Vinyals%2C%20O.%20Beyond%20short%20snippets%3A%20Deep%20networks%20for%20video%20classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ng%2C%20J.Y.%20Hausknecht%2C%20M.J.%20Vijayanarasimhan%2C%20S.%20Vinyals%2C%20O.%20Beyond%20short%20snippets%3A%20Deep%20networks%20for%20video%20classification%202015"
        },
        {
            "id": "36",
            "entry": "[36] M. Noroozi, H. Pirsiavash, and P. Favaro. Representation learning by learning to count. In ICCV, pages 5899\u20135907, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noroozi%2C%20M.%20Pirsiavash%2C%20H.%20Favaro%2C%20P.%20Representation%20learning%20by%20learning%20to%20count%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noroozi%2C%20M.%20Pirsiavash%2C%20H.%20Favaro%2C%20P.%20Representation%20learning%20by%20learning%20to%20count%202017"
        },
        {
            "id": "37",
            "entry": "[37] E. Ohn-Bar and M. M. Trivedi. Joint angles similarities and HOG2 for action recognition. In CVPR workshops, pages 465\u2013470, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ohn-Bar%2C%20E.%20Trivedi%2C%20M.M.%20Joint%20angles%20similarities%20and%20HOG2%20for%20action%20recognition%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ohn-Bar%2C%20E.%20Trivedi%2C%20M.M.%20Joint%20angles%20similarities%20and%20HOG2%20for%20action%20recognition%202013"
        },
        {
            "id": "38",
            "entry": "[38] D. Oneata, J. J. Verbeek, and C. Schmid. Action and event recognition with fisher vectors on a compact feature set. In ICCV, pages 1817\u20131824, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oneata%2C%20D.%20Verbeek%2C%20J.J.%20Schmid%2C%20C.%20Action%20and%20event%20recognition%20with%20fisher%20vectors%20on%20a%20compact%20feature%20set%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oneata%2C%20D.%20Verbeek%2C%20J.J.%20Schmid%2C%20C.%20Action%20and%20event%20recognition%20with%20fisher%20vectors%20on%20a%20compact%20feature%20set%202013"
        },
        {
            "id": "39",
            "entry": "[39] O. Oreifej and Z. Liu. HON4D: histogram of oriented 4d normals for activity recognition from depth sequences. In CVPR, pages 716\u2013723, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Oreifej%2C%20O.%20Liu%2C%20Z.%20HON4D%3A%20histogram%20of%20oriented%204d%20normals%20for%20activity%20recognition%20from%20depth%20sequences%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Oreifej%2C%20O.%20Liu%2C%20Z.%20HON4D%3A%20histogram%20of%20oriented%204d%20normals%20for%20activity%20recognition%20from%20depth%20sequences%202013"
        },
        {
            "id": "40",
            "entry": "[40] V. Parameswaran and R. Chellappa. View invariance for human action recognition. IJCV, 66(1):83\u2013101, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parameswaran%2C%20V.%20Chellappa%2C%20R.%20View%20invariance%20for%20human%20action%20recognition%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parameswaran%2C%20V.%20Chellappa%2C%20R.%20View%20invariance%20for%20human%20action%20recognition%202006"
        },
        {
            "id": "41",
            "entry": "[41] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal video autoencoder with differentiable memory. In ICLR workshops, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patraucean%2C%20V.%20Handa%2C%20A.%20Cipolla%2C%20R.%20Spatio-temporal%20video%20autoencoder%20with%20differentiable%20memory%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patraucean%2C%20V.%20Handa%2C%20A.%20Cipolla%2C%20R.%20Spatio-temporal%20video%20autoencoder%20with%20differentiable%20memory%202016"
        },
        {
            "id": "42",
            "entry": "[42] H. Rahmani and M. Bennamoun. Learning action recognition model from depth and skeleton videos. In ICCV, pages 5833\u20135842, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahmani%2C%20H.%20Bennamoun%2C%20M.%20Learning%20action%20recognition%20model%20from%20depth%20and%20skeleton%20videos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahmani%2C%20H.%20Bennamoun%2C%20M.%20Learning%20action%20recognition%20model%20from%20depth%20and%20skeleton%20videos%202017"
        },
        {
            "id": "43",
            "entry": "[43] H. Rahmani, A. Mahmood, D. Q. Huynh, and A. S. Mian. Histogram of oriented principal components for cross-view action recognition. IEEE TPAMI, 38(12):2430\u20132443, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahmani%2C%20H.%20Mahmood%2C%20A.%20Huynh%2C%20D.Q.%20Mian%2C%20A.S.%20Histogram%20of%20oriented%20principal%20components%20for%20cross-view%20action%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahmani%2C%20H.%20Mahmood%2C%20A.%20Huynh%2C%20D.Q.%20Mian%2C%20A.S.%20Histogram%20of%20oriented%20principal%20components%20for%20cross-view%20action%20recognition%202016"
        },
        {
            "id": "44",
            "entry": "[44] H. Rahmani and A. S. Mian. 3d action recognition from novel viewpoints. In CVPR, pages 1506\u20131515, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahmani%2C%20H.%20Mian%2C%20A.S.%203d%20action%20recognition%20from%20novel%20viewpoints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahmani%2C%20H.%20Mian%2C%20A.S.%203d%20action%20recognition%20from%20novel%20viewpoints%202016"
        },
        {
            "id": "45",
            "entry": "[45] H. Rahmani, A. S. Mian, and M. Shah. Learning a deep model for human action recognition from novel viewpoints. IEEE TPAMI, 40(3):667\u2013681, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahmani%2C%20H.%20Mian%2C%20A.S.%20Shah%2C%20M.%20Learning%20a%20deep%20model%20for%20human%20action%20recognition%20from%20novel%20viewpoints%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahmani%2C%20H.%20Mian%2C%20A.S.%20Shah%2C%20M.%20Learning%20a%20deep%20model%20for%20human%20action%20recognition%20from%20novel%20viewpoints%202018"
        },
        {
            "id": "46",
            "entry": "[46] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and S. Chopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6604"
        },
        {
            "id": "47",
            "entry": "[47] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In AISTATS, pages 448\u2013455, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Hinton%2C%20G.E.%20Deep%20boltzmann%20machines%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Hinton%2C%20G.E.%20Deep%20boltzmann%20machines%202009"
        },
        {
            "id": "48",
            "entry": "[48] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multistage feature learning. In CVPR, pages 3626\u20133633, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sermanet%2C%20P.%20Kavukcuoglu%2C%20K.%20Chintala%2C%20S.%20LeCun%2C%20Y.%20Pedestrian%20detection%20with%20unsupervised%20multistage%20feature%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sermanet%2C%20P.%20Kavukcuoglu%2C%20K.%20Chintala%2C%20S.%20LeCun%2C%20Y.%20Pedestrian%20detection%20with%20unsupervised%20multistage%20feature%20learning%202013"
        },
        {
            "id": "49",
            "entry": "[49] A. Shahroudy, J. Liu, T. Ng, and G. Wang. NTU RGB+D: A large scale dataset for 3D human activity analysis. In CVPR, pages 1010\u20131019, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shahroudy%2C%20A.%20Liu%2C%20J.%20Ng%2C%20T.%20Wang%2C%20G.%20NTU%20RGB%2BD%3A%20A%20large%20scale%20dataset%20for%203D%20human%20activity%20analysis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shahroudy%2C%20A.%20Liu%2C%20J.%20Ng%2C%20T.%20Wang%2C%20G.%20NTU%20RGB%2BD%3A%20A%20large%20scale%20dataset%20for%203D%20human%20activity%20analysis%202016"
        },
        {
            "id": "50",
            "entry": "[50] A. Shahroudy, T.-T. Ng, Y. Gong, and G. Wang. Deep multimodal feature analysis for action recognition in RGB+D videos. IEEE TPAMI, 40(5):1045\u20131058, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shahroudy%2C%20A.%20Ng%2C%20T.-T.%20Gong%2C%20Y.%20Wang%2C%20G.%20Deep%20multimodal%20feature%20analysis%20for%20action%20recognition%20in%20RGB%2BD%20videos%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shahroudy%2C%20A.%20Ng%2C%20T.-T.%20Gong%2C%20Y.%20Wang%2C%20G.%20Deep%20multimodal%20feature%20analysis%20for%20action%20recognition%20in%20RGB%2BD%20videos%202018"
        },
        {
            "id": "51",
            "entry": "[51] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In NIPS, pages 568\u2013576, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Two-stream%20convolutional%20networks%20for%20action%20recognition%20in%20videos%202014"
        },
        {
            "id": "52",
            "entry": "[52] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6806"
        },
        {
            "id": "53",
            "entry": "[53] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using lstms. In ICML, pages 843\u2013852, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhutdinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20N.%20Mansimov%2C%20E.%20Salakhutdinov%2C%20R.%20Unsupervised%20learning%20of%20video%20representations%20using%20lstms%202015"
        },
        {
            "id": "54",
            "entry": "[54] R. Vemulapalli, F. Arrate, and R. Chellappa. Human action recognition by representing 3d skeletons as points in a lie group. In CVPR, pages 588\u2013595, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vemulapalli%2C%20R.%20Arrate%2C%20F.%20Chellappa%2C%20R.%20Human%20action%20recognition%20by%20representing%203d%20skeletons%20as%20points%20in%20a%20lie%20group%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vemulapalli%2C%20R.%20Arrate%2C%20F.%20Chellappa%2C%20R.%20Human%20action%20recognition%20by%20representing%203d%20skeletons%20as%20points%20in%20a%20lie%20group%202014"
        },
        {
            "id": "55",
            "entry": "[55] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, pages 1096\u20131103, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vincent%2C%20P.%20Larochelle%2C%20H.%20Bengio%2C%20Y.%20Manzagol%2C%20P.%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vincent%2C%20P.%20Larochelle%2C%20H.%20Bengio%2C%20Y.%20Manzagol%2C%20P.%20Extracting%20and%20composing%20robust%20features%20with%20denoising%20autoencoders%202008"
        },
        {
            "id": "56",
            "entry": "[56] H. Wang and C. Schmid. Action recognition with improved trajectories. In ICCV, pages 3551\u20133558, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20H.%20Schmid%2C%20C.%20Action%20recognition%20with%20improved%20trajectories%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20H.%20Schmid%2C%20C.%20Action%20recognition%20with%20improved%20trajectories%202013"
        },
        {
            "id": "57",
            "entry": "[57] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras. In CVPR, pages 1290\u20131297, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20J.%20Liu%2C%20Z.%20Wu%2C%20Y.%20Yuan%2C%20J.%20Mining%20actionlet%20ensemble%20for%20action%20recognition%20with%20depth%20cameras%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20J.%20Liu%2C%20Z.%20Wu%2C%20Y.%20Yuan%2C%20J.%20Mining%20actionlet%20ensemble%20for%20action%20recognition%20with%20depth%20cameras%202012"
        },
        {
            "id": "58",
            "entry": "[58] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Learning actionlet ensemble for 3d human action recognition. IEEE TPAMI, 36(5):914\u2013927, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20J.%20Liu%2C%20Z.%20Wu%2C%20Y.%20Yuan%2C%20J.%20Learning%20actionlet%20ensemble%20for%203d%20human%20action%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20J.%20Liu%2C%20Z.%20Wu%2C%20Y.%20Yuan%2C%20J.%20Learning%20actionlet%20ensemble%20for%203d%20human%20action%20recognition%202014"
        },
        {
            "id": "59",
            "entry": "[59] J. Wang, X. Nie, Y. Xia, Y. Wu, and S. Zhu. Cross-view action modeling, learning, and recognition. In CVPR, pages 2649\u20132656, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20J.%20Nie%2C%20X.%20Xia%2C%20Y.%20Wu%2C%20Y.%20Cross-view%20action%20modeling%2C%20learning%2C%20and%20recognition%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20J.%20Nie%2C%20X.%20Xia%2C%20Y.%20Wu%2C%20Y.%20Cross-view%20action%20modeling%2C%20learning%2C%20and%20recognition%202014"
        },
        {
            "id": "60",
            "entry": "[60] P. Wang, W. Li, Z. Gao, Y. Zhang, C. Tang, and P. Ogunbona. Scene flow to action map: A new representation for RGB-D based action recognition with convolutional neural networks. In CVPR, pages 416\u2013425, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20P.%20Li%2C%20W.%20Gao%2C%20Z.%20Zhang%2C%20Y.%20Scene%20flow%20to%20action%20map%3A%20A%20new%20representation%20for%20RGB-D%20based%20action%20recognition%20with%20convolutional%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20P.%20Li%2C%20W.%20Gao%2C%20Z.%20Zhang%2C%20Y.%20Scene%20flow%20to%20action%20map%3A%20A%20new%20representation%20for%20RGB-D%20based%20action%20recognition%20with%20convolutional%20neural%20networks%202017"
        },
        {
            "id": "61",
            "entry": "[61] X. Wang, K. He, and A. Gupta. Transitive invariance for self-supervised visual representation learning. In ICCV, pages 1338\u20131347, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20X.%20He%2C%20K.%20Gupta%2C%20A.%20Transitive%20invariance%20for%20self-supervised%20visual%20representation%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20X.%20He%2C%20K.%20Gupta%2C%20A.%20Transitive%20invariance%20for%20self-supervised%20visual%20representation%20learning%202017"
        },
        {
            "id": "62",
            "entry": "[62] X. Yang and Y. Tian. Super normal vector for activity recognition using depth sequences. In CVPR, pages 804\u2013811, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20X.%20Tian%2C%20Y.%20Super%20normal%20vector%20for%20activity%20recognition%20using%20depth%20sequences%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20X.%20Tian%2C%20Y.%20Super%20normal%20vector%20for%20activity%20recognition%20using%20depth%20sequences%202014"
        },
        {
            "id": "63",
            "entry": "[63] P. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, and N. Zheng. View adaptive recurrent neural networks for high performance human action recognition from skeleton data. In ICCV, pages 2136\u20132145, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20P.%20Lan%2C%20C.%20Xing%2C%20J.%20Zeng%2C%20W.%20View%20adaptive%20recurrent%20neural%20networks%20for%20high%20performance%20human%20action%20recognition%20from%20skeleton%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20P.%20Lan%2C%20C.%20Xing%2C%20J.%20Zeng%2C%20W.%20View%20adaptive%20recurrent%20neural%20networks%20for%20high%20performance%20human%20action%20recognition%20from%20skeleton%20data%202017"
        },
        {
            "id": "64",
            "entry": "[64] Z. Zhang, C. Wang, B. Xiao, W. Zhou, S. Liu, and C. Shi. Cross-view action recognition via a continuous virtual path. In CVPR, pages 2690\u20132697, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Z.%20Wang%2C%20C.%20Xiao%2C%20B.%20Zhou%2C%20W.%20Cross-view%20action%20recognition%20via%20a%20continuous%20virtual%20path%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Z.%20Wang%2C%20C.%20Xiao%2C%20B.%20Zhou%2C%20W.%20Cross-view%20action%20recognition%20via%20a%20continuous%20virtual%20path%202013"
        }
    ]
}
