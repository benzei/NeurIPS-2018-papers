{
    "filename": "7895-contamination-attacks-and-mitigation-in-multi-party-machine-learning.pdf",
    "metadata": {
        "title": "Contamination Attacks and Mitigation in Multi-Party Machine Learning",
        "author": "Jamie Hayes, Olga Ohrimenko",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7895-contamination-attacks-and-mitigation-in-multi-party-machine-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Machine learning is data hungry; the more data a model has access to in training, the more likely it is to perform well at inference time. Distinct parties may want to combine their local data to gain the benefits of a model trained on a large corpus of data. We consider such a case: parties get access to the model trained on their joint data but do not see each others individual datasets. We show that one needs to be careful when using this multi-party model since a potentially malicious party can taint the model by providing contaminated data. We then show how adversarial training can defend against such attacks by preventing the model from learning trends specific to individual parties data, thereby also guaranteeing party-level membership privacy."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "multi-party computation",
            "url": "https://en.wikipedia.org/wiki/multi-party_computation"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "Multi-party machine learning allows several parties to combine their datasets and run algorithms on their joint data in order to get insights that may not be present in their individual datasets",
        "Secure multi-party computation can be enabled with cryptographic techniques [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], and systems based on trusted processors such as Intel SGX [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Datasets We evaluated the attack on three datasets: UCI Adult 3 (ADULT), UCI Credit Card 4 (CREDIT CARD), and News20 5 (NEWS20)",
        "Figure 2a shows how adversarial training mitigates contamination attacks launched as described in Section 2 for the Adult 3 dataset with 10% of the training set containing contaminated records, and CREDIT CARD and News20 5 datasets with 10%, and 5%, respectively",
        "We showed that adversarial training mitigates this kind of attack while providing protection against party membership inference attacks, at no cost to model performance",
        "Distributed or collaborative machine learning, where each party trains the model locally, provides an additional attack vector compared to the centralized model considered here, since the attack can be updated throughout training"
    ],
    "key_statements": [
        "Multi-party machine learning allows several parties to combine their datasets and run algorithms on their joint data in order to get insights that may not be present in their individual datasets",
        "Secure multi-party computation can be enabled with cryptographic techniques [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>], and systems based on trusted processors such as Intel SGX [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We are interested in the setting where a small number of parties wish to use a secure centralized multi-party machine learning service to train a model on their joint data",
        "We show that adversarial training [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] is successful at defending against contamination attacks while being unaware of which attributes and class labels are targeted by the malicious party",
        "We show that adversarial training mitigates such attacks, even when the attribute and label under attack is unknown",
        "We show that in addition to protecting against contamination attacks, adversarial training can be used to mitigate privacy-related attacks such as party membership inference of individual records",
        "Datasets We evaluated the attack on three datasets: UCI Adult 3 (ADULT), UCI Credit Card 4 (CREDIT CARD), and News20 5 (NEWS20)",
        "We present adversarial training as a general defense against contamination attacks",
        "Adversarial training minimizes the party-level information output by a prediction, minimizing the effect that contaminated records have on the multi-party model",
        "We evaluate adversarial training as a method for training a multi-party model and as a defense against contamination attacks",
        "Figure 2a shows how adversarial training mitigates contamination attacks launched as described in Section 2 for the Adult 3 dataset with 10% of the training set containing contaminated records, and CREDIT CARD and News20 5 datasets with 10%, and 5%, respectively",
        "Figure 2b shows for the Adult 3 dataset, that contamination accuracy of the adversarially trained model was close to the baseline of the local model regardless of the fraction of contaminated records in the training set",
        "For adversarial training to be an efficient training method in multi-party machine learning, it must not decrease the validation accuracy when data comes from dissimilar distributions",
        "We find that adversarial training decreases the validation accuracy by only 0.6%, as shown in the fist column of Table 2",
        "We showed that adversarial training mitigates this kind of attack while providing protection against party membership inference attacks, at no cost to model performance",
        "Distributed or collaborative machine learning, where each party trains the model locally, provides an additional attack vector compared to the centralized model considered here, since the attack can be updated throughout training"
    ],
    "summary": [
        "Multi-party machine learning allows several parties to combine their datasets and run algorithms on their joint data in order to get insights that may not be present in their individual datasets.",
        "We are interested in the setting where a small number of parties wish to use a secure centralized multi-party machine learning service to train a model on their joint data.",
        "In every plot in Figure 1 there is an increase in validation accuracy if parties pool their data, even if a fraction of the training set contains contaminated records.",
        "As expected, the validation accuracy difference between the multi-party and local model narrows as more contaminated records are introduced into the training set.",
        "Contaminated records leak information about the party identity through predictions since the attacker has created a strong correlation between the contaminated attribute and label that is not present in other parties\u2019 data.",
        "Adversarial training minimizes the party-level information output by a prediction, minimizing the effect that contaminated records have on the multi-party model.",
        "We extend the theoretical results of Louppe et al [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] and show that if f is trained with party identifier as a pivot attribute we obtain (1) party-level membership privacy for the records in the training data and (2) the classifier learns only the trends that are common to all the parties, thereby not learning information from contaminated records.",
        "We evaluate adversarial training as a method for training a multi-party model and as a defense against contamination attacks.",
        "Figure 2a shows how adversarial training mitigates contamination attacks launched as described in Section 2 for the ADULT dataset with 10% of the training set containing contaminated records, and CREDIT CARD and NEWS20 datasets with 10%, and 5%, respectively.",
        "Figure 2b shows for the ADULT dataset, that contamination accuracy of the adversarially trained model was close to the baseline of the local model regardless of the fraction of contaminated records in the training set.",
        "For adversarial training to be an efficient training method in multi-party machine learning, it must not decrease the validation accuracy when data comes from dissimilar distributions.",
        "In multi-party machine learning, given a training record, predicting which party it belongs to is a form of a membership inference attack and has real privacy concerns.",
        "0.0 Fractio0n.02of conta0m.0i4nated re0c.o06rds in tra0i.n08ing set 0.1 (a) Training set contains 10%, 10%, and 5% contaminated records for ADULT, CREDIT CARD, and NEWS20 dataset, respectively.",
        "Investigating efficacy of contamination attacks and our mitigation in this setting is an interesting direction to explore"
    ],
    "headline": "We show that one needs to be careful when using this multi-party model since a potentially malicious party can taint the model by providing contaminated data",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In ACM Conference on Computer and Communications Security (CCS), pages 308\u2013318, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Chu%2C%20A.%20Goodfellow%2C%20I.%20McMahan%2C%20H.B.%20Deep%20learning%20with%20differential%20privacy%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20M.%20Chu%2C%20A.%20Goodfellow%2C%20I.%20McMahan%2C%20H.B.%20Deep%20learning%20with%20differential%20privacy%202016"
        },
        {
            "id": "2",
            "entry": "[2] S. Alfeld, X. Zhu, and P. Barford. Data poisoning attacks against autoregressive models. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1452\u20131458, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alfeld%2C%20S.%20Zhu%2C%20X.%20Barford%2C%20P.%20Data%20poisoning%20attacks%20against%20autoregressive%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alfeld%2C%20S.%20Zhu%2C%20X.%20Barford%2C%20P.%20Data%20poisoning%20attacks%20against%20autoregressive%20models%202016"
        },
        {
            "id": "3",
            "entry": "[3] J. Allen, B. Ding, J. Kulkarni, H. Nori, O. Ohrimenko, and S. Yekhanin. An algorithmic framework for differentially private data analysis on trusted processors. CoRR, abs/1807.00736, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.00736"
        },
        {
            "id": "4",
            "entry": "[4] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In International Conference on Machine Learning (ICML), pages 1467\u20131474, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20B.%20Nelson%2C%20B.%20Laskov%2C%20P.%20Poisoning%20attacks%20against%20support%20vector%20machines%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20B.%20Nelson%2C%20B.%20Laskov%2C%20P.%20Poisoning%20attacks%20against%20support%20vector%20machines%202012"
        },
        {
            "id": "5",
            "entry": "[5] A. Bittau, U. Erlingsson, P. Maniatis, I. Mironov, A. Raghunathan, D. Lie, M. Rudominer, U. Kode, J. Tinnes, and B. Seefeld. Prochlo: Strong privacy for analytics in the crowd. In ACM Symposium on Operating Systems Principles (SOSP), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bittau%2C%20A.%20Erlingsson%2C%20U.%20Maniatis%2C%20P.%20Mironov%2C%20I.%20Prochlo%3A%20Strong%20privacy%20for%20analytics%20in%20the%20crowd%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bittau%2C%20A.%20Erlingsson%2C%20U.%20Maniatis%2C%20P.%20Mironov%2C%20I.%20Prochlo%3A%20Strong%20privacy%20for%20analytics%20in%20the%20crowd%202017"
        },
        {
            "id": "6",
            "entry": "[6] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning. In ACM Conference on Computer and Communications Security (CCS), pages 1175\u20131191, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonawitz%2C%20K.%20Ivanov%2C%20V.%20Kreuter%2C%20B.%20Marcedone%2C%20A.%20Practical%20secure%20aggregation%20for%20privacy-preserving%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bonawitz%2C%20K.%20Ivanov%2C%20V.%20Kreuter%2C%20B.%20Marcedone%2C%20A.%20Practical%20secure%20aggregation%20for%20privacy-preserving%20machine%20learning%202017"
        },
        {
            "id": "7",
            "entry": "[7] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (S&P), pages 39\u201357, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20N.%20Wagner%2C%20D.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "8",
            "entry": "[8] X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05526"
        },
        {
            "id": "9",
            "entry": "[9] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Conference on Innovations in Theoretical Computer Science Conference (ITCS), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dwork%2C%20C.%20Hardt%2C%20M.%20Pitassi%2C%20T.%20Reingold%2C%20O.%20Fairness%20through%20awareness%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dwork%2C%20C.%20Hardt%2C%20M.%20Pitassi%2C%20T.%20Reingold%2C%20O.%20Fairness%20through%20awareness%202012"
        },
        {
            "id": "10",
            "entry": "[10] H. Edwards and A. Storkey. Censoring representations with an adversary. In International Conference on Learning Representations (ICLR), 2 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Edwards%2C%20H.%20Storkey%2C%20A.%20Censoring%20representations%20with%20an%20adversary%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Edwards%2C%20H.%20Storkey%2C%20A.%20Censoring%20representations%20with%20an%20adversary%202016"
        },
        {
            "id": "11",
            "entry": "[11] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In International Conference on Machine Learning (ICML), pages 201\u2013210, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gilad-Bachrach%2C%20R.%20Dowlin%2C%20N.%20Laine%2C%20K.%20Lauter%2C%20K.%20Cryptonets%3A%20Applying%20neural%20networks%20to%20encrypted%20data%20with%20high%20throughput%20and%20accuracy%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gilad-Bachrach%2C%20R.%20Dowlin%2C%20N.%20Laine%2C%20K.%20Lauter%2C%20K.%20Cryptonets%3A%20Applying%20neural%20networks%20to%20encrypted%20data%20with%20high%20throughput%20and%20accuracy%202016"
        },
        {
            "id": "12",
            "entry": "[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Conference on Neural Information Processing Systems (NIPS), pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "13",
            "entry": "[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "14",
            "entry": "[14] T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.06733"
        },
        {
            "id": "15",
            "entry": "[15] J. Hamm. Minimax filter: Learning to preserve privacy from inference attacks. Journal of Machine Learning Research, 18:129:1\u2013129:31, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamm%2C%20J.%20Minimax%20filter%3A%20Learning%20to%20preserve%20privacy%20from%20inference%20attacks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamm%2C%20J.%20Minimax%20filter%3A%20Learning%20to%20preserve%20privacy%20from%20inference%20attacks%202017"
        },
        {
            "id": "16",
            "entry": "[16] J. Hamm, P. Cao, and M. Belkin. Learning privately from multiparty data. In International Conference on Machine Learning (ICML), pages 555\u2013563, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hamm%2C%20J.%20Cao%2C%20P.%20Belkin%2C%20M.%20Learning%20privately%20from%20multiparty%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hamm%2C%20J.%20Cao%2C%20P.%20Belkin%2C%20M.%20Learning%20privately%20from%20multiparty%20data%202016"
        },
        {
            "id": "17",
            "entry": "[17] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro. Membership Inference Attacks Against Generative Models. Proceedings on Privacy Enhancing Technologies (PoPETs), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hayes%2C%20J.%20Melis%2C%20L.%20Danezis%2C%20G.%20Cristofaro%2C%20E.De%20Membership%20Inference%20Attacks%20Against%20Generative%20Models.%20Proceedings%20on%20Privacy%20Enhancing%20Technologies%20%28PoPETs%29%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hayes%2C%20J.%20Melis%2C%20L.%20Danezis%2C%20G.%20Cristofaro%2C%20E.De%20Membership%20Inference%20Attacks%20Against%20Generative%20Models.%20Proceedings%20on%20Privacy%20Enhancing%20Technologies%20%28PoPETs%29%202018"
        },
        {
            "id": "18",
            "entry": "[18] E. Hesamifard, H. Takabi, M. Ghasemi, and R. N. Wright. Privacy-preserving machine learning as a service. Proceedings on Privacy Enhancing Technologies (PoPETs), 2018(3):123\u2013142, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hesamifard%2C%20E.%20Takabi%2C%20H.%20Ghasemi%2C%20M.%20Wright%2C%20R.N.%20Privacy-preserving%20machine%20learning%20as%20a%20service.%20Proceedings%20on%20Privacy%20Enhancing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hesamifard%2C%20E.%20Takabi%2C%20H.%20Ghasemi%2C%20M.%20Wright%2C%20R.N.%20Privacy-preserving%20machine%20learning%20as%20a%20service.%20Proceedings%20on%20Privacy%20Enhancing%202018"
        },
        {
            "id": "19",
            "entry": "[19] B. Hitaj, G. Ateniese, and F. Perez-Cruz. Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning. In ACM Conference on Computer and Communications Security (CCS), pages 603\u2013618, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hitaj%2C%20B.%20Ateniese%2C%20G.%20Perez-Cruz%2C%20F.%20Deep%20Models%20Under%20the%20GAN%3A%20Information%20Leakage%20from%20Collaborative%20Deep%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hitaj%2C%20B.%20Ateniese%2C%20G.%20Perez-Cruz%2C%20F.%20Deep%20Models%20Under%20the%20GAN%3A%20Information%20Leakage%20from%20Collaborative%20Deep%20Learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] M. Hoekstra, R. Lal, P. Pappachan, C. Rozas, V. Phegade, and J. del Cuvillo. Using innovative instructions to create trustworthy software solutions. In Workshop on Hardware and Architectural Support for Security and Privacy (HASP), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoekstra%2C%20M.%20Lal%2C%20R.%20Pappachan%2C%20P.%20Rozas%2C%20C.%20Using%20innovative%20instructions%20to%20create%20trustworthy%20software%20solutions.%20In%20Workshop%20on%20Hardware%20and%20Architectural%20Support%20for%20Security%20and%20Privacy%20%28HASP%29%202013"
        },
        {
            "id": "21",
            "entry": "[21] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li. Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In IEEE Symposium on Security and Privacy (S&P), pages 19\u201335, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jagielski%2C%20M.%20Oprea%2C%20A.%20Biggio%2C%20B.%20Liu%2C%20C.%20Manipulating%20machine%20learning%3A%20Poisoning%20attacks%20and%20countermeasures%20for%20regression%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jagielski%2C%20M.%20Oprea%2C%20A.%20Biggio%2C%20B.%20Liu%2C%20C.%20Manipulating%20machine%20learning%3A%20Poisoning%20attacks%20and%20countermeasures%20for%20regression%20learning%202018"
        },
        {
            "id": "22",
            "entry": "[22] Y. Kim. Convolutional neural networks for sentence classification. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014"
        },
        {
            "id": "23",
            "entry": "[23] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning (ICML), pages 1885\u20131894, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koh%2C%20P.W.%20Liang%2C%20P.%20Understanding%20black-box%20predictions%20via%20influence%20functions%202017"
        },
        {
            "id": "24",
            "entry": "[24] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "25",
            "entry": "[25] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter, and K. Chen. Understanding membership inferences on well-generalized learning models. arXiv preprint arXiv:1802.04889, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04889"
        },
        {
            "id": "26",
            "entry": "[26] G. Louppe, M. Kagan, and K. Cranmer. Learning to pivot with adversarial networks. In Conference on Neural Information Processing Systems (NIPS), pages 982\u2013991, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Louppe%2C%20G.%20Kagan%2C%20M.%20Cranmer%2C%20K.%20Learning%20to%20pivot%20with%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Louppe%2C%20G.%20Kagan%2C%20M.%20Cranmer%2C%20K.%20Learning%20to%20pivot%20with%20adversarial%20networks%202017"
        },
        {
            "id": "27",
            "entry": "[27] H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas. Federated learning of deep networks using model averaging. CoRR, abs/1602.05629, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.05629"
        },
        {
            "id": "28",
            "entry": "[28] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent language models. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20H.B.%20Ramage%2C%20D.%20Talwar%2C%20K.%20Zhang%2C%20L.%20Learning%20differentially%20private%20recurrent%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20H.B.%20Ramage%2C%20D.%20Talwar%2C%20K.%20Zhang%2C%20L.%20Learning%20differentially%20private%20recurrent%20language%20models%202018"
        },
        {
            "id": "29",
            "entry": "[29] P. Mohassel and Y. Zhang. SecureML: A System for Scalable Privacy-Preserving Machine Learning. In IEEE Symposium on Security and Privacy (S&P), pages 19\u201338, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohassel%2C%20P.%20Zhang%2C%20Y.%20SecureML%3A%20A%20System%20for%20Scalable%20Privacy-Preserving%20Machine%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohassel%2C%20P.%20Zhang%2C%20Y.%20SecureML%3A%20A%20System%20for%20Scalable%20Privacy-Preserving%20Machine%20Learning%202017"
        },
        {
            "id": "30",
            "entry": "[30] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 2574\u20132582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20A%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20S.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20A%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "31",
            "entry": "[31] M. Nasr, R. Shokri, and A. Houmansadr. Machine learning with membership privacy using adversarial regularization. In ACM Conference on Computer and Communications Security (CCS), pages 634\u2013646, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nasr%2C%20M.%20Shokri%2C%20R.%20Houmansadr%2C%20A.%20Machine%20learning%20with%20membership%20privacy%20using%20adversarial%20regularization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nasr%2C%20M.%20Shokri%2C%20R.%20Houmansadr%2C%20A.%20Machine%20learning%20with%20membership%20privacy%20using%20adversarial%20regularization%202018"
        },
        {
            "id": "32",
            "entry": "[32] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye, N. Taft, and D. Boneh. Privacy-preserving matrix factorization. In ACM Conference on Computer and Communications Security (CCS), pages 801\u2013812, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nikolaenko%2C%20V.%20Ioannidis%2C%20S.%20Weinsberg%2C%20U.%20Joye%2C%20M.%20Privacy-preserving%20matrix%20factorization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nikolaenko%2C%20V.%20Ioannidis%2C%20S.%20Weinsberg%2C%20U.%20Joye%2C%20M.%20Privacy-preserving%20matrix%20factorization%202013"
        },
        {
            "id": "33",
            "entry": "[33] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani, and M. Costa. Oblivious multi-party machine learning on trusted processors. In USENIX Security Symposium, pages 619\u2013636, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ohrimenko%2C%20O.%20Schuster%2C%20F.%20Fournet%2C%20C.%20Mehta%2C%20A.%20Oblivious%20multi-party%20machine%20learning%20on%20trusted%20processors%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ohrimenko%2C%20O.%20Schuster%2C%20F.%20Fournet%2C%20C.%20Mehta%2C%20A.%20Oblivious%20multi-party%20machine%20learning%20on%20trusted%20processors%202016"
        },
        {
            "id": "34",
            "entry": "[34] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy (EuroS&P), pages 372\u2013387, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20N.%20McDaniel%2C%20P.%20Jha%2C%20S.%20Fredrikson%2C%20M.%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20N.%20McDaniel%2C%20P.%20Jha%2C%20S.%20Fredrikson%2C%20M.%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "35",
            "entry": "[35] M. A. Pathak, S. Rane, and B. Raj. Multiparty differential privacy via aggregation of locally trained classifiers. In Conference on Neural Information Processing Systems (NIPS), pages 1876\u20131884, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20M.A.%20Rane%2C%20S.%20Raj%2C%20B.%20Multiparty%20differential%20privacy%20via%20aggregation%20of%20locally%20trained%20classifiers%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20M.A.%20Rane%2C%20S.%20Raj%2C%20B.%20Multiparty%20differential%20privacy%20via%20aggregation%20of%20locally%20trained%20classifiers%202010"
        },
        {
            "id": "36",
            "entry": "[36] A. Rajkumar and S. Agarwal. A differentially private stochastic gradient descent algorithm for multiparty classification. In Conference on Artificial Intelligence and Statistics (AISTATS), pages 933\u2013941, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajkumar%2C%20A.%20Agarwal%2C%20S.%20A%20differentially%20private%20stochastic%20gradient%20descent%20algorithm%20for%20multiparty%20classification%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rajkumar%2C%20A.%20Agarwal%2C%20S.%20A%20differentially%20private%20stochastic%20gradient%20descent%20algorithm%20for%20multiparty%20classification%202012"
        },
        {
            "id": "37",
            "entry": "[37] F. Schuster, M. Costa, C. Fournet, C. Gkantsidis, M. Peinado, G. Mainar-Ruiz, and M. Russinovich. V C3: Trustworthy data analytics in the cloud using SGX. In IEEE Symposium on Security and Privacy (S&P), pages 38\u201354, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schuster%2C%20F.%20Costa%2C%20M.%20Fournet%2C%20C.%20Gkantsidis%2C%20C.%20V%20C3%3A%20Trustworthy%20data%20analytics%20in%20the%20cloud%20using%20SGX%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schuster%2C%20F.%20Costa%2C%20M.%20Fournet%2C%20C.%20Gkantsidis%2C%20C.%20V%20C3%3A%20Trustworthy%20data%20analytics%20in%20the%20cloud%20using%20SGX%202015"
        },
        {
            "id": "38",
            "entry": "[38] S. Shen, S. Tople, and P. Saxena. Auror: Defending against poisoning attacks in collaborative deep learning systems. In Conference on Computer Security Applications (ACSAC), pages 508\u2013519, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20S.%20Tople%2C%20S.%20Saxena%2C%20P.%20Auror%3A%20Defending%20against%20poisoning%20attacks%20in%20collaborative%20deep%20learning%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20S.%20Tople%2C%20S.%20Saxena%2C%20P.%20Auror%3A%20Defending%20against%20poisoning%20attacks%20in%20collaborative%20deep%20learning%20systems%202016"
        },
        {
            "id": "39",
            "entry": "[39] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In ACM Conference on Computer and Communications Security (CCS), pages 1310\u20131321, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shokri%2C%20R.%20Shmatikov%2C%20V.%20Privacy-preserving%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shokri%2C%20R.%20Shmatikov%2C%20V.%20Privacy-preserving%20deep%20learning%202015"
        },
        {
            "id": "40",
            "entry": "[40] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In IEEE Symposium on Security and Privacy (S&P), pages 3\u201318, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shokri%2C%20R.%20Stronati%2C%20M.%20Song%2C%20C.%20Shmatikov%2C%20V.%20Membership%20inference%20attacks%20against%20machine%20learning%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shokri%2C%20R.%20Stronati%2C%20M.%20Song%2C%20C.%20Shmatikov%2C%20V.%20Membership%20inference%20attacks%20against%20machine%20learning%20models%202017"
        },
        {
            "id": "41",
            "entry": "[41] C. Song, T. Ristenpart, and V. Shmatikov. Machine learning models that remember too much. In ACM Conference on Computer and Communications Security (CCS), pages 587\u2013601, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Song%2C%20C.%20Ristenpart%2C%20T.%20Shmatikov%2C%20V.%20Machine%20learning%20models%20that%20remember%20too%20much%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Song%2C%20C.%20Ristenpart%2C%20T.%20Shmatikov%2C%20V.%20Machine%20learning%20models%20that%20remember%20too%20much%202017"
        },
        {
            "id": "42",
            "entry": "[42] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. Is feature selection secure against training data poisoning? In International Conference on Machine Learning (ICML), pages 1689\u20131698, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20H.%20Biggio%2C%20B.%20Brown%2C%20G.%20Fumera%2C%20G.%20Is%20feature%20selection%20secure%20against%20training%20data%20poisoning%3F%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20H.%20Biggio%2C%20B.%20Brown%2C%20G.%20Fumera%2C%20G.%20Is%20feature%20selection%20secure%20against%20training%20data%20poisoning%3F%202015"
        },
        {
            "id": "43",
            "entry": "[43] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. Support vector machines under adversarial label contamination. Neurocomputing, 160:53\u201362, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20H.%20Biggio%2C%20B.%20Nelson%2C%20B.%20Xiao%2C%20H.%20Support%20vector%20machines%20under%20adversarial%20label%20contamination%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20H.%20Biggio%2C%20B.%20Nelson%2C%20B.%20Xiao%2C%20H.%20Support%20vector%20machines%20under%20adversarial%20label%20contamination%202015"
        },
        {
            "id": "44",
            "entry": "[44] M. B. Zafar, I. Valera, M. Gomez-Rodriguez, and K. P. Gummadi. Fairness constraints: Mechanisms for fair classification. In Conference on Artificial Intelligence and Statistics (AISTATS), pages 962\u2013970, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zafar%2C%20M.B.%20Valera%2C%20I.%20Gomez-Rodriguez%2C%20M.%20Gummadi%2C%20K.P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zafar%2C%20M.B.%20Valera%2C%20I.%20Gomez-Rodriguez%2C%20M.%20Gummadi%2C%20K.P.%20Fairness%20constraints%3A%20Mechanisms%20for%20fair%20classification%202017"
        },
        {
            "id": "45",
            "entry": "[45] R. S. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In International Conference on Machine Learning (ICML), pages 325\u2013333, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zemel%2C%20R.S.%20Wu%2C%20Y.%20Swersky%2C%20K.%20Pitassi%2C%20T.%20Dwork.%20Learning%20fair%20representations%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zemel%2C%20R.S.%20Wu%2C%20Y.%20Swersky%2C%20K.%20Pitassi%2C%20T.%20Dwork.%20Learning%20fair%20representations%202013"
        }
    ]
}
