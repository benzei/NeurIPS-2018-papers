{
    "filename": "7893-unsupervised-learning-of-artistic-styles-with-archetypal-style-analysis.pdf",
    "metadata": {
        "title": "Unsupervised Learning of Artistic Styles with Archetypal Style Analysis",
        "author": "Daan Wynen, Cordelia Schmid, Julien Mairal",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7893-unsupervised-learning-of-artistic-styles-with-archetypal-style-analysis.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to deep image representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes."
    },
    "keywords": [
        {
            "term": "target painting",
            "url": "https://en.wikipedia.org/wiki/target_painting"
        },
        {
            "term": "convex combination",
            "url": "https://en.wikipedia.org/wiki/convex_combination"
        },
        {
            "term": "archetypal analysis",
            "url": "https://en.wikipedia.org/wiki/archetypal_analysis"
        },
        {
            "term": "sparse coding",
            "url": "https://en.wikipedia.org/wiki/sparse_coding"
        },
        {
            "term": "unsupervised learning",
            "url": "https://en.wikipedia.org/wiki/unsupervised_learning"
        },
        {
            "term": "wavelet coefficient",
            "url": "https://en.wikipedia.org/wiki/wavelet_coefficient"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        }
    ],
    "highlights": [
        "Artistic style transfer consists in manipulating the appearance of an input image such that its semantic content and its scene organization are preserved, but a human may perceive the modified image as having been painted in a similar fashion as a given target painting",
        "Related to previous approaches to texture synthesis based on modeling statistics of wavelet coefficients [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], the seminal work of Gatys et al [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] has recently shown that a deep convolutional neural network originally trained for classification tasks yields a powerful representation for style and texture modeling",
        "The description of \u201cstyle\u201d in [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] consists of local statistics obtained from deep visual features, represented by the covariance matrices of feature responses computed at each network layer",
        "Three practical cases come to mind: (i) \u03b1 may be a canonical vector that selects a single archetype; \u03b1 may be any convex combination of archetypes for archetypal style interpolation; \u03b1 may be a modification of an existing archetypal decomposition to enhance a style already present in an input image I\u2014that is, \u03b1 is a variation of \u03b1 defined in (2)",
        "Whereas other techniques may be used for that purpose, archetypal analysis admits a dual interpretation which makes it particularly appropriate for the task: On the one hand, archetypes are represented as convex combinations of input image styles and are directly interpretable; on the other hand, an image style is approximated by a convex combination of archetypes allowing various kinds of visualizations"
    ],
    "key_statements": [
        "Artistic style transfer consists in manipulating the appearance of an input image such that its semantic content and its scene organization are preserved, but a human may perceive the modified image as having been painted in a similar fashion as a given target painting",
        "Related to previous approaches to texture synthesis based on modeling statistics of wavelet coefficients [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], the seminal work of Gatys et al [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] has recently shown that a deep convolutional neural network originally trained for classification tasks yields a powerful representation for style and texture modeling",
        "The description of \u201cstyle\u201d in [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] consists of local statistics obtained from deep visual features, represented by the covariance matrices of feature responses computed at each network layer",
        "Three practical cases come to mind: (i) \u03b1 may be a canonical vector that selects a single archetype; \u03b1 may be any convex combination of archetypes for archetypal style interpolation; \u03b1 may be a modification of an existing archetypal decomposition to enhance a style already present in an input image I\u2014that is, \u03b1 is a variation of \u03b1 defined in (2)",
        "We introduced archetypal style analysis as a means to identify styles in a collection of artworks without supervision, and to use them for the manipulation of artworks and photos",
        "Whereas other techniques may be used for that purpose, archetypal analysis admits a dual interpretation which makes it particularly appropriate for the task: On the one hand, archetypes are represented as convex combinations of input image styles and are directly interpretable; on the other hand, an image style is approximated by a convex combination of archetypes allowing various kinds of visualizations"
    ],
    "summary": [
        "Artistic style transfer consists in manipulating the appearance of an input image such that its semantic content and its scene organization are preserved, but a human may perceive the modified image as having been painted in a similar fashion as a given target painting.",
        "Whereas the goal of these previous approaches was to improve style transfer, we address a different objective and propose to use an unsupervised method to learn style representations from a potentially large collection of paintings.",
        "We learn archetypal representations of style from image collections.",
        "Archetypes are simple to interpret since they are related to convex combinations of a few image style representations from the original dataset, which can be visualized.",
        "Transfer to an archetypal style is achieved by selecting a single archetype in the decomposition; style enhancement consist of increasing the contribution of an existing archetype, making the input image more \u201carchetypal\u201d.",
        "The paper is organized as follows: Section 2 presents the archetypal style analysis model, and its application to a large collection of paintings.",
        "We show how to use archetypal analysis to learn style from large collections of paintings, and subsequently perform style decomposition on arbitrary images.",
        "Each painting\u2019s style can be represented by a sparse low-dimensional code \u03b1 in \u2206k, and each archetype is itself associated to a few input paintings, which is crucial for their interpretation.",
        "We briefly present the universal style transfer approach of [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and introduce a modification that allows us to better preserve the content details of the original images, before presenting how to use the framework for archetypal style manipulation.",
        "Only, that we are given a content image Ic and a style image Is. We assume that we are given pairs of encoders/decoders such that el(I) produces the l-th feature map previously selected from the VGG network and dl is a decoder that has been trained to approximately \u201cinvert\u201d el\u2014that is, dl) \u2248 I.",
        "Three practical cases come to mind: (i) \u03b1 may be a canonical vector that selects a single archetype; \u03b1 may be any convex combination of archetypes for archetypal style interpolation; \u03b1 may be a modification of an existing archetypal decomposition to enhance a style already present in an input image I\u2014that is, \u03b1 is a variation of \u03b1 defined in (2).",
        "In Figure 6, while increasing the contributions of the individual archetypes, we vary \u03b3 = \u03b4, so that the middle image is very close visually to the original image (\u03b3 = \u03b4 = 0.5), while the outer panels put a strong emphasis on the modified styles.",
        "We introduced archetypal style analysis as a means to identify styles in a collection of artworks without supervision, and to use them for the manipulation of artworks and photos."
    ],
    "headline": "We introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua. StyleBank: an explicit representation for neural image style transfer. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20D.%20Yuan%2C%20L.%20Liao%2C%20J.%20Yu%2C%20N.%20StyleBank%3A%20an%20explicit%20representation%20for%20neural%20image%20style%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20D.%20Yuan%2C%20L.%20Liao%2C%20J.%20Yu%2C%20N.%20StyleBank%3A%20an%20explicit%20representation%20for%20neural%20image%20style%20transfer%202017"
        },
        {
            "id": "2",
            "entry": "[2] Y. Chen, J. Mairal, and Z. Harchaoui. Fast and robust archetypal analysis for representation learning. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Y.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20Fast%20and%20robust%20archetypal%20analysis%20for%20representation%20learning%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Y.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20Fast%20and%20robust%20archetypal%20analysis%20for%20representation%20learning%202014"
        },
        {
            "id": "3",
            "entry": "[3] A. Cutler and L. Breiman. Archetypal analysis. Technometrics, 36(4):338\u2013347, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cutler%2C%20A.%20Breiman%2C%20L.%20Archetypal%20analysis%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cutler%2C%20A.%20Breiman%2C%20L.%20Archetypal%20analysis%201994"
        },
        {
            "id": "4",
            "entry": "[4] V. Dumoulin, J. Shlens, and M. Kudlur. A learned representation for artistic style. In Proc. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dumoulin%2C%20V.%20Shlens%2C%20J.%20Kudlur%2C%20M.%20A%20learned%20representation%20for%20artistic%20style%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dumoulin%2C%20V.%20Shlens%2C%20J.%20Kudlur%2C%20M.%20A%20learned%20representation%20for%20artistic%20style%202017"
        },
        {
            "id": "5",
            "entry": "[5] L. A. Gatys, A. S. Ecker, and M. Bethge. A Neural Algorithm of Artistic Style. preprint arXiv:1508.06576, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.06576"
        },
        {
            "id": "6",
            "entry": "[6] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis using convolutional neural networks. In Adv. in Neural Information Processing Systems (NIPS), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Texture%20synthesis%20using%20convolutional%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatys%2C%20L.A.%20Ecker%2C%20A.S.%20Bethge%2C%20M.%20Texture%20synthesis%20using%20convolutional%20neural%20networks%202015"
        },
        {
            "id": "7",
            "entry": "[7] G. Ghiasi, H. Leeu, M. Kudlur, V. Dumoulin, and J. Shlens. Exploring the structure of a real-time, arbitrary neural artistic stylization network. In Proc. British Machine Vision Conference (BMVC), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghiasi%2C%20G.%20Leeu%2C%20H.%20Kudlur%2C%20M.%20Dumoulin%2C%20V.%20Exploring%20the%20structure%20of%20a%20real-time%2C%20arbitrary%20neural%20artistic%20stylization%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghiasi%2C%20G.%20Leeu%2C%20H.%20Kudlur%2C%20M.%20Dumoulin%2C%20V.%20Exploring%20the%20structure%20of%20a%20real-time%2C%20arbitrary%20neural%20artistic%20stylization%20network%202017"
        },
        {
            "id": "8",
            "entry": "[8] D. J. Heeger and J. R. Bergen. Pyramid-based texture analysis/synthesis. In Proc. 22nd annual conference on Computer graphics and interactive techniques (SIGGRAPH), 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heeger%2C%20D.J.%20Bergen%2C%20J.R.%20Pyramid-based%20texture%20analysis/synthesis%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heeger%2C%20D.J.%20Bergen%2C%20J.R.%20Pyramid-based%20texture%20analysis/synthesis%201995"
        },
        {
            "id": "9",
            "entry": "[9] X. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proc. International Conference on Computer Vision (ICCV), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Belongie%2C%20S.%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Belongie%2C%20S.%20Arbitrary%20style%20transfer%20in%20real-time%20with%20adaptive%20instance%20normalization%202017"
        },
        {
            "id": "10",
            "entry": "[10] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision (ECCV), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20J.%20Alahi%2C%20A.%20Fei-Fei%2C%20L.%20Perceptual%20losses%20for%20real-time%20style%20transfer%20and%20super-resolution%202016"
        },
        {
            "id": "11",
            "entry": "[11] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang. Universal style transfer via feature transforms. In Adv. Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Fang%2C%20C.%20Yang%2C%20J.%20Wang%2C%20Z.%20Universal%20style%20transfer%20via%20feature%20transforms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Fang%2C%20C.%20Yang%2C%20J.%20Wang%2C%20Z.%20Universal%20style%20transfer%20via%20feature%20transforms%202017"
        },
        {
            "id": "12",
            "entry": "[12] F. Luan, S. Paris, E. Shechtman, and K. Bala. Deep photo style transfer. In Proc. Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luan%2C%20F.%20Paris%2C%20S.%20Shechtman%2C%20E.%20Bala%2C%20K.%20Deep%20photo%20style%20transfer%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luan%2C%20F.%20Paris%2C%20S.%20Shechtman%2C%20E.%20Bala%2C%20K.%20Deep%20photo%20style%20transfer%202017"
        },
        {
            "id": "13",
            "entry": "[13] J. Mairal, F. Bach, J. Ponce, et al. Sparse modeling for image and vision processing. Foundations and Trends in Computer Graphics and Vision, 8(2-3):85\u2013283, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20J.%20Bach%2C%20F.%20Ponce%2C%20J.%20Sparse%20modeling%20for%20image%20and%20vision%20processing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20J.%20Bach%2C%20F.%20Ponce%2C%20J.%20Sparse%20modeling%20for%20image%20and%20vision%20processing%202014"
        },
        {
            "id": "14",
            "entry": "[14] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research (JMLR), 11(Jan):19\u201360, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20J.%20Bach%2C%20F.%20Ponce%2C%20J.%20Sapiro%2C%20G.%20Online%20learning%20for%20matrix%20factorization%20and%20sparse%20coding%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20J.%20Bach%2C%20F.%20Ponce%2C%20J.%20Sapiro%2C%20G.%20Online%20learning%20for%20matrix%20factorization%20and%20sparse%20coding%202010"
        },
        {
            "id": "15",
            "entry": "[15] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381:607\u2013609, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshausen%2C%20B.A.%20Field%2C%20D.J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Olshausen%2C%20B.A.%20Field%2C%20D.J.%20Emergence%20of%20simple-cell%20receptive%20field%20properties%20by%20learning%20a%20sparse%20code%20for%20natural%20images%201996"
        },
        {
            "id": "16",
            "entry": "[16] P. Paatero and U. Tapper. Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values. Environmetrics, 5(2):111\u2013126, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paatero%2C%20P.%20Tapper%2C%20U.%20Positive%20matrix%20factorization%3A%20a%20non-negative%20factor%20model%20with%20optimal%20utilization%20of%20error%20estimates%20of%20data%20values%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paatero%2C%20P.%20Tapper%2C%20U.%20Positive%20matrix%20factorization%3A%20a%20non-negative%20factor%20model%20with%20optimal%20utilization%20of%20error%20estimates%20of%20data%20values%201994"
        },
        {
            "id": "17",
            "entry": "[17] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20A.%20Gross%2C%20S.%20Chintala%2C%20S.%20Chanan%2C%20G.%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "18",
            "entry": "[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. Journal of machine learning research (JMLR), 12(Oct):2825\u20132830, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20python%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20python%202011"
        },
        {
            "id": "19",
            "entry": "[19] J. Portilla and E. P. Simoncelli. A parametric texture model based on joint statistics of complex wavelet coefficients. International journal of computer vision, 40(1):49\u201370, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Portilla%2C%20J.%20Simoncelli%2C%20E.P.%20A%20parametric%20texture%20model%20based%20on%20joint%20statistics%20of%20complex%20wavelet%20coefficients%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Portilla%2C%20J.%20Simoncelli%2C%20E.P.%20A%20parametric%20texture%20model%20based%20on%20joint%20statistics%20of%20complex%20wavelet%20coefficients%202000"
        },
        {
            "id": "20",
            "entry": "[20] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer for videos and spherical images. International Journal on Computer Vision (IJCV), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruder%2C%20M.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Artistic%20style%20transfer%20for%20videos%20and%20spherical%20images%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruder%2C%20M.%20Dosovitskiy%2C%20A.%20Brox%2C%20T.%20Artistic%20style%20transfer%20for%20videos%20and%20spherical%20images%202018"
        },
        {
            "id": "21",
            "entry": "[21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. preprint arXiv:1409.1556, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1409.1556"
        },
        {
            "id": "22",
            "entry": "[22] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Texture networks: feed-forward synthesis of textures and stylized images. In Proc. International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ulyanov%2C%20D.%20Lebedev%2C%20V.%20Vedaldi%2C%20A.%20Lempitsky%2C%20V.%20Texture%20networks%3A%20feed-forward%20synthesis%20of%20textures%20and%20stylized%20images%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ulyanov%2C%20D.%20Lebedev%2C%20V.%20Vedaldi%2C%20A.%20Lempitsky%2C%20V.%20Texture%20networks%3A%20feed-forward%20synthesis%20of%20textures%20and%20stylized%20images%202016"
        },
        {
            "id": "23",
            "entry": "[23] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research (JMLR), 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20L.%20Hinton%2C%20G.%20Visualizing%20data%20using%20t-SNE%202008"
        },
        {
            "id": "24",
            "entry": "[24] M.-C. Yeh, S. Tang, A. Bhattad, and D. A. Forsyth. Quantitative evaluation of style transfer. preprint arXiv:1804.00118, 2018. ",
            "arxiv_url": "https://arxiv.org/pdf/1804.00118"
        }
    ]
}
