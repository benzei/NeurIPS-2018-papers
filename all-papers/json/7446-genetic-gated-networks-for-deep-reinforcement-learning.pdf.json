{
    "filename": "7446-genetic-gated-networks-for-deep-reinforcement-learning.pdf",
    "metadata": {
        "title": "Genetic-Gated Networks for Deep Reinforcement Learning",
        "author": "Simyung Chang, John Yang, Jaeseok Choi, Nojun Kwak",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7446-genetic-gated-networks-for-deep-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce the Genetic-Gated Networks (G2Ns), simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer(s) of networks. Our method can take both advantages of gradient-free optimization and gradient-based optimization methods, of which the former is effective for problems with multiple local minima, while the latter can quickly find local minima. In addition, multiple chromosomes can define different models, making it easy to construct multiple models and can be effectively applied to problems that require multiple models. We show that this G2N can be applied to typical reinforcement learning algorithms to achieve a large improvement in sample efficiency and performance."
    },
    "keywords": [
        {
            "term": "local minima",
            "url": "https://en.wikipedia.org/wiki/local_minima"
        },
        {
            "term": "genetic algorithm",
            "url": "https://en.wikipedia.org/wiki/genetic_algorithm"
        },
        {
            "term": "policy gradient method",
            "url": "https://en.wikipedia.org/wiki/policy_gradient_method"
        },
        {
            "term": "data structure",
            "url": "https://en.wikipedia.org/wiki/data_structure"
        },
        {
            "term": "National Research Foundation of Korea",
            "url": "https://en.wikipedia.org/wiki/National_Research_Foundation_of_Korea"
        },
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "local minimum",
            "url": "https://en.wikipedia.org/wiki/local_minimum"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Genetic Algorithm : A genetic algorithm [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] is a parallel and global search algorithm",
        "This data structure is usually expressed as a binary vector each dimension of which decides the activation of each gene",
        "The algorithm activates new genes with a fixed probability of mutations in every generation, and these random mutations allow the genetic algorithm to escape from local minima",
        "G2AC and G2PPO, in the second phase, use all the perturbed models to collect experiences, while updating the model parameters using the loss based on the elite model; this phase is intended to explore much more so that the elite model can be switched to the actor with a better individual if found",
        "G2Ns can define multiple models without increasing the amount of computation and use the advantage of gradient-free optimization by gating with a chromosome based on a genetic algorithm in a hidden layer",
        "As applications of G2Ns, we have proposed Genetic-Gated Actor-Critic methods(G2AC, G2PPO) which can be used to problems within the RL domain where the multiple models can be useful while having the local minima problem"
    ],
    "key_statements": [
        "Genetic Algorithm : A genetic algorithm [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] is a parallel and global search algorithm",
        "This data structure is usually expressed as a binary vector each dimension of which decides the activation of each gene",
        "The algorithm activates new genes with a fixed probability of mutations in every generation, and these random mutations allow the genetic algorithm to escape from local minima",
        "G2AC and G2PPO, in the second phase, use all the perturbed models to collect experiences, while updating the model parameters using the loss based on the elite model; this phase is intended to explore much more so that the elite model can be switched to the actor with a better individual if found",
        "G2Ns can define multiple models without increasing the amount of computation and use the advantage of gradient-free optimization by gating with a chromosome based on a genetic algorithm in a hidden layer",
        "As applications of G2Ns, we have proposed Genetic-Gated Actor-Critic methods(G2AC, G2PPO) which can be used to problems within the RL domain where the multiple models can be useful while having the local minima problem"
    ],
    "summary": [
        "Genetic Algorithm : A genetic algorithm [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] is a parallel and global search algorithm.",
        "While meaningful explorations can be achieved by applying learned noise in the parameter space and perturbing neural policy models [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], there have been genetic evolution approaches for the exploration control system of an optimal policy, considering the gradient-free methods are able to overcome confined local optima [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "Implementations of multiple chromosomes in a parameter space as a form of gated vector enables generating various models through perturbations on the original neural model to better search over a solution space.",
        "But every once in a generation the training model is allowed to switch its gating vector that results the best fitness-score, which comes as a crucial feature of our optimization for highly multi-modal solution spaces.",
        "Among a population, an elite neural network model f (x; \u03b8, \u03bcelite) is selected, where \u03bcelite denotes the chromosome that causes an individual to yield the best fitness-score.",
        "The proposed Genetic-Gated actor critic methods (G2AC and G2PPO) have the following main features : Multi Model Race: A single critic evaluates the state values while multiple agents follow unique policies, so the population matrix can be applied to the last hidden layer of actors.",
        "G2AC and G2PPO, in the second phase, use all the perturbed models to collect experiences, while updating the model parameters using the loss based on the elite model; this phase is intended to explore much more so that the elite model can be switched to the actor with a better individual if found.",
        "Unlike our method that is engaged with multiple actors, original PPO is reported to use a single actor model and a batch size of 2,048 steps when learning in the MuJoCo environments.",
        "Comparison with random multi-policy: To compare with a simple multi-policy exploration, we have defined a multi-model structure with random binary vectors as gates and have compared the performance of it with our methods.",
        "G2Ns can define multiple models without increasing the amount of computation and use the advantage of gradient-free optimization by gating with a chromosome based on a genetic algorithm in a hidden layer.",
        "As applications of G2Ns, we have proposed Genetic-Gated Actor-Critic methods(G2AC, G2PPO) which can be used to problems within the RL domain where the multiple models can be useful while having the local minima problem.",
        "It is not just a development of a RL algorithm, but it shows that a combination of two completely different machine learning algorithms can be a better algorithm"
    ],
    "headline": "We introduce the Genetic-Gated Networks , simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer of networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01540"
        },
        {
            "id": "2",
            "entry": "[2] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.10295"
        },
        {
            "id": "3",
            "entry": "[3] Matthew Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. A neuroevolution approach to general atari game playing. IEEE Transactions on Computational Intelligence and AI in Games, 6(4):355\u2013366, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hausknecht%2C%20Matthew%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Stone%2C%20Peter%20A%20neuroevolution%20approach%20to%20general%20atari%20game%20playing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hausknecht%2C%20Matthew%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Stone%2C%20Peter%20A%20neuroevolution%20approach%20to%20general%20atari%20game%20playing%202014"
        },
        {
            "id": "4",
            "entry": "[4] John H Holland. Genetic algorithms. Scientific american, 267(1):66\u201373, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Holland%2C%20John%20H.%20Genetic%20algorithms%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Holland%2C%20John%20H.%20Genetic%20algorithms%201992"
        },
        {
            "id": "5",
            "entry": "[5] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "6",
            "entry": "[6] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adria%20Puigdomenech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "7",
            "entry": "[7] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in Neural Information Processing Systems, pages 4026\u20134034, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20Ian%20Blundell%2C%20Charles%20Pritzel%2C%20Alexander%20Roy%2C%20Benjamin%20Van%20Deep%20exploration%20via%20bootstrapped%20dqn%202016"
        },
        {
            "id": "8",
            "entry": "[8] Ian Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy. Deep exploration via randomized value functions. arXiv preprint arXiv:1703.07608, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07608"
        },
        {
            "id": "9",
            "entry": "[9] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.01905"
        },
        {
            "id": "10",
            "entry": "[10] Thomas R\u00fcckstie\u00df, Martin Felder, and J\u00fcrgen Schmidhuber. State-dependent exploration for policy gradient methods. Machine Learning and Knowledge Discovery in Databases, pages 234\u2013249, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thomas%20R%C3%BCckstie%C3%9F%2C%20Martin%20Felder%20Schmidhuber%2C%20J%C3%BCrgen%20State-dependent%20exploration%20for%20policy%20gradient%20methods%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thomas%20R%C3%BCckstie%C3%9F%2C%20Martin%20Felder%20Schmidhuber%2C%20J%C3%BCrgen%20State-dependent%20exploration%20for%20policy%20gradient%20methods%202008"
        },
        {
            "id": "11",
            "entry": "[11] E Ruffio, D Saury, D Petit, and M Girault. Tutorial 2: Zero-order optimization algorithms. Eurotherm School METTI, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ruffio%2C%20E.%20Saury%2C%20D.%20Petit%2C%20D.%20Girault%2C%20M.%20Tutorial%202%3A%20Zero-order%20optimization%20algorithms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ruffio%2C%20E.%20Saury%2C%20D.%20Petit%2C%20D.%20Girault%2C%20M.%20Tutorial%202%3A%20Zero-order%20optimization%20algorithms%202011"
        },
        {
            "id": "12",
            "entry": "[12] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.03864"
        },
        {
            "id": "13",
            "entry": "[13] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "14",
            "entry": "[14] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 387\u2013395, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Lever%2C%20Guy%20Heess%2C%20Nicolas%20Degris%2C%20Thomas%20Deterministic%20policy%20gradient%20algorithms%202014"
        },
        {
            "id": "15",
            "entry": "[15] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20E.%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "16",
            "entry": "[16] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06567"
        },
        {
            "id": "17",
            "entry": "[17] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057\u20131063, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20McAllester%2C%20David%20A.%20Singh%2C%20Satinder%20P.%20Mansour%2C%20Yishay%20Policy%20gradient%20methods%20for%20reinforcement%20learning%20with%20function%20approximation%202000"
        },
        {
            "id": "18",
            "entry": "[18] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026\u20135033. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20Emanuel%20Erez%2C%20Tom%20Tassa%2C%20Yuval%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012"
        },
        {
            "id": "19",
            "entry": "[19] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06581"
        },
        {
            "id": "20",
            "entry": "[20] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20Ronald%20J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "21",
            "entry": "[21] Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in Neural Information Processing Systems, pages 5285\u20135294, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Mansimov%2C%20Elman%20Grosse%2C%20Roger%20B.%20Liao%2C%20Shun%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Mansimov%2C%20Elman%20Grosse%2C%20Roger%20B.%20Liao%2C%20Shun%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20kronecker-factored%20approximation%202017"
        }
    ]
}
