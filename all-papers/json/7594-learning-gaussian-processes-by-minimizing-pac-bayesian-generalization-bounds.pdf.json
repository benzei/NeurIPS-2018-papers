{
    "filename": "7594-learning-gaussian-processes-by-minimizing-pac-bayesian-generalization-bounds.pdf",
    "metadata": {
        "date": 2018,
        "title": "Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds",
        "author": "David Reeb",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7594-learning-gaussian-processes-by-minimizing-pac-bayesian-generalization-bounds.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Gaussian Processes (GPs) are a generic modelling tool for supervised learning. While they have been successfully applied on large datasets, their use in safetycritical applications is hindered by the lack of good performance guarantees. To this end, we propose a method to learn GPs and their sparse approximations by directly optimizing a PAC-Bayesian bound on their generalization performance, instead of maximizing the marginal likelihood. Besides its theoretical appeal, we find in our evaluation that our learning method is robust and yields significantly better generalization guarantees than other common GP approaches on several regression benchmark datasets."
    },
    "keywords": [
        {
            "term": "gaussian process regression",
            "url": "https://en.wikipedia.org/wiki/gaussian_process_regression"
        },
        {
            "term": "marginal likelihood",
            "url": "https://en.wikipedia.org/wiki/marginal_likelihood"
        },
        {
            "term": "maximum likelihood",
            "url": "https://en.wikipedia.org/wiki/maximum_likelihood"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "gaussian processes",
            "url": "https://en.wikipedia.org/wiki/gaussian_processes"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        }
    ],
    "highlights": [
        "Gaussian Processes (GPs) are a powerful modelling method due to their non-parametric nature [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We proposed and explored the use of PAC-Bayesian bounds as an optimization objective for Gaussian Processes training",
        "We were able to achieve significantly better guarantees on the outof-sample performance compared to state-of-the-art Gaussian Processes methods, such as VFE or FITC, while maintaining computational scalability",
        "Despite the much better generalization guarantees obtained by our method, it often yields worse test error, in particular test mean squared error, than standard Gaussian Processes regression methods; this largely persists even when using more distance-sensitive loss functions than the 0-1-loss",
        "The underlying reason could be that all loss functions considered in this work were bounded, as necessitated by our desire to provide generalization guarantees irrespective of the true data distribution",
        "While rigorous PACBayesian bounds exist for mean squared error-like unbounded loss functions under special assumptions on the data distribution [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], it may be worthwhile to investigate whether these training objectives lead to better test mean squared error in examples"
    ],
    "key_statements": [
        "Gaussian Processes (GPs) are a powerful modelling method due to their non-parametric nature [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We demonstrate our Gaussian Processes learning method on regression tasks, whereas PAC-Bayes bounds have so far mostly been used in a classification setting",
        "The tighter objective (5) in the kl-PAC-Gaussian Processes allows learning a slightly more complex Gaussian Processes Q in terms of the KL-divergence compared to the sqrt-PAC-Gaussian Processes, which results in better test risks and at the same time better guarantees",
        "To the boston dataset, the higher test errors of kl-PAC-SGP compared to VFE and FITC can be explained by underfitting due to the stronger regularization, again shown by lower KL and significantly larger learned \u03c3n2, cf",
        "We proposed and explored the use of PAC-Bayesian bounds as an optimization objective for Gaussian Processes training",
        "We were able to achieve significantly better guarantees on the outof-sample performance compared to state-of-the-art Gaussian Processes methods, such as VFE or FITC, while maintaining computational scalability",
        "We further found that using the tighter generalization bound B(Q) (5) based on the inverse binary kl-divergence leads to an increase in the performance on all metrics compared to a looser bound BPin as employed in previous works (e.g. [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>])",
        "Despite the much better generalization guarantees obtained by our method, it often yields worse test error, in particular test mean squared error, than standard Gaussian Processes regression methods; this largely persists even when using more distance-sensitive loss functions than the 0-1-loss",
        "The underlying reason could be that all loss functions considered in this work were bounded, as necessitated by our desire to provide generalization guarantees irrespective of the true data distribution",
        "While rigorous PACBayesian bounds exist for mean squared error-like unbounded loss functions under special assumptions on the data distribution [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], it may be worthwhile to investigate whether these training objectives lead to better test mean squared error in examples",
        "A drawback is that those assumptions are usually impossible to verify, the generalization guarantees are not comparable",
        "Note that the design of a loss function is dependent on the application domain and there is no ubiquitous choice across all settings"
    ],
    "summary": [
        "Gaussian Processes (GPs) are a powerful modelling method due to their non-parametric nature [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "To remain distribution-free as in the usual PAC setting, we employ and investigate a generic bounded loss function for regression.",
        "The basic idea of our method, which we call \u201cPAC-GP\u201d is to learn the parameters4 \u03b8 and \u03c3n by minimizing the upper bound B(Q\u03b8,\u03c3n ) from Eq (5), selecting the GP predictor Q\u03b8,\u03c3n with the best generalization performance guarantee within the scope of the PAC-Bayesian bound.",
        "For the experiments below we always employ the FITC parametrization in our proposed PAC-SGP method, i.e. our optimization parameters are \u03c3n2 and {zi} besides the length scale hyperparameters \u03b8.",
        "To get a first intuition, we illustrate in Fig. 1 the effect of varying \u03b5 in the loss function on the predictive distribution of our sparse PAC-SGP.",
        "Our PAC-GP yields significantly better generalization guarantees for all accuracy goals \u03b5 compared to full-GP, since we are directly optimizing the bound (5).",
        "The tighter objective (5) in the kl-PAC-GP allows learning a slightly more complex GP Q in terms of the KL-divergence compared to the sqrt-PAC-GP, which results in better test risks and at the same time better guarantees.",
        "Our kl-PAC-SGP achieves significantly better upper bounds than VFE and FITC, by more than a factor of 3 on sarcos, a factor of roughly 2 on pol, and a factor between 1.3 and 2 on kin40k (Fig. 3, cf.",
        "Our kl-PAC-SGP behaves more favorably in terms of generalization guarantee when inducing points are added and more complex models are allowed: our upper bound improves substantially with M or does at least not degrade, as opposed to VFE and FITC, whose complexities KL/N grow substantially with M .",
        "To the boston dataset, the higher test errors of kl-PAC-SGP compared to VFE and FITC can be explained by underfitting due to the stronger regularization, again shown by lower KL and significantly larger learned \u03c3n2, cf.",
        "We proposed and explored the use of PAC-Bayesian bounds as an optimization objective for GP training.",
        "The underlying reason could be that all loss functions considered in this work were bounded, as necessitated by our desire to provide generalization guarantees irrespective of the true data distribution.",
        "While rigorous PACBayesian bounds exist for MSE-like unbounded loss functions under special assumptions on the data distribution [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], it may be worthwhile to investigate whether these training objectives lead to better test MSE in examples.",
        "In many safety-critical applications, small deviations are tolerable whereas larger deviations are all catastrophic, a 0-1-loss as ours and a rigorous bound on it can be more useful than the MSE test error"
    ],
    "headline": "We propose a method to learn Gaussian Processes and their sparse approximations by directly optimizing a PAC-Bayesian bound on their generalization performance, instead of maximizing the marginal likelihood",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. E. Rasmussen, C. K. I. Williams, \u201cGaussian Processes for Machine Learning\u201d, The MIT Press (2006).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Williams%2C%20C.K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202006"
        },
        {
            "id": "2",
            "entry": "[2] M. Bauer, M. v. d. Wilk, C. E. Rasmussen, \u201cUnderstanding Probabilistic Sparse Gaussian Process Approximations\u201d, In NIPS (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bauer%2C%20M.%20v.%20d.%20Wilk%2C%20M.%20Rasmussen%2C%20C.E.%20Understanding%20Probabilistic%20Sparse%20Gaussian%20Process%20Approximations%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bauer%2C%20M.%20v.%20d.%20Wilk%2C%20M.%20Rasmussen%2C%20C.E.%20Understanding%20Probabilistic%20Sparse%20Gaussian%20Process%20Approximations%202016"
        },
        {
            "id": "3",
            "entry": "[3] S. Shalev-Shwartz, S. Ben-David, \u201cUnderstanding Machine Learning: From Theory to Algorithms\u201d, Cambridge University Press (2014).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Ben-David%2C%20S.%20Understanding%20Machine%20Learning%3A%20From%20Theory%20to%20Algorithms%202014"
        },
        {
            "id": "4",
            "entry": "[4] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, \u201cUnderstanding deep learning requires rethinking generalization\u201d, In ICLR (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "5",
            "entry": "[5] M. Jordan, Z. Ghahramani, T. Jaakkola, L. Saul, \u201cIntroduction to variational methods for graphical models\u201d, Machine Learning, 37, 183-233 (1999).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jordan%2C%20M.%20Ghahramani%2C%20Z.%20Jaakkola%2C%20T.%20Saul%2C%20L.%20Introduction%20to%20variational%20methods%20for%20graphical%20models%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jordan%2C%20M.%20Ghahramani%2C%20Z.%20Jaakkola%2C%20T.%20Saul%2C%20L.%20Introduction%20to%20variational%20methods%20for%20graphical%20models%201999"
        },
        {
            "id": "6",
            "entry": "[6] M. Titsias, \u201cVariational Learning of Inducing Variables in Sparse Gaussian Processes\u201d, In AISTATS (2009).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Titsias%2C%20M.%20Variational%20Learning%20of%20Inducing%20Variables%20in%20Sparse%20Gaussian%20Processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Titsias%2C%20M.%20Variational%20Learning%20of%20Inducing%20Variables%20in%20Sparse%20Gaussian%20Processes%202009"
        },
        {
            "id": "7",
            "entry": "[7] D. McAllester, \u201cPAC-Bayesian model averaging\u201d, COLT (1999).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20D.%20PAC-Bayesian%20model%20averaging%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20D.%20PAC-Bayesian%20model%20averaging%201999"
        },
        {
            "id": "8",
            "entry": "[8] D. McAllester, \u201cPAC-Bayesian Stochastic Model Selection\u201d, Machine Learning 51, 5-21 (2003).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20D.%20PAC-Bayesian%20Stochastic%20Model%20Selection%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20D.%20PAC-Bayesian%20Stochastic%20Model%20Selection%202003"
        },
        {
            "id": "9",
            "entry": "[9] O. Catoni, \u201cPac-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning\u201d, IMS Lecture Notes Monograph Series 2007, Vol. 56 (2007).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Catoni%2C%20O.%20Pac-Bayesian%20Supervised%20Classification%3A%20The%20Thermodynamics%20of%20Statistical%20Learning%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Catoni%2C%20O.%20Pac-Bayesian%20Supervised%20Classification%3A%20The%20Thermodynamics%20of%20Statistical%20Learning%202007"
        },
        {
            "id": "10",
            "entry": "[10] M. Seeger, \u201cPAC-Bayesian Generalization Error Bounds for Gaussian Process Classification\u201d, Journal of Machine Learning Research 3, 233-269 (2002).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20M.%20PAC-Bayesian%20Generalization%20Error%20Bounds%20for%20Gaussian%20Process%20Classification%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20M.%20PAC-Bayesian%20Generalization%20Error%20Bounds%20for%20Gaussian%20Process%20Classification%202002"
        },
        {
            "id": "11",
            "entry": "[11] A. Ambroladze, E. Parrado-Hern\u00e1ndez, J. Shawe-Taylor, \u201cTighter PAC-Bayes bounds\u201d, In NIPS (2007).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ambroladze%2C%20A.%20Parrado-Hern%C3%A1ndez%2C%20E.%20Shawe-Taylor%2C%20J.%20Tighter%20PAC-Bayes%20bounds%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ambroladze%2C%20A.%20Parrado-Hern%C3%A1ndez%2C%20E.%20Shawe-Taylor%2C%20J.%20Tighter%20PAC-Bayes%20bounds%202007"
        },
        {
            "id": "12",
            "entry": "[12] J. Langford, J. Shawe-Tayor, \u201cPAC-Bayes & margins\u201d, In NIPS (2002).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20J.%20Shawe-Tayor%2C%20%E2%80%9CPAC-Bayes%20%26%20margins%E2%80%9D%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20J.%20J.%20Shawe-Tayor%2C%20%E2%80%9CPAC-Bayes%20%26%20margins%E2%80%9D%202002"
        },
        {
            "id": "13",
            "entry": "[13] P. Germain, A. Lacasse, F. Laviolette, M. Marchand, \u201cPAC-Bayesian Learning of Linear Classifiers\u201d, In ICML (2009).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20P.%20Lacasse%2C%20A.%20Laviolette%2C%20F.%20Marchand%2C%20M.%20PAC-Bayesian%20Learning%20of%20Linear%20Classifiers%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20P.%20Lacasse%2C%20A.%20Laviolette%2C%20F.%20Marchand%2C%20M.%20PAC-Bayesian%20Learning%20of%20Linear%20Classifiers%202009"
        },
        {
            "id": "14",
            "entry": "[14] G. K. Dziugaite, D. M. Roy, \u201cComputing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data\u201d, In UAI (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20G.K.%20Roy%2C%20D.M.%20Computing%20Nonvacuous%20Generalization%20Bounds%20for%20Deep%20%28Stochastic%29%20Neural%20Networks%20with%20Many%20More%20Parameters%20than%20Training%20Data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20G.K.%20Roy%2C%20D.M.%20Computing%20Nonvacuous%20Generalization%20Bounds%20for%20Deep%20%28Stochastic%29%20Neural%20Networks%20with%20Many%20More%20Parameters%20than%20Training%20Data%202017"
        },
        {
            "id": "15",
            "entry": "[15] E. Snelson, Z. Ghahramani, \u201cSparse Gaussian Processes using Pseudo-inputs\u201d, In NIPS (2005).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20Processes%20using%20Pseudo-inputs%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snelson%2C%20E.%20Ghahramani%2C%20Z.%20Sparse%20Gaussian%20Processes%20using%20Pseudo-inputs%202005"
        },
        {
            "id": "16",
            "entry": "[16] M. Seeger, C. K. I. Williams, N. Lawrence, \u201cFast Forward Selection to Seepd Up Sparse Gaussian Process Regression\u201d, In AISTATS (2003).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20M.%20Williams%2C%20C.K.I.%20Lawrence%2C%20N.%20Fast%20Forward%20Selection%20to%20Seepd%20Up%20Sparse%20Gaussian%20Process%20Regression%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20M.%20Williams%2C%20C.K.I.%20Lawrence%2C%20N.%20Fast%20Forward%20Selection%20to%20Seepd%20Up%20Sparse%20Gaussian%20Process%20Regression%202003"
        },
        {
            "id": "17",
            "entry": "[17] P. Germain, F. Bach, A. Lacoste, S. Lacoste-Julien, \u201cPAC-Bayesian Theory Meets Bayesian Inference\u201d, In NIPS (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Germain%2C%20P.%20Bach%2C%20F.%20Lacoste%2C%20A.%20S.%20Lacoste-Julien%2C%20%E2%80%9CPAC-Bayesian%20Theory%20Meets%20Bayesian%20Inference%E2%80%9D%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Germain%2C%20P.%20Bach%2C%20F.%20Lacoste%2C%20A.%20S.%20Lacoste-Julien%2C%20%E2%80%9CPAC-Bayesian%20Theory%20Meets%20Bayesian%20Inference%E2%80%9D%202016"
        },
        {
            "id": "18",
            "entry": "[18] R. Sheth, R. Khardon, \u201cExcess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models\u201d, In NIPS (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sheth%2C%20R.%20Khardon%2C%20R.%20Excess%20Risk%20Bounds%20for%20the%20Bayes%20Risk%20using%20Variational%20Inference%20in%20Latent%20Gaussian%20Models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sheth%2C%20R.%20Khardon%2C%20R.%20Excess%20Risk%20Bounds%20for%20the%20Bayes%20Risk%20using%20Variational%20Inference%20in%20Latent%20Gaussian%20Models%202017"
        },
        {
            "id": "19",
            "entry": "[19] V. Vapnik, \u201cThe Nature of Statistical Learning Theory\u201d, Springer (1995).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20V.%20The%20Nature%20of%20Statistical%20Learning%20Theory%201995"
        },
        {
            "id": "20",
            "entry": "[20] A. Maurer, \u201cA Note on the PAC Bayesian Theorem\u201d, arXiv:cs/0411099 (2004).",
            "arxiv_url": "https://arxiv.org/pdf/cs/0411099"
        },
        {
            "id": "21",
            "entry": "[21] L. Begin, P. Germain, F. Laviolette, J.-F. Roy, \u201cPAC-Bayesian Bounds based on the Renyi Divergence\u201d, In AISTATS (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Begin%2C%20L.%20Germain%2C%20P.%20Laviolette%2C%20F.%20Roy%2C%20J.-F.%20PAC-Bayesian%20Bounds%20based%20on%20the%20Renyi%20Divergence%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Begin%2C%20L.%20Germain%2C%20P.%20Laviolette%2C%20F.%20Roy%2C%20J.-F.%20PAC-Bayesian%20Bounds%20based%20on%20the%20Renyi%20Divergence%202016"
        },
        {
            "id": "22",
            "entry": "[22] S. Kullback, R. Leibler, \u201cOn information and sufficiency\u201d, Annals of Mathematical Statistics 22, 79-86 (1951).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kullback%2C%20S.%20Leibler%2C%20R.%20On%20information%20and%20sufficiency%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kullback%2C%20S.%20Leibler%2C%20R.%20On%20information%20and%20sufficiency%201951"
        },
        {
            "id": "23",
            "entry": "[23] A. G. de G. Matthews, J. Hensman, R. Turner, Z. Ghahramani, \u201cOn Sparse Variational Methods and the Kullback-Leibler Divergence between Stochastic Processes\u201d, In AISTATS (2016).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=de%20G.%20Matthews%2C%20A.G.%20Hensman%2C%20J.%20Turner%2C%20R.%20Ghahramani%2C%20Z.%20On%20Sparse%20Variational%20Methods%20and%20the%20Kullback-Leibler%20Divergence%20between%20Stochastic%20Processes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=de%20G.%20Matthews%2C%20A.G.%20Hensman%2C%20J.%20Turner%2C%20R.%20Ghahramani%2C%20Z.%20On%20Sparse%20Variational%20Methods%20and%20the%20Kullback-Leibler%20Divergence%20between%20Stochastic%20Processes%202016"
        },
        {
            "id": "24",
            "entry": "[24] J. Quinonero-Candela, C. E. Rasmussen, \u201cA Unifying View of Sparse Approximate Gaussian Process Regression\u201d, Journal of Machine Learning Research 6, 1939-1959 (2005).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quinonero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20Unifying%20View%20of%20Sparse%20Approximate%20Gaussian%20Process%20Regression%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quinonero-Candela%2C%20J.%20Rasmussen%2C%20C.E.%20A%20Unifying%20View%20of%20Sparse%20Approximate%20Gaussian%20Process%20Regression%202005"
        },
        {
            "id": "25",
            "entry": "[25] T. D. Bui, J. Yan, R. E. Turner, \u201cA Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation\u201d, Journal of Machine Learning Research 18, 1-72 (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bui%2C%20T.D.%20Yan%2C%20J.%20Turner%2C%20R.E.%20A%20Unifying%20Framework%20for%20Gaussian%20Process%20Pseudo-Point%20Approximations%20using%20Power%20Expectation%20Propagation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bui%2C%20T.D.%20Yan%2C%20J.%20Turner%2C%20R.E.%20A%20Unifying%20Framework%20for%20Gaussian%20Process%20Pseudo-Point%20Approximations%20using%20Power%20Expectation%20Propagation%202017"
        },
        {
            "id": "26",
            "entry": "[26] J. Hensman, A. Matthews, Z. Ghahramani, \u201cScalable Variational Gaussian Process Classification\u201d, In AISTATS (2015).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hensman%2C%20J.%20Matthews%2C%20A.%20Ghahramani%2C%20Z.%20Scalable%20Variational%20Gaussian%20Process%20Classification%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hensman%2C%20J.%20Matthews%2C%20A.%20Ghahramani%2C%20Z.%20Scalable%20Variational%20Gaussian%20Process%20Classification%202015"
        },
        {
            "id": "27",
            "entry": "[27] A. Matthews, M. van der Wilk, T. Nickson, K. Fujii, A. Boukouvalas, P. Le\u00f3n-Villagr\u00e1, Z. Ghahramani, J. Hensman, \u201cGPflow: A Gaussian process library using TensorFlow\u201d, Journal of Machine Learning Research 18, 1-6 (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matthews%2C%20A.%20van%20der%20Wilk%2C%20M.%20Nickson%2C%20T.%20Fujii%2C%20K.%20GPflow%3A%20A%20Gaussian%20process%20library%20using%20TensorFlow%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matthews%2C%20A.%20van%20der%20Wilk%2C%20M.%20Nickson%2C%20T.%20Fujii%2C%20K.%20GPflow%3A%20A%20Gaussian%20process%20library%20using%20TensorFlow%202017"
        },
        {
            "id": "28",
            "entry": "[28] M. Abadi et al., \u201cTensorFlow: Large-Scale Machine Learning on Heterogeneous Systems\u201d, https://www.tensorflow.org/ (2015).",
            "url": "https://www.tensorflow.org/"
        },
        {
            "id": "29",
            "entry": "[29] C.-A. Cheng, B. Boots, \u201cVariational Inference for Gaussian Process Models with Linear Complexity\u201d, In NIPS (2017). ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Variational%20Inference%20for%20Gaussian%20Process%20Models%20with%20Linear%20Complexity%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20C.-A.%20Boots%2C%20B.%20Variational%20Inference%20for%20Gaussian%20Process%20Models%20with%20Linear%20Complexity%202017"
        }
    ]
}
