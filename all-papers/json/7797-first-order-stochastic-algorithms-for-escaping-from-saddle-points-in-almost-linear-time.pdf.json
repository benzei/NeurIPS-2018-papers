{
    "filename": "7797-first-order-stochastic-algorithms-for-escaping-from-saddle-points-in-almost-linear-time.pdf",
    "metadata": {
        "title": "First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time",
        "author": "Yi Xu, Jing Rong, Tianbao Yang",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7797-first-order-stochastic-algorithms-for-escaping-from-saddle-points-in-almost-linear-time.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we consider first-order methods for solving stochastic non-convex optimization problems. The key building block of the proposed algorithms is firstorder procedures to extract negative curvature from the Hessian matrix through a principled sequence starting from noise, which are referred to NEgative-curvature-"
    },
    "keywords": [
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "hessian matrix",
            "url": "https://en.wikipedia.org/wiki/hessian_matrix"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "negative curvature",
            "url": "https://en.wikipedia.org/wiki/negative_curvature"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "The problem of interest in this paper is Stochastic Non-Convex Optimization given by min F (x) E\u03be[f (x; \u03be)], (1)<br/><br/>x\u2208Rd where \u03be is a random variable and f (x; \u03be) is a random smooth non-convex function of x",
        "We provide a formal analysis of simple procedures based on gradient descent and accelerated gradient method for exacting a negative curvature direction from the Hessian",
        "We develop a general framework of first-order algorithms for stochastic non-convex optimization by combining the proposed first-order NEON procedures to extract negative curvature with existing first-order stochastic algorithms that aim at a first-order critical point",
        "We notice several differences between the two works: (i) they used Gaussian random noise with a variance proportional to d\u2212C, where C is a large unknown constant, in contrast our NEON and NEON+ procedures use random noise sampled from the sphere of an Euclidean ball with radius proportional to log\u22122(d); the update of their deterministic NEON2det is constructed based on the Chebyshev polynomial, in contrast our NEON+ with a similar iteration complexity is based on the well-known Nesterov\u2019s accelerated gradient method; we provide a general framework/analysis for promoting first-order algorithms to enjoy second-order convergence, which could be useful for promoting new first-order stochastic algorithms; the reported iteration complexity of their NEON2online is better \u221athan our stochastic variants of NEON",
        "We discuss a second-order method based on Hessian-vector product to escape from a non-degenerate saddle point x of a function f (x) that satisfies \u03bbmin(\u22072f (x)) \u2264 \u2212\u03b3, which can be found in many previous studies [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "We have proposed novel first-order procedures to extract negative curvature from a Hessian matrix by using a noise-initiated sequence, which are of independent interest"
    ],
    "key_statements": [
        "The problem of interest in this paper is Stochastic Non-Convex Optimization given by min F (x) E\u03be[f (x; \u03be)], (1)<br/><br/>x\u2208Rd where \u03be is a random variable and f (x; \u03be) is a random smooth non-convex function of x",
        "We develop a general framework of first-order stochastic algorithms with a secondorder convergence guarantee based on our new technique and existing algorithms that may only converge second-order stationary to a first-order stationary point x such that \u2207F (x)",
        "We provide a formal analysis of simple procedures based on gradient descent and accelerated gradient method for exacting a negative curvature direction from the Hessian",
        "We develop a general framework of first-order algorithms for stochastic non-convex optimization by combining the proposed first-order NEON procedures to extract negative curvature with existing first-order stochastic algorithms that aim at a first-order critical point",
        "The second-order methods used in these studies for computing negative curvature can be replaced by the proposed NEON procedures",
        "We notice several differences between the two works: (i) they used Gaussian random noise with a variance proportional to d\u2212C, where C is a large unknown constant, in contrast our NEON and NEON+ procedures use random noise sampled from the sphere of an Euclidean ball with radius proportional to log\u22122(d); the update of their deterministic NEON2det is constructed based on the Chebyshev polynomial, in contrast our NEON+ with a similar iteration complexity is based on the well-known Nesterov\u2019s accelerated gradient method; we provide a general framework/analysis for promoting first-order algorithms to enjoy second-order convergence, which could be useful for promoting new first-order stochastic algorithms; the reported iteration complexity of their NEON2online is better \u221athan our stochastic variants of NEON",
        "We discuss a second-order method based on Hessian-vector product to escape from a non-degenerate saddle point x of a function f (x) that satisfies \u03bbmin(\u22072f (x)) \u2264 \u2212\u03b3, which can be found in many previous studies [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "We present an accelerated gradient method to extract the NC to match the iteration complexity of the Lanczos method",
        "We mention that the proposed NEON or NEON+ can be used in existing second-order stochastic algorithms that require a NC direction as a substitute of second-order methods [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]. [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] developed Natasha2, which uses second-order online Oja\u2019s algorithm for finding the NC. [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] developed a stochastic algorithm for solving a finite-sum problem by using SVRG and a second-order stochastic algorithm for computing the NC",
        "The results are presented in Figure 2, showing that two variants of NEON-stochastic gradient descent methods can escape saddles faster than Noisy stochastic gradient descent",
        "We have proposed novel first-order procedures to extract negative curvature from a Hessian matrix by using a noise-initiated sequence, which are of independent interest"
    ],
    "summary": [
        "The problem of interest in this paper is Stochastic Non-Convex Optimization given by min F (x) E\u03be[f (x; \u03be)], (1)<br/><br/>x\u2208Rd where \u03be is a random variable and f (x; \u03be) is a random smooth non-convex function of x.",
        "We develop a general framework of first-order algorithms for stochastic non-convex optimization by combining the proposed first-order NEON procedures to extract negative curvature with existing first-order stochastic algorithms that aim at a first-order critical point.",
        "The second-order methods used in these studies for computing negative curvature can be replaced by the proposed NEON procedures.",
        "We discuss a second-order method based on HVPs to escape from a non-degenerate saddle point x of a function f (x) that satisfies \u03bbmin(\u22072f (x)) \u2264 \u2212\u03b3, which can be found in many previous studies [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>].",
        "Our HVP-free stochastic algorithms with provable guarantees for solving (1) presented in section are based on a key building block, i.e., extracting NC from noise using only first-order information.",
        "A key building block of the proposed method is a first-order procedure to extract NC for a non-convex function f (x) 1.",
        "Hessian has a negative eigen-value not just at non-degenerate saddle points, which can be used in some stochastic or deterministic algorithms [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "The proposed procedure NEON based on the above sequence for finding a NC direction of \u22072f (x) is presented in Algorithm 1, where fx(u) is defined in (7).",
        "None of these studies provide an explicit complexity guarantee on extracting NC from the Hessian matrix for a general non-convex problem.",
        "Nesterov\u2019s accelerated gradient (SNAG) method) are popular stochastic algorithms for solving a stochastic non-convex optimization problem.",
        "SCSG runs with multiple epochs, and each epoch uses similar updates as SVRG with three distinct features: (i) it was applied to a sub-sampled function FS1 ; it allows for using a mini-batch samples of size b independent of S1 to compute stochastic gradients; the number of updates of each epoch is a random number following a geometric distribution dependent on b and |S1|.",
        "We mention that the proposed NEON or NEON+ can be used in existing second-order stochastic algorithms that require a NC direction as a substitute of second-order methods [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "The results are presented in Figure 2, showing that two variants of NEON-SGD methods can escape saddles faster than Noisy SGD.",
        "We have proposed novel first-order procedures to extract negative curvature from a Hessian matrix by using a noise-initiated sequence, which are of independent interest.",
        "Based on the proposed general framework, we designed several first-order stochastic algorithms with state-of-the-art second-order convergence guarantee."
    ],
    "headline": "We develop a general framework of first-order stochastic algorithms with a secondorder convergence guarantee based on our new technique and existing algorithms that may only converge second-order stationary to a first-order stationary point x such that \u2207F ",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Z. Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. CoRR, /abs/1708.08694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "2",
            "entry": "[2] Z. Allen-Zhu and Y. Li. Neon2: Finding local minima via first-order oracles. CoRR, abs/1711.06673, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.06673"
        },
        {
            "id": "3",
            "entry": "[3] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. \"convex until proven guilty\": Dimension-free acceleration of gradient descent on non-convex functions. In ICML, pages 654\u2013663, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20%22convex%20until%20proven%20guilty%22%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20%22convex%20until%20proven%20guilty%22%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017"
        },
        {
            "id": "4",
            "entry": "[4] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Accelerated methods for nonconvex optimization. SIAM Journal on Optimization, 28(2):1751\u20131772, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20Accelerated%20methods%20for%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20Sidford%2C%20A.%20Accelerated%20methods%20for%20nonconvex%20optimization%202018"
        },
        {
            "id": "5",
            "entry": "[5] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points \u2014 online stochastic gradient for tensor decomposition. In COLT, pages 797\u2013842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20%E2%80%94%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "6",
            "entry": "[6] S. Ghadimi and G. Lan. Stochastic firstand zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Stochastic%20firstand%20zeroth-order%20methods%20for%20nonconvex%20stochastic%20programming%202013"
        },
        {
            "id": "7",
            "entry": "[7] S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Math. Program., 156(1-2):59\u201399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016"
        },
        {
            "id": "8",
            "entry": "[8] S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Math. Program., 155(1-2):267\u2013305, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016"
        },
        {
            "id": "9",
            "entry": "[9] E. Hazan, K. Y. Levy, and S. Shalev-Shwartz. On graduated optimization for stochastic non-convex problems. In ICML, pages 1833\u20131841, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Levy%2C%20K.Y.%20Shalev-Shwartz%2C%20S.%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Levy%2C%20K.Y.%20Shalev-Shwartz%2C%20S.%20On%20graduated%20optimization%20for%20stochastic%20non-convex%20problems%202016"
        },
        {
            "id": "10",
            "entry": "[10] P. Jain, P. Kar, et al. Non-convex optimization for machine learning. Foundations and Trends R in Machine Learning, 10(3-4):142\u2013336, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20P.%20Kar%2C%20P.%20Non-convex%20optimization%20for%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20P.%20Kar%2C%20P.%20Non-convex%20optimization%20for%20machine%20learning%202017"
        },
        {
            "id": "11",
            "entry": "[11] C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently. In ICML, pages 1724\u20131732, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017"
        },
        {
            "id": "12",
            "entry": "[12] C. Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. In COLT, pages 1042\u20131085, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20C.%20Netrapalli%2C%20P.%20Jordan%2C%20M.I.%20Accelerated%20gradient%20descent%20escapes%20saddle%20points%20faster%20than%20gradient%20descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20C.%20Netrapalli%2C%20P.%20Jordan%2C%20M.I.%20Accelerated%20gradient%20descent%20escapes%20saddle%20points%20faster%20than%20gradient%20descent%202018"
        },
        {
            "id": "13",
            "entry": "[13] J. Kuczynski and H. Wozniakowski. Estimating the largest eigenvalue by the power and lanczos algorithms with a random start. SIAM Journal on Matrix Analysis and Applications, 13(4):1094\u20131122, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuczynski%2C%20J.%20Wozniakowski%2C%20H.%20Estimating%20the%20largest%20eigenvalue%20by%20the%20power%20and%20lanczos%20algorithms%20with%20a%20random%20start%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuczynski%2C%20J.%20Wozniakowski%2C%20H.%20Estimating%20the%20largest%20eigenvalue%20by%20the%20power%20and%20lanczos%20algorithms%20with%20a%20random%20start%201992"
        },
        {
            "id": "14",
            "entry": "[14] L. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via SCSG methods. In NIPS, pages 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20SCSG%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20SCSG%20methods%202017"
        },
        {
            "id": "15",
            "entry": "[15] H. Li and Z. Lin. Accelerated proximal gradient methods for nonconvex programming. In NIPS, pages 379\u2013387, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20H.%20Lin%2C%20Z.%20Accelerated%20proximal%20gradient%20methods%20for%20nonconvex%20programming%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20H.%20Lin%2C%20Z.%20Accelerated%20proximal%20gradient%20methods%20for%20nonconvex%20programming%202015"
        },
        {
            "id": "16",
            "entry": "[16] M. Liu and T. Yang. On noisy negative curvature descent: Competing with gradient descent for faster non-convex optimization. CoRR, abs/1709.08571, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.08571"
        },
        {
            "id": "17",
            "entry": "[17] M. Liu and T. Yang. Stochastic non-convex optimization with strong high probability secondorder convergence. CoRR, abs/1710.09447, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.09447"
        },
        {
            "id": "18",
            "entry": "[18] Y. Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization. Kluwer Academic Publ., 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%20%3A%20a%20basic%20course.%20Applied%20optimization%202004"
        },
        {
            "id": "19",
            "entry": "[19] M. O\u2019Neill and S. J. Wright. Behavior of accelerated gradient methods near critical points of nonconvex problems. CoRR, abs/1706.07993, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.07993"
        },
        {
            "id": "20",
            "entry": "[20] S. Reddi, M. Zaheer, S. Sra, B. Poczos, F. Bach, R. Salakhutdinov, and A. Smola. A generic approach for escaping saddle points. In AISTATS, pages 1233\u20131242, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018"
        },
        {
            "id": "21",
            "entry": "[21] C. W. Royer and S. J. Wright. Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization. SIAM Journal on Optimization, 28(2):1448\u20131477, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Royer%2C%20C.W.%20Wright%2C%20S.J.%20Complexity%20analysis%20of%20second-order%20line-search%20algorithms%20for%20smooth%20nonconvex%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Royer%2C%20C.W.%20Wright%2C%20S.J.%20Complexity%20analysis%20of%20second-order%20line-search%20algorithms%20for%20smooth%20nonconvex%20optimization%202018"
        },
        {
            "id": "22",
            "entry": "[22] Y. Yan, T. Yang, Z. Li, Q. Lin, and Y. Yang. A unified analysis of stochastic momentum methods for deep learning. In IJCAI, pages 2955\u20132961, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yan%2C%20Y.%20Yang%2C%20T.%20Li%2C%20Z.%20Lin%2C%20Q.%20A%20unified%20analysis%20of%20stochastic%20momentum%20methods%20for%20deep%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yan%2C%20Y.%20Yang%2C%20T.%20Li%2C%20Z.%20Lin%2C%20Q.%20A%20unified%20analysis%20of%20stochastic%20momentum%20methods%20for%20deep%20learning%202018"
        },
        {
            "id": "23",
            "entry": "[23] Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient langevin dynamics. In COLT, pages 1980\u20132022, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Y.%20Liang%2C%20P.%20Charikar%2C%20M.%20A%20hitting%20time%20analysis%20of%20stochastic%20gradient%20langevin%20dynamics%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Y.%20Liang%2C%20P.%20Charikar%2C%20M.%20A%20hitting%20time%20analysis%20of%20stochastic%20gradient%20langevin%20dynamics%201980"
        }
    ]
}
