{
    "filename": "7852-online-convex-optimization-for-cumulative-constraints.pdf",
    "metadata": {
        "title": "Online convex optimization for cumulative constraints",
        "author": "Jianjun Yuan, Andrew Lamperski",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7852-online-convex-optimization-for-cumulative-constraints.pdf"
        },
        "abstract": "We propose the algorithms for online convex optimization which lead to cumulative\nPT squared constraint violations of the form\n[g(xt)]+ 2 = O(T 1\n), where t=1\n2 (0, 1) . Previous literature has focused on long-term constraints of the form"
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "online optimization",
            "url": "https://en.wikipedia.org/wiki/online_optimization"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        }
    ],
    "highlights": [
        "Online optimization is a popular framework for machine learning, with applications such as dictionary learning [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], auctions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], classification, and regression [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We receive a loss function ft : S ! R drawn from a family of convex functions and we obtain the loss ft",
        "We propose two algorithms for the following two different cases: Convex Case: The first algorithm is for the convex case, which has the user-determined tradeoff as in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], while the constraint violation is more strict",
        "We propose two algorithms for online convex optimization with both convex and strongly convex objective functions",
        "The bounds for the strongly convex case is an improvement compared with the previous efforts in the literature"
    ],
    "key_statements": [
        "Online optimization is a popular framework for machine learning, with applications such as dictionary learning [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], auctions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], classification, and regression [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We receive a loss function ft : S ! R drawn from a family of convex functions and we obtain the loss ft",
        "We propose two algorithms for the following two different cases: Convex Case: The first algorithm is for the convex case, which has the user-determined tradeoff as in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], while the constraint violation is more strict",
        "From Theorem 1, we can see that by setting appropriate stepsize, \u2318, and constanpt, , we can obtain the upper bound for the regret of the loss function being less than or equal to O( T ), which is shown in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]",
        "We propose two algorithms for online convex optimization with both convex and strongly convex objective functions",
        "The bounds for the strongly convex case is an improvement compared with the previous efforts in the literature"
    ],
    "summary": [
        "Online optimization is a popular framework for machine learning, with applications such as dictionary learning [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], auctions [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>], classification, and regression [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "The algorithm gives a cumulative regret RegretT (x\u21e4) which is upper bounded by O( T ), but the constraint g",
        "O( T ) regret and a bound of O( T ) on the long-term constraint violation.",
        "In both algorithms of [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] and [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], the bound for the violation of the long-term constraint is that 8i, PT gi",
        "Algorithm 1 Generalized Online Convex Optimization with Long-term Constraint",
        "Based on the update rule in Algorithm 1, the following theorem gives the upper bounds for both the",
        "From Theorem 1, we can see that by setting appropriate stepsize, \u2318, and constanpt, , we can obtain the upper bound for the regret of the loss function being less than or equal to O( T ), which is shown in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>].",
        "2. The proposed algorithm can upper bound the constraint violation for each single step [gi]+, which is not bounded in the previous literature.",
        "PT bounds for both objective regret and the clipped long-term constraint [gi]+ compared with t=1",
        "The first result puts a bound on the clipped long-term constraint, rather than the sum-of-squares that appears in Theorem 1.",
        "This result shows that our algorithm generalizes the regret and long-term constraint bounds of [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>].",
        "We will show that previous algorithms can use our proposed augmented Lagrangian function to have their own clipped long-term constraint bound.",
        "Propositions 4 and 5 show that clipped long-term constraints can be bounded by combining the algorithms of [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] with our augmented Lagrangian.",
        "These results are similar in part to our Propositions 2 and 3, they do not imply the results in Theorems 1 and 2 as well as the new single step constraint violation bound in Lemma 1, which are our key contributions.",
        "From the result we can see that, for our designed strongly convex algorithm Our-Strong, its result is around the best ones in not only the clipped constraint violation, but the objective regret.",
        "From these results we can see that our algorithm has very small constraint violation for each time step, which is desired by the requirement.",
        "By applying different update strategies that utilize a modified augmented Lagrangian function, they can solve OCO with a squared/clipped long-term constraints requirement.",
        "The algorithm for general convex case provides the useful bounds for both the long-term constraint violation and the constraint violation at each timestep.",
        "Experiments show that our algorithms can follow the constraint boundary tightly and have relatively smaller clipped long-term constraint violation with reasonably low objective regret.",
        "It would be useful if future work could explore the noisy versions of the constraints and obtain the similar upper bounds"
    ],
    "headline": "We propose the algorithms for online convex optimization which lead to cumulative",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Avrim Blum, Vijay Kumar, Atri Rudra, and Felix Wu. Online learning in online auctions. Theoretical Computer Science, 324(2-3):137\u2013146, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Kumar%2C%20Vijay%20Rudra%2C%20Atri%20Wu%2C%20Felix%20Online%20learning%20in%20online%20auctions%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Kumar%2C%20Vijay%20Rudra%2C%20Atri%20Wu%2C%20Felix%20Online%20learning%20in%20online%20auctions%202004"
        },
        {
            "id": "2",
            "entry": "[2] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cesa-Bianchi%2C%20Nicolo%20Lugosi%2C%20G%C3%A1bor%20Prediction%2C%20learning%2C%20and%20games%202006"
        },
        {
            "id": "3",
            "entry": "[3] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar):551\u2013585, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Crammer%2C%20Koby%20Dekel%2C%20Ofer%20Keshet%2C%20Joseph%20Shalev-Shwartz%2C%20Shai%20Online%20passive-aggressive%20algorithms%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Crammer%2C%20Koby%20Dekel%2C%20Ofer%20Keshet%2C%20Joseph%20Shalev-Shwartz%2C%20Shai%20Online%20passive-aggressive%20algorithms%202006"
        },
        {
            "id": "4",
            "entry": "[4] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20Bach%2C%20Francis%20Lacoste-Julien%2C%20Simon%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "5",
            "entry": "[5] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimization. Journal of Machine Learning Research, 17(83):1\u20135, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diamond%2C%20Steven%20Boyd%2C%20Stephen%20CVXPY%3A%20A%20Python-embedded%20modeling%20language%20for%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diamond%2C%20Steven%20Boyd%2C%20Stephen%20CVXPY%3A%20A%20Python-embedded%20modeling%20language%20for%20convex%20optimization%202016"
        },
        {
            "id": "6",
            "entry": "[6] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 272\u2013279. ACM, 2008. 1https://www.iso-ne.com/isoexpress/web/reports/load-and-demand",
            "url": "http://www.iso-ne.com/isoexpress/web/reports/load-and-demand",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Chandra%2C%20Tushar%20Efficient%20projections%20onto%20the%20l%201-ball%20for%20learning%20in%20high%20dimensions%202008"
        },
        {
            "id": "7",
            "entry": "[7] John C Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari. Composite objective mirror descent. In COLT, pages 14\u201326, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Tewari%2C%20Ambuj%20Composite%20objective%20mirror%20descent%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Shalev-Shwartz%2C%20Shai%20Singer%2C%20Yoram%20Tewari%2C%20Ambuj%20Composite%20objective%20mirror%20descent%202010"
        },
        {
            "id": "8",
            "entry": "[8] Maryam Fazel, Rong Ge, Sham M Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for linearized control problems. arXiv preprint arXiv:1801.05039, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.05039"
        },
        {
            "id": "9",
            "entry": "[9] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2):169\u2013192, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Agarwal%2C%20Amit%20Kale%2C%20Satyen%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Agarwal%2C%20Amit%20Kale%2C%20Satyen%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007"
        },
        {
            "id": "10",
            "entry": "[10] Rodolphe Jenatton, Jim Huang, and C\u00e9dric Archambeau. Adaptive algorithms for online convex optimization with long-term constraints. In International Conference on Machine Learning, pages 402\u2013411, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rodolphe%20Jenatton%2C%20Jim%20Huang%20Archambeau%2C%20C%C3%A9dric%20Adaptive%20algorithms%20for%20online%20convex%20optimization%20with%20long-term%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rodolphe%20Jenatton%2C%20Jim%20Huang%20Archambeau%2C%20C%C3%A9dric%20Adaptive%20algorithms%20for%20online%20convex%20optimization%20with%20long-term%20constraints%202016"
        },
        {
            "id": "11",
            "entry": "[11] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "12",
            "entry": "[12] Yingying Li, Guannan Qu, and Na Li. Online optimization with predictions and switching costs: Fast algorithms and the fundamental limit. arXiv preprint arXiv:1801.07780, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.07780"
        },
        {
            "id": "13",
            "entry": "[13] Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for efficiency: online convex optimization with long term constraints. Journal of Machine Learning Research, 13(Sep):2503\u2013 2528, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Yang%2C%20Tianbao%20Trading%20regret%20for%20efficiency%3A%20online%20convex%20optimization%20with%20long%20term%20constraints%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahdavi%2C%20Mehrdad%20Jin%2C%20Rong%20Yang%2C%20Tianbao%20Trading%20regret%20for%20efficiency%3A%20online%20convex%20optimization%20with%20long%20term%20constraints%202012"
        },
        {
            "id": "14",
            "entry": "[14] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In Proceedings of the 26th annual international conference on machine learning, pages 689\u2013696. ACM, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sapiro%2C%20Guillermo%20Online%20dictionary%20learning%20for%20sparse%20coding%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20Julien%20Bach%2C%20Francis%20Ponce%2C%20Jean%20Sapiro%2C%20Guillermo%20Online%20dictionary%20learning%20for%20sparse%20coding%202009"
        },
        {
            "id": "15",
            "entry": "[15] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "16",
            "entry": "[16] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127\u2013152, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yu%20Smooth%20minimization%20of%20non-smooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yu%20Smooth%20minimization%20of%20non-smooth%20functions%202005"
        },
        {
            "id": "17",
            "entry": "[17] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "18",
            "entry": "[18] K Senthil and K Manikandan. Economic thermal power dispatch with emission constraint and valve point effect loading using improved tabu search algorithm. International Journal of Computer Applications, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Senthil%2C%20K.%20Manikandan%2C%20K.%20Economic%20thermal%20power%20dispatch%20with%20emission%20constraint%20and%20valve%20point%20effect%20loading%20using%20improved%20tabu%20search%20algorithm%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Senthil%2C%20K.%20Manikandan%2C%20K.%20Economic%20thermal%20power%20dispatch%20with%20emission%20constraint%20and%20valve%20point%20effect%20loading%20using%20improved%20tabu%20search%20algorithm%202010"
        },
        {
            "id": "19",
            "entry": "[19] Hao Yu, Michael Neely, and Xiaohan Wei. Online convex optimization with stochastic constraints. In Advances in Neural Information Processing Systems, pages 1427\u20131437, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Hao%20Neely%2C%20Michael%20Wei%2C%20Xiaohan%20Online%20convex%20optimization%20with%20stochastic%20constraints%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Hao%20Neely%2C%20Michael%20Wei%2C%20Xiaohan%20Online%20convex%20optimization%20with%20stochastic%20constraints%202017"
        },
        {
            "id": "20",
            "entry": "[20] Jianjun Yuan and Andrew Lamperski. Online control basis selection by a regularized actor critic algorithm. In American Control Conference (ACC), 2017, pages 4448\u20134453. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yuan%2C%20Jianjun%20Lamperski%2C%20Andrew%20Online%20control%20basis%20selection%20by%20a%20regularized%20actor%20critic%20algorithm%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yuan%2C%20Jianjun%20Lamperski%2C%20Andrew%20Online%20control%20basis%20selection%20by%20a%20regularized%20actor%20critic%20algorithm%202017"
        },
        {
            "id": "21",
            "entry": "[21] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 928\u2013936, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
