{
    "filename": "7295-an-off-policy-policy-gradient-theorem-using-emphatic-weightings.pdf",
    "metadata": {
        "title": "An Off-policy Policy Gradient Theorem Using Emphatic Weightings",
        "author": "Ehsan Imani, Eric Graves, Martha White",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7295-an-off-policy-policy-gradient-theorem-using-emphatic-weightings.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Policy gradient methods are widely used for control in reinforcement learning, particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting, due to the existence of the policy gradient theorem which provides a simplified form for the gradient."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "optimal solution",
            "url": "https://en.wikipedia.org/wiki/optimal_solution"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "policy learning",
            "url": "https://en.wikipedia.org/wiki/policy_learning"
        }
    ],
    "highlights": [
        "Off-policy learning holds great promise for learningOinn-paonliocnyline setting, where an agent generates a single stream of interaction with its enviroofnf-mpeonlitc.y methods are limited to learning about the agent\u2019s current policy",
        "We empirically investigate the utility of using the true off-policy gradient, as opposed to the previous approximation used by OffPAC; the impact of the choice of a; and the efficacy of estimating emphatic weightings in Actor-Critic with Emphatic weightings",
        "We present a toy problem to highlight the fact that OffPAC\u2014which uses an approximate semi-gradient\u2014can converge to suboptimal solutions, even in ideal conditions, whereas Actor-Critic with Emphatic weightings\u2014with the true gradient\u2014converges to the optimal solution",
        "In this paper we proved the off-policy policy gradient theorem, using emphatic weightings",
        "We designed a simple MDP to highlight issues with existing methods\u2014namely OffPAC and Deterministic Policy Gradient\u2014 particularly highlighting that the stationary points of these semi-gradient methods for this problem do not include the optimal solution",
        "We conclude with a result suggesting that more work needs to be done to effectively estimate emphatic weightings, and that important steps for developing Actor-Critic algorithm for the off-policy setting are to improve estimation of these weightings"
    ],
    "key_statements": [
        "Off-policy learning holds great promise for learningOinn-paonliocnyline setting, where an agent generates a single stream of interaction with its enviroofnf-mpeonlitc.y methods are limited to learning about the agent\u2019s current policy",
        "Actor Critic with Emphatic weightings (ACE)\u2014that approximates the simplified gradients provided by the theorem",
        "We provide an off-policy policy gradient theorem, for general policy parametrization",
        "We demonstrate both the theorem and the counterexample under stochastic and deterministic policies, and that Actor-Critic with Emphatic weightings converges to the optimal solution.\n2 Problem Formulation",
        "We show that the policy gradient theorem does hold in the off-policy setting, when using function approximation for the policy, as long as we use emphatic weightings",
        "We provide the complete Actor-Critic with Emphatic weightings (ACE) algorithm, with pseudo-code and additional algorithm details, in Appendix B",
        "We empirically investigate the utility of using the true off-policy gradient, as opposed to the previous approximation used by OffPAC; the impact of the choice of a; and the efficacy of estimating emphatic weightings in Actor-Critic with Emphatic weightings",
        "We present a toy problem to highlight the fact that OffPAC\u2014which uses an approximate semi-gradient\u2014can converge to suboptimal solutions, even in ideal conditions, whereas Actor-Critic with Emphatic weightings\u2014with the true gradient\u2014converges to the optimal solution",
        "In Section 5.3 and Figure 5, that this is a problem in the continuous action setting with Deterministic Policy Gradient",
        "In this paper we proved the off-policy policy gradient theorem, using emphatic weightings",
        "We derived an off-policy actor-critic algorithm that follows the gradient of the objective function, as opposed to previous method like OffPAC and Deterministic Policy Gradient that followed an approximate semi-gradient",
        "We designed a simple MDP to highlight issues with existing methods\u2014namely OffPAC and Deterministic Policy Gradient\u2014 particularly highlighting that the stationary points of these semi-gradient methods for this problem do not include the optimal solution",
        "We conclude with a result suggesting that more work needs to be done to effectively estimate emphatic weightings, and that important steps for developing Actor-Critic algorithm for the off-policy setting are to improve estimation of these weightings"
    ],
    "summary": [
        "Off-policy learning holds great promise for learningOinn-paonliocnyline setting, where an agent generates a single stream of interaction with its enviroofnf-mpeonlitc.y methods are limited to learning about the agent\u2019s current policy.",
        "Actor Critic with Emphatic weightings (ACE)\u2014that approximates the simplified gradients provided by the theorem.",
        "We use previous methods for incrementally estimating these emphatic weightings [Yu, 2015, Sutton et al, 2016] to design a new off-policy actor-critic algorithm, called ActorCritic with Emphatic weightings (ACE).",
        "We show that the policy gradient theorem does hold in the off-policy setting, when using function approximation for the policy, as long as we use emphatic weightings.",
        "These results parallel those in off-policy policy evaluation, for learning the value function, where issues of convergence for temporal difference methods are ameliorated by using an emphatic weighting [Sutton et al, 2016].",
        "We develop an incremental actor-critic algorithm with emphatic weightings, that uses the above off-policy policy gradient theorem.",
        "The policy gradient theorem assumes access to the true value function, and provides a Bellman equation that defines the optimal fixed point.",
        "Selecting a between 0 and 1 could provide a reasonable balance, obtaining a nearly unbiased gradient to enable convergence to a valid stationary point but potentially reducing some variability when estimating the emphatic weighting.",
        "We empirically investigate the utility of using the true off-policy gradient, as opposed to the previous approximation used by OffPAC; the impact of the choice of a; and the efficacy of estimating emphatic weightings in ACE.",
        "Derivation of the online algorithm assumes a fixed target policy [Sutton et al, 2016], while the actor is updated at every time step.",
        "The actor is initialized with zero weights and uses true state values in its updates.",
        ". We trained an actor, called True-ACE, that uses true emphatic weightings for the current target policy and behaviour policy, computed at each timestep.",
        "We derived an off-policy actor-critic algorithm that follows the gradient of the objective function, as opposed to previous method like OffPAC and DPG that followed an approximate semi-gradient.",
        "We designed a simple MDP to highlight issues with existing methods\u2014namely OffPAC and DPG\u2014 particularly highlighting that the stationary points of these semi-gradient methods for this problem do not include the optimal solution.",
        "Called Actor-Critic with Emphatic Weightings, on the other hand, which follows the gradient, reaches the optimal solution, both for an idealized setting given the true critic and when learning the critic.",
        "We conclude with a result suggesting that more work needs to be done to effectively estimate emphatic weightings, and that important steps for developing Actor-Critic algorithm for the off-policy setting are to improve estimation of these weightings."
    ],
    "headline": "We develop a new actor-critic algorithm\u2014called",
    "reference_links": [
        {
            "id": ",_2008_a",
            "entry": ", (5):834\u2013846, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=5834846%201983"
        },
        {
            "id": ",_2004_a",
            "entry": ", pages 105\u2013112, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Nov%29%202004"
        },
        {
            "id": "Grondman_et+al_2012_a",
            "entry": ", 5(Nov):1471\u20131530, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=5Nov14711530%202004"
        },
        {
            "id": "Ivo_2000_a",
            "entry": "Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert BaIbEuEsEkaT. rAanssuarcvtieoynosfoanctSoyrs-tcermitisc, rMeiannf,oarcnedmCeynbtelrenaerntiicnsg, :PaSrttanCd(aArdppalnicdantiaotnusraalnpdoRliecvyiegwrasd) ients. , 42(6):1291\u20131307, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ivo%20Grondman%20Lucian%20Busoniu%20Gabriel%20AD%20Lopes%20and%20Robert%20BaIbEuEsEkaT%20rAanssuarcvtieoynosfoanctSoyrstcermitisc%20rMeiannfoarcnedmCeynbtelrenaerntiicnsg%20PaSrttanCdaArdppalnicdantiaotnusraalnpdoRliecvyiegwrasd%20ients%20%2042612911307%202012"
        },
        {
            "id": "Lillicrap_et+al_2015_a",
            "entry": ", pages 1008\u20131014, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lillicrap%2C%20Timothy%20P.%20Hunt%2C%20Jonathan%20J.%20Pritzel%2C%20Alexander%20Heess%2C%20Nicolas%20va%3A%201n5d0D9.a0a2n97W1ierstra.%20Continuous%20control%20with%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "Timothy_1992_a",
            "entry": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval TaarsXsiav, DpraevpirdinStialvreXri,va:1n5d0D9.a0a2n97W1ierstra. Continuous control with deep reinforcement learning. , 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20LoMnga-cJhi%20iLnien.leSaerlnf-inimgproving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20LoMnga-cJhi%20iLnien.leSaerlnf-inimgproving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching%201992"
        },
        {
            "id": "LoMnga-cJhi_2011_a",
            "entry": "LoMnga-cJhi iLnien.leSaerlnf-inimgproving reactive agents based on reinforcement learning, planning and teaching. , 8(3-4):293\u2013321, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LoMngacJhi%20iLnienleSaerlnfinimgproving%20reactive%20agents%20based%20on%20reinforcement%20learning%20planning%20and%20teaching%20%20834293321%201992"
        },
        {
            "id": "._2001_a",
            "entry": ". PhD thesis, University of Alberta, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20PeIteErEMEaTrrbaancshacatnidonJsoohnn%20ANuTtosmitasitkiclisC.oSntirmoul%20lation-based%20optimization%20of%20markov%20reward%20processes%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20PeIteErEMEaTrrbaancshacatnidonJsoohnn%20ANuTtosmitasitkiclisC.oSntirmoul%20lation-based%20optimization%20of%20markov%20reward%20processes%202001"
        },
        {
            "id": "PeIteErEMEaTrrbaancshacatnidonJsoohnn_2000_b",
            "entry": "PeIteErEMEaTrrbaancshacatnidonJsoohnn ANuTtosmitasitkiclisC.oSntirmoul lation-based optimization of markov reward processes. , 46(2):191\u2013209, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=PeIteErEMEaTrrbaancshacatnidonJsoohnn%20ANuTtosmitasitkiclisCoSntirmoul%20lationbased%20optimization%20of%20markov%20reward%20processes%20%20462191209%202001"
        },
        {
            "id": "D_2014_a",
            "entry": ", 518(7540):529, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deterministic%20policy%20gradient%20algorithms%202014"
        },
        {
            "id": "Sutton_et+al_2011_a",
            "entry": ". PhD thesis, PhD thesis, University of Massachusetts Amherst, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Modayil%2C%20Joseph%20Delp%2C%20Michael%20Degris%2C%20Thomas%20Horde%3A%20A%20scalabTlheere1a0l-thtimInetearrncahtiitoenctaulrCe%20ofonrfelereanrncienognknAouwtolneodmgeofursomAguennstsupaenrdvMisueltdiasgeennsot%20rSiymstoetmors-inVtoelruamcteio2n%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutton%2C%20Richard%20S.%20Modayil%2C%20Joseph%20Delp%2C%20Michael%20Degris%2C%20Thomas%20Horde%3A%20A%20scalabTlheere1a0l-thtimInetearrncahtiitoenctaulrCe%20ofonrfelereanrncienognknAouwtolneodmgeofursomAguennstsupaenrdvMisueltdiasgeennsot%20rSiymstoetmors-inVtoelruamcteio2n%202011"
        },
        {
            "id": "D_2014_a",
            "entry": "D SPirlovceer,edGinLgesvoefr,thNeH31eestss.,. T. Degris, and D Wierstra. Deterministic policy gradient algorithms. In , 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D%20SPirlovceeredGinLgesvoefrthNeH31eestss%20T%20Degris%20and%20D%20Wierstra%20Deterministic%20policy%20gradient%20algorithms%20In%20%202014"
        },
        {
            "id": "Inc_2001_a",
            "entry": "Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalabTlheere1a0l-thtimInetearrncahtiitoenctaulrCe ofonrfelereanrncienognknAouwtolneodmgeofursomAguennstsupaenrdvMisueltdiasgeennsot rSiymstoetmors-inVtoelruamcteio2n. In , pages 761\u2013768. International Foundation for Autonomous Agents and Multiagent Systems, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Richard%20S%20Sutton%20Joseph%20Modayil%20Michael%20Delp%20Thomas%20Degris%20Patrick%20M%20Pilarski%20Adam%20White%20and%20Doina%20Precup%20Horde%20A%20scalabTlheere1a0lthtimInetearrncahtiitoenctaulrCe%20ofonrfelereanrncienognknAouwtolneodmgeofursomAguennstsupaenrdvMisueltdiasgeennsot%20rSiymstoetmorsinVtoelruamcteio2n%20In%20%20pages%20761768%20International%20Foundation%20for%20Autonomous%20Agents%20and%20Multiagent%20Systems%202011"
        },
        {
            "id": ",_2017_a",
            "entry": ", pages 441\u2013448, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=pages%20441448%202014"
        }
    ]
}
