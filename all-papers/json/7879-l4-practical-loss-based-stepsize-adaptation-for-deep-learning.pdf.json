{
    "filename": "7879-l4-practical-loss-based-stepsize-adaptation-for-deep-learning.pdf",
    "metadata": {
        "title": "L4: Practical loss-based stepsize adaptation for deep learning",
        "author": "Michal Rolinek, Georg Martius",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7879-l4-practical-loss-based-stepsize-adaptation-for-deep-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose a stepsize adaptation scheme for stochastic gradient descent. It operates directly with the loss function and rescales the gradient in order to make fixed predicted progress on the loss. We demonstrate its capabilities by conclusively improving the performance of Adam and Momentum optimizers. The enhanced optimizers with default hyperparameters consistently outperform their constant stepsize counterparts, even the best ones, without a measurable increase in computational cost. The performance is validated on multiple architectures including dense nets, CNNs, ResNets, and the recurrent Differential Neural Computer on classical datasets MNIST, fashion MNIST, CIFAR10 and others."
    },
    "keywords": [
        {
            "term": "MNIST",
            "url": "https://en.wikipedia.org/wiki/MNIST"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Stochastic gradient methods are the driving force behind the recent boom of deep learning",
        "We introduce two variants of L4 leading to two optimizers: (1) with momentum gradient descent, denoted by L4Mom, and (2) with Adam [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], denoted by L4Adam",
        "We propose a stepsize adaptation scheme L4 compatible with currently most prominent gradient methods",
        "Two arising optimizers were tested on a multitude of datasets, spanning across different batch sizes, loss functions and network structures",
        "Ali Rahimi and Benjamin Recht suggested in their NIPS 2017 talk [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] that the failure to drive loss to zero within machine precision might be an actual bottleneck of deep learning",
        "We show on this example and on MNIST that our method can break this \u201coptimization floor\u201d"
    ],
    "key_statements": [
        "Stochastic gradient methods are the driving force behind the recent boom of deep learning",
        "The demand for practical efficiency as well as for theoretical understanding has never been stronger. This has inspired a lot of research and has given rise to new and currently very popular optimization methods such as Adam [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], AdaGrad [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], or RMSProp [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>], which serve as competitive alternatives to classical stochastic gradient descent (SGD)",
        "We introduce two variants of L4 leading to two optimizers: (1) with momentum gradient descent, denoted by L4Mom, and (2) with Adam [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], denoted by L4Adam",
        "We evaluate the proposed method on five different setups, spanning over different architectures, datasets, and loss functions",
        "For each of the methods, the performance is evaluated for the best setting of the stepsize/learning rate parameter",
        "We propose a stepsize adaptation scheme L4 compatible with currently most prominent gradient methods",
        "Two arising optimizers were tested on a multitude of datasets, spanning across different batch sizes, loss functions and network structures",
        "Ali Rahimi and Benjamin Recht suggested in their NIPS 2017 talk [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] that the failure to drive loss to zero within machine precision might be an actual bottleneck of deep learning",
        "We show on this example and on MNIST that our method can break this \u201coptimization floor\u201d"
    ],
    "summary": [
        "Stochastic gradient methods are the driving force behind the recent boom of deep learning.",
        "Our adaptation method is called Linearized Loss-based optimaL Learning-rate (L4) and it has two main features.",
        "We describe how the stepsize is chosen for a gradient update proposed by an underlying optimizer (e.",
        "Let L(\u03b8) be the loss function depending on the parameters \u03b8 and let v be the update step provided by some standard optimizer, e.",
        "In addition there are the following reasons to be more conservative: the problems in deep learning are non-convex, and minimizing the currently seen batch loss is very likely to not generalize to the whole dataset.",
        "The algorithm is called Linearized Loss-based optimaL Learning-rate (L4) and it works on top of provided gradient estimator and an update direction algorithm, see",
        "For each of the methods, the performance is evaluated for the best setting of the stepsize/learning rate parameter.",
        "The performance of these default settings is comparable with handcrafted optimization policies on more complicated architectures.",
        "Training loss Effective learning rate",
        "The deployed optimization policy is momentum gradient with manually crafted piece-wise constant stepsize adaptation.",
        "The adaptive learning rates are much more conservative in behavior compared to MNIST, possibly signaling for different nature of the datasets.",
        "Comparing performance against optimized constant learning rates is favorable for L4 optimizers both in terms of loss and test accuracy.",
        "That the constant learning rate 10\u22123 provided in [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] can be further tuned and we compare our results against the improved value 0.005.",
        "We used the best performing constant learning rates 0.01 for Adam and 1.2 for momentum SGD as baselines.",
        "Plateau regions with small gradients will force very high learning rates in order to leave them.",
        "This is a fundamental difference to methods that at each step make a small update to the previous learning rate.",
        "RMSProp and Adam use best performing learning rates 0.005 and 0.01, respectively.",
        "We propose a stepsize adaptation scheme L4 compatible with currently most prominent gradient methods.",
        "Two arising optimizers were tested on a multitude of datasets, spanning across different batch sizes, loss functions and network structures.",
        "This default setting performs well when compared to hand-tuned optimization policies from official repositories of modern high-performing architectures.",
        "The ability of the proposed method to drive loss to convergence creates an opportunity to better evaluate regularization strategies and develop new ones."
    ],
    "headline": "We propose a stepsize adaptation scheme for stochastic gradient descent",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Marcin Andrychowicz, Misha Denil, Sergio G\u00f3mez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems 29, pages 3981\u20133989. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20G%C3%B3mez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andrychowicz%2C%20Marcin%20Denil%2C%20Misha%20G%C3%B3mez%2C%20Sergio%20Hoffman%2C%20Matthew%20W.%20Learning%20to%20learn%20by%20gradient%20descent%20by%20gradient%20descent%202016"
        },
        {
            "id": "2",
            "entry": "[2] Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank D. Wood. Online learning rate adaptation with hypergradient descent. CoRR, abs/1703.04782, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04782"
        },
        {
            "id": "3",
            "entry": "[3] R. H. Byrd, S. L. Hansen, Jorge Nocedal, and Y. Singer. A stochastic quasi-newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008\u20131031, 1 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20R.H.%20Hansen%2C%20S.L.%20Nocedal%2C%20Jorge%20Singer%2C%20Y.%20A%20stochastic%20quasi-newton%20method%20for%20large-scale%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20R.H.%20Hansen%2C%20S.L.%20Nocedal%2C%20Jorge%20Singer%2C%20Y.%20A%20stochastic%20quasi-newton%20method%20for%20large-scale%20optimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] Google Deepmind. Official implementation of the differential neural computer, 2017. https://github.com/deepmind/dnc Commit a4debae.",
            "url": "https://github.com/deepmind/dnc",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deepmind%2C%20Google%20Official%20implementation%20of%20the%20differential%20neural%20computer%202017"
        },
        {
            "id": "5",
            "entry": "[5] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121\u20132159, July 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20Hazan%2C%20Elad%20Singer%2C%20Yoram%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011-07"
        },
        {
            "id": "6",
            "entry": "[6] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alex%20Graves%20Greg%20Wayne%20and%20Ivo%20Danihelka%20Neural%20turing%20machines%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alex%20Graves%20Greg%20Wayne%20and%20Ivo%20Danihelka%20Neural%20turing%20machines%202014"
        },
        {
            "id": "7",
            "entry": "[7] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio G\u00f3mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adri\u00e0 Puigdom\u00e8nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471\u2013476, October 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Graves%2C%20Alex%20Wayne%2C%20Greg%20Reynolds%2C%20Malcolm%20Harley%2C%20Tim%20Hybrid%20computing%20using%20a%20neural%20network%20with%20dynamic%20external%20memory%202016-10"
        },
        {
            "id": "8",
            "entry": "[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778. IEEE Computer Society, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "9",
            "entry": "[9] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In in Proceedings of ICLR, 2015. arXiv preprint https://arxiv.org/abs/1412.6980.",
            "url": "https://arxiv.org/abs/1412.6980",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "10",
            "entry": "[10] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced Research), 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alex%20Krizhevsky%20Vinod%20Nair%20and%20Geoffrey%20Hinton%20CIFAR10%20Canadian%20Institute%20for%20Advanced%20Research%202009"
        },
        {
            "id": "11",
            "entry": "[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, November 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11"
        },
        {
            "id": "12",
            "entry": "[12] Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01885"
        },
        {
            "id": "13",
            "entry": "[13] M. Mahsereci and P. Hennig. Probabilistic line searches for stochastic optimization. In Advances in Neural Information Processing Systems 28, pages 181\u2013189. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mahsereci%2C%20M.%20Hennig%2C%20P.%20Probabilistic%20line%20searches%20for%20stochastic%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mahsereci%2C%20M.%20Hennig%2C%20P.%20Probabilistic%20line%20searches%20for%20stochastic%20optimization%202015"
        },
        {
            "id": "14",
            "entry": "[14] Franziska Meier, Daniel Kappler, and Stefan Schaal. Online learning of a memory for learning rates. arXiv preprint https://arxiv.org/abs/1709.06709, 2017.",
            "url": "https://arxiv.org/abs/1709.06709",
            "arxiv_url": "https://arxiv.org/pdf/1709.06709"
        },
        {
            "id": "15",
            "entry": "[15] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York, 2nd edition, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nocedal%2C%20J.%20Wright%2C%20S.J.%20Numerical%20Optimization%202006"
        },
        {
            "id": "16",
            "entry": "[16] B. T. Polyak. Introduction to optimization. Translations Series in Mathematics and Engineering, Optimization Software, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20B.T.%20Introduction%20to%20optimization%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20B.T.%20Introduction%20to%20optimization%201987"
        },
        {
            "id": "17",
            "entry": "[17] Benjamin Recht. Gradient descent doesn\u2019t find a local minimum, 2017. https://github.com/benjamin-recht/shallow-linear-net Commit d192d96.",
            "url": "https://github.com/benjamin-recht/shallow-linear-net",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Gradient%20descent%20doesn%E2%80%99t%20find%20a%20local%20minimum%202017"
        },
        {
            "id": "18",
            "entry": "[18] Benjamin Recht and Ali Rahimi. Reflections on random kitchen sinks, 2017. http://www.argmin.net/2017/12/05/kitchen-sinks, Dec.5 2017.",
            "url": "http://www.argmin.net/2017/12/05/kitchen-sinks"
        },
        {
            "id": "19",
            "entry": "[19] Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28/3 of Proceedings of Machine Learning Research, pages 343\u2013351, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schaul%2C%20Tom%20Zhang%2C%20Sixin%20LeCun%2C%20Yann%20No%20more%20pesky%20learning%20rates%202013-06-17",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schaul%2C%20Tom%20Zhang%2C%20Sixin%20LeCun%2C%20Yann%20No%20more%20pesky%20learning%20rates%202013-06-17"
        },
        {
            "id": "20",
            "entry": "[20] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pages 2440\u20132448. Curran Associates, Inc., 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sukhbaatar%2C%20Sainbayar%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sukhbaatar%2C%20Sainbayar%20Weston%2C%20Jason%20Fergus%2C%20Rob%20End-to-end%20memory%20networks%202015"
        },
        {
            "id": "21",
            "entry": "[21] TensorFlow GitHub Repository. Tensorflow implementation of ResNets, 2016. Commit 1f34fcaf.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=TensorFlow%20GitHub%20Repository%20Tensorflow%20implementation%20of%20ResNets%202016%20Commit%201f34fcaf"
        },
        {
            "id": "22",
            "entry": "[22] T. Tieleman and G. Hinton. Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5%E2%80%94RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tieleman%2C%20T.%20Hinton%2C%20G.%20Lecture%206.5%E2%80%94RmsProp%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude%202012"
        },
        {
            "id": "23",
            "entry": "[23] Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in stochastic meta-optimization. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Ren%2C%20Mengye%20Liao%2C%20Renjie%20Grosse%2C%20Roger%20Understanding%20short-horizon%20bias%20in%20stochastic%20meta-optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Ren%2C%20Mengye%20Liao%2C%20Renjie%20Grosse%2C%20Roger%20Understanding%20short-horizon%20bias%20in%20stochastic%20meta-optimization%202018"
        },
        {
            "id": "24",
            "entry": "[24] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20Han%20Rasul%2C%20Kashif%20Vollgraf%2C%20Roland%20Fashion-mnist%3A%20a%20novel%20image%20dataset%20for%20benchmarking%20machine%20learning%20algorithms%202017"
        }
    ]
}
