{
    "filename": "7321-algorithmic-regularization-in-learning-deep-homogeneous-models-layers-are-automatically-balanced.pdf",
    "metadata": {
        "title": "Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced",
        "author": "Simon S. Du, Wei Hu, Jason D. Lee",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7321-algorithmic-regularization-in-learning-deep-homogeneous-models-layers-are-automatically-balanced.pdf"
        },
        "abstract": "We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "For rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate",
        "We show that on the gradient flow trajectory induced by any differentiable loss function, for a large class of homogeneous models, including fully connected and convolutional neural networks with linear, Rectified Linear Unit and Leaky Rectified Linear Unit activations, the differences between squared norms across layers remain invariant",
        "We study the implicit regularization imposed by gradient descent with infinitesimal step size in training deep neural networks",
        "We show that gradient flow automatically balances the magnitudes of all layers in a deep neural network with homogeneous activations",
        "For the concrete model of asymmetric matrix factorization, we further use the balancedness property to show that gradient descent converges to global minimum",
        "We believe our findings on the invariance in deep models could serve as a fundamental building block for understanding optimization in deep learning"
    ],
    "key_statements": [
        "For rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate",
        "A direct consequence of homogeneity is that a solution can produce small function value while being unbounded, because one can always multiply one layer by a huge number and divide anotherThe full version of this paper is available at https://arxiv.org/abs/1806.00900. :Machine Learning Department, School of Computer Science, Carnegie Mellon University",
        "We show that on the gradient flow trajectory induced by any differentiable loss function, for a large class of homogeneous models, including fully connected and convolutional neural networks with linear, Rectified Linear Unit and Leaky Rectified Linear Unit activations, the differences between squared norms across layers remain invariant",
        "We study the implicit regularization imposed by gradient descent with infinitesimal step size in training deep neural networks",
        "We show that the conservation property in Corollary 2.1 can be generalized to convolutional neural networks",
        "We show that gradient flow automatically balances the magnitudes of all layers in a deep neural network with homogeneous activations",
        "For the concrete model of asymmetric matrix factorization, we further use the balancedness property to show that gradient descent converges to global minimum",
        "We believe our findings on the invariance in deep models could serve as a fundamental building block for understanding optimization in deep learning",
        "In this paper we focus on the invariance induced by gradient descent"
    ],
    "summary": [
        "For rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate.",
        "We show that on the gradient flow trajectory induced by any differentiable loss function, for a large class of homogeneous models, including fully connected and convolutional neural networks with linear, ReLU and Leaky ReLU activations, the differences between squared norms across layers remain invariant.",
        "For rank-1 asymmetric matrix factorization, we give a finer analysis and show that randomly initialized gradient descent with constant step size converges to the global minimum at a globally linear rate.",
        "In Section 2, we present our main theoretical result on the implicit regularization property of gradient flow for optimizing neural networks.",
        "In Section 3, we analyze the dynamics of randomly initialized gradient descent for asymmetric matrix factorization problem with unregularized objective function (1).",
        "In Section 2.1 we consider fully connected neural networks, and our main result (Theorem 2.1) shows that gradient flow automatically balances the incoming and outgoing weights at every neuron.",
        "0. for a neural network with arbitrary sparsity pattern and weight sharing structure, gradient flow still balances the magnitudes of all layers.",
        "We constrain ourselves to the asymmetric matrix factorization problem and analyze the gradient descent algorithm with random initialization.",
        "Our main theorem below says that if we use a random small initialization pU0, V0q, and set step sizes \u2318t to be appropriately small, gradient descent",
        "This is the first result showing that gradient descent with random initialization directly solves the un-regularized asymmetric matrix factorization problem (8).",
        "Our main finding is that with constant step size, the norms of two layers are always within a constant factor of each other, and we utilize this property to prove the linear convergence of GD to a global minimum.",
        "Notice that our theorems in Section 2 hold for gradient flow but in practice we can only choose a positive step size, so we cannot hope the difference between the squared Frobenius norms to remain exactly the same but can only hope to observe that the differences remain small.",
        "We show that gradient flow automatically balances the magnitudes of all layers in a deep neural network with homogeneous activations.",
        "For the concrete model of asymmetric matrix factorization, we further use the balancedness property to show that gradient descent converges to global minimum."
    ],
    "headline": "We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, Rectified Linear Unit or Leaky Rectified Linear Unit activation",
    "reference_links": [
        {
            "id": "Absil_et+al_2005_a",
            "entry": "Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of descent methods for analytic cost functions. SIAM Journal on Optimization, 16(2):531\u2013547, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Absil%2C%20Pierre-Antoine%20Mahony%2C%20Robert%20Andrews%2C%20Benjamin%20Convergence%20of%20the%20iterates%20of%20descent%20methods%20for%20analytic%20cost%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Absil%2C%20Pierre-Antoine%20Mahony%2C%20Robert%20Andrews%2C%20Benjamin%20Convergence%20of%20the%20iterates%20of%20descent%20methods%20for%20analytic%20cost%20functions%202005"
        },
        {
            "id": "Arora_et+al_2018_a",
            "entry": "Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06509"
        },
        {
            "id": "Bartlett_et+al_2018_a",
            "entry": "Peter L Bartlett, David P Helmbold, and Philip M Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. arXiv preprint arXiv:1802.06093, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06093"
        },
        {
            "id": "Brutzkus_2017_a",
            "entry": "Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07966"
        },
        {
            "id": "Brutzkus_et+al_2017_b",
            "entry": "Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10174"
        },
        {
            "id": "Choromanska_et+al_2015_a",
            "entry": "Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20Gerard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "Clarke_et+al_2008_a",
            "entry": "Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. Nonsmooth analysis and control theory, volume 178. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarke%2C%20Francis%20H.%20Ledyaev%2C%20Yuri%20S.%20Stern%2C%20Ronald%20J.%20Wolenski%2C%20Peter%20R.%20Nonsmooth%20analysis%20and%20control%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarke%2C%20Francis%20H.%20Ledyaev%2C%20Yuri%20S.%20Stern%2C%20Ronald%20J.%20Wolenski%2C%20Peter%20R.%20Nonsmooth%20analysis%20and%20control%202008"
        },
        {
            "id": "Davis_et+al_2018_a",
            "entry": "Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method converges on tame functions. arXiv preprint arXiv:1804.07795, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07795"
        },
        {
            "id": "Drusvyatskiy_et+al_2015_a",
            "entry": "Dmitriy Drusvyatskiy, Alexander D Ioffe, and Adrian S Lewis. Curves of descent. SIAM Journal on Control and Optimization, 53(1):114\u2013138, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drusvyatskiy%2C%20Dmitriy%20Ioffe%2C%20Alexander%20D.%20Lewis%2C%20Adrian%20S.%20Curves%20of%20descent%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drusvyatskiy%2C%20Dmitriy%20Ioffe%2C%20Alexander%20D.%20Lewis%2C%20Adrian%20S.%20Curves%20of%20descent%202015"
        },
        {
            "id": "Du_2018_a",
            "entry": "Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01206"
        },
        {
            "id": "Du_et+al_2017_a",
            "entry": "Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06129"
        },
        {
            "id": "Du_et+al_2017_b",
            "entry": "Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don\u2019t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00779"
        },
        {
            "id": "Freeman_2016_a",
            "entry": "C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. arXiv preprint arXiv:1611.01540, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01540"
        },
        {
            "id": "Ge_et+al_2015_a",
            "entry": "Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pages 797\u2013842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20pointsonline%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Huang%2C%20Furong%20Jin%2C%20Chi%20Yuan%2C%20Yang%20Escaping%20from%20saddle%20pointsonline%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "Ge_et+al_2017_a",
            "entry": "Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proceedings of the 34th International Conference on Machine Learning, pages 1233\u20131242, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20Rong%20Jin%2C%20Chi%20Zheng%2C%20Yi%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20Rong%20Jin%2C%20Chi%20Zheng%2C%20Yi%20No%20spurious%20local%20minima%20in%20nonconvex%20low%20rank%20problems%3A%20A%20unified%20geometric%20analysis%202017"
        },
        {
            "id": "Ge_et+al_0000_a",
            "entry": "Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "Glorot_2010_a",
            "entry": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Glorot%2C%20Xavier%20Bengio%2C%20Yoshua%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%202010"
        },
        {
            "id": "Haeffele_2015_a",
            "entry": "Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.07540"
        },
        {
            "id": "Hardt_2016_a",
            "entry": "Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04231"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "Kawaguchi_2016_a",
            "entry": "Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information Processing Systems, pages 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "Lee_et+al_2016_a",
            "entry": "Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pages 1246\u20131257, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20Jason%20D.%20Simchowitz%2C%20Max%20Jordan%2C%20Michael%20I.%20Recht%2C%20Benjamin%20Gradient%20descent%20only%20converges%20to%20minimizers%202016"
        },
        {
            "id": "Li_2017_a",
            "entry": "Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. arXiv preprint arXiv:1705.09886, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09886"
        },
        {
            "id": "Ma_et+al_2017_a",
            "entry": "Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of sgd in modern over-parametrized learning. arXiv preprint arXiv:1712.06559, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06559"
        },
        {
            "id": "Neyshabur_et+al_2015_a",
            "entry": "Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pages 2422\u20132430, 2015a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Behnam%20Neyshabur%20Ruslan%20R%20Salakhutdinov%20and%20Nati%20Srebro%20PathSGD%20Pathnormalized%20optimization%20in%20deep%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2024222430%202015a",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Behnam%20Neyshabur%20Ruslan%20R%20Salakhutdinov%20and%20Nati%20Srebro%20PathSGD%20Pathnormalized%20optimization%20in%20deep%20neural%20networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2024222430%202015a"
        },
        {
            "id": "Neyshabur_et+al_0000_a",
            "entry": "Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path normalization in neural networks. arXiv preprint arXiv:1511.06747, 2015b.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06747"
        },
        {
            "id": "Nguyen_0000_a",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017a.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08045"
        },
        {
            "id": "Nguyen_0000_b",
            "entry": "Quynh Nguyen and Matthias Hein. The loss surface and expressivity of deep convolutional neural networks. arXiv preprint arXiv:1710.10928, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1710.10928"
        },
        {
            "id": "Panageas_2016_a",
            "entry": "Ioannis Panageas and Georgios Piliouras. Gradient descent only converges to minimizers: Nonisolated critical points and invariant regions. arXiv preprint arXiv:1605.00405, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.00405"
        },
        {
            "id": "Safran_2016_a",
            "entry": "Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pages 774\u2013782, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Safran%2C%20Itay%20Shamir%2C%20Ohad%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Safran%2C%20Itay%20Shamir%2C%20Ohad%20On%20the%20quality%20of%20the%20initial%20basin%20in%20overspecified%20neural%20networks%202016"
        },
        {
            "id": "Safran_2017_a",
            "entry": "Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08968"
        },
        {
            "id": "Shamir_2018_a",
            "entry": "O. Shamir. Are resnets provably better than linear predictors? arXiv preprint arXiv:1804.06739, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06739"
        },
        {
            "id": "Su_et+al_2014_a",
            "entry": "Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterovs accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pages 2510\u20132518, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20Weijie%20Boyd%2C%20Stephen%20Candes%2C%20Emmanuel%20A%20differential%20equation%20for%20modeling%20nesterovs%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Su%2C%20Weijie%20Boyd%2C%20Stephen%20Candes%2C%20Emmanuel%20A%20differential%20equation%20for%20modeling%20nesterovs%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014"
        },
        {
            "id": "Tian_2017_a",
            "entry": "Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00560"
        },
        {
            "id": "Tu_et+al_2015_a",
            "entry": "Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear matrix equations via procrustes flow. arXiv preprint arXiv:1507.03566, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.03566"
        },
        {
            "id": "Vidal_et+al_2017_a",
            "entry": "Rene Vidal, Joan Bruna, Raja Giryes, and Stefano Soatto. Mathematics of deep learning. arXiv preprint arXiv:1712.04741, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04741"
        },
        {
            "id": "Zhang_et+al_2018_a",
            "entry": "Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and Ali Jadbabaie. Direct runge-kutta discretization achieves acceleration. arXiv preprint arXiv:1805.00521, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00521"
        },
        {
            "id": "Zhong_et+al_2017_a",
            "entry": "Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        },
        {
            "id": "Zhou_2017_a",
            "entry": "Pan Zhou and Jiashi Feng. The landscape of deep learning algorithms. arXiv preprint arXiv:1705.07038, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07038"
        }
    ]
}
