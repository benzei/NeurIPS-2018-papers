{
    "filename": "7713-pg-ts-improved-thompson-sampling-for-logistic-contextual-bandits.pdf",
    "metadata": {
        "title": "PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits",
        "author": "Bianca Dumitrascu, Karen Feng, Barbara Engelhardt",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7713-pg-ts-improved-thompson-sampling-for-logistic-contextual-bandits.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with P\u00f3lya-Gamma distributed augmentation variables, we propose an improved version of Thompson Sampling, a Bayesian formulation of contextual bandits with near-optimal performance. Our approach, P\u00f3lya-Gamma augmented Thompson Sampling (PG-TS), achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms, quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of P\u00f3lyaGamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits."
    },
    "keywords": [
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "multi armed bandit",
            "url": "https://en.wikipedia.org/wiki/multi_armed_bandit"
        },
        {
            "term": "click-through rate",
            "url": "https://en.wikipedia.org/wiki/click-through_rate"
        },
        {
            "term": "thompson sampling",
            "url": "https://en.wikipedia.org/wiki/thompson_sampling"
        },
        {
            "term": "gibbs sampler",
            "url": "https://en.wikipedia.org/wiki/gibbs_sampler"
        }
    ],
    "highlights": [
        "In a contextual bandit algorithm, a learner is given a choice among K actions or arms, for which contexts are available as d-dimensional feature vectors, across T sequential rounds",
        "We evaluate and compare our P\u00f3lya-Gamma augmented Thompson sampling method with Laplace-Thompson Sampling",
        "Laplace-Thompson Sampling has been shown to outperform its Upper Confidence Bound algorithms competitors in all settings considered here [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "We evaluate our algorithm in three scenarios: simulated data sets with parameters sampled from Gaussian and mixed Gaussian distributions, a toy data set based on the Forest Cover Type data set from the UCI repository [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and an offline evaluation method for bandit algorithms that relies on real-world log data [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>].\n4.1",
        "We introduced P\u00f3lya-Gamma augmented Thompson sampling, a fully Bayesian algorithm based on the P\u00f3lya-Gamma augmentation scheme for contextual bandits with logistic rewards",
        "We showed through extensive evaluation in both simulated and real-world data that our approach offers improved empirical performance while maintaining comparable computational costs by leveraging the simplicity of the Thompson sampling framework"
    ],
    "key_statements": [
        "In a contextual bandit algorithm, a learner is given a choice among K actions or arms, for which contexts are available as d-dimensional feature vectors, across T sequential rounds",
        "Recent work suggests that a double sampling approach via MCMC can improve Thompson Sampling [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>]. This approach provides MCMC schemes for bandits with binary and Gaussian rewards, but these algorithms do not generalize to the logistic contextual bandit",
        "We propose P\u00f3lya-Gamma augmented Thompson sampling (PG-Thompson Sampling), a fully Bayesian alternative to Laplace-Thompson Sampling",
        "P\u00f3lya-Gamma augmented Thompson sampling, uses the PG augmentation scheme to represent the binomial distributions of the sequential rewards in terms of latent variables with Gaussian distributions to perform tractable Bayesian logistic regression in a Thompson sampling setting",
        "We evaluate and compare our P\u00f3lya-Gamma augmented Thompson sampling method with Laplace-Thompson Sampling",
        "Laplace-Thompson Sampling has been shown to outperform its Upper Confidence Bound algorithms competitors in all settings considered here [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "We evaluate our algorithm in three scenarios: simulated data sets with parameters sampled from Gaussian and mixed Gaussian distributions, a toy data set based on the Forest Cover Type data set from the UCI repository [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and an offline evaluation method for bandit algorithms that relies on real-world log data [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>].\n4.1",
        "We evaluated the performance of P\u00f3lya-Gamma augmented Thompson sampling in the context of news article recommendations on the public benchmark Yahoo! Today Module data through an unbiased offline evaluation protocol [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "We introduced P\u00f3lya-Gamma augmented Thompson sampling, a fully Bayesian algorithm based on the P\u00f3lya-Gamma augmentation scheme for contextual bandits with logistic rewards",
        "We showed through extensive evaluation in both simulated and real-world data that our approach offers improved empirical performance while maintaining comparable computational costs by leveraging the simplicity of the Thompson sampling framework",
        "We further envision a generalization of this approach to sampling in bandit problems where additional structure is imposed on the contexts; for example, settings where arm contexts are sampled from dynamic linear topic models [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], or settings in which social network information is available for users and contexts [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>]"
    ],
    "summary": [
        "In a contextual bandit algorithm, a learner is given a choice among K actions or arms, for which contexts are available as d-dimensional feature vectors, across T sequential rounds.",
        "This approach provides MCMC schemes for bandits with binary and Gaussian rewards, but these algorithms do not generalize to the logistic contextual bandit.",
        "TS for the contextual bandit is broadly defined in Bayesian terms, where a prior distribution p(\u03b8) over the parameter \u03b8 is updated iteratively using a set of historical observations Dt\u22121 =",
        "Where Atmax is the set of arms with maximum rewards at step t if the true parameter were \u03b8t.",
        "PG-TS, uses the PG augmentation scheme to represent the binomial distributions of the sequential rewards in terms of latent variables with Gaussian distributions to perform tractable Bayesian logistic regression in a Thompson sampling setting.",
        "We evaluate our algorithm in three scenarios: simulated data sets with parameters sampled from Gaussian and mixed Gaussian distributions, a toy data set based on the Forest Cover Type data set from the UCI repository [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and an offline evaluation method for bandit algorithms that relies on real-world log data [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>].",
        "We generated a simulated data set with 100 arms and 10 features per context across 1, 000 trials.",
        "Larger M led to lower regret, as algorithms on the simulated data set with Gaussian \u03b8\u2217 over 100 runs with 1, 000 trials.",
        "We explored a prior misspecification scenario, where true parameter \u03b8\u2217 is sampled from a four-component Gaussian mixture model, as opposed to the Gaussian distribution assumed by both algorithms.",
        "We introduced PG-TS, a fully Bayesian algorithm based on the P\u00f3lya-Gamma augmentation scheme for contextual bandits with logistic rewards.",
        "We showed through extensive evaluation in both simulated and real-world data that our approach offers improved empirical performance while maintaining comparable computational costs by leveraging the simplicity of the Thompson sampling framework.",
        "We further envision a generalization of this approach to sampling in bandit problems where additional structure is imposed on the contexts; for example, settings where arm contexts are sampled from dynamic linear topic models [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], or settings in which social network information is available for users and contexts [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>].",
        "The state transition probabilities are multinomial distributions; our online P\u00f3lya-Gamma Gibbs sampling procedure can be extended to approximate the emerging intractable posteriors."
    ],
    "headline": "We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] N. Abe and A. Nakamura. Learning to optimally schedule internet banner advertisements. In ICML, volume 99, pages 12\u201321, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abe%2C%20N.%20Nakamura%2C%20A.%20Learning%20to%20optimally%20schedule%20internet%20banner%20advertisements%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abe%2C%20N.%20Nakamura%2C%20A.%20Learning%20to%20optimally%20schedule%20internet%20banner%20advertisements%201999"
        },
        {
            "id": "2",
            "entry": "[2] M. Abeille and A. Lazaric. Linear Thompson Sampling Revisited. In AISTATS 2017-20th International Conference on Artificial Intelligence and Statistics, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abeille%2C%20M.%20Lazaric%2C%20A.%20Linear%20Thompson%20Sampling%20Revisited%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abeille%2C%20M.%20Lazaric%2C%20A.%20Linear%20Thompson%20Sampling%20Revisited%202017"
        },
        {
            "id": "3",
            "entry": "[3] R. Agrawal. Sample mean based index policies by o (log n) regret for the multi-armed bandit problem. Advances in Applied Probability, 27(4):1054\u20131078, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20R.%20Sample%20mean%20based%20index%20policies%20by%20o%20%28log%20n%29%20regret%20for%20the%20multi-armed%20bandit%20problem%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20R.%20Sample%20mean%20based%20index%20policies%20by%20o%20%28log%20n%29%20regret%20for%20the%20multi-armed%20bandit%20problem%201995"
        },
        {
            "id": "4",
            "entry": "[4] S. Agrawal and N. Goyal. Thompson Sampling for contextual bandits with linear payoffs. In International Conference on Machine Learning, pages 127\u2013135, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20S.%20Goyal%2C%20N.%20Thompson%20Sampling%20for%20contextual%20bandits%20with%20linear%20payoffs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20S.%20Goyal%2C%20N.%20Thompson%20Sampling%20for%20contextual%20bandits%20with%20linear%20payoffs%202013"
        },
        {
            "id": "5",
            "entry": "[5] S. Agrawal and N. Goyal. Near-Optimal Regret Bounds for Thompson Sampling. Journal of the ACM (JACM), 64(5):30, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20S.%20Goyal%2C%20N.%20Near-Optimal%20Regret%20Bounds%20for%20Thompson%20Sampling%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20S.%20Goyal%2C%20N.%20Near-Optimal%20Regret%20Bounds%20for%20Thompson%20Sampling%202017"
        },
        {
            "id": "6",
            "entry": "[6] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235\u2013256, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Fischer%2C%20P.%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Fischer%2C%20P.%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "7",
            "entry": "[7] R. F. Barber, M. Drton, and K. M. Tan. Laplace approximation in high-dimensional Bayesian regression. In Statistical Analysis for High-Dimensional Data, pages 15\u201336.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barber%2C%20R.F.%20Drton%2C%20M.%20Tan%2C%20K.M.%20Laplace%20approximation%20in%20high-dimensional%20Bayesian%20regression.%20In%20Statistical%20Analysis%20for%20High-Dimensional%20Data"
        },
        {
            "id": "8",
            "entry": "[8] S. D. Bay, D. Kibler, M. J. Pazzani, and P. Smyth. The uci kdd archive of large data sets for data mining research and experimentation. ACM SIGKDD Explorations Newsletter, 2(2):81\u201385, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bay%2C%20S.D.%20Kibler%2C%20D.%20Pazzani%2C%20M.J.%20Smyth%2C%20P.%20The%20uci%20kdd%20archive%20of%20large%20data%20sets%20for%20data%20mining%20research%20and%20experimentation%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bay%2C%20S.D.%20Kibler%2C%20D.%20Pazzani%2C%20M.J.%20Smyth%2C%20P.%20The%20uci%20kdd%20archive%20of%20large%20data%20sets%20for%20data%20mining%20research%20and%20experimentation%202000"
        },
        {
            "id": "9",
            "entry": "[9] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Advances in neural information processing systems, pages 2249\u20132257, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chapelle%2C%20O.%20Li%2C%20L.%20An%20empirical%20evaluation%20of%20Thompson%20sampling%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chapelle%2C%20O.%20Li%2C%20L.%20An%20empirical%20evaluation%20of%20Thompson%20sampling%202011"
        },
        {
            "id": "10",
            "entry": "[10] H. M. Choi, J. P. Hobert, et al. The Polya-Gamma Gibbs sampler for Bayesian logistic regression is uniformly ergodic. Electronic Journal of Statistics, 7:2054\u20132064, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choi%2C%20H.M.%20Hobert%2C%20J.P.%20The%20Polya-Gamma%20Gibbs%20sampler%20for%20Bayesian%20logistic%20regression%20is%20uniformly%20ergodic%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choi%2C%20H.M.%20Hobert%2C%20J.P.%20The%20Polya-Gamma%20Gibbs%20sampler%20for%20Bayesian%20logistic%20regression%20is%20uniformly%20ergodic%202013"
        },
        {
            "id": "11",
            "entry": "[11] W. Chu, S. taek Park, T. Beaupre, N. Motgi, A. Phadke, S. Chakraborty, and J. Zachariah. A case study of behavior-driven conjoint analysis on Yahoo! Front Page Today module. In Proc. of KDD, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chu%2C%20W.%20taek%20Park%2C%20S.%20Beaupre%2C%20T.%20Motgi%2C%20N.%20A%20case%20study%20of%20behavior-driven%20conjoint%20analysis%20on%20Yahoo%21%20Front%20Page%20Today%20module%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chu%2C%20W.%20taek%20Park%2C%20S.%20Beaupre%2C%20T.%20Motgi%2C%20N.%20A%20case%20study%20of%20behavior-driven%20conjoint%20analysis%20on%20Yahoo%21%20Front%20Page%20Today%20module%202009"
        },
        {
            "id": "13",
            "entry": "[13] L. Devroye. On exact simulation algorithms for some distributions related to Jacobi theta functions. Statistics & Probability Letters, 79(21):2251\u20132259, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devroye%2C%20L.%20On%20exact%20simulation%20algorithms%20for%20some%20distributions%20related%20to%20Jacobi%20theta%20functions%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devroye%2C%20L.%20On%20exact%20simulation%20algorithms%20for%20some%20distributions%20related%20to%20Jacobi%20theta%20functions%202009"
        },
        {
            "id": "14",
            "entry": "[14] M. O. Duff. Design for an optimal probe. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 131\u2013138, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duff%2C%20M.O.%20Design%20for%20an%20optimal%20probe%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duff%2C%20M.O.%20Design%20for%20an%20optimal%20probe%202003"
        },
        {
            "id": "15",
            "entry": "[15] S. Filippi, O. Cappe, A. Garivier, and C. Szepesv\u00e1ri. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586\u2013594, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Filippi%2C%20S.%20Cappe%2C%20O.%20Garivier%2C%20A.%20Szepesv%C3%A1ri%2C%20C.%20Parametric%20bandits%3A%20The%20generalized%20linear%20case%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Filippi%2C%20S.%20Cappe%2C%20O.%20Garivier%2C%20A.%20Szepesv%C3%A1ri%2C%20C.%20Parametric%20bandits%3A%20The%20generalized%20linear%20case%202010"
        },
        {
            "id": "16",
            "entry": "[16] C. Gentile, S. Li, and G. Zappella. Online clustering of bandits. In International Conference on Machine Learning, pages 757\u2013765, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gentile%2C%20C.%20Li%2C%20S.%20Zappella%2C%20G.%20Online%20clustering%20of%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gentile%2C%20C.%20Li%2C%20S.%20Zappella%2C%20G.%20Online%20clustering%20of%20bandits%202014"
        },
        {
            "id": "17",
            "entry": "[17] C. Glynn, S. T. Tokdar, D. L. Banks, and B. Howard. Bayesian Analysis of Dynamic Linear Topic Models. arXiv preprint arXiv:1511.03947, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.03947"
        },
        {
            "id": "18",
            "entry": "[18] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169\u2013192, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Agarwal%2C%20A.%20Kale%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Agarwal%2C%20A.%20Kale%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007"
        },
        {
            "id": "19",
            "entry": "[19] E. Hazan, T. Koren, and K. Y. Levy. Logistic regression: Tight bounds for stochastic and online optimization. In Conference on Learning Theory, pages 197\u2013209, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20E.%20Koren%2C%20T.%20Levy%2C%20K.Y.%20Logistic%20regression%3A%20Tight%20bounds%20for%20stochastic%20and%20online%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20E.%20Koren%2C%20T.%20Levy%2C%20K.Y.%20Logistic%20regression%3A%20Tight%20bounds%20for%20stochastic%20and%20online%20optimization%202014"
        },
        {
            "id": "20",
            "entry": "[20] K.-S. Jun, A. Bhargava, R. Nowak, and R. Willett. Scalable Generalized Linear Bandits: Online Computation and Hashing. arXiv preprint arXiv:1706.00136, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00136"
        },
        {
            "id": "21",
            "entry": "[21] J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in neural information processing systems, pages 817\u2013824, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Langford%2C%20J.%20Zhang%2C%20T.%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Langford%2C%20J.%20Zhang%2C%20T.%20The%20epoch-greedy%20algorithm%20for%20multi-armed%20bandits%20with%20side%20information%202008"
        },
        {
            "id": "22",
            "entry": "[22] L. Li, W. Chu, J. Langford, and R. E. Schapire. A Contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Schapire%2C%20R.E.%20A%20Contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Schapire%2C%20R.E.%20A%20Contextual-bandit%20approach%20to%20personalized%20news%20article%20recommendation%202010"
        },
        {
            "id": "23",
            "entry": "[23] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased offline evaluation of contextual-banditbased news article recommendation algorithms. In Proceedings of the fourth ACM International Conference on Web search and Data Mining, pages 297\u2013306. ACM, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Wang%2C%20X.%20Unbiased%20offline%20evaluation%20of%20contextual-banditbased%20news%20article%20recommendation%20algorithms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20L.%20Chu%2C%20W.%20Langford%2C%20J.%20Wang%2C%20X.%20Unbiased%20offline%20evaluation%20of%20contextual-banditbased%20news%20article%20recommendation%20algorithms%202011"
        },
        {
            "id": "24",
            "entry": "[24] S. Linderman, M. Johnson, and R. P. Adams. Dependent multinomial models made easy: Stickbreaking with the p\u00f3lya-gamma augmentation. In Advances in Neural Information Processing Systems, pages 3456\u20133464, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Linderman%2C%20S.%20Johnson%2C%20M.%20Adams%2C%20R.P.%20Dependent%20multinomial%20models%20made%20easy%3A%20Stickbreaking%20with%20the%20p%C3%B3lya-gamma%20augmentation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Linderman%2C%20S.%20Johnson%2C%20M.%20Adams%2C%20R.P.%20Dependent%20multinomial%20models%20made%20easy%3A%20Stickbreaking%20with%20the%20p%C3%B3lya-gamma%20augmentation%202015"
        },
        {
            "id": "25",
            "entry": "[25] H. B. McMahan and M. Streeter. Open problem: Better bounds for online logistic regression. In Conference on Learning Theory, pages 44\u20131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20H.B.%20Streeter%2C%20M.%20Open%20problem%3A%20Better%20bounds%20for%20online%20logistic%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20H.B.%20Streeter%2C%20M.%20Open%20problem%3A%20Better%20bounds%20for%20online%20logistic%20regression%202012"
        },
        {
            "id": "26",
            "entry": "[26] I. Osband and B. Van Roy. Bootstrapped Thompson sampling and deep exploration. arXiv preprint arXiv:1507.00300, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00300"
        },
        {
            "id": "27",
            "entry": "[27] N. G. Polson, J. G. Scott, and J. Windle. Bayesian inference for logistic models using P\u00f3lyaGamma latent variables. Journal of the American statistical Association, 108(504):1339\u20131349, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polson%2C%20N.G.%20Scott%2C%20J.G.%20Windle%2C%20J.%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B3lyaGamma%20latent%20variables%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polson%2C%20N.G.%20Scott%2C%20J.G.%20Windle%2C%20J.%20Bayesian%20inference%20for%20logistic%20models%20using%20P%C3%B3lyaGamma%20latent%20variables%202013"
        },
        {
            "id": "28",
            "entry": "[28] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221\u20131243, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russo%2C%20D.%20Roy%2C%20B.Van%20Learning%20to%20optimize%20via%20posterior%20sampling%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russo%2C%20D.%20Roy%2C%20B.Van%20Learning%20to%20optimize%20via%20posterior%20sampling%202014"
        },
        {
            "id": "29",
            "entry": "[29] D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. The Journal of Machine Learning Research, 17(1):2442\u20132471, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Russo%2C%20D.%20Roy%2C%20B.Van%20An%20information-theoretic%20analysis%20of%20Thompson%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Russo%2C%20D.%20Roy%2C%20B.Van%20An%20information-theoretic%20analysis%20of%20Thompson%20sampling%202016"
        },
        {
            "id": "30",
            "entry": "[30] D. Russo, B. Van Roy, A. Kazerouni, and I. Osband. A Tutorial on Thompson Sampling. arXiv preprint arXiv:1707.02038, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02038"
        },
        {
            "id": "31",
            "entry": "[31] J. Scott and J. W. Pillow. Fully Bayesian inference for neural models with negative-binomial spiking. In Advances in Neural Information Processing Systems, pages 1898\u20131906, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scott%2C%20J.%20Pillow%2C%20J.W.%20Fully%20Bayesian%20inference%20for%20neural%20models%20with%20negative-binomial%20spiking%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scott%2C%20J.%20Pillow%2C%20J.W.%20Fully%20Bayesian%20inference%20for%20neural%20models%20with%20negative-binomial%20spiking%202012"
        },
        {
            "id": "32",
            "entry": "[32] M. Strens. A Bayesian framework for reinforcement learning. In ICML, pages 943\u2013950, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Strens%2C%20M.%20A%20Bayesian%20framework%20for%20reinforcement%20learning%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Strens%2C%20M.%20A%20Bayesian%20framework%20for%20reinforcement%20learning%202000"
        },
        {
            "id": "33",
            "entry": "[33] A. Tewari and S. A. Murphy. From ads to interventions: Contextual bandits in mobile health. In Mobile Health, pages 495\u2013517.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tewari%2C%20A.%20Murphy%2C%20S.A.%20From%20ads%20to%20interventions%3A%20Contextual%20bandits%20in%20mobile%20health",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tewari%2C%20A.%20Murphy%2C%20S.A.%20From%20ads%20to%20interventions%3A%20Contextual%20bandits%20in%20mobile%20health"
        },
        {
            "id": "34",
            "entry": "[34] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thompson%2C%20W.R.%20On%20the%20likelihood%20that%20one%20unknown%20probability%20exceeds%20another%20in%20view%20of%20the%20evidence%20of%20two%20samples%201933"
        },
        {
            "id": "35",
            "entry": "[35] I. Urteaga and C. H. Wiggins. Bayesian bandits: balancing the exploration-exploitation tradeoff via double sampling. arXiv preprint arXiv:1709.03162, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.03162"
        },
        {
            "id": "36",
            "entry": "[36] J. Windle, N. G. Polson, and J. G. Scott. Sampling Polya-Gamma random variates: alternate and approximate techniques. arXiv preprint arXiv:1405.0506, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1405.0506"
        },
        {
            "id": "37",
            "entry": "[37] M. Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the American Statistical Association, 74(368):799\u2013806, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodroofe%2C%20M.%20A%20one-armed%20bandit%20problem%20with%20a%20concomitant%20variable%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodroofe%2C%20M.%20A%20one-armed%20bandit%20problem%20with%20a%20concomitant%20variable%201979"
        },
        {
            "id": "38",
            "entry": "[38] M. Zhou, L. Li, D. Dunson, and L. Carin. Lognormal and gamma mixed negative binomial regression. In Proceedings of the 29th International Conference on Machine Learning, volume 2012, page 1343. NIH Public Access, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhou%2C%20M.%20Li%2C%20L.%20Dunson%2C%20D.%20Carin%2C%20L.%20Lognormal%20and%20gamma%20mixed%20negative%20binomial%20regression%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhou%2C%20M.%20Li%2C%20L.%20Dunson%2C%20D.%20Carin%2C%20L.%20Lognormal%20and%20gamma%20mixed%20negative%20binomial%20regression%202012"
        }
    ]
}
