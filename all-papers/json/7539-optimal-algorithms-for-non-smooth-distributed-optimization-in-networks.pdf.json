{
    "filename": "7539-optimal-algorithms-for-non-smooth-distributed-optimization-in-networks.pdf",
    "metadata": {
        "title": "Optimal Algorithms for Non-Smooth Distributed Optimization in Networks",
        "author": "Kevin Scaman1 Francis Bach2 S\u00e9bastien Bubeck3 Yin Tat Lee3,4 Laurent Massouli\u00e92,5 1 Huawei Noah\u2019s Ark Lab, 2 INRIA, Ecole Normale Sup\u00e9rieure, PSL Research University, 3 Microsoft Research, 4 University of Washington, 5 MSR-INRIA Joint Centre",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7539-optimal-algorithms-for-non-smooth-distributed-optimization-in-networks.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function, and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is t\u221ahat, for non-smooth functions, while the dominant term of the error is in O(1/ t), the structure of the communication network only impacts a second-order term in O(1/t), where t is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption, we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function, and show that DRS is within a d1/4 multiplicative factor of the optimal convergence rate, where d is the underlying dimension."
    },
    "keywords": [
        {
            "term": "multiplicative factor",
            "url": "https://en.wikipedia.org/wiki/multiplicative_factor"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "objective function",
            "url": "https://en.wikipedia.org/wiki/objective_function"
        },
        {
            "term": "lipschitz continuity",
            "url": "https://en.wikipedia.org/wiki/lipschitz_continuity"
        }
    ],
    "highlights": [
        "Distributed optimization finds many applications in machine learning, for example when the dataset is large and training is achieved using a cluster of computing units",
        "Under the more challenging global regularity assumption, we show in Section 3 that distributing the simple smoothing approach introduced in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] yields fast convergence rates with respect to communication",
        "We show that the distributed randomized smoothing algorithm converges to the optimal parameter under the global regularity assumption",
        "We provide optimal convergence rates for non-smooth and convex distributed optimization in two settings: Lipschitz continuity of the global objective function, and Lipschitz continuity of local individual functions",
        "Under the local regularity assumption, we provide optimal convergence rates that depend on the 2-average of the local Lipschitz constants and the eigengap of the gossip matrix",
        "Under the global regularity assumption, we provide a lower complexity bound that depends on the Lipschitz constant of the objective function, as well as a distributed version of the smoothing approach of [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and show that this algorithm is within a d1/4 multiplicative factor of the optimal convergence rate"
    ],
    "key_statements": [
        "Distributed optimization finds many applications in machine learning, for example when the dataset is large and training is achieved using a cluster of computing units",
        "Under the local regularity assumption, we provide in Section 4 matching upper and lower bounds of complexity in a decentralized setting in which communication is performed using the gossip algorithm [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>]",
        "Under the more challenging global regularity assumption, we show in Section 3 that distributing the simple smoothing approach introduced in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] yields fast convergence rates with respect to communication",
        "Our analysis differs from the smooth and strongly-convex setting in two major aspects: (1) the na\u00efve master/slave distributed algorithm is in this case not optimal, and (2) the convergence rates differ between communication and local computations",
        "For the global regularity assumption, we only provide an algorithm with a d1/4 competitive ratio, where d is the dimension of the problem",
        "We show that the distributed randomized smoothing algorithm converges to the optimal parameter under the global regularity assumption",
        "Assuming that the dimension d is large compared to the characteristic values of the problem (a standard set-up for lower bounds in non-smooth optimization [20, Theorem 3.2.1]), Theorem 2 implies that, under the global regularity assumption (A1), the time to reach a precision \u03b5 > 0 with any black-box procedure is lower bounded by",
        "To the smooth and strongly-convex case of [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], the lower bound on the optimal convergence rate is obtained by replacing the diameter of the network with 1/ \u03b3(W ), where \u03b3(W ) = \u03bbn\u22121(W )/\u03bb1(W ) is the ratio between smallest non-zero and largest eigenvalues of W , known as the normalized eigengap",
        "Assuming that the dimension d is large compared to the characteristic values of the problem, Theorem 3 implies that, under the local regularity assumption (A2) and for a gossip matrix W with eigengap \u03b3(W ), the time to reach a precision \u03b5 > 0 with any decentralized black-box procedure is lower-bounded by",
        "We provide optimal convergence rates for non-smooth and convex distributed optimization in two settings: Lipschitz continuity of the global objective function, and Lipschitz continuity of local individual functions",
        "Under the local regularity assumption, we provide optimal convergence rates that depend on the 2-average of the local Lipschitz constants and the eigengap of the gossip matrix",
        "Under the global regularity assumption, we provide a lower complexity bound that depends on the Lipschitz constant of the objective function, as well as a distributed version of the smoothing approach of [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and show that this algorithm is within a d1/4 multiplicative factor of the optimal convergence rate"
    ],
    "summary": [
        "Distributed optimization finds many applications in machine learning, for example when the dataset is large and training is achieved using a cluster of computing units.",
        "Under the local regularity assumption, we provide in Section 4 matching upper and lower bounds of complexity in a decentralized setting in which communication is performed using the gossip algorithm [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>].",
        "Our analysis differs from the smooth and strongly-convex setting in two major aspects: (1) the na\u00efve master/slave distributed algorithm is in this case not optimal, and (2) the convergence rates differ between communication and local computations.",
        "Assuming that the dimension d is large compared to the characteristic values of the problem (a standard set-up for lower bounds in non-smooth optimization [20, Theorem 3.2.1]), Theorem 2 implies that, under the global regularity assumption (A1), the time to reach a precision \u03b5 > 0 with any black-box procedure is lower bounded by",
        "The proof of Theorem 2 relies on the use of two objective functions: first, the standard worst case function used for single machine convex optimization is used to obtain a lower bound on the local computation time of individual machines.",
        "To the smooth and strongly-convex case of [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>], the lower bound on the optimal convergence rate is obtained by replacing the diameter of the network with 1/ \u03b3(W ), where \u03b3(W ) = \u03bbn\u22121(W )/\u03bb1(W ) is the ratio between smallest non-zero and largest eigenvalues of W , known as the normalized eigengap.",
        "Assuming that the dimension d is large compared to the characteristic values of the problem, Theorem 3 implies that, under the local regularity assumption (A2) and for a gossip matrix W with eigengap \u03b3(W ), the time to reach a precision \u03b5 > 0 with any decentralized black-box procedure is lower-bounded by",
        "We provide optimal convergence rates for non-smooth and convex distributed optimization in two settings: Lipschitz continuity of the global objective function, and Lipschitz continuity of local individual functions.",
        "Under the local regularity assumption, we provide optimal convergence rates that depend on the 2-average of the local Lipschitz constants and the eigengap of the gossip matrix.",
        "Under the global regularity assumption, we provide a lower complexity bound that depends on the Lipschitz constant of the objective function, as well as a distributed version of the smoothing approach of [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and show that this algorithm is within a d1/4 multiplicative factor of the optimal convergence rate.",
        "The analysis presented in this paper allows several natural extensions, including time-varying communication networks, asynchronous algorithms, stochastic settings, and an analysis of unequal node compute speeds going beyond Remark 5.",
        "We provide the first optimal decentralized algorithm, called multi-step primal-dual (MSPD).<br/><br/>Under the global regularity assumption, we provide a lower complexity bound that depends on the Lipschitz constant of the objective function, as well as a distributed version of the smoothing approach of [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>] and show that this algorithm is within a d1/4 multiplicative factor of the optimal convergence rate"
    ],
    "headline": "We investigate this problem under two regularity assumptions:  the Lipschitz continuity of the global objective function, and  the Lipschitz continuity of local individual functions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48\u201361, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20Angelia%20Ozdaglar%2C%20Asuman%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20Angelia%20Ozdaglar%2C%20Asuman%20Distributed%20subgradient%20methods%20for%20multi-agent%20optimization%202009"
        },
        {
            "id": "2",
            "entry": "[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1\u2013122, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Parikh%2C%20Neal%20Chu%2C%20Eric%20Peleato%2C%20Borja%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20Stephen%20Parikh%2C%20Neal%20Chu%2C%20Eric%20Peleato%2C%20Borja%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers%202011"
        },
        {
            "id": "3",
            "entry": "[3] John C. Duchi, Alekh Agarwal, and Martin J. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):592\u2013606, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Agarwal%2C%20Alekh%20Wainwright%2C%20Martin%20J.%20Dual%20averaging%20for%20distributed%20optimization%3A%20Convergence%20analysis%20and%20network%20scaling%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Agarwal%2C%20Alekh%20Wainwright%2C%20Martin%20J.%20Dual%20averaging%20for%20distributed%20optimization%3A%20Convergence%20analysis%20and%20network%20scaling%202012"
        },
        {
            "id": "4",
            "entry": "[4] Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. EXTRA: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944\u2013966, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Wei%20Ling%2C%20Qing%20Wu%2C%20Gang%20Yin%2C%20Wotao%20EXTRA%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Wei%20Ling%2C%20Qing%20Wu%2C%20Gang%20Yin%2C%20Wotao%20EXTRA%3A%20An%20exact%20first-order%20algorithm%20for%20decentralized%20consensus%20optimization%202015"
        },
        {
            "id": "5",
            "entry": "[5] Wei Shi, Qing Ling, Kun Yuan, Gang Wu, and Wotao Yin. On the linear convergence of the ADMM in decentralized consensus optimization. IEEE Transactions on Signal Processing, 62(7):1750\u20131761, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shi%2C%20Wei%20Ling%2C%20Qing%20Yuan%2C%20Kun%20Wu%2C%20Gang%20On%20the%20linear%20convergence%20of%20the%20ADMM%20in%20decentralized%20consensus%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shi%2C%20Wei%20Ling%2C%20Qing%20Yuan%2C%20Kun%20Wu%2C%20Gang%20On%20the%20linear%20convergence%20of%20the%20ADMM%20in%20decentralized%20consensus%20optimization%202014"
        },
        {
            "id": "6",
            "entry": "[6] Du\u0161an Jakovetic, Jos\u00e9 M. F. Moura, and Joao Xavier. Linear convergence rate of a class of distributed augmented lagrangian algorithms. IEEE Transactions on Automatic Control, 60(4):922\u2013936, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jakovetic%2C%20Du%C5%A1an%20Moura%2C%20Jos%C3%A9%20M.F.%20Xavier%2C%20Joao%20Linear%20convergence%20rate%20of%20a%20class%20of%20distributed%20augmented%20lagrangian%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jakovetic%2C%20Du%C5%A1an%20Moura%2C%20Jos%C3%A9%20M.F.%20Xavier%2C%20Joao%20Linear%20convergence%20rate%20of%20a%20class%20of%20distributed%20augmented%20lagrangian%20algorithms%202015"
        },
        {
            "id": "7",
            "entry": "[7] Angelia Nedic, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM Journal on Optimization, 27(4):2597\u20132633, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nedic%2C%20Angelia%20Olshevsky%2C%20Alex%20Shi%2C%20Wei%20Achieving%20geometric%20convergence%20for%20distributed%20optimization%20over%20time-varying%20graphs%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nedic%2C%20Angelia%20Olshevsky%2C%20Alex%20Shi%2C%20Wei%20Achieving%20geometric%20convergence%20for%20distributed%20optimization%20over%20time-varying%20graphs%202017"
        },
        {
            "id": "8",
            "entry": "[8] Kevin Scaman, Francis Bach, S\u00e9bastien Bubeck, Yin Tat Lee, and Laurent Massouli\u00e9. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In Proceedings of the 34th International Conference on Machine Learning ICML, pages 3027\u20133036, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scaman%2C%20Kevin%20Bach%2C%20Francis%20Bubeck%2C%20S%C3%A9bastien%20Lee%2C%20Yin%20Tat%20Optimal%20algorithms%20for%20smooth%20and%20strongly%20convex%20distributed%20optimization%20in%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scaman%2C%20Kevin%20Bach%2C%20Francis%20Bubeck%2C%20S%C3%A9bastien%20Lee%2C%20Yin%20Tat%20Optimal%20algorithms%20for%20smooth%20and%20strongly%20convex%20distributed%20optimization%20in%20networks%202017"
        },
        {
            "id": "9",
            "entry": "[9] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE/ACM Transactions on Networking (TON), 14(SI):2508\u20132530, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Stephen%20Ghosh%2C%20Arpita%20Prabhakar%2C%20Balaji%20Shah%2C%20Devavrat%20Randomized%20gossip%20algorithms%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20Stephen%20Ghosh%2C%20Arpita%20Prabhakar%2C%20Balaji%20Shah%2C%20Devavrat%20Randomized%20gossip%20algorithms%202006"
        },
        {
            "id": "10",
            "entry": "[10] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674\u2013701, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Bartlett%2C%20Peter%20L.%20Wainwright%2C%20Martin%20J.%20Randomized%20smoothing%20for%20stochastic%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Bartlett%2C%20Peter%20L.%20Wainwright%2C%20Martin%20J.%20Randomized%20smoothing%20for%20stochastic%20optimization%202012"
        },
        {
            "id": "11",
            "entry": "[11] Du\u0161an Jakovetic, Joao Xavier, and Jos\u00e9 M. F. Moura. Fast distributed gradient methods. IEEE Transactions on Automatic Control, 59(5):1131\u20131146, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Du%C5%A1an%20Jakovetic%2C%20Joao%20Xavier%20Moura%2C%20Jos%C3%A9%20M.%20F.%20Fast%20distributed%20gradient%20methods%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Du%C5%A1an%20Jakovetic%2C%20Joao%20Xavier%20Moura%2C%20Jos%C3%A9%20M.%20F.%20Fast%20distributed%20gradient%20methods%202014"
        },
        {
            "id": "12",
            "entry": "[12] Aryan Mokhtari and Alejandro Ribeiro. DSA: Decentralized double stochastic averaging gradient algorithm. Journal of Machine Learning Research, 17(1):2165\u20132199, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mokhtari%2C%20Aryan%20Ribeiro%2C%20Alejandro%20DSA%3A%20Decentralized%20double%20stochastic%20averaging%20gradient%20algorithm%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mokhtari%2C%20Aryan%20Ribeiro%2C%20Alejandro%20DSA%3A%20Decentralized%20double%20stochastic%20averaging%20gradient%20algorithm%202016"
        },
        {
            "id": "13",
            "entry": "[13] Ermin Wei and Asuman Ozdaglar. Distributed alternating direction method of multipliers. In 51st Annual Conference on Decision and Control (CDC), pages 5445\u20135450. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20Ermin%20Ozdaglar%2C%20Asuman%20Distributed%20alternating%20direction%20method%20of%20multipliers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20Ermin%20Ozdaglar%2C%20Asuman%20Distributed%20alternating%20direction%20method%20of%20multipliers%202012"
        },
        {
            "id": "14",
            "entry": "[14] Guanghui Lan, Soomin Lee, and Yi Zhou. Communication-efficient algorithms for decentralized and stochastic optimization. arXiv preprint arXiv:1701.03961, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.03961"
        },
        {
            "id": "15",
            "entry": "[15] Martin Jaggi, Virginia Smith, Martin Tak\u00e1c, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems 27, pages 3068\u20133076, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Tak%C3%A1c%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaggi%2C%20Martin%20Smith%2C%20Virginia%20Tak%C3%A1c%2C%20Martin%20Terhorst%2C%20Jonathan%20Communication-efficient%20distributed%20dual%20coordinate%20ascent%202014"
        },
        {
            "id": "16",
            "entry": "[16] Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning and estimation. In Advances in Neural Information Processing Systems 27, pages 163\u2013171, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shamir%2C%20Ohad%20Fundamental%20limits%20of%20online%20and%20distributed%20algorithms%20for%20statistical%20learning%20and%20estimation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shamir%2C%20Ohad%20Fundamental%20limits%20of%20online%20and%20distributed%20algorithms%20for%20statistical%20learning%20and%20estimation%202014"
        },
        {
            "id": "17",
            "entry": "[17] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. In Advances in Neural Information Processing Systems 28, pages 1756\u20131764, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjevani%2C%20Yossi%20Shamir%2C%20Ohad%20Communication%20complexity%20of%20distributed%20convex%20learning%20and%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjevani%2C%20Yossi%20Shamir%2C%20Ohad%20Communication%20complexity%20of%20distributed%20convex%20learning%20and%20optimization%202015"
        },
        {
            "id": "18",
            "entry": "[18] S\u00e9bastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231\u2013357, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S%C3%A9bastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S%C3%A9bastien%20Convex%20optimization%3A%20Algorithms%20and%20complexity%202015"
        },
        {
            "id": "19",
            "entry": "[19] J. J. Moreau. Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien. Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France, 93:273\u2013299, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moreau%2C%20J.J.%20Proximit%C3%A9%20et%20dualit%C3%A9%20dans%20un%20espace%20hilbertien%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moreau%2C%20J.J.%20Proximit%C3%A9%20et%20dualit%C3%A9%20dans%20un%20espace%20hilbertien%201965"
        },
        {
            "id": "20",
            "entry": "[20] Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Kluwer Academic Publishers, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%20%3A%20a%20basic%20course%202004"
        },
        {
            "id": "21",
            "entry": "[21] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):120\u2013145, May 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20Antonin%20Pock%2C%20Thomas%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20Antonin%20Pock%2C%20Thomas%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011-05"
        },
        {
            "id": "22",
            "entry": "[22] Niao He, Anatoli Juditsky, and Arkadi Nemirovski. Mirror prox algorithm for multi-term composite minimization and semi-separable problems. Computational Optimization and Applications, 61(2):275\u2013319, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Niao%20Juditsky%2C%20Anatoli%20Nemirovski%2C%20Arkadi%20Mirror%20prox%20algorithm%20for%20multi-term%20composite%20minimization%20and%20semi-separable%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Niao%20Juditsky%2C%20Anatoli%20Nemirovski%2C%20Arkadi%20Mirror%20prox%20algorithm%20for%20multi-term%20composite%20minimization%20and%20semi-separable%20problems%202015"
        },
        {
            "id": "23",
            "entry": "[23] Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method. Technical Report 1212.2002, arXiv, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste-Julien%2C%20Simon%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20A%20simpler%20approach%20to%20obtaining%20an%20O%281/t%29%20convergence%20rate%20for%20the%20projected%20stochastic%20subgradient%20method%202012"
        },
        {
            "id": "24",
            "entry": "[24] W. Auzinger. Iterative Solution of Large Linear Systems. Lecture notes, TU Wien, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auzinger%2C%20W.%20Iterative%20Solution%20of%20Large%20Linear%20Systems.%20Lecture%20notes%202011"
        },
        {
            "id": "25",
            "entry": "[25] M. Arioli and J. Scott. Chebyshev acceleration of iterative refinement. Numerical Algorithms, 66(3):591\u2013608, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arioli%2C%20M.%20Scott%2C%20J.%20Chebyshev%20acceleration%20of%20iterative%20refinement%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arioli%2C%20M.%20Scott%2C%20J.%20Chebyshev%20acceleration%20of%20iterative%20refinement%202014"
        }
    ]
}
