{
    "filename": "7558-dialog-to-action-conversational-question-answering-over-a-large-scale-knowledge-base.pdf",
    "metadata": {
        "title": "Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base",
        "author": "Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7558-dialog-to-action-conversational-question-answering-over-a-large-scale-knowledge-base.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin."
    },
    "keywords": [
        {
            "term": "question answering",
            "url": "https://en.wikipedia.org/wiki/question_answering"
        },
        {
            "term": "natural language",
            "url": "https://en.wikipedia.org/wiki/natural_language"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "gated recurrent unit",
            "url": "https://en.wikipedia.org/wiki/gated_recurrent_unit"
        },
        {
            "term": "knowledge base",
            "url": "https://en.wikipedia.org/wiki/knowledge_base"
        },
        {
            "term": "semantic parsing",
            "url": "https://en.wikipedia.org/wiki/semantic_parsing"
        },
        {
            "term": "logical form",
            "url": "https://en.wikipedia.org/wiki/logical_form"
        }
    ],
    "highlights": [
        "We consider the problem of mapping conversational natural language questions to formal representations of their underlying meanings, which would be executed to produce the answer [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "We study the problem in a realistic setting that (1) only denotations are available for model training while the underlying logical forms remain unknown, and (2) logical forms will be executed on a large-scale knowledge base (KB) consisting of tens of millions of entities",
        "To cope with ellipsis phenomena in conversation, we introduce a dialog memory management component that leverages historical entities, predicates, and action subsequences when generating the logical form for a current utterance",
        "We present the Dialog-to-Action, a generative model that converts an utterance in conversation to a logical form, which will be executed on a large-scale knowledge base to produce the answer",
        "The model works in a top-down manner following a small and flexible grammar, in which the generation of a logical form is equivalent to the prediction of a sequence of actions",
        "Results on a large-scale dataset demonstrate the effectiveness of considering the dialog memory, and show that our model performs significantly better than a strong memory network-based encoder-decoder model"
    ],
    "key_statements": [
        "We consider the problem of mapping conversational natural language questions to formal representations of their underlying meanings, which would be executed to produce the answer [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "We study the problem in a realistic setting that (1) only denotations are available for model training while the underlying logical forms remain unknown, and (2) logical forms will be executed on a large-scale knowledge base (KB) consisting of tens of millions of entities",
        "We believe that knowledge base-based conversational question answering plays an important role in both search engines and intelligent personal assistants (e.g., Siri, Alexa, Cortana/Xiaoice, and Google ) [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] to improve the ability of multi-turn question answering",
        "Online learning by searching legitimate logical forms requires repeated execution on a large-scale knowledge base, which is extremely time-consuming and intolerable",
        "We introduce a generative model that interprets the logical form of an utterance in a top-down manner",
        "To cope with ellipsis phenomena in conversation, we introduce a dialog memory management component that leverages historical entities, predicates, and action subsequences when generating the logical form for a current utterance",
        "We develop a grammar-guided decoder to control the generation of an action sequence, and a dialog memory management component to leverage historical contexts.\n4.1",
        "The first action subsequence in the dialog memory of Figure 2 is identical to the logical form f ind(U nitedStates, isP residentOf ), which means the president of the United States",
        "Our approach is a semantic parsing based method, which explicitly manipulates the actions/functions and lets the Seq2Seq model learn how these actions are used to derive the logical form of the question",
        "We present the Dialog-to-Action, a generative model that converts an utterance in conversation to a logical form, which will be executed on a large-scale knowledge base to produce the answer",
        "The model works in a top-down manner following a small and flexible grammar, in which the generation of a logical form is equivalent to the prediction of a sequence of actions",
        "The model is effectively learned from denotations without using annotated logical forms",
        "Results on a large-scale dataset demonstrate the effectiveness of considering the dialog memory, and show that our model performs significantly better than a strong memory network-based encoder-decoder model"
    ],
    "summary": [
        "We consider the problem of mapping conversational natural language questions to formal representations of their underlying meanings, which would be executed to produce the answer [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>\u2013<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "To cope with ellipsis phenomena in conversation, we introduce a dialog memory management component that leverages historical entities, predicates, and action subsequences when generating the logical form for a current utterance.",
        "Based on sequence-to-sequence learning [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], the model takes a question and its context from interaction history as the input and generates an action sequence.",
        "The first action subsequence in the dialog memory of Figure 2 is identical to the logical form f ind(U nitedStates, isP residentOf ), which means the president of the United States.",
        "HRED+KVmem [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] is a sequenceto-sequence learning method, which uses a hierarchical encoder and a key-value memory network [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] to compute the representation for the question and its contexts, and uses an RNN as the decoder to directly produce answers.",
        "Our approach is a semantic parsing based method, which explicitly manipulates the actions/functions and lets the Seq2Seq model learn how these actions are used to derive the logical form of the question.",
        "It could naturally leverage parsed results of previous turn including entities, predicates and action subsequences to handle various ellipsis phenomena.",
        "The 2nd and 3rd examples show that the dialog memory helps the parser replicate entity and predicate from history interaction.",
        "The problem of error propagation occurs because our model learns to replicate previously generated action sequences, which might be incorrect despite we consider the probability of the previous logical form.",
        "Compared to their method that only learns to replicate the entire logical form of previous utterance, our model is more flexible and capable of replicating various information from dialog memory including entities, predicates, and action subsequences.",
        "Our task differs from this line of work in that our logical forms interact with a large-scale knowledge base, which poses new challenges for model training.",
        "Our semantic parsing-based model is essentially different from them in that deep question understanding is required to produce the explicit logical form of the underlying meaning.",
        "We present the Dialog-to-Action, a generative model that converts an utterance in conversation to a logical form, which will be executed on a large-scale knowledge base to produce the answer.",
        "The model works in a top-down manner following a small and flexible grammar, in which the generation of a logical form is equivalent to the prediction of a sequence of actions.",
        "A dialog memory management is developed and naturally integrated in the model, so that historical entities, predicates, and action subsequences could be selectively replicated.",
        "Results on a large-scale dataset demonstrate the effectiveness of considering the dialog memory, and show that our model performs significantly better than a strong memory network-based encoder-decoder model"
    ],
    "headline": "We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Luke S Zettlemoyer and Michael Collins. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 976\u2013984, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zettlemoyer%2C%20Luke%20S.%20Collins%2C%20Michael%20Learning%20context-dependent%20mappings%20from%20sentences%20to%20logical%20form%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zettlemoyer%2C%20Luke%20S.%20Collins%2C%20Michael%20Learning%20context-dependent%20mappings%20from%20sentences%20to%20logical%20form%202009"
        },
        {
            "id": "2",
            "entry": "[2] Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association of Computational Linguistics, 1:49\u201362, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Artzi%2C%20Yoav%20Zettlemoyer%2C%20Luke%20Weakly%20supervised%20learning%20of%20semantic%20parsers%20for%20mapping%20instructions%20to%20actions%202013"
        },
        {
            "id": "3",
            "entry": "[3] Andreas Vlachos and Stephen Clark. A new corpus and imitation learning framework for context-dependent semantic parsing. Transactions of the Association for Computational Linguistics, 2:547\u2013559, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vlachos%2C%20Andreas%20Clark%2C%20Stephen%20A%20new%20corpus%20and%20imitation%20learning%20framework%20for%20context-dependent%20semantic%20parsing%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vlachos%2C%20Andreas%20Clark%2C%20Stephen%20A%20new%20corpus%20and%20imitation%20learning%20framework%20for%20context-dependent%20semantic%20parsing%202014"
        },
        {
            "id": "4",
            "entry": "[4] Reginald Long, Panupong Pasupat, and Percy Liang. Simpler context-dependent logical forms via model projections. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1456\u20131465. Association for Computational Linguistics, August 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Long%2C%20Reginald%20Pasupat%2C%20Panupong%20Liang%2C%20Percy%20Simpler%20context-dependent%20logical%20forms%20via%20model%20projections%202016-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Long%2C%20Reginald%20Pasupat%2C%20Panupong%20Liang%2C%20Percy%20Simpler%20context-dependent%20logical%20forms%20via%20model%20projections%202016-08"
        },
        {
            "id": "5",
            "entry": "[5] Kelvin Guu, Panupong Pasupat, Evan Liu, and Percy Liang. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1051\u20131062, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guu%2C%20Kelvin%20Pasupat%2C%20Panupong%20Liu%2C%20Evan%20Liang%2C%20Percy%20From%20language%20to%20programs%3A%20Bridging%20reinforcement%20learning%20and%20maximum%20marginal%20likelihood%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guu%2C%20Kelvin%20Pasupat%2C%20Panupong%20Liu%2C%20Evan%20Liang%2C%20Percy%20From%20language%20to%20programs%3A%20Bridging%20reinforcement%20learning%20and%20maximum%20marginal%20likelihood%202017"
        },
        {
            "id": "6",
            "entry": "[6] Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1821\u20131831, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Iyyer%2C%20Mohit%20Yih%2C%20Wen-tau%20Chang%2C%20Ming-Wei%20Search-based%20neural%20structured%20learning%20for%20sequential%20question%20answering%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Iyyer%2C%20Mohit%20Yih%2C%20Wen-tau%20Chang%2C%20Ming-Wei%20Search-based%20neural%20structured%20learning%20for%20sequential%20question%20answering%202017"
        },
        {
            "id": "7",
            "entry": "[7] Alane Suhr, Srinivasan Iyer, and Yoav Artzi. Learning to map context-dependent sentences to executable formal queries. arXiv preprint arXiv:1804.06868, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.06868"
        },
        {
            "id": "8",
            "entry": "[8] Heung-Yeung Shum, Xiaodong He, and Di Li. From eliza to xiaoice: Challenges and opportunities with social chatbots. arXiv preprint arXiv:1801.01957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01957"
        },
        {
            "id": "9",
            "entry": "[9] Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S Zettlemoyer. A generative model for parsing natural language to meaning representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 783\u2013792. Association for Computational Linguistics, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lu%2C%20Wei%20Ng%2C%20Hwee%20Tou%20Lee%2C%20Wee%20Sun%20Zettlemoyer%2C%20Luke%20S.%20A%20generative%20model%20for%20parsing%20natural%20language%20to%20meaning%20representations%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lu%2C%20Wei%20Ng%2C%20Hwee%20Tou%20Lee%2C%20Wee%20Sun%20Zettlemoyer%2C%20Luke%20S.%20A%20generative%20model%20for%20parsing%20natural%20language%20to%20meaning%20representations%202008"
        },
        {
            "id": "10",
            "entry": "[10] Dipendra Kumar Misra and Yoav Artzi. Neural shift-reduce ccg semantic parsing. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1775\u20131786, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Misra%2C%20Dipendra%20Kumar%20Artzi%2C%20Yoav%20Neural%20shift-reduce%20ccg%20semantic%20parsing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Misra%2C%20Dipendra%20Kumar%20Artzi%2C%20Yoav%20Neural%20shift-reduce%20ccg%20semantic%20parsing%202016"
        },
        {
            "id": "11",
            "entry": "[11] Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. Neural semantic parsing with type constraints for semi-structured tables. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516\u20131526. Association for Computational Linguistics, September 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krishnamurthy%2C%20Jayant%20Dasigi%2C%20Pradeep%20Gardner%2C%20Matt%20Neural%20semantic%20parsing%20with%20type%20constraints%20for%20semi-structured%20tables%202017-09",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krishnamurthy%2C%20Jayant%20Dasigi%2C%20Pradeep%20Gardner%2C%20Matt%20Neural%20semantic%20parsing%20with%20type%20constraints%20for%20semi-structured%20tables%202017-09"
        },
        {
            "id": "12",
            "entry": "[12] Bo Chen, Le Sun, and Xianpei Han. Sequence-to-action: End-to-end semantic graph generation for semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Bo%20Sun%2C%20Le%20Han%2C%20Xianpei%20Sequence-to-action%3A%20End-to-end%20semantic%20graph%20generation%20for%20semantic%20parsing%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Bo%20Sun%2C%20Le%20Han%2C%20Xianpei%20Sequence-to-action%3A%20End-to-end%20semantic%20graph%20generation%20for%20semantic%20parsing%202018"
        },
        {
            "id": "13",
            "entry": "[13] Amrita Saha, Vardaan Pahuja, Mitesh M Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. arXiv preprint arXiv:1801.10314, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.10314"
        },
        {
            "id": "14",
            "entry": "[14] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "15",
            "entry": "[15] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. Proceeding of ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bahdanau%2C%20Dzmitry%20Cho%2C%20Kyunghyun%20Bengio%2C%20Yoshua%20Neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate%202015"
        },
        {
            "id": "16",
            "entry": "[16] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93decoder%20for%20statistical%20machine%20translation%202014"
        },
        {
            "id": "17",
            "entry": "[17] Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. empirical methods in natural language processing, pages 1412\u20131421, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luong%2C%20Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luong%2C%20Thang%20Pham%2C%20Hieu%20Manning%2C%20Christopher%20D.%20Effective%20approaches%20to%20attention-based%20neural%20machine%20translation%202015"
        },
        {
            "id": "18",
            "entry": "[18] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.03126"
        },
        {
            "id": "19",
            "entry": "[19] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems, pages 2787\u20132795, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bordes%2C%20Antoine%20Usunier%2C%20Nicolas%20Garcia-Duran%2C%20Alberto%20Weston%2C%20Jason%20Translating%20embeddings%20for%20modeling%20multi-relational%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bordes%2C%20Antoine%20Usunier%2C%20Nicolas%20Garcia-Duran%2C%20Alberto%20Weston%2C%20Jason%20Translating%20embeddings%20for%20modeling%20multi-relational%20data%202013"
        },
        {
            "id": "20",
            "entry": "[20] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In Advances in neural information processing systems, pages 926\u2013934, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Chen%2C%20Danqi%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Reasoning%20with%20neural%20tensor%20networks%20for%20knowledge%20base%20completion%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Socher%2C%20Richard%20Chen%2C%20Danqi%20Manning%2C%20Christopher%20D.%20Ng%2C%20Andrew%20Reasoning%20with%20neural%20tensor%20networks%20for%20knowledge%20base%20completion%202013"
        },
        {
            "id": "21",
            "entry": "[21] Alane Suhr and Yoav Artzi. Situated mapping of sequential instructions to actions with single-step reward observation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 2072\u20132082. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/P18-1193.",
            "url": "http://aclweb.org/anthology/P18-1193",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suhr%2C%20Alane%20Artzi%2C%20Yoav%20Situated%20mapping%20of%20sequential%20instructions%20to%20actions%20with%20single-step%20reward%20observation%202018"
        },
        {
            "id": "22",
            "entry": "[22] Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems. arXiv preprint arXiv:1804.08217, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.08217"
        },
        {
            "id": "23",
            "entry": "[23] Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine Bordes, Sumit Chopra, Alexander Miller, Arthur Szlam, and Jason Weston. Evaluating prerequisite qualities for learning end-to-end dialog systems. arXiv preprint arXiv:1511.06931, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.06931"
        },
        {
            "id": "24",
            "entry": "[24] Jason E Weston. Dialog-based language learning. In Advances in Neural Information Processing Systems, pages 829\u2013837, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weston%2C%20Jason%20E.%20Dialog-based%20language%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weston%2C%20Jason%20E.%20Dialog-based%20language%20learning%202016"
        }
    ]
}
