{
    "filename": "7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf",
    "metadata": {
        "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport",
        "author": "L\u00e9na\u00efc Chizat, Francis Bach",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf"
        },
        "abstract": "Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension."
    },
    "keywords": [
        {
            "term": "gradient flow",
            "url": "https://en.wikipedia.org/wiki/gradient_flow"
        },
        {
            "term": "optimal transport",
            "url": "https://en.wikipedia.org/wiki/optimal_transport"
        },
        {
            "term": "inverse problem",
            "url": "https://en.wikipedia.org/wiki/inverse_problem"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "signal processing",
            "url": "https://en.wikipedia.org/wiki/signal_processing"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "linear combination",
            "url": "https://en.wikipedia.org/wiki/linear_combination"
        },
        {
            "term": "convex function",
            "url": "https://en.wikipedia.org/wiki/convex_function"
        },
        {
            "term": "mean field",
            "url": "https://en.wikipedia.org/wiki/mean_field"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "parameter space",
            "url": "https://en.wikipedia.org/wiki/parameter_space"
        }
    ],
    "highlights": [
        "A classical task in machine learning and signal processing is to search for an element in a Hilbert space F that minimizes a smooth, convex loss function R : F \u2192 R+ and that is a linear combination of a few elements from a large given parameterized set {\u03c6(\u03b8)}\u03b8\u2208\u0398 \u2282 F",
        "In Section 3, under assumptions on \u03c6 and the initialization, we prove that if this Wasserstein gradient flow converges, the limit is a global minimizer of J",
        "A measure \u03bc \u2208 M+(\u03a9) such that F (\u03bc) < \u221e minimizes F on M+(\u03a9) iff F (\u03bc) \u2265 0 and F (\u03bc)(u) = 0 for \u03bc-a.e. u \u2208 \u03a9. Despite these strong differences between stationarity and global optimality, we show that Wasserstein gradient flows converge to global minimizers, under two main conditions:",
        "We have established asymptotic global optimality properties for a family of non-convex gradient flows. These results were enabled by the study of a Wasserstein gradient flow: this object simplifies the handling of many-particle regimes, analogously to a mean-field limit",
        "The particle-complexity to reach global optimality turns out very favorable on synthetic numerical problems. This confirms the relevance of our qualitative results and calls for quantitative ones that would further exploit the properties of such particle gradient flows"
    ],
    "key_statements": [
        "A classical task in machine learning and signal processing is to search for an element in a Hilbert space F that minimizes a smooth, convex loss function R : F \u2192 R+ and that is a linear combination of a few elements from a large given parameterized set {\u03c6(\u03b8)}\u03b8\u2208\u0398 \u2282 F",
        "In Section 3, under assumptions on \u03c6 and the initialization, we prove that if this Wasserstein gradient flow converges, the limit is a global minimizer of J",
        "A measure \u03bc \u2208 M+(\u03a9) such that F (\u03bc) < \u221e minimizes F on M+(\u03a9) iff F (\u03bc) \u2265 0 and F (\u03bc)(u) = 0 for \u03bc-a.e. u \u2208 \u03a9. Despite these strong differences between stationarity and global optimality, we show that Wasserstein gradient flows converge to global minimizers, under two main conditions:",
        "If the Wasserstein gradient flow of F converges in W2 to \u03bc\u221e, \u03bc\u221e is a global minimizer of F",
        "We have established asymptotic global optimality properties for a family of non-convex gradient flows. These results were enabled by the study of a Wasserstein gradient flow: this object simplifies the handling of many-particle regimes, analogously to a mean-field limit",
        "The particle-complexity to reach global optimality turns out very favorable on synthetic numerical problems. This confirms the relevance of our qualitative results and calls for quantitative ones that would further exploit the properties of such particle gradient flows"
    ],
    "summary": [
        "A classical task in machine learning and signal processing is to search for an element in a Hilbert space F that minimizes a smooth, convex loss function R : F \u2192 R+ and that is a linear combination of a few elements from a large given parameterized set {\u03c6(\u03b8)}\u03b8\u2208\u0398 \u2282 F.",
        "In the space of measures, has many local minima, but we build gradient flows that avoids them, relying mainly on the homogeneity properties of Jm. The novelty is to see (2) as a discretization of (1)\u2014a point of view present in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] but not yet exploited for global optimality guarantees.",
        "In Section 3, under assumptions on \u03c6 and the initialization, we prove that if this Wasserstein gradient flow converges, the limit is a global minimizer of J.",
        "We consider the following class of problems on the space of non-negative finite measures on a domain \u03a9 \u2282 Rd which, as explained below, is more general than (1): F \u2217 = min F (\u03bc) where F (\u03bc) = R \u03a6d\u03bc + V d\u03bc, (3)",
        "We characterize the many-particle limit of classical gradient flows, under Assumptions 2.1.",
        "Despite these strong differences between stationarity and global optimality, we show that Wasserstein gradient flows converge to global minimizers, under two main conditions:",
        "This covers the case of lifted problems of Section 2.1 when \u03c6 is 1-homogeneous and neural networks with ReLU activation functions.",
        "If the projection (h1)t of the Wasserstein gradient flow of F weakly converges to \u03bd \u2208 M(\u0398), \u03bd is a global minimizer of",
        "If the Wasserstein gradient flow of F converges in W2 to \u03bc\u221e, \u03bc\u221e is a global minimizer of F .",
        "We display on Figure 2 particle gradient flows for training a neural network with a single hidden layer and ReLU activation in the classical parameterization, with d = 2.",
        "Particle gradient flow optimal positions limit measure",
        "Since our convergence results are non-quantitative, one might argue that similar\u2014and much simpler to prove\u2014asymptotical results hold for the method of distributing particles on the whole of \u0398 and optimizing on the weights, which is a convex problem.",
        "While exponential particle-complexity is unavoidable for the convex approach, we observed on several synthetic problems that particle gradient descent only needs a slight over-parameterization m > m0 to find global minimizers within optimization error.",
        "10 2 10 3 10 4 particle gradient flow convex minimization below optim.",
        "We have established asymptotic global optimality properties for a family of non-convex gradient flows.",
        "These results were enabled by the study of a Wasserstein gradient flow: this object simplifies the handling of many-particle regimes, analogously to a mean-field limit.",
        "Multiple layer neural networks are an interesting avenue for future research"
    ],
    "headline": "We study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Ralph Abraham and Joel Robbin. Transversal mappings and flows. WA Benjamin New York, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abraham%2C%20Ralph%20Robbin%2C%20Joel%20Transversal%20mappings%20and%20flows%201967"
        },
        {
            "id": "2",
            "entry": "[2] Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of descent methods for analytic cost functions. SIAM Journal on Optimization, 16(2):531\u2013547, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Absil%2C%20Pierre-Antoine%20Mahony%2C%20Robert%20Andrews%2C%20Benjamin%20Convergence%20of%20the%20iterates%20of%20descent%20methods%20for%20analytic%20cost%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Absil%2C%20Pierre-Antoine%20Mahony%2C%20Robert%20Andrews%2C%20Benjamin%20Convergence%20of%20the%20iterates%20of%20descent%20methods%20for%20analytic%20cost%20functions%202005"
        },
        {
            "id": "3",
            "entry": "[3] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ambrosio%2C%20Luigi%20Gigli%2C%20Nicola%20Savar%C3%A9%2C%20Giuseppe%20Gradient%20flows%3A%20in%20metric%20spaces%20and%20in%20the%20space%20of%20probability%20measures%202008"
        },
        {
            "id": "4",
            "entry": "[4] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1\u201353, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Adrien Blanchet and J\u00e9r\u00f4me Bolte. A family of functional inequalities: \u0141ojasiewicz inequalities and displacement convex functions. Journal of Functional Analysis, 275(7):1650\u20131673, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blanchet%2C%20Adrien%20Bolte%2C%20J%C3%A9r%C3%B4me%20A%20family%20of%20functional%20inequalities%3A%20%C5%81ojasiewicz%20inequalities%20and%20displacement%20convex%20functions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blanchet%2C%20Adrien%20Bolte%2C%20J%C3%A9r%C3%B4me%20A%20family%20of%20functional%20inequalities%3A%20%C5%81ojasiewicz%20inequalities%20and%20displacement%20convex%20functions%202018"
        },
        {
            "id": "6",
            "entry": "[6] Nicholas Boyd, Geoffrey Schiebinger, and Benjamin Recht. The alternating descent conditional gradient method for sparse inverse problems. SIAM Journal on Optimization, 27(2):616\u2013639, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20Nicholas%20Schiebinger%2C%20Geoffrey%20Recht%2C%20Benjamin%20The%20alternating%20descent%20conditional%20gradient%20method%20for%20sparse%20inverse%20problems%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20Nicholas%20Schiebinger%2C%20Geoffrey%20Recht%2C%20Benjamin%20The%20alternating%20descent%20conditional%20gradient%20method%20for%20sparse%20inverse%20problems%202017"
        },
        {
            "id": "7",
            "entry": "[7] Kristian Bredies and Hanna Katriina Pikkarainen. Inverse problems in spaces of measures. ESAIM: Control, Optimisation and Calculus of Variations, 19(1):190\u2013218, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bredies%2C%20Kristian%20Pikkarainen%2C%20Hanna%20Katriina%20Inverse%20problems%20in%20spaces%20of%20measures%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bredies%2C%20Kristian%20Pikkarainen%2C%20Hanna%20Katriina%20Inverse%20problems%20in%20spaces%20of%20measures%202013"
        },
        {
            "id": "8",
            "entry": "[8] Felix E. Browder. Fixed point theory and nonlinear problems. Proc. Sym. Pure. Math, 39:49\u201388, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Browder%2C%20Felix%20E.%20Fixed%20point%20theory%20and%20nonlinear%20problems%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Browder%2C%20Felix%20E.%20Fixed%20point%20theory%20and%20nonlinear%20problems%201983"
        },
        {
            "id": "9",
            "entry": "[9] Paul Catala, Vincent Duval, and Gabriel Peyr\u00e9. A low-rank approach to off-the-grid sparse deconvolution. Journal of Physics: Conference Series, 904(1):012015, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Catala%2C%20Paul%20Duval%2C%20Vincent%20Peyr%C3%A9%2C%20Gabriel%20A%20low-rank%20approach%20to%20off-the-grid%20sparse%20deconvolution%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Catala%2C%20Paul%20Duval%2C%20Vincent%20Peyr%C3%A9%2C%20Gabriel%20A%20low-rank%20approach%20to%20off-the-grid%20sparse%20deconvolution%202017"
        },
        {
            "id": "10",
            "entry": "[10] Donald L. Cohn. Measure theory, volume 165.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Donald%20L%20Cohn%20Measure%20theory%20volume%20165",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Donald%20L%20Cohn%20Measure%20theory%20volume%20165"
        },
        {
            "id": "11",
            "entry": "[11] Patrick L. Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering, pages 185\u2013212.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Patrick%2C%20L.%20Combettes%20and%20Jean-Christophe%20Pesquet.%20Proximal%20splitting%20methods%20in%20signal%20processing.%20In%20Fixed-point%20algorithms%20for%20inverse%20problems",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Patrick%2C%20L.%20Combettes%20and%20Jean-Christophe%20Pesquet.%20Proximal%20splitting%20methods%20in%20signal%20processing.%20In%20Fixed-point%20algorithms%20for%20inverse%20problems"
        },
        {
            "id": "12",
            "entry": "[12] Yohann De Castro and Fabrice Gamboa. Exact reconstruction using Beurling minimal extrapolation. Journal of Mathematical Analysis and applications, 395(1):336\u2013354, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Castro%2C%20Yohann%20De%20Gamboa%2C%20Fabrice%20Exact%20reconstruction%20using%20Beurling%20minimal%20extrapolation%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Castro%2C%20Yohann%20De%20Gamboa%2C%20Fabrice%20Exact%20reconstruction%20using%20Beurling%20minimal%20extrapolation%202012"
        },
        {
            "id": "13",
            "entry": "[13] Vincent Duval and Gabriel Peyr\u00e9. Exact support recovery for sparse spikes deconvolution. Foundations of Computational Mathematics, 15(5):1315\u20131355, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duval%2C%20Vincent%20Peyr%C3%A9%2C%20Gabriel%20Exact%20support%20recovery%20for%20sparse%20spikes%20deconvolution%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duval%2C%20Vincent%20Peyr%C3%A9%2C%20Gabriel%20Exact%20support%20recovery%20for%20sparse%20spikes%20deconvolution%202015"
        },
        {
            "id": "14",
            "entry": "[14] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Bengio%2C%20Yoshua%20Courville%2C%20Aaron%20Deep%20Learning%202016"
        },
        {
            "id": "15",
            "entry": "[15] Suriya Gunasekar, Blake E. Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems 30, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gunasekar%2C%20Suriya%20Woodworth%2C%20Blake%20E.%20Bhojanapalli%2C%20Srinadh%20Neyshabur%2C%20Behnam%20Implicit%20regularization%20in%20matrix%20factorization%202017"
        },
        {
            "id": "16",
            "entry": "[16] Benjamin D. Haeffele and Ren\u00e9 Vidal. Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7331\u20137339, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haeffele%2C%20Benjamin%20D.%20Vidal%2C%20Ren%C3%A9%20Global%20optimality%20in%20neural%20network%20training%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haeffele%2C%20Benjamin%20D.%20Vidal%2C%20Ren%C3%A9%20Global%20optimality%20in%20neural%20network%20training%202017"
        },
        {
            "id": "17",
            "entry": "[17] Daniel Hauer and Jos\u00e9 Maz\u00f3n. Kurdyka-\u0141ojasiewicz-Simon inequality for gradient flows in metric spaces. arXiv preprint arXiv:1707.03129, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.03129"
        },
        {
            "id": "18",
            "entry": "[18] Simon Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haykin%2C%20Simon%20Neural%20Networks%3A%20A%20Comprehensive%20Foundation%201994"
        },
        {
            "id": "19",
            "entry": "[19] Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Proceedings of the International Conference on Machine Learning (ICML), 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaggi%2C%20Martin%20Revisiting%20Frank-Wolfe%3A%20Projection-free%20sparse%20convex%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaggi%2C%20Martin%20Revisiting%20Frank-Wolfe%3A%20Projection-free%20sparse%20convex%20optimization%202013"
        },
        {
            "id": "20",
            "entry": "[20] Michel Journ\u00e9e, Francis Bach, P-A Absil, and Rodolphe Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327\u20132351, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Journ%C3%A9e%2C%20Michel%20Francis%20Bach%2C%20P.-A.Absil%20Sepulchre%2C%20Rodolphe%20Low-rank%20optimization%20on%20the%20cone%20of%20positive%20semidefinite%20matrices%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Journ%C3%A9e%2C%20Michel%20Francis%20Bach%2C%20P.-A.Absil%20Sepulchre%2C%20Rodolphe%20Low-rank%20optimization%20on%20the%20cone%20of%20positive%20semidefinite%20matrices%202010"
        },
        {
            "id": "21",
            "entry": "[21] Harold Kushner and G. George Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kushner%2C%20Harold%20Yin%2C%20G.George%20Stochastic%20approximation%20and%20recursive%20algorithms%20and%20applications%2C%20volume%2035%202003"
        },
        {
            "id": "22",
            "entry": "[22] Jean-Bernard Lasserre. Moments, positive polynomials and their applications, volume 1. World Scientific, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lasserre%2C%20Jean-Bernard%20Moments%2C%20positive%20polynomials%20and%20their%20applications%2C%20volume%201%202010"
        },
        {
            "id": "23",
            "entry": "[23] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. In Advances in Neural Information Processing Systems, pages 597\u2013607, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20ReLU%20activation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yuanzhi%20Yuan%2C%20Yang%20Convergence%20analysis%20of%20two-layer%20neural%20networks%20with%20ReLU%20activation%202017"
        },
        {
            "id": "24",
            "entry": "[24] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013 E7671, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mei%2C%20Song%20Montanari%2C%20Andrea%20Nguyen%2C%20Phan-Minh%20A%20mean%20field%20view%20of%20the%20landscape%20of%20two-layer%20neural%20networks%202018"
        },
        {
            "id": "25",
            "entry": "[25] Atsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for infinite ensembles. arXiv preprint arXiv:1712.05438, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05438"
        },
        {
            "id": "26",
            "entry": "[26] Clarice Poon, Nicolas Keriven, and Gabriel Peyr\u00e9. A dual certificates analysis of compressive off-the-grid recovery. arXiv preprint arXiv:1802.08464, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08464"
        },
        {
            "id": "27",
            "entry": "[27] Ralph T. Rockafellar. Convex Analysis. Princeton University Press, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20Ralph%20T.%20Convex%20Analysis%201997"
        },
        {
            "id": "28",
            "entry": "[28] Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. arXiv preprint arXiv:1805.00915, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.00915"
        },
        {
            "id": "29",
            "entry": "[29] Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Santambrogio%2C%20Filippo%20Optimal%20transport%20for%20applied%20mathematicians%202015"
        },
        {
            "id": "30",
            "entry": "[30] Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin of Mathematical Sciences, 7(1):87\u2013154, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=and%20Wasserstein%7D%20gradient%20flows%3A%20an%20overview%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=and%20Wasserstein%7D%20gradient%20flows%3A%20an%20overview%202017"
        },
        {
            "id": "31",
            "entry": "[31] Damien Scieur, Vincent Roulet, Francis Bach, and Alexandre d\u2019Aspremont. Integration methods and optimization algorithms. In Advances in Neural Information Processing Systems, pages 1109\u20131118, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Damien%20Scieur%20Vincent%20Roulet%20Francis%20Bach%20and%20Alexandre%20dAspremont%20Integration%20methods%20and%20optimization%20algorithms%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2011091118%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Damien%20Scieur%20Vincent%20Roulet%20Francis%20Bach%20and%20Alexandre%20dAspremont%20Integration%20methods%20and%20optimization%20algorithms%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%2011091118%202017"
        },
        {
            "id": "32",
            "entry": "[32] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv preprint arXiv:1805.01053, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.01053"
        },
        {
            "id": "33",
            "entry": "[33] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20Mahdi%20Javanmard%2C%20Adel%20Lee%2C%20Jason%20D.%20Theoretical%20insights%20into%20the%20optimization%20landscape%20of%20over-parameterized%20shallow%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20Mahdi%20Javanmard%2C%20Adel%20Lee%2C%20Jason%20D.%20Theoretical%20insights%20into%20the%20optimization%20landscape%20of%20over-parameterized%20shallow%20neural%20networks%202018"
        },
        {
            "id": "34",
            "entry": "[34] Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv preprint arXiv:1702.05777, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05777"
        },
        {
            "id": "35",
            "entry": "[35] Luca Venturi, Afonso Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network optimization landscapes. arXiv preprint arXiv:1802.06384, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06384"
        },
        {
            "id": "36",
            "entry": "[36] Chu Wang, Yingfei Wang, Robert Schapire, et al. Functional Frank-Wolfe boosting for general loss functions. arXiv preprint arXiv:1510.02558, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1510.02558"
        },
        {
            "id": "37",
            "entry": "[37] Hassler Whitney et al. A function not constant on a connected set of critical points. Duke Mathematical Journal, 1(4):514\u2013517, 1935. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Whitney%2C%20Hassler%20A%20function%20not%20constant%20on%20a%20connected%20set%20of%20critical%20points%201935",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Whitney%2C%20Hassler%20A%20function%20not%20constant%20on%20a%20connected%20set%20of%20critical%20points%201935"
        }
    ]
}
