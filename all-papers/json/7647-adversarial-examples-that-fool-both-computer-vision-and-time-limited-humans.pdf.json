{
    "filename": "7647-adversarial-examples-that-fool-both-computer-vision-and-time-limited-humans.pdf",
    "metadata": {
        "title": "Adversarial Examples that Fool both Computer Vision and Time-Limited Humans",
        "author": "Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7647-adversarial-examples-that-fool-both-computer-vision-and-time-limited-humans.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers."
    },
    "keywords": [
        {
            "term": "Institutional Review Board",
            "url": "https://en.wikipedia.org/wiki/Institutional_Review_Board"
        },
        {
            "term": "object recognition",
            "url": "https://en.wikipedia.org/wiki/object_recognition"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "visual system",
            "url": "https://en.wikipedia.org/wiki/visual_system"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "time limited",
            "url": "https://en.wikipedia.org/wiki/time_limited"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        }
    ],
    "highlights": [
        "Adversarial Examples<br/><br/>Goodfellow et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] define adversarial examples as \u201cinputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.\u201d In the context of visual object recognition, adversarial examples are images usually formed by applying a small perturbation to a naturally occurring image in a way that breaks the predictions made by a machine learning classifier",
        "We construct adversarial examples that transfer from computer vision models to the human visual system",
        "We find that adversarial examples that transfer across computer vision models do successfully influence the perception of human observers, uncovering a new class of illusions that are shared between computer vision models and the human brain.\n2 Background and Related Work\n2.1",
        "Goodfellow et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] define adversarial examples as \u201cinputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.\u201d In the context of visual object recognition, adversarial examples are images usually formed by applying a small perturbation to a naturally occurring image in a way that breaks the predictions made by a machine learning classifier",
        "Adversarial examples are known to transfer across machine learning models, which suggest that these adversarial perturbations may carry information about target adversarial classes",
        "Our findings demonstrate striking similarities between the decision boundaries of convolutional neural networks and the human visual system"
    ],
    "key_statements": [
        "Adversarial Examples<br/><br/>Goodfellow et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] define adversarial examples as \u201cinputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.\u201d In the context of visual object recognition, adversarial examples are images usually formed by applying a small perturbation to a naturally occurring image in a way that breaks the predictions made by a machine learning classifier",
        "We construct adversarial examples that transfer from computer vision models to the human visual system",
        "We find that adversarial examples that transfer across computer vision models do successfully influence the perception of human observers, uncovering a new class of illusions that are shared between computer vision models and the human brain.\n2 Background and Related Work\n2.1",
        "Goodfellow et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] define adversarial examples as \u201cinputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.\u201d In the context of visual object recognition, adversarial examples are images usually formed by applying a small perturbation to a naturally occurring image in a way that breaks the predictions made by a machine learning classifier",
        "See Figure Supp.1a for a canonical example where adding a small perturbation to an image of a panda causes it to be misclassified as a gibbon. This perturbation is small enough to be imperceptible. This perturbation relies on carefully chosen structure based on the parameters of the neural network\u2014but when magnified to be perceptible, human observers cannot recognize any meaningful structure",
        "Adversarial examples are known to transfer across machine learning models, which suggest that these adversarial perturbations may carry information about target adversarial classes",
        "Recent research has found similarities in representation and behavior between deep convolutional neural networks (CNNs) and the primate visual system [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]. This further motivates the possibility that adversarial examples may transfer from computer vision models to humans",
        "We first assess the transfer of our constructed images to two test models that were not included in the ensemble used to generate adversarial examples",
        "We show that adversarial examples transfer to time-limited humans",
        "When averaging across all trials, this effect was very significant for the pets and vegetables group (p < 0.001), and less significant for the hazard group (p = 0.05) (Figure 2b). These results suggest that the direction of the adversarial image perturbation, in combination with a specific image, is perceptually relevant to features that the human visual system uses to classify objects",
        "We did not design controlled experiments to prove that the adversarial examples work in any specific way, but we informally observed a few apparent patterns illustrated in Figure 3: disrupting object edges, especially by mid-frequency modulations perpendicular to the edge; enhancing edges both by increasing contrast and creating texture boundaries; modifying texture; and taking advantage of dark regions in the image, where the perceptual magnitude of small \u270f perturbations can be larger.\n5.3",
        "Our findings demonstrate striking similarities between the decision boundaries of convolutional neural networks and the human visual system"
    ],
    "summary": [
        "Adversarial Examples<br/><br/>Goodfellow et al [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>] define adversarial examples as \u201cinputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.\u201d In the context of visual object recognition, adversarial examples are images usually formed by applying a small perturbation to a naturally occurring image in a way that breaks the predictions made by a machine learning classifier.",
        "The current understanding is that this class of transferable adversarial examples has no effect on human visual perception, yet no thorough empirical investigation has yet been performed.",
        "We construct adversarial examples that transfer from computer vision models to the human visual system.",
        "Adversarial examples are known to transfer across machine learning models, which suggest that these adversarial perturbations may carry information about target adversarial classes.",
        "This further motivates the possibility that adversarial examples may transfer from computer vision models to humans.",
        "For a given image group, we wish to generate targeted adversarial examples that strongly transfer across models.",
        "Adv: we added adversarial perturbation adv to image, crafted to cause machine learning models to misclassify adv as the opposite class in the group.",
        "We might show an airplane adversarially perturbed toward the dog class, while a subject is in a session classifying images as cats or dogs.",
        "We first assess the transfer of our constructed images to two test models that were not included in the ensemble used to generate adversarial examples.",
        "Attacks in the adv and false conditions succeeded against the test models between 57% and 89% of the time, depending on image class and experimental condition.",
        "As described in Section 3.2.2, we used the false condition to test whether adversarial perturbations can influence which of two incorrect classes a subject chooses.",
        "This demonstrates that the adversarial perturbations generated using CNNs biased human perception towards the targeted class.",
        "These results suggest that the direction of the adversarial image perturbation, in combination with a specific image, is perceptually relevant to features that the human visual system uses to classify objects.",
        "The perturbations we made were small enough that they generally do not change the output class for a human who has no time limit.",
        "We did not design controlled experiments to prove that the adversarial examples work in any specific way, but we informally observed a few apparent patterns illustrated in Figure 3: disrupting object edges, especially by mid-frequency modulations perpendicular to the edge; enhancing edges both by increasing contrast and creating texture boundaries; modifying texture; and taking advantage of dark regions in the image, where the perceptual magnitude of small \u270f perturbations can be larger.",
        "We correct this assumption by showing that adversarial examples based on perceptible but classpreserving perturbations that fool multiple machine learning models fool time-limited humans.",
        "We expect this observation to lead to advances in both neuroscience and machine learning research"
    ],
    "headline": "We address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Anish Athalye. Robust adversarial examples, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Athalye%2C%20Anish%20Robust%20adversarial%20examples%202017"
        },
        {
            "id": "2",
            "entry": "[2] Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07397"
        },
        {
            "id": "3",
            "entry": "[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III, pages 387\u2013402, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Biggio%2C%20Battista%20Corona%2C%20Igino%20Maiorca%2C%20Davide%20Nelson%2C%20Blaine%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time%202013-09-23",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Biggio%2C%20Battista%20Corona%2C%20Igino%20Maiorca%2C%20Davide%20Nelson%2C%20Blaine%20Evasion%20attacks%20against%20machine%20learning%20at%20test%20time%202013-09-23"
        },
        {
            "id": "4",
            "entry": "[4] Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09665"
        },
        {
            "id": "5",
            "entry": "[5] Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. International Conference on Learning Representations, 2018. accepted as poster.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Buckman%2C%20Jacob%20Roy%2C%20Aurko%20Raffel%2C%20Colin%20Goodfellow%2C%20Ian%20Thermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Buckman%2C%20Jacob%20Roy%2C%20Aurko%20Raffel%2C%20Colin%20Goodfellow%2C%20Ian%20Thermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%202018"
        },
        {
            "id": "6",
            "entry": "[6] Charles F Cadieu, Ha Hong, Daniel LK Yamins, Nicolas Pinto, Diego Ardila, Ethan A Solomon, Najib J Majaj, and James J DiCarlo. Deep neural networks rival the representation of primate it cortex for core visual object recognition. PLoS computational biology, 10(12):e1003963, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cadieu%2C%20Charles%20F.%20Hong%2C%20Ha%20Yamins%2C%20Daniel%20L.K.%20Pinto%2C%20Nicolas%20Ethan%20A%20Solomon%2C%20Najib%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cadieu%2C%20Charles%20F.%20Hong%2C%20Ha%20Yamins%2C%20Daniel%20L.K.%20Pinto%2C%20Nicolas%20Ethan%20A%20Solomon%2C%20Najib%202014"
        },
        {
            "id": "7",
            "entry": "[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "8",
            "entry": "[8] Miguel P Eckstein, Kathryn Koehler, Lauren E Welbourne, and Emre Akbas. Humans, but not deep neural networks, often miss giant targets in scenes. Current Biology, 27(18):2827\u20132832, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eckstein%2C%20Miguel%20P.%20Koehler%2C%20Kathryn%20Welbourne%2C%20Lauren%20E.%20Akbas%2C%20Emre%20Humans%2C%20but%20not%20deep%20neural%20networks%2C%20often%20miss%20giant%20targets%20in%20scenes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eckstein%2C%20Miguel%20P.%20Koehler%2C%20Kathryn%20Welbourne%2C%20Lauren%20E.%20Akbas%2C%20Emre%20Humans%2C%20but%20not%20deep%20neural%20networks%2C%20often%20miss%20giant%20targets%20in%20scenes%202017"
        },
        {
            "id": "9",
            "entry": "[9] Jeremy Freeman and Eero P Simoncelli. Metamers of the ventral stream. Nature neuroscience, 14(9):1195, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Freeman%2C%20Jeremy%20Simoncelli%2C%20Eero%20P.%20Metamers%20of%20the%20ventral%20stream%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Freeman%2C%20Jeremy%20Simoncelli%2C%20Eero%20P.%20Metamers%20of%20the%20ventral%20stream%202011"
        },
        {
            "id": "10",
            "entry": "[10] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1508.06576"
        },
        {
            "id": "11",
            "entry": "[11] Robert Geirhos, David HJ Janssen, Heiko H Sch\u00fctt, Jonas Rauber, Matthias Bethge, and Felix A Wichmann. Comparing deep neural networks against humans: object recognition when the signal gets weaker. arXiv preprint arXiv:1706.06969, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06969"
        },
        {
            "id": "12",
            "entry": "[12] Ian Goodfellow, Nicolas Papernot, Sandy Huang, Yan Duan, Pieter Abbeel, and Jack Clark. Attacking machine learning with adversarial examples, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20Papernot%2C%20Nicolas%20Huang%2C%20Sandy%20Duan%2C%20Yan%20Attacking%20machine%20learning%20with%20adversarial%20examples%202017"
        },
        {
            "id": "13",
            "entry": "[13] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "14",
            "entry": "[14] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick D. McDaniel. Adversarial examples for malware detection. In ESORICS 2017, pages 62\u201379, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grosse%2C%20Kathrin%20Papernot%2C%20Nicolas%20Manoharan%2C%20Praveen%20Backes%2C%20Michael%20Adversarial%20examples%20for%20malware%20detection%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grosse%2C%20Kathrin%20Papernot%2C%20Nicolas%20Manoharan%2C%20Praveen%20Backes%2C%20Michael%20Adversarial%20examples%20for%20malware%20detection%202017"
        },
        {
            "id": "15",
            "entry": "[15] Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick. Neuroscience-inspired artificial intelligence. Neuron, 95(2):245\u2013258, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hassabis%2C%20Demis%20Kumaran%2C%20Dharshan%20Summerfield%2C%20Christopher%20Botvinick%2C%20Matthew%20Neuroscience-inspired%20artificial%20intelligence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hassabis%2C%20Demis%20Kumaran%2C%20Dharshan%20Summerfield%2C%20Christopher%20Botvinick%2C%20Matthew%20Neuroscience-inspired%20artificial%20intelligence%202017"
        },
        {
            "id": "16",
            "entry": "[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity Mappings in Deep Residual Networks. ArXiv e-prints, March 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20Mappings%20in%20Deep%20Residual%20Networks%202016-03",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Identity%20Mappings%20in%20Deep%20Residual%20Networks%202016-03"
        },
        {
            "id": "17",
            "entry": "[17] James M Hillis, Marc O Ernst, Martin S Banks, and Michael S Landy. Combining sensory information: mandatory fusion within, but not between, senses. Science, 298(5598):1627\u20131630, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hillis%2C%20James%20M.%20Ernst%2C%20Marc%20O.%20Banks%2C%20Martin%20S.%20Landy%2C%20Michael%20S.%20Combining%20sensory%20information%3A%20mandatory%20fusion%20within%2C%20but%20not%20between%2C%20senses%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hillis%2C%20James%20M.%20Ernst%2C%20Marc%20O.%20Banks%2C%20Martin%20S.%20Landy%2C%20Michael%20S.%20Combining%20sensory%20information%3A%20mandatory%20fusion%20within%2C%20but%20not%20between%2C%20senses%202002"
        },
        {
            "id": "18",
            "entry": "[18] Michael Ibbotson and Bart Krekelberg. Visual perception and saccadic eye movements. Current opinion in neurobiology, 21(4):553\u2013558, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ibbotson%2C%20Michael%20Krekelberg%2C%20Bart%20Visual%20perception%20and%20saccadic%20eye%20movements%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ibbotson%2C%20Michael%20Krekelberg%2C%20Bart%20Visual%20perception%20and%20saccadic%20eye%20movements%202011"
        },
        {
            "id": "19",
            "entry": "[19] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00851"
        },
        {
            "id": "20",
            "entry": "[20] GYULA KovAcs, Rufin Vogels, and Guy A Orban. Cortical correlate of pattern backward masking. Proceedings of the National Academy of Sciences, 92(12):5587\u20135591, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KovAcs%2C%20G.Y.U.L.A.%20Rufin%20Vogels%2C%20and%20Guy%20A%20Orban.%20Cortical%20correlate%20of%20pattern%20backward%20masking%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=KovAcs%2C%20G.Y.U.L.A.%20Rufin%20Vogels%2C%20and%20Guy%20A%20Orban.%20Cortical%20correlate%20of%20pattern%20backward%20masking%201995"
        },
        {
            "id": "21",
            "entry": "[21] Matthias K\u00fcmmerer, Lucas Theis, and Matthias Bethge. Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet. arXiv preprint arXiv:1411.1045, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1411.1045"
        },
        {
            "id": "22",
            "entry": "[22] Matthias K\u00fcmmerer, Tom Wallis, and Matthias Bethge. Deepgaze ii: Predicting fixations from deep features over time and tasks. Journal of Vision, 17(10):1147\u20131147, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%C3%BCmmerer%2C%20Matthias%20Wallis%2C%20Tom%20Bethge%2C%20Matthias%20Deepgaze%20ii%3A%20Predicting%20fixations%20from%20deep%20features%20over%20time%20and%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%C3%BCmmerer%2C%20Matthias%20Wallis%2C%20Tom%20Bethge%2C%20Matthias%20Deepgaze%20ii%3A%20Predicting%20fixations%20from%20deep%20features%20over%20time%20and%20tasks%202017"
        },
        {
            "id": "23",
            "entry": "[23] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial Machine Learning at Scale. ArXiv e-prints, November 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20A.%20Goodfellow%2C%20I.%20Bengio%2C%20S.%20Adversarial%20Machine%20Learning%20at%20Scale.%20ArXiv%20e-prints%202016-11"
        },
        {
            "id": "24",
            "entry": "[24] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In ICLR\u20192017 Workshop, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20Bengio%2C%20Samy%20Adversarial%20examples%20in%20the%20physical%20world%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurakin%2C%20Alexey%20Goodfellow%2C%20Ian%20Bengio%2C%20Samy%20Adversarial%20examples%20in%20the%20physical%20world%202016"
        },
        {
            "id": "25",
            "entry": "[25] Michael F Land and Dan-Eric Nilsson. Animal eyes. Oxford University Press, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Michael%2C%20F.%20Land%20and%20Dan-Eric%20Nilsson.%20Animal%20eyes%202012"
        },
        {
            "id": "26",
            "entry": "[26] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.02770"
        },
        {
            "id": "27",
            "entry": "[27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.06083"
        },
        {
            "id": "28",
            "entry": "[28] Lane McIntosh, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, and Stephen Baccus. Deep learning models of the retinal response to natural scenes. In Advances in neural information processing systems, pages 1369\u20131377, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McIntosh%2C%20Lane%20Maheswaranathan%2C%20Niru%20Nayebi%2C%20Aran%20Ganguli%2C%20Surya%20Deep%20learning%20models%20of%20the%20retinal%20response%20to%20natural%20scenes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McIntosh%2C%20Lane%20Maheswaranathan%2C%20Niru%20Nayebi%2C%20Aran%20Ganguli%2C%20Surya%20Deep%20learning%20models%20of%20the%20retinal%20response%20to%20natural%20scenes%202016"
        },
        {
            "id": "29",
            "entry": "[29] Bruno A Olshausen. 20 years of learning about vision: Questions answered, questions unanswered, and questions not yet asked. In 20 Years of Computational Neuroscience, pages 243\u2013270.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Olshausen%2C%20Bruno%20A.%20years%20of%20learning%20about%20vision%3A%20Questions%20answered%2C%20questions%20unanswered%2C%20and%20questions%20not%20yet%20asked.%20In%2020%20Years%20of%20Computational%20Neuroscience%2020"
        },
        {
            "id": "30",
            "entry": "[30] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.07277"
        },
        {
            "id": "31",
            "entry": "[31] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pages 506\u2013519. ACM, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Goodfellow%2C%20Ian%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "32",
            "entry": "[32] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pages 582\u2013597. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20Wu%2C%20Xi%20Jha%2C%20Somesh%20Distillation%20as%20a%20defense%20to%20adversarial%20perturbations%20against%20deep%20neural%20networks%202016"
        },
        {
            "id": "33",
            "entry": "[33] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. CoRR, abs/1511.07528, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.07528"
        },
        {
            "id": "34",
            "entry": "[34] Mary C Potter, Brad Wyble, Carl Erick Hagmann, and Emily S McCourt. Detecting meaning in rsvp at 13 ms per picture. Attention, Perception, & Psychophysics, 76(2):270\u2013279, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Potter%2C%20Mary%20C.%20Wyble%2C%20Brad%20Hagmann%2C%20Carl%20Erick%20McCourt%2C%20Emily%20S.%20Detecting%20meaning%20in%20rsvp%20at%2013%20ms%20per%20picture%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Potter%2C%20Mary%20C.%20Wyble%2C%20Brad%20Hagmann%2C%20Carl%20Erick%20McCourt%2C%20Emily%20S.%20Detecting%20meaning%20in%20rsvp%20at%2013%20ms%20per%20picture%202014"
        },
        {
            "id": "35",
            "entry": "[35] Rishi Rajalingham, Elias B. Issa, Pouya Bashivan, Kohitij Kar, Kailyn Schmidt, and James J DiCarlo. Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks. bioRxiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rajalingham%2C%20Rishi%20Issa%2C%20Elias%20B.%20Bashivan%2C%20Pouya%20Kar%2C%20Kohitij%20Large-scale%2C%20high-resolution%20comparison%20of%20the%20core%20visual%20object%20recognition%20behavior%20of%20humans%2C%20monkeys%2C%20and%20state-of-the-art%20deep%20artificial%20neural%20networks%202018"
        },
        {
            "id": "36",
            "entry": "[36] Maximilian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in cortex. Nature neuroscience, 2(11):1019, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Riesenhuber%2C%20Maximilian%20Poggio%2C%20Tomaso%20Hierarchical%20models%20of%20object%20recognition%20in%20cortex%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Riesenhuber%2C%20Maximilian%20Poggio%2C%20Tomaso%20Hierarchical%20models%20of%20object%20recognition%20in%20cortex%201999"
        },
        {
            "id": "37",
            "entry": "[37] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. ArXiv e-prints, February 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Ioffe%2C%20S.%20Vanhoucke%2C%20V.%20Inception-v4%2C%20A.Alemi%20Inception-ResNet%20and%20the%20Impact%20of%20Residual%20Connections%20on%20Learning.%20ArXiv%20e-prints%202016-02"
        },
        {
            "id": "38",
            "entry": "[38] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception Architecture for Computer Vision. ArXiv e-prints, December 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20Inception%20Architecture%20for%20Computer%20Vision.%20ArXiv%20e-prints%202015-12"
        },
        {
            "id": "39",
            "entry": "[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6199"
        },
        {
            "id": "40",
            "entry": "[40] F. Tram\u00e8r, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble Adversarial Training: Attacks and Defenses. ArXiv e-prints, May 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tram%C3%A8r%2C%20F.%20Kurakin%2C%20A.%20Papernot%2C%20N.%20Boneh%2C%20D.%20Ensemble%20Adversarial%20Training%3A%20Attacks%20and%20Defenses.%20ArXiv%20e-prints%202017-05"
        },
        {
            "id": "41",
            "entry": "[41] D. C. Van Essen and C. H Anderson. Information processing strategies and pathways in the primate visual system. In Zornetzer S. F., Davis J. L., Lau C., and McKenna T., editors, An introduction to neural and electronic networks, page 45\u201376, San Diego, CA, 1995. Academic Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Essen%2C%20D.C.Van%20Anderson%2C%20C.H.%20Information%20processing%20strategies%20and%20pathways%20in%20the%20primate%20visual%20system",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Essen%2C%20D.C.Van%20Anderson%2C%20C.H.%20Information%20processing%20strategies%20and%20pathways%20in%20the%20primate%20visual%20system"
        },
        {
            "id": "42",
            "entry": "[42] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.01155"
        },
        {
            "id": "43",
            "entry": "[43] Daniel L. K. Yamins and James J. DiCarlo. Using goal-driven deep learning models to understand sensory cortex. Nature Neuroscience, 19:356\u2013365, 2016. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yamins%2C%20Daniel%20L.K.%20DiCarlo%2C%20James%20J.%20Using%20goal-driven%20deep%20learning%20models%20to%20understand%20sensory%20cortex%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yamins%2C%20Daniel%20L.K.%20DiCarlo%2C%20James%20J.%20Using%20goal-driven%20deep%20learning%20models%20to%20understand%20sensory%20cortex%202016"
        }
    ]
}
