{
    "filename": "7791-algorithmic-assurance-an-active-approach-to-algorithmic-testing-using-bayesian-optimisation.pdf",
    "metadata": {
        "title": "Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation",
        "author": "Shivapratap Gopakumar, Sunil Gupta, Santu Rana, Vu Nguyen, Svetha Venkatesh",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7791-algorithmic-assurance-an-active-approach-to-algorithmic-testing-using-bayesian-optimisation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce algorithmic assurance, the problem of testing whether machine learning algorithms are conforming to their intended design goal. We address this problem by proposing an efficient framework for algorithmic testing. To provide assurance, we need to efficiently discover scenarios where an algorithm decision deviates maximally from its intended gold standard. We mathematically formulate this task as an optimisation problem of an expensive, black-box function. We use an active learning approach based on Bayesian optimisation to solve this optimisation problem. We extend this framework to algorithms with vector-valued outputs by making appropriate modification in Bayesian optimisation via the EXP3 algorithm. We theoretically analyse our methods for convergence. Using two real-world applications, we demonstrate the efficiency of our methods. The significance of our problem formulation and initial solutions is that it will serve as the foundation in assuring humans about machines making complex decisions."
    },
    "keywords": [
        {
            "term": "gold standard",
            "url": "https://en.wikipedia.org/wiki/gold_standard"
        },
        {
            "term": "bayesian optimization",
            "url": "https://en.wikipedia.org/wiki/bayesian_optimization"
        },
        {
            "term": "decision process",
            "url": "https://en.wikipedia.org/wiki/decision_process"
        },
        {
            "term": "Australian Research Council",
            "url": "https://en.wikipedia.org/wiki/Australian_Research_Council"
        }
    ],
    "highlights": [
        "Supervised learning algorithms today serve as proxies for decision processes traditionally performed by humans",
        "If an algorithm is built to serve as a proxy for this decision process, can we provide assurance that the difference in the decision made by the algorithm and the metallurgist is within a stipulated bound? if an algorithm has been trained to recognize digits, can we ensure that the recognition error of the algorithm is acceptable across all allowable visual variations within which a human can recognise digits correctly? To provide such assurance we need to compare an algorithm against its gold standard and find the maximum deviation",
        "We develop a Bayesian Optimisation (BO) framework to efficiently discover the scenario wherein an algorithm maximally deviates from its gold standard",
        "We have introduced a novel problem of algorithmic assurance to assess the deviation of an algorithm from its intended use",
        "We have developed an efficient framework for algorithmic testing for singletask and multi-task settings",
        "In the modern era of artificial intelligence, algorithms are increasingly taking decisions pertinent to our life, it is very timely to build the confidence that algorithms can be trusted and our proposed algorithmic assurance framework is an early attempt towards this goal"
    ],
    "key_statements": [
        "Supervised learning algorithms today serve as proxies for decision processes traditionally performed by humans",
        "If an algorithm is built to serve as a proxy for this decision process, can we provide assurance that the difference in the decision made by the algorithm and the metallurgist is within a stipulated bound? if an algorithm has been trained to recognize digits, can we ensure that the recognition error of the algorithm is acceptable across all allowable visual variations within which a human can recognise digits correctly? To provide such assurance we need to compare an algorithm against its gold standard and find the maximum deviation",
        "We develop a Bayesian Optimisation (BO) framework to efficiently discover the scenario wherein an algorithm maximally deviates from its gold standard",
        "We model f (x) using a Gaussian process [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>], and its predictive distribution is used to predict the deviation of the algorithm from the gold standard along with any epistemic uncertainty",
        "For our multi-task algorithmic testing framework we use EXP3 algorithm [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] which is capable of working in a partial reward feedback scenario",
        "We have introduced a novel problem of algorithmic assurance to assess the deviation of an algorithm from its intended use",
        "We have developed an efficient framework for algorithmic testing for singletask and multi-task settings",
        "In the modern era of artificial intelligence, algorithms are increasingly taking decisions pertinent to our life, it is very timely to build the confidence that algorithms can be trusted and our proposed algorithmic assurance framework is an early attempt towards this goal"
    ],
    "summary": [
        "Supervised learning algorithms today serve as proxies for decision processes traditionally performed by humans.",
        "Bayesian optimisation is one such efficient active learning method with a convergence guarantee on the average regret as O where T is the number of iterations/samples.",
        "We develop a Bayesian Optimisation (BO) framework to efficiently discover the scenario wherein an algorithm maximally deviates from its gold standard.",
        "We demonstrate our framework on two problems: Prediction of strength-determining phases in an alloy design process, and recognition of handwritten digits under visual distortions.",
        "In our proposed algorithmic testing framework, our goal is to efficiently identify a scenario x\u2217 wherein the algorithm output A (x\u2217) maximally deviates from the function a(x\u2217).",
        "In our proposed multi-task algorithmic testing, we aim to efficiently discover the scenario wherein the algorithm maximally differs from the true function for any of the outputs or tasks.",
        "For our multi-task algorithmic testing framework we use EXP3 algorithm [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] which is capable of working in a partial reward feedback scenario.",
        "The probability vector pt = [pt1, ..., pCt ] indicates the promise of different tasks for obtaining high values and is used to select a function for performing Bayesian optimisation.",
        "We evaluate single and multi-task assurance using the two real world applications: (1) Alloy design, and (2) hand written digit recognition.",
        "We use this neural network model for single and multi-task assurance.",
        "We use EXP3BO for multi-task assurance to discover the composition with the maximal deviation from Thermocalc for any alloy phase.",
        "To evaluate the optimisation efficiency of our proposed EXP3BO algorithm we compare it against the baselines - Round-robin BO, Random",
        "We found the maximal error for \u201cAL12MN\u201d phase for RMSE=1.05, at a substantially different element composition compared to the one found during the algorithm training stage.",
        "We construct a proxy algorithm for recognising digits and the task is to identify the level of distortion causing the largest error in a transformed MNIST[<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] dataset.",
        "We run BO for our single task assurance to discover the distortion for maximal recognition error.",
        "We use EXP3BO for multi-task assurance to discover the maximal recognition error for any digits.",
        "This is important Figure 5 \u2013 Single task assurance for digit because the error in recognising each digit may differ recognitionoptimisation results showing depending on its visual complexity and distortion.",
        "We have developed an efficient framework for algorithmic testing for singletask and multi-task settings.",
        "The usefulness of our framework is demonstrated on two problems: prediction of strength-determining phases in alloy design and recognition of handwritten digits under shear and rotation distortions.",
        "In the modern era of artificial intelligence, algorithms are increasingly taking decisions pertinent to our life, it is very timely to build the confidence that algorithms can be trusted and our proposed algorithmic assurance framework is an early attempt towards this goal"
    ],
    "headline": "The problem of testing whether machine learning algorithms are conforming to their intended design goal",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235\u2013256, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Fischer%2C%20P.%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Fischer%2C%20P.%20Finite-time%20analysis%20of%20the%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "2",
            "entry": "[2] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48\u201377, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "3",
            "entry": "[3] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281\u2013305, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20J.%20Bengio%2C%20Y.%20Random%20search%20for%20hyper-parameter%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20J.%20Bengio%2C%20Y.%20Random%20search%20for%20hyper-parameter%20optimization%202012"
        },
        {
            "id": "4",
            "entry": "[4] A. D. Bull. Convergence rates of efficient global optimization algorithms. The Journal of Machine Learning Research, 12:2879\u20132904, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bull%2C%20A.D.%20Convergence%20rates%20of%20efficient%20global%20optimization%20algorithms%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bull%2C%20A.D.%20Convergence%20rates%20of%20efficient%20global%20optimization%20algorithms%202011"
        },
        {
            "id": "5",
            "entry": "[5] T. Dai Nguyen, S. Gupta, S. Rana, and S. Venkatesh. Stable bayesian optimization. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 578\u2013591.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20T.Dai%20Gupta%2C%20S.%20Rana%2C%20S.%20Venkatesh%2C%20S.%20Stable%20bayesian%20optimization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20T.Dai%20Gupta%2C%20S.%20Rana%2C%20S.%20Venkatesh%2C%20S.%20Stable%20bayesian%20optimization"
        },
        {
            "id": "6",
            "entry": "[6] P. Hennig and C. J. Schuler. Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 13:1809\u20131837, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hennig%2C%20P.%20Schuler%2C%20C.J.%20Entropy%20search%20for%20information-efficient%20global%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hennig%2C%20P.%20Schuler%2C%20C.J.%20Entropy%20search%20for%20information-efficient%20global%20optimization%202012"
        },
        {
            "id": "7",
            "entry": "[7] J. M. Hern\u00e1ndez-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918\u2013926, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hern%C3%A1ndez-Lobato%2C%20J.M.%20Hoffman%2C%20M.W.%20Ghahramani%2C%20Z.%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hern%C3%A1ndez-Lobato%2C%20J.M.%20Hoffman%2C%20M.W.%20Ghahramani%2C%20Z.%20Predictive%20entropy%20search%20for%20efficient%20global%20optimization%20of%20black-box%20functions%202014"
        },
        {
            "id": "8",
            "entry": "[8] M. Hoffman, E. Brochu, and N. de Freitas. Portfolio allocation for bayesian optimization. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 327\u2013336. AUAI Press, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20M.%20Brochu%2C%20E.%20de%20Freitas%2C%20N.%20Portfolio%20allocation%20for%20bayesian%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20M.%20Brochu%2C%20E.%20de%20Freitas%2C%20N.%20Portfolio%20allocation%20for%20bayesian%20optimization%202011"
        },
        {
            "id": "9",
            "entry": "[9] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Learning and Intelligent Optimization, pages 507\u2013523.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hutter%2C%20F.%20Hoos%2C%20H.H.%20Leyton-Brown%2C%20K.%20Sequential%20model-based%20optimization%20for%20general%20algorithm%20configuration",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hutter%2C%20F.%20Hoos%2C%20H.H.%20Leyton-Brown%2C%20K.%20Sequential%20model-based%20optimization%20for%20general%20algorithm%20configuration"
        },
        {
            "id": "10",
            "entry": "[10] D. R. Jones, M. Schonlau, and W. J. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455\u2013492, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jones%2C%20D.R.%20Schonlau%2C%20M.%20Welch%2C%20W.J.%20Efficient%20global%20optimization%20of%20expensive%20black-box%20functions%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jones%2C%20D.R.%20Schonlau%2C%20M.%20Welch%2C%20W.J.%20Efficient%20global%20optimization%20of%20expensive%20black-box%20functions%201998"
        },
        {
            "id": "11",
            "entry": "[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "12",
            "entry": "[12] J. Nogueira, R. Martinez-Cantin, A. Bernardino, and L. Jamone. Unscented bayesian optimization for safe robot grasping. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pages 1967\u20131972. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nogueira%2C%20J.%20Martinez-Cantin%2C%20R.%20Bernardino%2C%20A.%20Jamone%2C%20L.%20Unscented%20bayesian%20optimization%20for%20safe%20robot%20grasping%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nogueira%2C%20J.%20Martinez-Cantin%2C%20R.%20Bernardino%2C%20A.%20Jamone%2C%20L.%20Unscented%20bayesian%20optimization%20for%20safe%20robot%20grasping%202016"
        },
        {
            "id": "13",
            "entry": "[13] S. Rana, C. Li, S. Gupta, V. Nguyen, and S. Venkatesh. High dimensional Bayesian optimization with elastic gaussian process. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 2883\u20132891, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rana%2C%20S.%20Li%2C%20C.%20Gupta%2C%20S.%20Nguyen%2C%20V.%20High%20dimensional%20Bayesian%20optimization%20with%20elastic%20gaussian%20process%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rana%2C%20S.%20Li%2C%20C.%20Gupta%2C%20S.%20Nguyen%2C%20V.%20High%20dimensional%20Bayesian%20optimization%20with%20elastic%20gaussian%20process%202017"
        },
        {
            "id": "14",
            "entry": "[14] C. E. Rasmussen. The infinite gaussian mixture model. In NIPS, volume 12, pages 554\u2013560, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20The%20infinite%20gaussian%20mixture%20model%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20C.E.%20The%20infinite%20gaussian%20mixture%20model%201999"
        },
        {
            "id": "15",
            "entry": "[15] C. E. Rasmussen. Gaussian processes for machine learning. Citeseer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20C.E.%20Gaussian%20processes%20for%20machine%20learning%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rasmussen%2C%20C.E.%20Gaussian%20processes%20for%20machine%20learning%202006"
        },
        {
            "id": "16",
            "entry": "[16] Y. Seldin, C. Szepesv\u00e1ri, P. Auer, and Y. Abbasi-Yadkori. Evaluation and analysis of the performance of the exp3 algorithm in stochastic environments. In EWRL, pages 103\u2013116, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seldin%2C%20Y.%20Szepesv%C3%A1ri%2C%20C.%20Auer%2C%20P.%20Abbasi-Yadkori%2C%20Y.%20Evaluation%20and%20analysis%20of%20the%20performance%20of%20the%20exp3%20algorithm%20in%20stochastic%20environments%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seldin%2C%20Y.%20Szepesv%C3%A1ri%2C%20C.%20Auer%2C%20P.%20Abbasi-Yadkori%2C%20Y.%20Evaluation%20and%20analysis%20of%20the%20performance%20of%20the%20exp3%20algorithm%20in%20stochastic%20environments%202012"
        },
        {
            "id": "17",
            "entry": "[17] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20J.%20Larochelle%2C%20H.%20Adams%2C%20R.P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "18",
            "entry": "[18] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Prabhat, and R. Adams. Scalable bayesian optimization using deep neural networks. In Proceedings of the 32nd International Conference on Machine Learning, pages 2171\u20132180, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20J.%20Rippel%2C%20O.%20Swersky%2C%20K.%20Kiros%2C%20R.%20Scalable%20bayesian%20optimization%20using%20deep%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20J.%20Rippel%2C%20O.%20Swersky%2C%20K.%20Kiros%2C%20R.%20Scalable%20bayesian%20optimization%20using%20deep%20neural%20networks%202015"
        },
        {
            "id": "19",
            "entry": "[19] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015\u20131022, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srinivas%2C%20N.%20Krause%2C%20A.%20Kakade%2C%20S.%20Seeger%2C%20M.%20Gaussian%20process%20optimization%20in%20the%20bandit%20setting%3A%20No%20regret%20and%20experimental%20design%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srinivas%2C%20N.%20Krause%2C%20A.%20Kakade%2C%20S.%20Seeger%2C%20M.%20Gaussian%20process%20optimization%20in%20the%20bandit%20setting%3A%20No%20regret%20and%20experimental%20design%202010"
        },
        {
            "id": "20",
            "entry": "[20] N. Srinivas, A. Krause, S. M. Kakade, and M. W. Seeger. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory, 58(5):3250\u20133265, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srinivas%2C%20N.%20Krause%2C%20A.%20Kakade%2C%20S.M.%20Seeger%2C%20M.W.%20Information-theoretic%20regret%20bounds%20for%20gaussian%20process%20optimization%20in%20the%20bandit%20setting%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srinivas%2C%20N.%20Krause%2C%20A.%20Kakade%2C%20S.M.%20Seeger%2C%20M.W.%20Information-theoretic%20regret%20bounds%20for%20gaussian%20process%20optimization%20in%20the%20bandit%20setting%202012"
        },
        {
            "id": "21",
            "entry": "[21] K. Swersky, J. Snoek, and R. P. Adams. Multi-task Bayesian optimization. In Advances in neural information processing systems, pages 2004\u20132012, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Swersky%2C%20K.%20Snoek%2C%20J.%20Adams%2C%20R.P.%20Multi-task%20Bayesian%20optimization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Swersky%2C%20K.%20Snoek%2C%20J.%20Adams%2C%20R.P.%20Multi-task%20Bayesian%20optimization%202004"
        }
    ]
}
