{
    "filename": "7621-escaping-saddle-points-in-constrained-optimization.pdf",
    "metadata": {
        "title": "Escaping Saddle Points in Constrained Optimization",
        "author": "Aryan Mokhtari, Asuman Ozdaglar, Ali Jadbabaie",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7621-escaping-saddle-points-in-constrained-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we study the problem of escaping from saddle points in smooth nonconvex optimization problems subject to a convex set C. We propose a generic framework that yields convergence to a second-order stationary point of the problem, if the convex set C is simple for a quadratic objective function. Specifically, our results hold if one can find a \u21e2-approximate solution of a quadratic program subject to C in polynomial time, where \u21e2 < 1 is a positive constant that depends on the structure of the set C. Under this condition, we show that the sequence of iterates generated by the proposed framework reaches an (\u270f, )-second order stationary point (SOSP) in at most O {\u270f 2, \u21e2 3 3} iterations. We further"
    },
    "keywords": [
        {
            "term": "complexity analysis",
            "url": "https://en.wikipedia.org/wiki/complexity_analysis"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "trust region",
            "url": "https://en.wikipedia.org/wiki/trust_region"
        },
        {
            "term": "saddle point",
            "url": "https://en.wikipedia.org/wiki/saddle_point"
        },
        {
            "term": "quadratic program",
            "url": "https://en.wikipedia.org/wiki/quadratic_program"
        },
        {
            "term": "nonlinear programming",
            "url": "https://en.wikipedia.org/wiki/nonlinear_programming"
        }
    ],
    "highlights": [
        "There has been a recent revival of interest in non-convex optimization, due to obvious applications in machine learning",
        "We show that the proposed approach leads to an (\u270f, )-second-order stationary point (SOSP) for Problem (1)",
        "As in the unconstrained setting, the first-order and second-order optimality conditions may not be satisfied in finite number of iterations, and we focus on finding an approximate second-order stationary point",
        "In Section 4, we study in detail projected gradient descent and conditional gradient algorithms for the first order phase of the proposed framework",
        "We show that the number of required iterations for projected gradient descent to reach an \u270f-first-order stationary points is of O(\u270f 2)",
        "We study the second stage of the framework in Algorithm 1 which corresponds to the case that the current iterate is an \u270f-first-order stationary points"
    ],
    "key_statements": [
        "There has been a recent revival of interest in non-convex optimization, due to obvious applications in machine learning",
        "We show that the proposed approach leads to an (\u270f, )-second-order stationary point (SOSP) for Problem (1)",
        "As in the unconstrained setting, the first-order and second-order optimality conditions may not be satisfied in finite number of iterations, and we focus on finding an approximate second-order stationary point",
        "In Section 4, we study in detail projected gradient descent and conditional gradient algorithms for the first order phase of the proposed framework",
        "We show that the number of required iterations for projected gradient descent to reach an \u270f-first-order stationary points is of O(\u270f 2)",
        "We study the second stage of the framework in Algorithm 1 which corresponds to the case that the current iterate is an \u270f-first-order stationary points"
    ],
    "summary": [
        "There has been a recent revival of interest in non-convex optimization, due to obvious applications in machine learning.",
        "As in the unconstrained setting, the first-order and second-order optimality conditions may not be satisfied in finite number of iterations, and we focus on finding an approximate SOSP.",
        "!R order and the convex stationary point of Problem (1) if the following conditions are satisfied.",
        "Objective function value of the program in (13), we focus on the cases that we can obtain a feasible point ut which is a \u21e2-approximate solution of Problem (13), i.e., ut 2 C satisfies the constraints in (13) and q u\u21e4 ()",
        "After computing a feasible point ut satisfying the condition in (14), we check the quadratic objective function value at the point ut, and if the inequality q < \u21e2 holds, we follow the update xt+1 = (1 )xt + ut, (15)",
        "Note that Algorithm 1 stops if we reach a point xt that satisfies the first-order stationary condition rf>(x xt) \u270f, and the objective function value for the \u21e2-approximate solution of the quadratic subproblem is larger than \u21e2 , i.e., q \u21e2 .",
        "To prove the claim in Theorem 1, we first review first-order conditional gradient and projected gradient algorithms and show that if the current iterate is not a first-order stationary point, by following either of these updates the objective function value decreases by a constant of O \u270f2 (Section 4).",
        "Focus on the second stage of Algorithm 1 which corresponds to the case that the current iterate is an \u270f-FOSP and we need to solve the quadratic program in (13) approximately (Section 5).",
        "We show that if the current iterate is not an \u270f-first order stationary point, by updating the variable according to (18)-(19) the objective function value decreases.",
        "If the iterate xt at step t is not an \u270f-first order stationary point, the objective function value at the updated variable xt+1 satisfies the inequality f",
        "The result in Proposition 2 shows that by following the update of the conditional gradient method the objective function value decreases by O(\u270f2), if an \u270f-FOSP is not achieved.",
        "By checking the first-order optimality condition of xt, the variable vt is already computed, and we need to solve only one linear program per iteration.",
        "We show that the updated variable according to (15) decreases the objective function value if the condition q < \u21e2 holds."
    ],
    "headline": "We study the problem of escaping from saddle points in smooth nonconvex optimization problems subject to a convex set C",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] N. Agarwal, Z. Allen Zhu, B. Bullins, E. Hazan, and T. Ma. Finding approximate local minima faster than gradient descent. In STOC, pages 1195\u20131199, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20N.%20Zhu%2C%20Z.Allen%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20N.%20Zhu%2C%20Z.Allen%20Bullins%2C%20B.%20Hazan%2C%20E.%20Finding%20approximate%20local%20minima%20faster%20than%20gradient%20descent%202017"
        },
        {
            "id": "2",
            "entry": "[2] Z. Allen-Zhu. Natasha 2: Faster non-convex optimization than SGD. CoRR, abs/1708.08694, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.08694"
        },
        {
            "id": "3",
            "entry": "[3] Z. Allen Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In ICML, pages 699\u2013707, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Z.Allen%20Hazan%2C%20E.%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Z.Allen%20Hazan%2C%20E.%20Variance%20reduction%20for%20faster%20non-convex%20optimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] D. P. Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20D.P.%20Nonlinear%20programming.%20Athena%20scientific%201999"
        },
        {
            "id": "5",
            "entry": "[5] W. Bian, X. Chen, and Y. Ye. Complexity analysis of interior point algorithms for non-lipschitz and nonconvex minimization. Math. Program., 149(1-2):301\u2013327, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bian%2C%20W.%20Chen%2C%20X.%20Ye%2C%20Y.%20Complexity%20analysis%20of%20interior%20point%20algorithms%20for%20non-lipschitz%20and%20nonconvex%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bian%2C%20W.%20Chen%2C%20X.%20Ye%2C%20Y.%20Complexity%20analysis%20of%20interior%20point%20algorithms%20for%20non-lipschitz%20and%20nonconvex%20minimization%202015"
        },
        {
            "id": "6",
            "entry": "[6] J. V. Burke, J. J. More, and G. Toraldo. Convergence properties of trust region methods for linear and convex constraints. Math. Program., 47(1-3):305\u2013336, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Burke%2C%20J.V.%20More%2C%20J.J.%20Toraldo%2C%20G.%20Convergence%20properties%20of%20trust%20region%20methods%20for%20linear%20and%20convex%20constraints%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Burke%2C%20J.V.%20More%2C%20J.J.%20Toraldo%2C%20G.%20Convergence%20properties%20of%20trust%20region%20methods%20for%20linear%20and%20convex%20constraints%201990"
        },
        {
            "id": "7",
            "entry": "[7] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Accelerated methods for non-convex optimization. CoRR, abs/1611.00756, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.00756"
        },
        {
            "id": "8",
            "entry": "[8] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. \u201dconvex until proven guilty\u201d: Dimension-free acceleration of gradient descent on non-convex functions. In ICML, pages 654\u2013663, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20A.%20Sidford.%E2%80%9Dconvex%20until%20proven%20guilty%E2%80%9D%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carmon%2C%20Y.%20Duchi%2C%20J.C.%20Hinder%2C%20O.%20A.%20Sidford.%E2%80%9Dconvex%20until%20proven%20guilty%E2%80%9D%3A%20Dimension-free%20acceleration%20of%20gradient%20descent%20on%20non-convex%20functions%202017"
        },
        {
            "id": "9",
            "entry": "[9] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points i. arXiv preprint arXiv:1710.11606, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11606"
        },
        {
            "id": "10",
            "entry": "[10] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower bounds for finding stationary points ii: First-order methods. arXiv preprint arXiv:1711.00841, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00841"
        },
        {
            "id": "11",
            "entry": "[11] C. Cartis, N. Gould, and P. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part I: motivation, convergence and numerical results. Math. Program., 127(2):245\u2013295, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20I%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20I%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011"
        },
        {
            "id": "12",
            "entry": "[12] C. Cartis, N. Gould, and P. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part II: worst-case functionand derivative-evaluation complexity. Math. Program., 130(2):295\u2013319, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20II%3A%20worst-case%20functionand%20derivative-evaluation%20complexity%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20II%3A%20worst-case%20functionand%20derivative-evaluation%20complexity%202011"
        },
        {
            "id": "13",
            "entry": "[13] C. Cartis, N. Gould, and P. Toint. Complexity bounds for second-order optimality in unconstrained optimization. J. Complexity, 28(1):93\u2013108, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Complexity%20bounds%20for%20second-order%20optimality%20in%20unconstrained%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Complexity%20bounds%20for%20second-order%20optimality%20in%20unconstrained%20optimization%202012"
        },
        {
            "id": "14",
            "entry": "[14] C. Cartis, N. Gould, and P. Toint. An adaptive cubic regularization algorithm for nonconvex optimization with convex constraints and its function-evaluation complexity. IMA Journal of Numerical Analysis, 32(4): 1662\u20131695, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20An%20adaptive%20cubic%20regularization%20algorithm%20for%20nonconvex%20optimization%20with%20convex%20constraints%20and%20its%20function-evaluation%20complexity%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20An%20adaptive%20cubic%20regularization%20algorithm%20for%20nonconvex%20optimization%20with%20convex%20constraints%20and%20its%20function-evaluation%20complexity%202012"
        },
        {
            "id": "15",
            "entry": "[15] C. Cartis, N. Gould, and P. Toint. On the evaluation complexity of cubic regularization methods for potentially rank-deficient nonlinear least-squares problems and its relevance to constrained nonlinear optimization. SIAM J. Opt., 23(3):1553\u20131574, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20On%20the%20evaluation%20complexity%20of%20cubic%20regularization%20methods%20for%20potentially%20rank-deficient%20nonlinear%20least-squares%20problems%20and%20its%20relevance%20to%20constrained%20nonlinear%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20On%20the%20evaluation%20complexity%20of%20cubic%20regularization%20methods%20for%20potentially%20rank-deficient%20nonlinear%20least-squares%20problems%20and%20its%20relevance%20to%20constrained%20nonlinear%20optimization%202013"
        },
        {
            "id": "16",
            "entry": "[16] C. Cartis, N. Gould, and P. Toint. On the evaluation complexity of constrained nonlinear least-squares and general constrained nonlinear optimization using second-order methods. SIAM Journal on Numerical Analysis, 53(2):836\u2013851, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20On%20the%20evaluation%20complexity%20of%20constrained%20nonlinear%20least-squares%20and%20general%20constrained%20nonlinear%20optimization%20using%20second-order%20methods%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20On%20the%20evaluation%20complexity%20of%20constrained%20nonlinear%20least-squares%20and%20general%20constrained%20nonlinear%20optimization%20using%20second-order%20methods%202015"
        },
        {
            "id": "17",
            "entry": "[17] C. Cartis, N. Gould, and P. Toint. Second-order optimality and beyond: Characterization and evaluation complexity in convexly constrained nonlinear optimization. Foundations of Computational Mathematics, pages 1\u201335, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Second-order%20optimality%20and%20beyond%3A%20Characterization%20and%20evaluation%20complexity%20in%20convexly%20constrained%20nonlinear%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.%20Toint%2C%20P.%20Second-order%20optimality%20and%20beyond%3A%20Characterization%20and%20evaluation%20complexity%20in%20convexly%20constrained%20nonlinear%20optimization%202017"
        },
        {
            "id": "18",
            "entry": "[18] A. R. Conn, N. Gould, A. Sartenaer, and P. Toint. Global convergence of a class of trust region algorithms for optimization using inexact projections on convex constraints. SIAM J. on Opt., 3(1):164\u2013221, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conn%2C%20A.R.%20Gould%2C%20N.%20Sartenaer%2C%20A.%20Toint%2C%20P.%20Global%20convergence%20of%20a%20class%20of%20trust%20region%20algorithms%20for%20optimization%20using%20inexact%20projections%20on%20convex%20constraints%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conn%2C%20A.R.%20Gould%2C%20N.%20Sartenaer%2C%20A.%20Toint%2C%20P.%20Global%20convergence%20of%20a%20class%20of%20trust%20region%20algorithms%20for%20optimization%20using%20inexact%20projections%20on%20convex%20constraints%201993"
        },
        {
            "id": "19",
            "entry": "[19] F. E. Curtis, D. P. Robinson, and M. Samadi. A trust region algorithm with a worst-case iteration complexity of \\mathcal {O}(\\epsilon{-3/2}) for nonconvex optimization. Math. Program., 162(1-2):1\u201332, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Curtis%2C%20F.E.%20Robinson%2C%20D.P.%20Samadi%2C%20M.%20A%20trust%20region%20algorithm%20with%20a%20worst-case%20iteration%20complexity%20of%20%5Cmathcal%20%7BO%7D%28%5Cepsilon%7B-3/2%7D%29%20for%20nonconvex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Curtis%2C%20F.E.%20Robinson%2C%20D.P.%20Samadi%2C%20M.%20A%20trust%20region%20algorithm%20with%20a%20worst-case%20iteration%20complexity%20of%20%5Cmathcal%20%7BO%7D%28%5Cepsilon%7B-3/2%7D%29%20for%20nonconvex%20optimization%202017"
        },
        {
            "id": "20",
            "entry": "[20] G. Di Pillo, S. Lucidi, and L. Palagi. Convergence to second-order stationary points of a primal-dual algorithm model for nonlinear programming. Mathematics of Operations Research, 30(4):897\u2013915, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pillo%2C%20G.Di%20Lucidi%2C%20S.%20Palagi%2C%20L.%20Convergence%20to%20second-order%20stationary%20points%20of%20a%20primal-dual%20algorithm%20model%20for%20nonlinear%20programming%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pillo%2C%20G.Di%20Lucidi%2C%20S.%20Palagi%2C%20L.%20Convergence%20to%20second-order%20stationary%20points%20of%20a%20primal-dual%20algorithm%20model%20for%20nonlinear%20programming%202005"
        },
        {
            "id": "21",
            "entry": "[21] F. Facchinei and S. Lucidi. Convergence to second order stationary points in inequality constrained optimization. Mathematics of Operations Research, 23(3):746\u2013766, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Facchinei%2C%20F.%20Lucidi%2C%20S.%20Convergence%20to%20second%20order%20stationary%20points%20in%20inequality%20constrained%20optimization%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Facchinei%2C%20F.%20Lucidi%2C%20S.%20Convergence%20to%20second%20order%20stationary%20points%20in%20inequality%20constrained%20optimization%201998"
        },
        {
            "id": "22",
            "entry": "[22] M. Fu, Z.-Q. Luo, and Y. Ye. Approximation algorithms for quadratic programming. Journal of combinatorial optimization, 2(1):29\u201350, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fu%2C%20M.%20Luo%2C%20Z.-Q.%20Ye%2C%20Y.%20Approximation%20algorithms%20for%20quadratic%20programming%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fu%2C%20M.%20Luo%2C%20Z.-Q.%20Ye%2C%20Y.%20Approximation%20algorithms%20for%20quadratic%20programming%201998"
        },
        {
            "id": "23",
            "entry": "[23] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. In COLT, pages 797\u2013842, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20-%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Huang%2C%20F.%20Jin%2C%20C.%20Yuan%2C%20Y.%20Escaping%20from%20saddle%20points%20-%20online%20stochastic%20gradient%20for%20tensor%20decomposition%202015"
        },
        {
            "id": "24",
            "entry": "[24] R. Ge, J. Lee, and T. Ma. Matrix completion has no spurious local minimum. In NIPS, pages 2973\u20132981, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ge%2C%20R.%20Lee%2C%20J.%20Ma%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ge%2C%20R.%20Lee%2C%20J.%20Ma%2C%20T.%20Matrix%20completion%20has%20no%20spurious%20local%20minimum%202016"
        },
        {
            "id": "25",
            "entry": "[25] S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Math. Program., 156(1-2):59\u201399, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Accelerated%20gradient%20methods%20for%20nonconvex%20nonlinear%20and%20stochastic%20programming%202016"
        },
        {
            "id": "26",
            "entry": "[26] S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Math. Program., 155(1-2):267\u2013305, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20S.%20Lan%2C%20G.%20Zhang%2C%20H.%20Mini-batch%20stochastic%20approximation%20methods%20for%20nonconvex%20stochastic%20composite%20optimization%202016"
        },
        {
            "id": "27",
            "entry": "[27] G. Haeser, H. Liu, and Y. Ye. Optimality condition and complexity analysis for linearly-constrained optimization without differentiability on the boundary. Math. Program., pages 1\u201337, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haeser%2C%20G.%20Liu%2C%20H.%20Ye%2C%20Y.%20Optimality%20condition%20and%20complexity%20analysis%20for%20linearly-constrained%20optimization%20without%20differentiability%20on%20the%20boundary%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haeser%2C%20G.%20Liu%2C%20H.%20Ye%2C%20Y.%20Optimality%20condition%20and%20complexity%20analysis%20for%20linearly-constrained%20optimization%20without%20differentiability%20on%20the%20boundary%202017"
        },
        {
            "id": "28",
            "entry": "[28] V. Jeyakumar and G. Li. Trust-region problems with linear inequality constraints: exact SDP relaxation, global optimality and robust optimization. Mathematical Programming, 147(1-2):171\u2013206, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jeyakumar%2C%20V.%20Li%2C%20G.%20Trust-region%20problems%20with%20linear%20inequality%20constraints%3A%20exact%20SDP%20relaxation%2C%20global%20optimality%20and%20robust%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jeyakumar%2C%20V.%20Li%2C%20G.%20Trust-region%20problems%20with%20linear%20inequality%20constraints%3A%20exact%20SDP%20relaxation%2C%20global%20optimality%20and%20robust%20optimization%202014"
        },
        {
            "id": "29",
            "entry": "[29] C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently. In ICML, pages 1724\u20131732, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jin%2C%20C.%20Ge%2C%20R.%20Netrapalli%2C%20P.%20Kakade%2C%20S.M.%20How%20to%20escape%20saddle%20points%20efficiently%202017"
        },
        {
            "id": "30",
            "entry": "[30] C. Jin, P. Netrapalli, and M. I. Jordan. Accelerated gradient descent escapes saddle points faster than gradient descent. CoRR, abs/1711.10456, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10456"
        },
        {
            "id": "31",
            "entry": "[31] S. Lacoste-Julien. Convergence rate of Frank-Wolfe for non-convex objectives. arXiv preprint arXiv:1607.00345, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.00345"
        },
        {
            "id": "32",
            "entry": "[32] L. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via SCSG methods. In Advances in Neural Information Processing Systems 30, pages 2345\u20132355, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20SCSG%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lei%2C%20L.%20Ju%2C%20C.%20Chen%2C%20J.%20Jordan%2C%20M.I.%20Non-convex%20finite-sum%20optimization%20via%20SCSG%20methods%202017"
        },
        {
            "id": "33",
            "entry": "[33] J. M. Mart\u0131nez and M. Raydan. Cubic-regularization counterpart of a variable-norm trust-region method for unconstrained minimization. J. Global Optimization, 68(2):367\u2013385, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mart%C4%B1nez%2C%20J.M.%20Raydan%2C%20M.%20Cubic-regularization%20counterpart%20of%20a%20variable-norm%20trust-region%20method%20for%20unconstrained%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mart%C4%B1nez%2C%20J.M.%20Raydan%2C%20M.%20Cubic-regularization%20counterpart%20of%20a%20variable-norm%20trust-region%20method%20for%20unconstrained%20minimization%202017"
        },
        {
            "id": "34",
            "entry": "[34] K. G. Murty and S. N. Kabadi. Some np-complete problems in quadratic and nonlinear programming. Math. Program., 39(2):117\u2013129, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murty%2C%20K.G.%20Kabadi%2C%20S.N.%20Some%20np-complete%20problems%20in%20quadratic%20and%20nonlinear%20programming%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murty%2C%20K.G.%20Kabadi%2C%20S.N.%20Some%20np-complete%20problems%20in%20quadratic%20and%20nonlinear%20programming%201987"
        },
        {
            "id": "35",
            "entry": "[35] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "36",
            "entry": "[36] Y. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance. Math. Program., 108(1):177\u2013205, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Polyak%2C%20B.T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20Polyak%2C%20B.T.%20Cubic%20regularization%20of%20newton%20method%20and%20its%20global%20performance%202006"
        },
        {
            "id": "37",
            "entry": "[37] S. Paternain, A. Mokhtari, and A. Ribeiro. A second order method for nonconvex optimization. arXiv preprint arXiv:1707.08028, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.08028"
        },
        {
            "id": "38",
            "entry": "[38] S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. J. Smola. Stochastic variance reduction for nonconvex optimization. In ICML, pages 314\u2013323, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "39",
            "entry": "[39] S. J. Reddi, S. Sra, B. Poczos, and A. J. Smola. Fast incremental method for smooth nonconvex optimization. In IEEE Conference on Decision and Control, CDC, pages 1971\u20131977, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Sra%2C%20S.%20Poczos%2C%20B.%20Smola%2C%20A.J.%20Fast%20incremental%20method%20for%20smooth%20nonconvex%20optimization%201971",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Sra%2C%20S.%20Poczos%2C%20B.%20Smola%2C%20A.J.%20Fast%20incremental%20method%20for%20smooth%20nonconvex%20optimization%201971"
        },
        {
            "id": "40",
            "entry": "[40] S. J. Reddi, M. Zaheer, S. Sra, B. Poczos, F. Bach, R. Salakhutdinov, and A. J. Smola. A generic approach for escaping saddle points. In AISTATS, pages 1233\u20131242, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Zaheer%2C%20M.%20Sra%2C%20S.%20Poczos%2C%20B.%20A%20generic%20approach%20for%20escaping%20saddle%20points%202018"
        },
        {
            "id": "41",
            "entry": "[41] C. W. Royer and S. J. Wright. Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03131"
        },
        {
            "id": "42",
            "entry": "[42] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. In IEEE International Symposium on Information Theory, ISIT 2016, pages 2379\u20132383, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20A%20geometric%20analysis%20of%20phase%20retrieval%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20A%20geometric%20analysis%20of%20phase%20retrieval%202016"
        },
        {
            "id": "43",
            "entry": "[43] J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere I: overview and the geometric picture. IEEE Trans. Information Theory, 63(2):853\u2013884, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20Complete%20dictionary%20recovery%20over%20the%20sphere%20I%3A%20overview%20and%20the%20geometric%20picture%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20J.%20Qu%2C%20Q.%20Wright%2C%20J.%20Complete%20dictionary%20recovery%20over%20the%20sphere%20I%3A%20overview%20and%20the%20geometric%20picture%202017"
        },
        {
            "id": "44",
            "entry": "[44] P. Tseng. Further results on approximating nonconvex quadratic optimization by semidefinite programming relaxation. SIAM Journal on Optimization, 14(1):268\u2013283, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tseng%2C%20P.%20Further%20results%20on%20approximating%20nonconvex%20quadratic%20optimization%20by%20semidefinite%20programming%20relaxation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tseng%2C%20P.%20Further%20results%20on%20approximating%20nonconvex%20quadratic%20optimization%20by%20semidefinite%20programming%20relaxation%202003"
        },
        {
            "id": "45",
            "entry": "[45] Y. Xu and T. Yang. First-order stochastic algorithms for escaping from saddle points in almost linear time. arXiv preprint arXiv:1711.01944, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.01944"
        },
        {
            "id": "46",
            "entry": "[46] Y. Ye. On affine scaling algorithms for nonconvex quadratic programming. Math. Program., 56(1-3): 285\u2013300, 1992. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ye%2C%20Y.%20On%20affine%20scaling%20algorithms%20for%20nonconvex%20quadratic%20programming%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ye%2C%20Y.%20On%20affine%20scaling%20algorithms%20for%20nonconvex%20quadratic%20programming%201992"
        }
    ]
}
