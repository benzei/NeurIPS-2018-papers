{
    "filename": "7466-pelee-a-real-time-object-detection-system-on-mobile-devices.pdf",
    "metadata": {
        "title": "Pelee: A Real-Time Object Detection System on Mobile Devices",
        "author": "Jun Wang, Tanner Bohn, Charles Ling",
        "date": 2012,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7466-pelee-a-real-time-object-detection-system-on-mobile-devices.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "An increasing need of running Convolutional Neural Network (CNN) models on mobile devices with limited computing power and memory resource encourages studies on efficient model design. A number of efficient architectures have been proposed in recent years, for example, MobileNet, ShuffleNet, and MobileNetV2. However, all these models are heavily dependent on depthwise separable convolution which lacks efficient implementation in most deep learning frameworks. In this study, we propose an efficient architecture named PeleeNet, which is built with conventional convolution instead. On ImageNet ILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and 1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile, PeleeNet is only 66% of the model size of MobileNet. We then propose a real-time object detection system by combining PeleeNet with Single Shot MultiBox Detector (SSD) method and optimizing the architecture for fast speed. Our proposed detection system2, named Pelee, achieves 76.4% mAP (mean average precision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of 23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size."
    },
    "keywords": [
        {
            "term": "mobile device",
            "url": "https://en.wikipedia.org/wiki/mobile_device"
        },
        {
            "term": "real time",
            "url": "https://en.wikipedia.org/wiki/real_time"
        },
        {
            "term": "NVIDIA",
            "url": "https://en.wikipedia.org/wiki/NVIDIA"
        },
        {
            "term": "convolutional neural network",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_network"
        },
        {
            "term": "COCO",
            "url": "https://en.wikipedia.org/wiki/COCO"
        }
    ],
    "highlights": [
        "There has been a rising interest in running high-quality Convolutional Neural Network models under strict constraints on memory and computational budget",
        "All these architectures are heavily dependent on depthwise separable convolution [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] which lacks efficient implementation",
        "Our PeleeNet is trained by PyTorch with mini-batch size 512 on two GPUs",
        "Our object detection system is based on the source code of SSD3 and is trained with Caffe [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "The models are trained on the COCO train+val dataset excluding 5000 minival images and evaluated on the test-dev2015 set",
        "Instead of using depthwise separable convolution, our proposed PeleeNet and Pelee are built with conventional convolution and have achieved compelling results on ILSVRC 2012, VOC 2007 and COCO"
    ],
    "key_statements": [
        "There has been a rising interest in running high-quality Convolutional Neural Network models under strict constraints on memory and computational budget",
        "All these architectures are heavily dependent on depthwise separable convolution [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] which lacks efficient implementation",
        "This research tries to explore the design of an efficient Convolutional Neural Network architecture for both image classification tasks and object detection tasks",
        "It has made a number of major contributions listed as follows: We propose a variant of DenseNet [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] architecture called PeleeNet for mobile devices",
        "PeleeNet achieves a compelling result on ImageNet ILSVRC 2012 [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] as well",
        "This can effectively reduce computational cost, we argue that early stage features are very important for vision tasks, and that premature reducing the feature map size can impair representational abilities",
        "Our PeleeNet is trained by PyTorch with mini-batch size 512 on two GPUs",
        "The model is trained with a cosine learning rate annealing schedule, similar to what is used by [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>] and [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>]",
        "Other hyper-parameters are the same as the one used on Stanford Dogs dataset",
        "MobileNetV2 achieves a high accuracy with 300 FLOPs, the actual speed of the model is slower than that of MobileNet with 569 FLOPs",
        "The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size",
        "Our object detection system is based on the source code of SSD3 and is trained with Caffe [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "Pelee achieves 76.4% mAP when we take the model trained on COCO trainval35k as described in Section 3.3 and fine-tuning it on the 07+12 dataset",
        "The models are trained on the COCO train+val dataset excluding 5000 minival images and evaluated on the test-dev2015 set",
        "Table 9 shows the results on test-dev2015",
        "Instead of using depthwise separable convolution, our proposed PeleeNet and Pelee are built with conventional convolution and have achieved compelling results on ILSVRC 2012, VOC 2007 and COCO"
    ],
    "summary": [
        "There has been a rising interest in running high-quality CNN models under strict constraints on memory and computational budget.",
        "Experimental results on Stanford Dogs [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] dataset show that our proposed PeleeNet is higher in accuracy than the one built with the original DenseNet architecture by 5.05% and higher than MobileNet [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] by 6.53%.",
        "We optimize the network architecture of Single Shot MultiBox Detector (SSD) [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>] for speed acceleration and combine it with PeleeNet. Our proposed system, named Pelee, achieves 76.4% mAP on PASCAL VOC [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>] 2007 and 22.4 mAP on COCO.",
        "After combining all these design choices, PeleeNet achieves 79.25% accuracy on Stanford Dogs, which is higher in accuracy by 4.23% than DenseNet-41 at less computational cost.",
        "As can be seen from Table 3, PeleeNet achieves a higher accuracy than that of MobileNet and ShuffleNet at no more than 66% model size and the lower computational cost.",
        "MobileNetV2 achieves a high accuracy with 300 FLOPs, the actual speed of the model is slower than that of MobileNet with 569 FLOPs. Using half precision float point (FP16) instead of single precision float point (FP32) is a widely used method to accelerate deep learning inference.",
        "Except for our efficient feature extraction network proposed in last section, we build the object detection network in a way different from the original SSD with a carefully selected set of 5 scale feature maps.",
        "The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size.",
        "We do not use 38 x 38 feature map layer to ensure a balance able to be reached between speed and accuracy.",
        "Pelee achieves 76.4% mAP when we take the model trained on COCO trainval35k as described in Section 3.3 and fine-tuning it on the 07+12 dataset.",
        "Pelee is 13.6 times lower in computational cost and 11.3 times smaller in model size than YOLOv2.",
        "Residual prediction block used in Pelee increases the computational cost, Pelee still runs slightly faster than SSD+MobileNet on iPhone and on TX2 in FP32 mode.",
        "Instead of using depthwise separable convolution, our proposed PeleeNet and Pelee are built with conventional convolution and have achieved compelling results on ILSVRC 2012, VOC 2007 and COCO.",
        "By combining efficient architecture design with mobile GPU and hardware-specified optimized runtime libraries, we are able to perform real-time prediction for image classification and object detection tasks on mobile devices.",
        "FPS on iPhone 8 and 125 FPS on NVIDIA TX2 with high accuracy"
    ],
    "headline": "We propose an efficient architecture named PeleeNet, which is built with conventional convolution instead",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.04861"
        },
        {
            "id": "2",
            "entry": "[2] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01083"
        },
        {
            "id": "3",
            "entry": "[3] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. arXiv preprint arXiv:1707.07012, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.07012"
        },
        {
            "id": "4",
            "entry": "[4] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4510\u20134520, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sandler%2C%20Mark%20Howard%2C%20Andrew%20Zhu%2C%20Menglong%20Zhmoginov%2C%20Andrey%20Mobilenetv2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sandler%2C%20Mark%20Howard%2C%20Andrew%20Zhu%2C%20Menglong%20Zhmoginov%2C%20Andrey%20Mobilenetv2%3A%20Inverted%20residuals%20and%20linear%20bottlenecks%202018"
        },
        {
            "id": "5",
            "entry": "[5] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Liu%2C%20Wei%20Jia%2C%20Yangqing%20Sermanet%2C%20Pierre%20Going%20deeper%20with%20convolutions%202015-06"
        },
        {
            "id": "6",
            "entry": "[6] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. arXiv preprint arXiv:1611.10012, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.10012"
        },
        {
            "id": "7",
            "entry": "[7] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06993"
        },
        {
            "id": "8",
            "entry": "[8] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained image categorization: Stanford dogs. In Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC), volume 2, page 1, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Khosla%2C%20Aditya%20Jayadevaprakash%2C%20Nityananda%20Yao%2C%20Bangpeng%20Li%2C%20Fei-Fei%20Novel%20dataset%20for%20fine-grained%20image%20categorization%3A%20Stanford%20dogs%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Khosla%2C%20Aditya%20Jayadevaprakash%2C%20Nityananda%20Yao%2C%20Bangpeng%20Li%2C%20Fei-Fei%20Novel%20dataset%20for%20fine-grained%20image%20categorization%3A%20Stanford%20dogs%202011"
        },
        {
            "id": "9",
            "entry": "[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20Jia%20Dong%2C%20Wei%20Socher%2C%20Richard%20Li%2C%20Li-Jia%20Imagenet%3A%20A%20large-scale%20hierarchical%20image%20database%202009"
        },
        {
            "id": "10",
            "entry": "[10] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In AAAI, pages 4278\u20134284, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inception-resnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20Christian%20Ioffe%2C%20Sergey%20Vanhoucke%2C%20Vincent%20Alemi%2C%20Alexander%20A.%20Inception-v4%2C%20inception-resnet%20and%20the%20impact%20of%20residual%20connections%20on%20learning%202017"
        },
        {
            "id": "11",
            "entry": "[11] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, and Xiangyang Xue. Dsod: Learning deeply supervised object detectors from scratch. In The IEEE International Conference on Computer Vision (ICCV), volume 3, page 7, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shen%2C%20Zhiqiang%20Liu%2C%20Zhuang%20Li%2C%20Jianguo%20Jiang%2C%20Yu-Gang%20Dsod%3A%20Learning%20deeply%20supervised%20object%20detectors%20from%20scratch%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shen%2C%20Zhiqiang%20Liu%2C%20Zhuang%20Li%2C%20Jianguo%20Jiang%2C%20Yu-Gang%20Dsod%3A%20Learning%20deeply%20supervised%20object%20detectors%20from%20scratch%202017"
        },
        {
            "id": "12",
            "entry": "[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20Sergey%20Szegedy%2C%20Christian%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "13",
            "entry": "[13] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21\u201337.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Wei%20Anguelov%2C%20Dragomir%20Erhan%2C%20Dumitru%20Szegedy%2C%20Christian%20Ssd%3A%20Single%20shot%20multibox%20detector",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Wei%20Anguelov%2C%20Dragomir%20Erhan%2C%20Dumitru%20Szegedy%2C%20Christian%20Ssd%3A%20Single%20shot%20multibox%20detector"
        },
        {
            "id": "14",
            "entry": "[14] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303\u2013338, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Everingham%2C%20Mark%20Gool%2C%20Luc%20Van%20Williams%2C%20Christopher%20K.I.%20Winn%2C%20John%20The%20pascal%20visual%20object%20classes%20%28voc%29%20challenge%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Everingham%2C%20Mark%20Gool%2C%20Luc%20Van%20Williams%2C%20Christopher%20K.I.%20Winn%2C%20John%20The%20pascal%20visual%20object%20classes%20%28voc%29%20challenge%202010"
        },
        {
            "id": "15",
            "entry": "[15] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.08242"
        },
        {
            "id": "16",
            "entry": "[16] Kyoungmin Lee, Jaeseok Choi, Jisoo Jeong, and Nojun Kwak. Residual features and unified prediction network for single stage detection. arXiv preprint arXiv:1707.05031, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.05031"
        },
        {
            "id": "17",
            "entry": "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20Kaiming%20Zhang%2C%20Xiangyu%20Ren%2C%20Shaoqing%20Sun%2C%20Jian%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "18",
            "entry": "[18] Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens van der Maaten, and Kilian Q Weinberger. Memory-efficient implementation of densenets. arXiv preprint arXiv:1707.06990, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06990"
        },
        {
            "id": "19",
            "entry": "[19] Ilya Loshchilov and Frank Hutter. Sgdr: stochastic gradient descent with restarts. arXiv preprint arXiv:1608.03983, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.03983"
        },
        {
            "id": "20",
            "entry": "[20] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia, pages 675\u2013678. ACM, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jia%2C%20Yangqing%20Shelhamer%2C%20Evan%20Donahue%2C%20Jeff%20Karayev%2C%20Sergey%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jia%2C%20Yangqing%20Shelhamer%2C%20Evan%20Donahue%2C%20Jeff%20Karayev%2C%20Sergey%20Caffe%3A%20Convolutional%20architecture%20for%20fast%20feature%20embedding%202014"
        }
    ]
}
