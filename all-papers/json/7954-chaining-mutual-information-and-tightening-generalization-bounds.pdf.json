{
    "filename": "7954-chaining-mutual-information-and-tightening-generalization-bounds.pdf",
    "metadata": {
        "title": "Chaining Mutual Information and Tightening Generalization Bounds",
        "author": "Amir Asadi, Emmanuel Abbe, Sergio Verdu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7954-chaining-mutual-information-and-tightening-generalization-bounds.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Bounding the generalization error of learning algorithms has a long history, which yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between the hypotheses, (ii) exploiting the dependence between the algorithm\u2019s input and output. Progress on the first point was made with the chaining method, originating from the work of Kolmogorov, and used in the VC-dimension bound. More recently, progress on the second point was made with the mutual information method by Russo and Zou \u201915. Yet, these two methods are currently disjoint. In this paper, we introduce a technique to combine chaining and mutual information methods, to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information bounds. As a corollary, we tighten Dudley\u2019s inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability."
    },
    "keywords": [
        {
            "term": "hypothesis",
            "url": "https://en.wikipedia.org/wiki/hypothesis"
        },
        {
            "term": "generalization error",
            "url": "https://en.wikipedia.org/wiki/generalization_error"
        },
        {
            "term": "vc dimension",
            "url": "https://en.wikipedia.org/wiki/vc_dimension"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "rademacher complexity",
            "url": "https://en.wikipedia.org/wiki/rademacher_complexity"
        },
        {
            "term": "information theory",
            "url": "https://en.wikipedia.org/wiki/information_theory"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "Motivation<br/><br/>Understanding the generalization phenomenon in machine learning has been a central question for many years and revived in recent years with the success and mystery of deep learning: why do neural nets generalize well, they operate in a classically overparametrized setting? In particular, classical generalization bounds do not explain this phenomenon",
        "Generalization bounds have evolved throughout the years, starting from the basic union bound over the hypothesis set, the refined union bound, Rademacher complexity, chaining and VC-dimension [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]; and algorithm-dependent bounds such as PAC-Bayesian bounds [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], uniform stability [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], compression bounds [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], and recently, the mutual information bound [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "By combining the ideas of the chaining method and the mutual information method, in this paper we obtain a chained mutual information bound on the expected generalization error which takes into account the dependencies between the hypotheses as well as the dependence between output and input of the algorithm",
        "We showed on an example that our chained mutual information bound significantly outperforms previous bounds and gets close to the true generalization error",
        "Under a natural regularization property of the learning algorithm, we provided a corollary of our bound which tightens Dudley\u2019s inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability"
    ],
    "key_statements": [
        "Motivation<br/><br/>Understanding the generalization phenomenon in machine learning has been a central question for many years and revived in recent years with the success and mystery of deep learning: why do neural nets generalize well, they operate in a classically overparametrized setting? In particular, classical generalization bounds do not explain this phenomenon",
        "Generalization bounds have evolved throughout the years, starting from the basic union bound over the hypothesis set, the refined union bound, Rademacher complexity, chaining and VC-dimension [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>], [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]; and algorithm-dependent bounds such as PAC-Bayesian bounds [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>], uniform stability [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>], compression bounds [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], and recently, the mutual information bound [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "By combining the ideas of the chaining method and the mutual information method, in this paper we obtain a chained mutual information bound on the expected generalization error which takes into account the dependencies between the hypotheses as well as the dependence between output and input of the algorithm",
        "We showed on an example that our chained mutual information bound significantly outperforms previous bounds and gets close to the true generalization error",
        "Under a natural regularization property of the learning algorithm, we provided a corollary of our bound which tightens Dudley\u2019s inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability"
    ],
    "summary": [
        "Motivation<br/><br/>Understanding the generalization phenomenon in machine learning has been a central question for many years and revived in recent years with the success and mystery of deep learning: why do neural nets generalize well, they operate in a classically overparametrized setting? In particular, classical generalization bounds do not explain this phenomenon.",
        "Consider the following example: let X1, X2, ..., Xn be standard normal random variables and assume that the algorithm output is index W .",
        "By combining the ideas of the chaining method and the mutual information method, in this paper we obtain a chained mutual information bound on the expected generalization error which takes into account the dependencies between the hypotheses as well as the dependence between output and input of the algorithm.",
        "In [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], the mutual information between the input and the output of binary classification learning algorithms is used to obtain high probability generalization bounds.",
        "In the probability theory literature, Fernique [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] gives upper and lower bounds on the expected bias of an algorithm which chooses its output from a Gaussian process, by using a chaining argument while taking into account the marginal distribution of the algorithm output.",
        "Both Theorem 3 and Theorem 4 capture the dependencies between the hypotheses by utilizing a metric d, and they are algorithm-dependent as the mutual information between the algorithm\u2019s discretized output and its input appears in their bounds.",
        "Note that in Example 1 there exists an independent additive noise term Z which has a degenerate part, causing the mutual information bound to blow up.",
        "Example 1 illustrates that combining the mutual information method with the chaining method as in our bound could give tight generalization bounds for such algorithms as well.",
        "Note that knowing the value of \u03c0Nk (W ), \u03c0Nk\u22121 (W ) is enough to determine which one of the random variables X\u03c0Nk (t) \u2212 X\u03c0Nk\u22121 (t) t\u2208T is chosen according to W .",
        "\u03a0Nk (W ), \u03c0Nk\u22121 (W ) is playing the role of the random index, and since X\u03c0Nk (t) \u2212 X\u03c0Nk\u22121 (t) is d2 \u03c0Nk (t), \u03c0Nk\u22121 (t) -subgaussian, based on Theorem 2, an application of data processing inequality and by summation, we have n",
        "We tighten Dudley\u2019s inequality (Theorem 1), given the following regularization property: the output W of an algorithm, with high probability, chooses a hypothesis from a subset of the hypothesis set with small covering numbers: Theorem 5 (Small subset property).",
        "We combined ideas from information theory and from high dimensional probability to obtain a generalization bound that takes into account both the dependencies between the hypotheses and the dependence between the input and the output of a learning algorithm.",
        "Under a natural regularization property of the learning algorithm, we provided a corollary of our bound which tightens Dudley\u2019s inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability"
    ],
    "headline": "We introduce a technique to combine chaining and mutual information methods, to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. Zhang, S. Bengio, M. Hardt, B. Recht and O. Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), Apr. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017-04"
        },
        {
            "id": "2",
            "entry": "[2] M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel learning. arXiv preprint arXiv:1802.01396, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01396"
        },
        {
            "id": "3",
            "entry": "[3] O. Bousquet, S. Boucheron, and G. Lugosi. Introduction to statistical learning theory. In Advanced Lectures on Machine Learning, pages 169\u2013207.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20O.%20Boucheron%2C%20S.%20Lugosi%2C%20G.%20Introduction%20to%20statistical%20learning%20theory",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20O.%20Boucheron%2C%20S.%20Lugosi%2C%20G.%20Introduction%20to%20statistical%20learning%20theory"
        },
        {
            "id": "4",
            "entry": "[4] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Ben-David%2C%20S.%20Understanding%20Machine%20Learning%3A%20From%20Theory%20to%20Algorithms%202014"
        },
        {
            "id": "5",
            "entry": "[5] D. A. McAllester. Some PAC-Bayesian theorems. Machine Learning, 37(3):355\u2013363, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McAllester%2C%20D.A.%20Some%20PAC-Bayesian%20theorems%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McAllester%2C%20D.A.%20Some%20PAC-Bayesian%20theorems%201999"
        },
        {
            "id": "6",
            "entry": "[6] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2(Mar):499\u2013526, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bousquet%2C%20O.%20Elisseeff%2C%20A.%20Stability%20and%20generalization%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bousquet%2C%20O.%20Elisseeff%2C%20A.%20Stability%20and%20generalization%202002"
        },
        {
            "id": "7",
            "entry": "[7] N. Littlestone and M. Warmuth. Relating data compression and learnability. Technical report, University of California, Santa Cruz, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littlestone%2C%20N.%20Warmuth%2C%20M.%20Relating%20data%20compression%20and%20learnability%201986"
        },
        {
            "id": "8",
            "entry": "[8] D. Russo and J. Zou. How much does your data exploration overfit? controlling bias via information usage. arXiv preprint arXiv:1511.05219, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05219"
        },
        {
            "id": "9",
            "entry": "[9] R. van Handel. Probability in high dimension. [Online]. Available: https://www.princeton.edu/~rvan/ ORF570.pdf , Dec.21 2016.",
            "url": "https://www.princeton.edu/~rvan/ORF570.pdf"
        },
        {
            "id": "10",
            "entry": "[10] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vershynin%2C%20R.%20High-Dimensional%20Probability%3A%20An%20Introduction%20with%20Applications%20in%20Data%20Science%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vershynin%2C%20R.%20High-Dimensional%20Probability%3A%20An%20Introduction%20with%20Applications%20in%20Data%20Science%202018"
        },
        {
            "id": "11",
            "entry": "[11] M. Talagrand. Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems, volume 60. Springer Science & Business Media, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Talagrand%2C%20M.%20Upper%20and%20Lower%20Bounds%20for%20Stochastic%20Processes%3A%20Modern%20Methods%20and%20Classical%20Problems%2C%20volume%2060%202014"
        },
        {
            "id": "12",
            "entry": "[12] S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boucheron%2C%20S.%20Lugosi%2C%20G.%20Massart%2C%20P.%20Concentration%20Inequalities%3A%20A%20Nonasymptotic%20Theory%20of%20Independence%202013"
        },
        {
            "id": "13",
            "entry": "[13] R. M Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian processes. Journal of Functional Analysis, 1(3):290\u2013330, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dudley%2C%20R.M.%20The%20sizes%20of%20compact%20subsets%20of%20Hilbert%20space%20and%20continuity%20of%20Gaussian%20processes%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dudley%2C%20R.M.%20The%20sizes%20of%20compact%20subsets%20of%20Hilbert%20space%20and%20continuity%20of%20Gaussian%20processes%201967"
        },
        {
            "id": "14",
            "entry": "[14] K. Kawaguchi, L. P. Kaelbling and Y. Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05468"
        },
        {
            "id": "15",
            "entry": "[15] Bousquet O. Bartlett, P. L and S. Mendelson. Local Rademacher complexities. The Annals of Statistics, 33(4):1497\u20131537, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20Bousquet%20O.%20L%2C%20P.%20Mendelson%2C%20S.%20Local%20Rademacher%20complexities%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20Bousquet%20O.%20L%2C%20P.%20Mendelson%2C%20S.%20Local%20Rademacher%20complexities%202005"
        },
        {
            "id": "16",
            "entry": "[16] J. Jiao, Y. Han and T. Weissman. Dependence measures bounding the exploration bias for general measurements. In Proc. of IEEE Symposium on Information Theory (ISIT), pages 1475\u20131479, Aachen, Germany, June 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiao%2C%20J.%20Han%2C%20Y.%20Weissman%2C%20T.%20Dependence%20measures%20bounding%20the%20exploration%20bias%20for%20general%20measurements%202017-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiao%2C%20J.%20Han%2C%20Y.%20Weissman%2C%20T.%20Dependence%20measures%20bounding%20the%20exploration%20bias%20for%20general%20measurements%202017-06"
        },
        {
            "id": "17",
            "entry": "[17] J. Jiao, Y. Han and T. Weissman. Generalizations of maximal inequalities to arbitrary selection rules. arXiv preprint arXiv:1708.09041, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.09041"
        },
        {
            "id": "18",
            "entry": "[18] A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In Advances in Neural Information Processing Systems (NIPS), pages 2524\u20132533, Dec. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20A.%20Raginsky%2C%20M.%20Information-theoretic%20analysis%20of%20generalization%20capability%20of%20learning%20algorithms%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20A.%20Raginsky%2C%20M.%20Information-theoretic%20analysis%20of%20generalization%20capability%20of%20learning%20algorithms%202017-12"
        },
        {
            "id": "19",
            "entry": "[19] A. Pensia, V. Jog and P. Loh. Generalization error bounds for noisy, iterative algorithms. arXiv preprint arXiv:1801.04295, 12",
            "arxiv_url": "https://arxiv.org/pdf/1801.04295"
        },
        {
            "id": "20",
            "entry": "[20] R. Bassily, S. Moran, I. Nachum, J. Shafer and A. Yehudayoff. Learners that leak little information. arXiv preprint arXiv:1710.05233, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.05233"
        },
        {
            "id": "21",
            "entry": "[21] J. Audibert and O. Bousquet. PAC-Bayesian generic chaining. In Advances in Neural Information Processing Systems (NIPS), pages 1125\u20131132, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.%20Bousquet%2C%20O.%20PAC-Bayesian%20generic%20chaining%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.%20Bousquet%2C%20O.%20PAC-Bayesian%20generic%20chaining%202004"
        },
        {
            "id": "22",
            "entry": "[22] J. Audibert and O. Bousquet. Combining PAC-Bayesian and generic chaining bounds. Journal of Machine Learning Research, 8(Apr):863\u2013889, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.%20Bousquet%2C%20O.%20Combining%20PAC-Bayesian%20and%20generic%20chaining%20bounds%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.%20Bousquet%2C%20O.%20Combining%20PAC-Bayesian%20and%20generic%20chaining%20bounds%202007"
        },
        {
            "id": "24",
            "entry": "[24] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cover%2C%20T.M.%20Thomas%2C%20J.A.%20Elements%20of%20Information%20Theory%202012"
        }
    ]
}
