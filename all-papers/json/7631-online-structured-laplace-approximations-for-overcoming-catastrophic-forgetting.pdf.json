{
    "filename": "7631-online-structured-laplace-approximations-for-overcoming-catastrophic-forgetting.pdf",
    "metadata": {
        "date": 2018,
        "title": "Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting",
        "author": "Hippolyt Ritter1\u2217",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7631-online-structured-laplace-approximations-for-overcoming-catastrophic-forgetting.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting."
    },
    "keywords": [
        {
            "term": "laplace approximation",
            "url": "https://en.wikipedia.org/wiki/laplace_approximation"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "neural networks",
            "url": "https://en.wikipedia.org/wiki/neural_networks"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Creating an agent that performs well across multiple tasks and continuously incorporates new knowledge has been a longstanding goal of research on artificial intelligence",
        "We find our online Laplace approximation to maintain higher test accuracy throughout training than placing a quadratic penalty around the MAP parameters of every task, in particular when using a simple diagonal approximation to the Hessian",
        "We investigate different values of the hyperparameter \u03bb on the permuted MNIST sequence of datasets for our online Laplace approximation",
        "We proposed the online Laplace approximation, a Bayesian online learning method for overcoming catastrophic forgetting in neural networks",
        "By further taking interactions between the parameters into account, we achieved considerable increases in test accuracy on the problems that we investigated, in particular for long sequences of datasets",
        "Our results demonstrate the importance of going beyond diagonal approximation methods which only measure the sensitivity of individual parameters"
    ],
    "key_statements": [
        "Creating an agent that performs well across multiple tasks and continuously incorporates new knowledge has been a longstanding goal of research on artificial intelligence",
        "In our experiments we show that this principled approximation of the posterior leads to substantial gains in performance over simpler diagonal methods, in particular for long sequences of tasks",
        "We review recent Kronecker factored approximations to the curvature of neural networks and how to use them to obtain a better fit to the posterior",
        "We find our online Laplace approximation to maintain higher test accuracy throughout training than placing a quadratic penalty around the MAP parameters of every task, in particular when using a simple diagonal approximation to the Hessian",
        "We investigate different values of the hyperparameter \u03bb on the permuted MNIST sequence of datasets for our online Laplace approximation",
        "We conclude from our results that the online Laplace approximation overestimates the uncertainty in the approximate posterior about the parameters for the permuted MNIST task, in particular with a diagonal approximation to the Hessian",
        "While we do not match the performance of the method developed in [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], we find the Laplace approximation to work significantly better than reported by the authors",
        "We proposed the online Laplace approximation, a Bayesian online learning method for overcoming catastrophic forgetting in neural networks",
        "By further taking interactions between the parameters into account, we achieved considerable increases in test accuracy on the problems that we investigated, in particular for long sequences of datasets",
        "Our results demonstrate the importance of going beyond diagonal approximation methods which only measure the sensitivity of individual parameters"
    ],
    "summary": [
        "Creating an agent that performs well across multiple tasks and continuously incorporates new knowledge has been a longstanding goal of research on artificial intelligence.",
        "We combine Bayesian online learning [<a class=\"ref-link\" id=\"c31\" href=\"#r31\">31</a>] with the Kronecker factored Laplace approximation [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] to update a quadratic penalty for every new task.",
        "We review recent Kronecker factored approximations to the curvature of neural networks and how to use them to obtain a better fit to the posterior.",
        "Our method requires calculating the expectations of the two Kronecker factors from Eq (9) over the data of the most recent task after training on it as well as calculating the quadratic penalty in Eq (5) using the identity in Eq (10) for every parameter update.",
        "In contrast to Bayesian online learning, as we cannot update the posterior over the weights in closed form, we use gradient-based methods to find a mode and perform a quadratic approximation around it, resulting in a Gaussian approximation.",
        "[<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] and [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>] have independently proposed the use of block-diagonal Kronecker factored curvature approximations [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] to sample from an approximate Gaussian posterior over the weights of a neural network.",
        "We find our online Laplace approximation to maintain higher test accuracy throughout training than placing a quadratic penalty around the MAP parameters of every task, in particular when using a simple diagonal approximation to the Hessian.",
        "The main difference between the methods lies in using a Kronecker factored approximation of the curvature over a diagonal one.5 Using this approximation, we achieve over 90% average test accuracy across 50 tasks, almost matching the performance of a network trained jointly on all observed data.",
        "We investigate different values of the hyperparameter \u03bb on the permuted MNIST sequence of datasets for our online Laplace approximation.",
        "Using \u03bb = 3, the value that achieves optimal validation error in our experiments, the Kronecker factored approximation leads to the network performing on the most recent and first tasks.",
        "We conclude from our results that the online Laplace approximation overestimates the uncertainty in the approximate posterior about the parameters for the permuted MNIST task, in particular with a diagonal approximation to the Hessian.",
        "In contrast, shows decaying performance on the initial tasks, but learns the fifth task almost as well as our method with a Kronecker factored approximation of the Hessian.",
        "By formulating a principled approximation to the posterior, we were able to substantially improve over EWC [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and SI [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>], two recent methods that add a quadratic regularizer to the objective for new tasks.",
        "Dealing with the complex interaction and correlation between parameters is necessary in moving towards a more complete response to the challenge of continual learning"
    ],
    "headline": "We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight Uncertainty in Neural Networks. In International Conference on Machine Learning, pages 1613\u20131622, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=C%20Blundell%20J%20Cornebise%20K%20Kavukcuoglu%20and%20D%20Wierstra%20Weight%20Uncertainty%20in%20Neural%20Networks%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2016131622%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=C%20Blundell%20J%20Cornebise%20K%20Kavukcuoglu%20and%20D%20Wierstra%20Weight%20Uncertainty%20in%20Neural%20Networks%20In%20International%20Conference%20on%20Machine%20Learning%20pages%2016131622%202015"
        },
        {
            "id": "2",
            "entry": "[2] A. Botev, H. Ritter, and D. Barber. Practical Gauss-Newton Optimisation for Deep Learning. In International Conference on Machine Learning, pages 557 \u2013 565, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Botev%2C%20A.%20Ritter%2C%20H.%20Barber%2C%20D.%20Practical%20Gauss-Newton%20Optimisation%20for%20Deep%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Botev%2C%20A.%20Ritter%2C%20H.%20Barber%2C%20D.%20Practical%20Gauss-Newton%20Optimisation%20for%20Deep%20Learning%202017"
        },
        {
            "id": "3",
            "entry": "[3] S. Dieleman, J. Schl\u00fcter, C. Raffel, E. Olson, S. K. S\u00f8nderby, D. Nouri, et al. Lasagne: First release., August 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=S%20Dieleman%20J%20Schl%C3%BCter%20C%20Raffel%20E%20Olson%20S%20K%20S%C3%B8nderby%20D%20Nouri%20et%20al%20Lasagne%20First%20release%20August%202015"
        },
        {
            "id": "4",
            "entry": "[4] E. Eskin, A. J. Smola, and S. Vishwanathan. Laplace Propagation. In Advances in Neural Information Processing Systems, pages 441\u2013448, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=E%20Eskin%20A%20J%20Smola%20and%20S%20Vishwanathan%20Laplace%20Propagation%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%20441448%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=E%20Eskin%20A%20J%20Smola%20and%20S%20Vishwanathan%20Laplace%20Propagation%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20pages%20441448%202004"
        },
        {
            "id": "5",
            "entry": "[5] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wierstra. Pathnet: Evolution Channels Gradient Descent in Super Neural Networks. arXiv preprint arXiv:1701.08734, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1701.08734"
        },
        {
            "id": "6",
            "entry": "[6] R. M. French. Catastrophic Forgetting in Connectionist Networks. Trends in Cognitive Sciences, 3:128\u2013135, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=French%2C%20R.M.%20Catastrophic%20Forgetting%20in%20Connectionist%20Networks%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=French%2C%20R.M.%20Catastrophic%20Forgetting%20in%20Connectionist%20Networks%201999"
        },
        {
            "id": "7",
            "entry": "[7] Z. Ghahramani. Online Variational Bayesian Learning. 2000. Slides from talk presented at NIPS 2000 workshop on Online Learning.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghahramani%2C%20Z.%20Online%20Variational%20Bayesian%20Learning%202000"
        },
        {
            "id": "8",
            "entry": "[8] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio. An Empirical Investigation of Catastrophic Forgetting in Gradient-based Neural Networks. arXiv preprint arXiv:1312.6211, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6211"
        },
        {
            "id": "9",
            "entry": "[9] E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Recasting Gradient-Based Meta-Learning as Hierarchical Bayes. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grant%2C%20E.%20Finn%2C%20C.%20Levine%2C%20S.%20Darrell%2C%20T.%20Recasting%20Gradient-Based%20Meta-Learning%20as%20Hierarchical%20Bayes%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grant%2C%20E.%20Finn%2C%20C.%20Levine%2C%20S.%20Darrell%2C%20T.%20Recasting%20Gradient-Based%20Meta-Learning%20as%20Hierarchical%20Bayes%202018"
        },
        {
            "id": "10",
            "entry": "[10] R. Grosse and J. Martens. A Kronecker-factored Approximate Fisher Matrix for Convolution Layers. In International Conference on Machine Learning, pages 573\u2013582, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grosse%2C%20R.%20Martens%2C%20J.%20A%20Kronecker-factored%20Approximate%20Fisher%20Matrix%20for%20Convolution%20Layers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grosse%2C%20R.%20Martens%2C%20J.%20A%20Kronecker-factored%20Approximate%20Fisher%20Matrix%20for%20Convolution%20Layers%202016"
        },
        {
            "id": "11",
            "entry": "[11] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions, volume 104. CRC Press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A%20K%20Gupta%20and%20D%20K%20Nagar%20Matrix%20Variate%20Distributions%20volume%20104%20CRC%20Press%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A%20K%20Gupta%20and%20D%20K%20Nagar%20Matrix%20Variate%20Distributions%20volume%20104%20CRC%20Press%201999"
        },
        {
            "id": "12",
            "entry": "[12] X. He and H. Jaeger. Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20X.%20Jaeger%2C%20H.%20Overcoming%20Catastrophic%20Interference%20using%20Conceptor-Aided%20Backpropagation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20X.%20Jaeger%2C%20H.%20Overcoming%20Catastrophic%20Interference%20using%20Conceptor-Aided%20Backpropagation%202018"
        },
        {
            "id": "13",
            "entry": "[13] A. Honkela and H. Valpola. On-line Variational Bayesian Learning. In 4th International Symposium on Independent Component Analysis and Blind Signal Separation, pages 803\u2013808, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Honkela%2C%20A.%20Valpola%2C%20H.%20On-line%20Variational%20Bayesian%20Learning%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Honkela%2C%20A.%20Valpola%2C%20H.%20On-line%20Variational%20Bayesian%20Learning%202003"
        },
        {
            "id": "14",
            "entry": "[14] F. Husz\u00e1r. Note on the Quadratic Penalties in Elastic Weight Consolidation. Proceedings of the National Academy of Sciences, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Husz%C3%A1r%2C%20F.%20Note%20on%20the%20Quadratic%20Penalties%20in%20Elastic%20Weight%20Consolidation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Husz%C3%A1r%2C%20F.%20Note%20on%20the%20Quadratic%20Penalties%20in%20Elastic%20Weight%20Consolidation%202018"
        },
        {
            "id": "15",
            "entry": "[15] D. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "16",
            "entry": "[16] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming Catastrophic Forgetting in Neural Networks. Proceedings of the National Academy of Sciences, pages 3521\u20133526, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Kirkpatrick%20R%20Pascanu%20N%20Rabinowitz%20J%20Veness%20G%20Desjardins%20A%20A%20Rusu%20K%20Milan%20J%20Quan%20T%20Ramalho%20A%20GrabskaBarwinska%20D%20Hassabis%20C%20Clopath%20D%20Kumaran%20and%20R%20Hadsell%20Overcoming%20Catastrophic%20Forgetting%20in%20Neural%20Networks%20Proceedings%20of%20the%20National%20Academy%20of%20Sciences%20pages%2035213526%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Kirkpatrick%20R%20Pascanu%20N%20Rabinowitz%20J%20Veness%20G%20Desjardins%20A%20A%20Rusu%20K%20Milan%20J%20Quan%20T%20Ramalho%20A%20GrabskaBarwinska%20D%20Hassabis%20C%20Clopath%20D%20Kumaran%20and%20R%20Hadsell%20Overcoming%20Catastrophic%20Forgetting%20in%20Neural%20Networks%20Proceedings%20of%20the%20National%20Academy%20of%20Sciences%20pages%2035213526%202017"
        },
        {
            "id": "17",
            "entry": "[17] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Reply to Husz\u00e1r: The Elastic Weight Consolidation Penalty is Empirically Valid. Proceedings of the National Academy of Sciences, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kirkpatrick%2C%20J.%20Pascanu%2C%20R.%20Rabinowitz%2C%20N.%20Veness%2C%20J.%20Reply%20to%20Husz%C3%A1r%3A%20The%20Elastic%20Weight%20Consolidation%20Penalty%20is%20Empirically%20Valid%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kirkpatrick%2C%20J.%20Pascanu%2C%20R.%20Rabinowitz%2C%20N.%20Veness%2C%20J.%20Reply%20to%20Husz%C3%A1r%3A%20The%20Elastic%20Weight%20Consolidation%20Penalty%20is%20Empirically%20Valid%202018"
        },
        {
            "id": "18",
            "entry": "[18] A. Krizhevsky and G. Hinton. Learning Multiple Layers of Features from Tiny Images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images%202009"
        },
        {
            "id": "19",
            "entry": "[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based Learning Applied to Document Recognition. In Proceedings of the IEEE, pages 2278 \u2013 2324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20Learning%20Applied%20to%20Document%20Recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20Learning%20Applied%20to%20Document%20Recognition%201998"
        },
        {
            "id": "20",
            "entry": "[20] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang. Overcoming Catastrophic Forgetting by Incremental Moment Matching. In Advances in Neural Information Processing Systems, pages 4655\u20134665, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20S.-W.%20Kim%2C%20J.-H.%20Jun%2C%20J.%20Ha%2C%20J.-W.%20Overcoming%20Catastrophic%20Forgetting%20by%20Incremental%20Moment%20Matching%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20S.-W.%20Kim%2C%20J.-H.%20Jun%2C%20J.%20Ha%2C%20J.-W.%20Overcoming%20Catastrophic%20Forgetting%20by%20Incremental%20Moment%20Matching%202017"
        },
        {
            "id": "21",
            "entry": "[21] D. Lopez-Paz and M. Ranzato. Gradient Episodic Memory for Continual Learning. In Advances in Neural Information Processing Systems, pages 6470\u20136479, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20D.%20Ranzato%2C%20M.%20Gradient%20Episodic%20Memory%20for%20Continual%20Learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20D.%20Ranzato%2C%20M.%20Gradient%20Episodic%20Memory%20for%20Continual%20Learning%202017"
        },
        {
            "id": "22",
            "entry": "[22] D. J. C. MacKay. A Practical Bayesian Framework for Backpropagation Networks. Neural Computation, 4:448\u2013472, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MacKay%2C%20D.J.C.%20A%20Practical%20Bayesian%20Framework%20for%20Backpropagation%20Networks%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MacKay%2C%20D.J.C.%20A%20Practical%20Bayesian%20Framework%20for%20Backpropagation%20Networks%201992"
        },
        {
            "id": "23",
            "entry": "[23] J. Martens and R. Grosse. Optimizing Neural Networks with Kronecker-factored Approximate Curvature. In International Conference on Machine Learning, pages 2408\u20132417, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20Neural%20Networks%20with%20Kronecker-factored%20Approximate%20Curvature%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20Neural%20Networks%20with%20Kronecker-factored%20Approximate%20Curvature%202015"
        },
        {
            "id": "24",
            "entry": "[24] J. Martens. New Insights and Perspectives on the Natural Gradient Method. arXiv preprint arXiv:1412.1193, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.1193"
        },
        {
            "id": "25",
            "entry": "[25] P. Maybeck. Stochastic Models, Estimation and Control, chapter 12.7. Academic Press, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maybeck%2C%20P.%20Stochastic%20Models%2C%20Estimation%20and%20Control%2C%20chapter%2012.7%201982"
        },
        {
            "id": "26",
            "entry": "[26] M. McCloskey and N. J. Cohen. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. Psychology of Learning and Motivation - Advances in Research and Theory, 24:109\u2013165, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McCloskey%2C%20M.%20Cohen%2C%20N.J.%20Catastrophic%20Interference%20in%20Connectionist%20Networks%3A%20The%20Sequential%20Learning%20Problem%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McCloskey%2C%20M.%20Cohen%2C%20N.J.%20Catastrophic%20Interference%20in%20Connectionist%20Networks%3A%20The%20Sequential%20Learning%20Problem%201989"
        },
        {
            "id": "27",
            "entry": "[27] T. P. Minka. Expectation Propagation for Approximate Bayesian Inference. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, pages 362\u2013369, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Minka%2C%20T.P.%20Expectation%20Propagation%20for%20Approximate%20Bayesian%20Inference%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Minka%2C%20T.P.%20Expectation%20Propagation%20for%20Approximate%20Bayesian%20Inference%202001"
        },
        {
            "id": "28",
            "entry": "[28] Y. Nesterov. A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2). Soviet Mathematics Doklady, 27:372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O%20%281/k2%29.%20Soviet%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20A%20Method%20of%20Solving%20a%20Convex%20Programming%20Problem%20with%20Convergence%20Rate%20O%20%281/k2%29.%20Soviet%201983"
        },
        {
            "id": "29",
            "entry": "[29] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading Digits in Natural Images with Unsupervised Feature Learning. In NIPS workshop on deep learning and unsupervised feature learning, page 5, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Netzer%2C%20Y.%20Wang%2C%20T.%20Coates%2C%20A.%20Bissacco%2C%20A.%20Reading%20Digits%20in%20Natural%20Images%20with%20Unsupervised%20Feature%20Learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Netzer%2C%20Y.%20Wang%2C%20T.%20Coates%2C%20A.%20Bissacco%2C%20A.%20Reading%20Digits%20in%20Natural%20Images%20with%20Unsupervised%20Feature%20Learning%202011"
        },
        {
            "id": "30",
            "entry": "[30] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner. Variational Continual Learning. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20C.V.%20Li%2C%20Y.%20Bui%2C%20T.D.%20Turner%2C%20R.E.%20Variational%20Continual%20Learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20C.V.%20Li%2C%20Y.%20Bui%2C%20T.D.%20Turner%2C%20R.E.%20Variational%20Continual%20Learning%202018"
        },
        {
            "id": "31",
            "entry": "[31] M. Opper and O. Winther. A Bayesian Approach to On-line Learning. On-line Learning in Neural Networks, ed. D. Saad, pages 363\u2013378, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Opper%2C%20M.%20Winther%2C%20O.%20A%20Bayesian%20Approach%20to%20On-line%20Learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Opper%2C%20M.%20Winther%2C%20O.%20A%20Bayesian%20Approach%20to%20On-line%20Learning%201998"
        },
        {
            "id": "32",
            "entry": "[32] B. T. Polyak. Some Methods of Speeding up the Convergence of Iteration Methods. USSR Computational Mathematics and Mathematical Physics, 4:1\u201317, 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Polyak%2C%20B.T.%20Some%20Methods%20of%20Speeding%20up%20the%20Convergence%20of%20Iteration%20Methods%201964",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Polyak%2C%20B.T.%20Some%20Methods%20of%20Speeding%20up%20the%20Convergence%20of%20Iteration%20Methods%201964"
        },
        {
            "id": "33",
            "entry": "[33] R. Ratcliff. Connectionist Models of Recognition Memory: Constraints Imposed by Learning and Forgetting Functions. Psychological Review, 97:285\u2013308, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ratcliff%2C%20R.%20Connectionist%20Models%20of%20Recognition%20Memory%3A%20Constraints%20Imposed%20by%20Learning%20and%20Forgetting%20Functions%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ratcliff%2C%20R.%20Connectionist%20Models%20of%20Recognition%20Memory%3A%20Constraints%20Imposed%20by%20Learning%20and%20Forgetting%20Functions%201990"
        },
        {
            "id": "34",
            "entry": "[34] H. Ritter, A. Botev, and D. Barber. A Scalable Laplace Approximation for Neural Networks. International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ritter%2C%20H.%20Botev%2C%20A.%20Barber%2C%20D.%20A%20Scalable%20Laplace%20Approximation%20for%20Neural%20Networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ritter%2C%20H.%20Botev%2C%20A.%20Barber%2C%20D.%20A%20Scalable%20Laplace%20Approximation%20for%20Neural%20Networks%202018"
        },
        {
            "id": "35",
            "entry": "[35] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive Neural Networks. arXiv preprint arXiv:1606.04671, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04671"
        },
        {
            "id": "36",
            "entry": "[36] L. Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. Empirical Analysis of the Hessian of Over-Parametrized Neural Networks. arXiv preprint arXiv:1706.04454, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04454"
        },
        {
            "id": "37",
            "entry": "[37] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou. Overcoming Catastrophic Forgetting with Hard Attention to the Task. arXiv preprint arXiv:1801.01423, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01423"
        },
        {
            "id": "38",
            "entry": "[38] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual Learning with Deep Generative Replay. arXiv preprint arXiv:1705.08690, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.08690"
        },
        {
            "id": "39",
            "entry": "[39] Theano Development Team. Theano: A Python Framework for Fast Computation of Mathematical Expressions. arXiv e-prints, abs/1605.02688, May 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.02688"
        },
        {
            "id": "40",
            "entry": "[40] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv preprint arXiv:1708.07747, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        },
        {
            "id": "41",
            "entry": "[41] F. Zenke, B. Poole, and S. Ganguli. Continual Learning through Synaptic Intelligence. In International Conference on Machine Learning, pages 3987\u20133995, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zenke%2C%20F.%20Poole%2C%20B.%20Ganguli%2C%20S.%20Continual%20Learning%20through%20Synaptic%20Intelligence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zenke%2C%20F.%20Poole%2C%20B.%20Ganguli%2C%20S.%20Continual%20Learning%20through%20Synaptic%20Intelligence%202017"
        }
    ]
}
