{
    "filename": "7602-actor-critic-policy-optimization-in-partially-observable-multiagent-environments.pdf",
    "metadata": {
        "date": 2018,
        "title": "Actor-Critic Policy Optimization in Partially Observable Multiagent Environments",
        "author": "Sriram Srinivasan\u2217,1 srsrinivasan@",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7602-actor-critic-policy-optimization-in-partially-observable-multiagent-environments.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero-sum games, without any domain-specific state space reductions."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "game theory",
            "url": "https://en.wikipedia.org/wiki/game_theory"
        },
        {
            "term": "extensive form",
            "url": "https://en.wikipedia.org/wiki/extensive_form"
        }
    ],
    "highlights": [
        "Variant, model-free outcome sampling (MFOS), the behavior policy at opponent states is defined as \u03c0\u2212i enabling online regret minimization",
        "We show that actor-critics reduce to a form of regret minimization and propose several policy update rules inspired by this connection",
        "Variant, model-free outcome sampling (MFOS), the behavior policy at opponent states is defined as \u03c0\u2212i enabling online regret minimization",
        "We show a connection between these algorithms and regret minimization, leading to previously unknown convergence properties underlying model-free multiagent RL in zero-sum games with imperfect information",
        "Our experiments show that these actor-critic algorithms converge to approximate Nash equilibria in commonly-used benchmark Poker domains with rates similar to or better than baseline model-free algorithms for zero-sum games",
        "Of the actor-critic variants, RPG and q-based Policy Gradient seem to outperform Regret Matching Policy Gradient in our experiments"
    ],
    "key_statements": [
        "Variant, model-free outcome sampling (MFOS), the behavior policy at opponent states is defined as \u03c0\u2212i enabling online regret minimization",
        "We show that actor-critics reduce to a form of regret minimization and propose several policy update rules inspired by this connection",
        "Variant, model-free outcome sampling (MFOS), the behavior policy at opponent states is defined as \u03c0\u2212i enabling online regret minimization",
        "We show a connection between these algorithms and regret minimization, leading to previously unknown convergence properties underlying model-free multiagent RL in zero-sum games with imperfect information",
        "Our experiments show that these actor-critic algorithms converge to approximate Nash equilibria in commonly-used benchmark Poker domains with rates similar to or better than baseline model-free algorithms for zero-sum games",
        "Of the actor-critic variants, RPG and q-based Policy Gradient seem to outperform Regret Matching Policy Gradient in our experiments",
        "We are curious about what role the connections between actor-critic methods and counterfactual regret could play in deriving convergence guarantees in model-free multiagent RL for cooperative and/or potential games"
    ],
    "summary": [
        "Variant, model-free outcome sampling (MFOS), the behavior policy at opponent states is defined as \u03c0\u2212i enabling online regret minimization.",
        "Actor-critics were recently studied in this setting for multiagent games [<a class=\"ref-link\" id=\"c68\" href=\"#r68\">68</a>], whereas we focus on partially-observable environments; only tabular methods are known to converge.",
        "REINFORCE [<a class=\"ref-link\" id=\"c95\" href=\"#r95\">95</a>] samples trajectories and computes gradients for each state st, updating \u03b8 toward \u2207\u03b8 logGt. A baseline is often subtracted from the return: Gt \u2212 v\u03c0, and policy gradients become actor-critics, training \u03c0 and v\u03c0 separately.",
        "We show the average policy dynamics and observe convergence to equilibrium in each game we tried, which is a known to be guaranteed in two-player zero-sum games using CFR, fictitious play [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], and continuous replicator dynamics [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>].",
        "This leads to a new interpretation of actor-critic algorithms in the multiagent partially observable setting: the advantage values q\u03c0,i \u2212 v\u03c0,i are immediate counterfactual regrets scaled by 1/B\u2212i(\u03c0, st).",
        "In two-player zero-sum games, PGPI/ACPI are gradient ascent-descent problems, because each player is trying to ascend their own score function, and when using tabular policies a solution exists due to the minimax theorem [<a class=\"ref-link\" id=\"c79\" href=\"#r79\">79</a>].",
        "If player i collects a batch of data from many sampled episodes and applies them all at once, the effective learning rates is scaled by the probability of reaching s: \u03b7i\u03c0(s)B\u2212i(\u03c0, s), which matches the value in the condition of Theorem 1.",
        "The analysis so far has concentrated on establishing guarantees for the optimization problem that underlies standard formulation of policy gradient and actor-critic algorithms.",
        "In two-player zero-sum games, when using tabular policies and projection P (\u03b8) as defined in Theorem 1 with learning rates \u03b1k",
        "We evaluate the actor-critic algorithms on two n-player games: Kuhn poker, and Leduc poker.",
        "We show a connection between these algorithms and regret minimization, leading to previously unknown convergence properties underlying model-free MARL in zero-sum games with imperfect information.",
        "Our experiments show that these actor-critic algorithms converge to approximate Nash equilibria in commonly-used benchmark Poker domains with rates similar to or better than baseline model-free algorithms for zero-sum games.",
        "We would like to formally develop the guarantees of the sample-based on-policy Monte Carlo CFR algorithms and/or extend to continuing tasks as in MDPs [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>].",
        "We are curious about what role the connections between actor-critic methods and CFR could play in deriving convergence guarantees in model-free MARL for cooperative and/or potential games.",
        "Of the actor-critic variants, RPG and QPG seem to outperform RMPG in our experiments"
    ],
    "headline": "We examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Sherief Abdallah and Victor Lesser. A multiagent reinforcement learning algorithm with non-linear dynamics. JAIR, 33(1):521\u2013549, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abdallah%2C%20Sherief%20Lesser%2C%20Victor%20A%20multiagent%20reinforcement%20learning%20algorithm%20with%20non-linear%20dynamics%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abdallah%2C%20Sherief%20Lesser%2C%20Victor%20A%20multiagent%20reinforcement%20learning%20algorithm%20with%20non-linear%20dynamics%202008"
        },
        {
            "id": "2",
            "entry": "[2] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Nicolas Heess Remi Munos, and Martin Riedmiller. Maximum a posteriori policy optimisation. CoRR, abs/1806.06920, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06920"
        },
        {
            "id": "3",
            "entry": "[3] Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. In Proceedings of the Sixth International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Al-Shedivat%2C%20Maruan%20Bansal%2C%20Trapit%20Burda%2C%20Yuri%20Sutskever%2C%20Ilya%20Continuous%20adaptation%20via%20meta-learning%20in%20nonstationary%20and%20competitive%20environments%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Al-Shedivat%2C%20Maruan%20Bansal%2C%20Trapit%20Burda%2C%20Yuri%20Sutskever%2C%20Ilya%20Continuous%20adaptation%20via%20meta-learning%20in%20nonstationary%20and%20competitive%20environments%202018"
        },
        {
            "id": "4",
            "entry": "[4] Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intelligence, 258:66\u201395, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Albrecht%2C%20Stefano%20V.%20Stone%2C%20Peter%20Autonomous%20agents%20modelling%20other%20agents%3A%20A%20comprehensive%20survey%20and%20open%20problems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Albrecht%2C%20Stefano%20V.%20Stone%2C%20Peter%20Autonomous%20agents%20modelling%20other%20agents%3A%20A%20comprehensive%20survey%20and%20open%20problems%202018"
        },
        {
            "id": "5",
            "entry": "[5] Cameron Allen, Melrose Roderick Kavosh Asadi, Abdel rahman Mohamed, George Konidaris, and Michael Littman. Mean actor critic. CoRR, abs/1709.00503, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.00503"
        },
        {
            "id": "6",
            "entry": "[6] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, pages 322\u2013331, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20Gambling%20in%20a%20rigged%20casino%3A%20The%20adversarial%20multi-armed%20bandit%20problem%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Auer%2C%20P.%20Cesa-Bianchi%2C%20N.%20Freund%2C%20Y.%20Schapire%2C%20R.E.%20Gambling%20in%20a%20rigged%20casino%3A%20The%20adversarial%20multi-armed%20bandit%20problem%201995"
        },
        {
            "id": "7",
            "entry": "[7] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent complexity via multi-agent competition. In Proceedings of the Sixth International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bansal%2C%20Trapit%20Pachocki%2C%20Jakub%20Sidor%2C%20Szymon%20Sutskever%2C%20Ilya%20Emergent%20complexity%20via%20multi-agent%20competition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bansal%2C%20Trapit%20Pachocki%2C%20Jakub%20Sidor%2C%20Szymon%20Sutskever%2C%20Ilya%20Emergent%20complexity%20via%20multi-agent%20competition%202018"
        },
        {
            "id": "8",
            "entry": "[8] Daan Bloembergen, Karl Tuyls, Daniel Hennes, and Michael Kaisers. Evolutionary dynamics of multi-agent learning: A survey. J. Artif. Intell. Res. (JAIR), 53:659\u2013697, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bloembergen%2C%20Daan%20Tuyls%2C%20Karl%20Hennes%2C%20Daniel%20Kaisers%2C%20Michael%20Evolutionary%20dynamics%20of%20multi-agent%20learning%3A%20A%20survey%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bloembergen%2C%20Daan%20Tuyls%2C%20Karl%20Hennes%2C%20Daniel%20Kaisers%2C%20Michael%20Evolutionary%20dynamics%20of%20multi-agent%20learning%3A%20A%20survey%202015"
        },
        {
            "id": "9",
            "entry": "[9] A. Blum and Y. Mansour. Learning, regret minimization, and equilibria. In Algorithmic Game Theory, chapter 4. Cambridge University Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20A.%20Mansour%2C%20Y.%20Learning%2C%20regret%20minimization%2C%20and%20equilibria.%20In%20Algorithmic%20Game%20Theory%2C%20chapter%204%202007"
        },
        {
            "id": "10",
            "entry": "[10] Branislav Bo\u0161ansk\u00fd, Viliam Lis\u00fd, Marc Lanctot, Jir\u00ed Cerm\u00e1k, and Mark H.M. Winands. Algorithms for computing strategies in two-player simultaneous move games. Artificial Intelligence, 237:1\u2014-40, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bo%C5%A1ansk%C3%BD%2C%20Branislav%20Lis%C3%BD%2C%20Viliam%20Lanctot%2C%20Marc%20Cerm%C3%A1k%2C%20Jir%C3%AD%20Algorithms%20for%20computing%20strategies%20in%20two-player%20simultaneous%20move%20games%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bo%C5%A1ansk%C3%BD%2C%20Branislav%20Lis%C3%BD%2C%20Viliam%20Lanctot%2C%20Marc%20Cerm%C3%A1k%2C%20Jir%C3%AD%20Algorithms%20for%20computing%20strategies%20in%20two-player%20simultaneous%20move%20games%202016"
        },
        {
            "id": "11",
            "entry": "[11] Michael Bowling. Convergence and no-regret in multiagent learning. In Advances in Neural Information Processing Systems 17 (NIPS), pages 209\u2013216, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20Michael%20Convergence%20and%20no-regret%20in%20multiagent%20learning%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20Michael%20Convergence%20and%20no-regret%20in%20multiagent%20learning%202005"
        },
        {
            "id": "12",
            "entry": "[12] Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up Limit Hold\u2019em Poker is solved. Science, 347(6218):145\u2013149, January 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20Michael%20Burch%2C%20Neil%20Johanson%2C%20Michael%20Tammelin%2C%20Oskari%20Heads-up%20Limit%20Hold%E2%80%99em%20Poker%20is%20solved%202015-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20Michael%20Burch%2C%20Neil%20Johanson%2C%20Michael%20Tammelin%2C%20Oskari%20Heads-up%20Limit%20Hold%E2%80%99em%20Poker%20is%20solved%202015-01"
        },
        {
            "id": "13",
            "entry": "[13] Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Artificial Intelligence, 136:215\u2013250, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Multiagent%20learning%20using%20a%20variable%20learning%20rate%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bowling%2C%20Michael%20Veloso%2C%20Manuela%20Multiagent%20learning%20using%20a%20variable%20learning%20rate%202002"
        },
        {
            "id": "14",
            "entry": "[14] G. W. Brown. Iterative solutions of games by fictitious play. In T.C. Koopmans, editor, Activity Analysis of Production and Allocation, pages 374\u2013376. John Wiley & Sons, Inc., 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20G.W.%20Iterative%20solutions%20of%20games%20by%20fictitious%20play%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brown%2C%20G.W.%20Iterative%20solutions%20of%20games%20by%20fictitious%20play%201951"
        },
        {
            "id": "15",
            "entry": "[15] Noam Brown, Christian Kroer, and Tuomas Sandholm. Dynamic thresholding and pruning for regret minimization. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20Noam%20Kroer%2C%20Christian%20Sandholm%2C%20Tuomas%20Dynamic%20thresholding%20and%20pruning%20for%20regret%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brown%2C%20Noam%20Kroer%2C%20Christian%20Sandholm%2C%20Tuomas%20Dynamic%20thresholding%20and%20pruning%20for%20regret%20minimization%202017"
        },
        {
            "id": "16",
            "entry": "[16] Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 360(6385), December 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20Noam%20Sandholm%2C%20Tuomas%20Superhuman%20AI%20for%20heads-up%20no-limit%20poker%3A%20Libratus%20beats%20top%20professionals%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brown%2C%20Noam%20Sandholm%2C%20Tuomas%20Superhuman%20AI%20for%20heads-up%20no-limit%20poker%3A%20Libratus%20beats%20top%20professionals%202017-12"
        },
        {
            "id": "17",
            "entry": "[17] L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transaction on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 38(2):156\u2013172, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Busoniu%2C%20L.%20Babuska%2C%20R.%20Schutter%2C%20B.De%20A%20comprehensive%20survey%20of%20multiagent%20reinforcement%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Busoniu%2C%20L.%20Babuska%2C%20R.%20Schutter%2C%20B.De%20A%20comprehensive%20survey%20of%20multiagent%20reinforcement%20learning%202008"
        },
        {
            "id": "18",
            "entry": "[18] Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark. Emergent communication through negotiation. In Proceedings of the Sixth International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20Kris%20Lazaridou%2C%20Angeliki%20Lanctot%2C%20Marc%20Leibo%2C%20Joel%20Z.%20Emergent%20communication%20through%20negotiation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20Kris%20Lazaridou%2C%20Angeliki%20Lanctot%2C%20Marc%20Leibo%2C%20Joel%20Z.%20Emergent%20communication%20through%20negotiation%202018"
        },
        {
            "id": "19",
            "entry": "[19] Kamil Ciosek and Shimon Whiteson. Expected policy gradients. In Proceedings of the Thirty-Second AAAI conference on Artificial Intelligence (AAAI-18), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciosek%2C%20Kamil%20Whiteson%2C%20Shimon%20Expected%20policy%20gradients%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciosek%2C%20Kamil%20Whiteson%2C%20Shimon%20Expected%20policy%20gradients%202018"
        },
        {
            "id": "20",
            "entry": "[20] Constantinos Daskalakis, Paul W. Goldberg, and Christos H. Papadimitriou. The complexity of computing a nash equilibrium. In Proceedings of the Thirty-eighth Annual ACM Symposium on Theory of Computing, STOC \u201906, pages 71\u201378, New York, NY, USA, 2006. ACM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20Constantinos%20Goldberg%2C%20Paul%20W.%20Papadimitriou%2C%20Christos%20H.%20The%20complexity%20of%20computing%20a%20nash%20equilibrium%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20Constantinos%20Goldberg%2C%20Paul%20W.%20Papadimitriou%2C%20Christos%20H.%20The%20complexity%20of%20computing%20a%20nash%20equilibrium%202006"
        },
        {
            "id": "21",
            "entry": "[21] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. CoRR, abs/1604.06778, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.06778"
        },
        {
            "id": "22",
            "entry": "[22] Lasse Espeholt, Hubert Soyer, R\u00e9mi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. CoRR, abs/1802.01561, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01561"
        },
        {
            "id": "23",
            "entry": "[23] Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Chen%2C%20Richard%20Y.%20Al-Shedivat%2C%20Maruan%20Whiteson%2C%20Shimon%20Learning%20with%20opponent-learning%20awareness%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Chen%2C%20Richard%20Y.%20Al-Shedivat%2C%20Maruan%20Whiteson%2C%20Shimon%20Learning%20with%20opponent-learning%20awareness%202017"
        },
        {
            "id": "24",
            "entry": "[24] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foerster%2C%20Jakob%20N.%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Foerster%2C%20Jakob%20N.%20Farquhar%2C%20Gregory%20Afouras%2C%20Triantafyllos%20Nardelli%2C%20Nantas%20Counterfactual%20multi-agent%20policy%20gradients%202017"
        },
        {
            "id": "25",
            "entry": "[25] N. Gatti, F. Panozzo, and M. Restelli. Efficient evolutionary dynamics with extensive-form games. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, pages 335\u2013341, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gatti%2C%20N.%20Panozzo%2C%20F.%20Restelli%2C%20M.%20Efficient%20evolutionary%20dynamics%20with%20extensive-form%20games%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gatti%2C%20N.%20Panozzo%2C%20F.%20Restelli%2C%20M.%20Efficient%20evolutionary%20dynamics%20with%20extensive-form%20games%202013"
        },
        {
            "id": "26",
            "entry": "[26] Richard Gibson. Regret minimization in non-zero-sum games with applications to building champion multiplayer computer poker agents. CoRR, abs/1305.0034, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1305.0034"
        },
        {
            "id": "27",
            "entry": "[27] Shixiang Gu, Ethan Holly, Timothy P. Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation. CoRR, abs/1610.00633, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.00633"
        },
        {
            "id": "28",
            "entry": "[28] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127\u20131150, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hart%2C%20S.%20Mas-Colell%2C%20A.%20A%20simple%20adaptive%20procedure%20leading%20to%20correlated%20equilibrium%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hart%2C%20S.%20Mas-Colell%2C%20A.%20A%20simple%20adaptive%20procedure%20leading%20to%20correlated%20equilibrium%202000"
        },
        {
            "id": "29",
            "entry": "[29] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3\u20134):157\u2013325, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Introduction%20to%20online%20convex%20optimization%202015"
        },
        {
            "id": "30",
            "entry": "[30] He He, Jordan L. Boyd-Graber, Kevin Kwok, and Hal Daum\u00e9 III. Opponent modeling in deep reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning (ICML 2016), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20He%20Boyd-Graber%2C%20Jordan%20L.%20Kwok%2C%20Kevin%20Daum%C3%A9%2C%20III%2C%20Hal%20Opponent%20modeling%20in%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20He%20Boyd-Graber%2C%20Jordan%20L.%20Kwok%2C%20Kevin%20Daum%C3%A9%2C%20III%2C%20Hal%20Opponent%20modeling%20in%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "31",
            "entry": "[31] Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heinrich%2C%20Johannes%20Lanctot%2C%20Marc%20Silver%2C%20David%20Fictitious%20self-play%20in%20extensive-form%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heinrich%2C%20Johannes%20Lanctot%2C%20Marc%20Silver%2C%20David%20Fictitious%20self-play%20in%20extensive-form%20games%202015"
        },
        {
            "id": "32",
            "entry": "[32] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfectinformation games. CoRR, abs/1603.01121, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.01121"
        },
        {
            "id": "33",
            "entry": "[33] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learning in multiagent environments: Dealing with non-stationarity. CoRR, abs/1707.09183, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09183"
        },
        {
            "id": "34",
            "entry": "[34] Josef Hofbauer and Karl Sigmund. Evolutionary Games and Population Dynamics. Cambridge University Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hofbauer%2C%20Josef%20Sigmund%2C%20Karl%20Evolutionary%20Games%20and%20Population%20Dynamics%201998"
        },
        {
            "id": "35",
            "entry": "[35] Josef Hofbauer, Sylvain Sorin, and Yannick Viossat. Time average replicator and best-reply dynamics. Mathematics of Operations Research, 34(2):263\u2013269, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hofbauer%2C%20Josef%20Sorin%2C%20Sylvain%20Viossat%2C%20Yannick%20Time%20average%20replicator%20and%20best-reply%20dynamics%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hofbauer%2C%20Josef%20Sorin%2C%20Sylvain%20Viossat%2C%20Yannick%20Time%20average%20replicator%20and%20best-reply%20dynamics%202009"
        },
        {
            "id": "36",
            "entry": "[36] Zhang-Wei Hong, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, and Chun-Yi Lee. A deep policy inference q-network for multi-agent systems. CoRR, abs/1712.07893, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07893"
        },
        {
            "id": "37",
            "entry": "[37] Max Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In Proceedings of the International Conference on Representation Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Max%20Jaderberg%2C%20V.Mnih%20Czarnecki%2C%20W.M.%20Schaul%2C%20T.%20Leibo%2C%20J.Z.%20Reinforcement%20learning%20with%20unsupervised%20auxiliary%20tasks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Max%20Jaderberg%2C%20V.Mnih%20Czarnecki%2C%20W.M.%20Schaul%2C%20T.%20Leibo%2C%20J.Z.%20Reinforcement%20learning%20with%20unsupervised%20auxiliary%20tasks%202017"
        },
        {
            "id": "38",
            "entry": "[38] Peter H. Jin, Sergey Levine, and Kurt Keutzer. Regret minimization for partially observable deep reinforcement learning. CoRR, abs/1710.11424, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.11424"
        },
        {
            "id": "39",
            "entry": "[39] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101:99\u2013134, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kaelbling%2C%20Leslie%20Pack%20Littman%2C%20Michael%20L.%20Cassandra%2C%20Anthony%20R.%20Planning%20and%20acting%20in%20partially%20observable%20stochastic%20domains%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kaelbling%2C%20Leslie%20Pack%20Littman%2C%20Michael%20L.%20Cassandra%2C%20Anthony%20R.%20Planning%20and%20acting%20in%20partially%20observable%20stochastic%20domains%201998"
        },
        {
            "id": "40",
            "entry": "[40] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the Nineteenth International Conference on Machine Learning, ICML \u201902, pages 267\u2013274, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20Langford%2C%20John%20Approximately%20optimal%20approximate%20reinforcement%20learning%202002"
        },
        {
            "id": "41",
            "entry": "[41] Ian A. Kash and Katja Hoffman. Combining no-regret and Q-learning. In European Workshop on Reinforcement Learning (EWRL) 14, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kash%2C%20Ian%20A.%20Hoffman%2C%20Katja%20Combining%20no-regret%20and%20Q-learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kash%2C%20Ian%20A.%20Hoffman%2C%20Katja%20Combining%20no-regret%20and%20Q-learning%202018"
        },
        {
            "id": "42",
            "entry": "[42] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "43",
            "entry": "[43] Vojtech Kovar\u00edk and Viliam Lis\u00fd. Analysis of hannan consistent selection for monte carlo tree search in simultaneous move games. CoRR, abs/1509.00149, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00149"
        },
        {
            "id": "44",
            "entry": "[44] H. W. Kuhn. Extensive games and the problem of information. Contributions to the Theory of Games, 2:193\u2013216, 1953.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuhn%2C%20H.W.%20Extensive%20games%20and%20the%20problem%20of%20information.%20Contributions%20to%20the%20Theory%20of%201953",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuhn%2C%20H.W.%20Extensive%20games%20and%20the%20problem%20of%20information.%20Contributions%20to%20the%20Theory%20of%201953"
        },
        {
            "id": "45",
            "entry": "[45] Shapley L. Some topics in two-person games. In Advances in Game Theory. Princeton University Press., 1964.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shapley%2C%20L.%20Some%20topics%20in%20two-person%20games.%20In%20Advances%20in%20Game%20Theory%201964"
        },
        {
            "id": "46",
            "entry": "[46] M. Lanctot, K. Waugh, M. Bowling, and M. Zinkevich. Sampling for regret minimization in extensive games. In Advances in Neural Information Processing Systems (NIPS 2009), pages 1078\u20131086, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20M.%20Waugh%2C%20K.%20Bowling%2C%20M.%20Zinkevich%2C%20M.%20Sampling%20for%20regret%20minimization%20in%20extensive%20games%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lanctot%2C%20M.%20Waugh%2C%20K.%20Bowling%2C%20M.%20Zinkevich%2C%20M.%20Sampling%20for%20regret%20minimization%20in%20extensive%20games%202009"
        },
        {
            "id": "47",
            "entry": "[47] Marc Lanctot. Monte Carlo Sampling and Regret Minimization for Equilibrium Computation and Decision-Making in Large Extensive Form Games. PhD thesis, Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada, June 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20Marc%20Monte%20Carlo%20Sampling%20and%20Regret%20Minimization%20for%20Equilibrium%20Computation%20and%20Decision-Making%20in%20Large%20Extensive%20Form%20Games%202013-06"
        },
        {
            "id": "48",
            "entry": "[48] Marc Lanctot. Further developments of extensive-form replicator dynamics using the sequenceform representation. In Proceedings of the Thirteenth International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 1257\u20131264, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20Marc%20Further%20developments%20of%20extensive-form%20replicator%20dynamics%20using%20the%20sequenceform%20representation%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lanctot%2C%20Marc%20Further%20developments%20of%20extensive-form%20replicator%20dynamics%20using%20the%20sequenceform%20representation%202014"
        },
        {
            "id": "49",
            "entry": "[49] Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte Carlo sampling for regret minimization in extensive games. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1078\u20131086, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20Marc%20Waugh%2C%20Kevin%20Zinkevich%2C%20Martin%20Bowling%2C%20Michael%20Monte%20Carlo%20sampling%20for%20regret%20minimization%20in%20extensive%20games%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lanctot%2C%20Marc%20Waugh%2C%20Kevin%20Zinkevich%2C%20Martin%20Bowling%2C%20Michael%20Monte%20Carlo%20sampling%20for%20regret%20minimization%20in%20extensive%20games%202009"
        },
        {
            "id": "50",
            "entry": "[50] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lanctot%2C%20Marc%20Zambaldi%2C%20Vinicius%20Gruslys%2C%20Audrunas%20Lazaridou%2C%20Angeliki%20and%20Thore%20Graepel.%20A%20unified%20game-theoretic%20approach%20to%20multiagent%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lanctot%2C%20Marc%20Zambaldi%2C%20Vinicius%20Gruslys%2C%20Audrunas%20Lazaridou%2C%20Angeliki%20and%20Thore%20Graepel.%20A%20unified%20game-theoretic%20approach%20to%20multiagent%20reinforcement%20learning%202017"
        },
        {
            "id": "51",
            "entry": "[51] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. In Proceedings of the International Conference on Learning Representations (ICLR), April 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lazaridou%2C%20Angeliki%20Peysakhovich%2C%20Alexander%20Baroni%2C%20Marco%20Multi-agent%20cooperation%20and%20the%20emergence%202017-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lazaridou%2C%20Angeliki%20Peysakhovich%2C%20Alexander%20Baroni%2C%20Marco%20Multi-agent%20cooperation%20and%20the%20emergence%202017-04"
        },
        {
            "id": "52",
            "entry": "[52] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:436\u2013444, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bengio%2C%20Yoshua%20Hinton%2C%20Geoffrey%20Deep%20learning%202015"
        },
        {
            "id": "53",
            "entry": "[53] Adam Lerer and Alexander Peysakhovich. Maintaining cooperation in complex social dilemmas using deep reinforcement learning. CoRR, abs/1707.01068, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.01068"
        },
        {
            "id": "54",
            "entry": "[54] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "55",
            "entry": "[55] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In In Proceedings of the Eleventh International Conference on Machine Learning, pages 157\u2013163. Morgan Kaufmann, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Littman%2C%20Michael%20L.%20Markov%20games%20as%20a%20framework%20for%20multi-agent%20reinforcement%20learning%201994"
        },
        {
            "id": "56",
            "entry": "[56] Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent control variates for policy optimization via stein identity. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Hao%20Feng%2C%20Yihao%20Mao%2C%20Yi%20Zhou%2C%20Dengyong%20Action-dependent%20control%20variates%20for%20policy%20optimization%20via%20stein%20identity%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Hao%20Feng%2C%20Yihao%20Mao%2C%20Yi%20Zhou%2C%20Dengyong%20Action-dependent%20control%20variates%20for%20policy%20optimization%20via%20stein%20identity%202018"
        },
        {
            "id": "57",
            "entry": "[57] Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6379\u20136390. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ryan%20Lowe%2C%20Y.I.W.U.%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Abbeel%2C%20OpenAI%20Pieter%20Multiagent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ryan%20Lowe%2C%20Y.I.W.U.%20Tamar%2C%20Aviv%20Harb%2C%20Jean%20Abbeel%2C%20OpenAI%20Pieter%20Multiagent%20actor-critic%20for%20mixed%20cooperative-competitive%20environments%202017"
        },
        {
            "id": "58",
            "entry": "[58] L. Matignon, G. J. Laurent, and N. Le Fort-Piat. Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems. The Knowledge Engineering Review, 27(01):1\u201331, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matignon%2C%20L.%20Laurent%2C%20G.J.%20Fort-Piat%2C%20N.Le%20Independent%20reinforcement%20learners%20in%20cooperative%20Markov%20games%3A%20a%20survey%20regarding%20coordination%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matignon%2C%20L.%20Laurent%2C%20G.J.%20Fort-Piat%2C%20N.Le%20Independent%20reinforcement%20learners%20in%20cooperative%20Markov%20games%3A%20a%20survey%20regarding%20coordination%20problems%202012"
        },
        {
            "id": "59",
            "entry": "[59] Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 1928\u20131937, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Badia%2C%20Adri%C3%A0%20Puigdom%C3%A8nech%20Mirza%2C%20Mehdi%20Graves%2C%20Alex%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016"
        },
        {
            "id": "60",
            "entry": "[60] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u2013533, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20Volodymyr%20Kavukcuoglu%2C%20Koray%20Silver%2C%20David%20Rusu%2C%20Andrei%20A.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015"
        },
        {
            "id": "61",
            "entry": "[61] Matej Moravc\u00edk, Martin Schmid, Neil Burch, Viliam Lis\u00fd, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 358(6362), October 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moravc%C3%ADk%2C%20Matej%20Schmid%2C%20Martin%20Burch%2C%20Neil%20Lis%C3%BD%2C%20Viliam%20Deepstack%3A%20Expert-level%20artificial%20intelligence%20in%20heads-up%20no-limit%20poker%202017-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moravc%C3%ADk%2C%20Matej%20Schmid%2C%20Martin%20Burch%2C%20Neil%20Lis%C3%BD%2C%20Viliam%20Deepstack%3A%20Expert-level%20artificial%20intelligence%20in%20heads-up%20no-limit%20poker%202017-10"
        },
        {
            "id": "62",
            "entry": "[62] Todd W. Neller and Marc Lanctot. An introduction to counterfactual regret minimization. In Proceedings of Model AI Assignments, The Fourth Symposium on Educational Advances in Artificial Intelligence (EAAI-2013), 2013. http://modelai.gettysburg.edu/2013/cfr/index.html.",
            "url": "http://modelai.gettysburg.edu/2013/cfr/index.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neller%2C%20Todd%20W.%20Lanctot%2C%20Marc%20An%20introduction%20to%20counterfactual%20regret%20minimization%202013"
        },
        {
            "id": "63",
            "entry": "[63] Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Policy gradient with value function approximation for collective multiagent planning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4319\u20134329. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20Duc%20Thien%20Kumar%2C%20Akshat%20Lau%2C%20Hoong%20Chuin%20Policy%20gradient%20with%20value%20function%20approximation%20for%20collective%20multiagent%20planning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20Duc%20Thien%20Kumar%2C%20Akshat%20Lau%2C%20Hoong%20Chuin%20Policy%20gradient%20with%20value%20function%20approximation%20for%20collective%20multiagent%20planning%202017"
        },
        {
            "id": "64",
            "entry": "[64] A. Now\u00e9, P. Vrancx, and Y-M. De Hauwere. Game theory and multi-agent reinforcement learning. In Reinforcement Learning: State-of-the-Art, chapter 14, pages 441\u2013470. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Now%C3%A9%2C%20A.%20Vrancx%2C%20P.%20Hauwere%2C%20Y.-M.De%20Game%20theory%20and%20multi-agent%20reinforcement%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Now%C3%A9%2C%20A.%20Vrancx%2C%20P.%20Hauwere%2C%20Y.-M.De%20Game%20theory%20and%20multi-agent%20reinforcement%20learning%202012"
        },
        {
            "id": "65",
            "entry": "[65] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frans%2C%20A.%20Oliehoek%20and%20Christopher%20Amato.%20A%20Concise%20Introduction%20to%20Decentralized%20POMDPs%202016"
        },
        {
            "id": "66",
            "entry": "[66] Fabio Panozzo, Nicola Gatti, and Marcello Restelli. Evolutionary dynamics of q-learning over the sequence form. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 2034\u20132040, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Panozzo%2C%20Fabio%20Gatti%2C%20Nicola%20Restelli%2C%20Marcello%20Evolutionary%20dynamics%20of%20q-learning%20over%20the%20sequence%20form%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Panozzo%2C%20Fabio%20Gatti%2C%20Nicola%20Restelli%2C%20Marcello%20Evolutionary%20dynamics%20of%20q-learning%20over%20the%20sequence%20form%202014"
        },
        {
            "id": "67",
            "entry": "[67] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills. CoRR, abs/1804.02717, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.02717"
        },
        {
            "id": "68",
            "entry": "[68] Julien Perolat, Bilal Piot, and Olivier Pietquin. Actor-critic fictitious play in simultaneous move multistage games. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 919\u2013928, Playa Blanca, Lanzarote, Canary Islands, 09\u201311 Apr 2018. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Perolat%2C%20Julien%20Piot%2C%20Bilal%20Pietquin%2C%20Olivier%20Actor-critic%20fictitious%20play%20in%20simultaneous%20move%20multistage%20games%202018-04",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Perolat%2C%20Julien%20Piot%2C%20Bilal%20Pietquin%2C%20Olivier%20Actor-critic%20fictitious%20play%20in%20simultaneous%20move%20multistage%20games%202018-04"
        },
        {
            "id": "69",
            "entry": "[69] Julien P\u00e9rolat, Bilal Piot, Bruno Scherrer, and Olivier Pietquin. On the use of non-stationary strategies for solving two-player zero-sum markov games. In The 19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=P%C3%A9rolat%2C%20Julien%20Piot%2C%20Bilal%20Scherrer%2C%20Bruno%20Pietquin%2C%20Olivier%20On%20the%20use%20of%20non-stationary%20strategies%20for%20solving%20two-player%20zero-sum%20markov%20games%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=P%C3%A9rolat%2C%20Julien%20Piot%2C%20Bilal%20Scherrer%2C%20Bruno%20Pietquin%2C%20Olivier%20On%20the%20use%20of%20non-stationary%20strategies%20for%20solving%20two-player%20zero-sum%20markov%20games%202016"
        },
        {
            "id": "70",
            "entry": "[70] Julien P\u00e9rolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming for two-player zero-sum markov games. In Proceedings of the International Conference on Machine Learning (ICML), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=P%C3%A9rolat%2C%20Julien%20Scherrer%2C%20Bruno%20Piot%2C%20Bilal%20Pietquin%2C%20Olivier%20Approximate%20dynamic%20programming%20for%20two-player%20zero-sum%20markov%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=P%C3%A9rolat%2C%20Julien%20Scherrer%2C%20Bruno%20Piot%2C%20Bilal%20Pietquin%2C%20Olivier%20Approximate%20dynamic%20programming%20for%20two-player%20zero-sum%20markov%20games%202015"
        },
        {
            "id": "71",
            "entry": "[71] Jan Peters. Policy gradient methods for control applications. Technical Report TR-CLMC2007-1, University of Southern California, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20Policy%20gradient%20methods%20for%20control%20applications%202002"
        },
        {
            "id": "72",
            "entry": "[72] Yu Qian, Fang Debin, Zhang Xiaoling, Jin Chen, and Ren Qiyu. Stochastic evolution dynamic of the rock\u2013scissors\u2013paper game based on a quasi birth and death process. Scientific Reports, 6(1):28585, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qian%2C%20Yu%20Debin%2C%20Fang%20Xiaoling%2C%20Zhang%20Chen%2C%20Jin%20Stochastic%20evolution%20dynamic%20of%20the%20rock%E2%80%93scissors%E2%80%93paper%20game%20based%20on%20a%20quasi%20birth%20and%20death%20process%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qian%2C%20Yu%20Debin%2C%20Fang%20Xiaoling%2C%20Zhang%20Chen%2C%20Jin%20Stochastic%20evolution%20dynamic%20of%20the%20rock%E2%80%93scissors%E2%80%93paper%20game%20based%20on%20a%20quasi%20birth%20and%20death%20process%202016"
        },
        {
            "id": "73",
            "entry": "[73] Deirdre Quillen, Eric Jang, Ofir Nachum, Chelsea Finn, Julian Ibarz, and Sergey Levine. Deep reinforcement learning for vision-based robotic grasping: A simulated comparative evaluation of off-policy methods. CoRR, abs/1802.10264, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.10264"
        },
        {
            "id": "74",
            "entry": "[74] N. A. Risk and D. Szafron. Using counterfactual regret minimization to create competitive multiplayer poker agents. In Proceedings of the International Conference on Autonomus Agents and Multiagent Systems (AAMAS), pages 159\u2013166, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Risk%2C%20N.A.%20Szafron%2C%20D.%20Using%20counterfactual%20regret%20minimization%20to%20create%20competitive%20multiplayer%20poker%20agents%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Risk%2C%20N.A.%20Szafron%2C%20D.%20Using%20counterfactual%20regret%20minimization%20to%20create%20competitive%20multiplayer%20poker%20agents%202010"
        },
        {
            "id": "75",
            "entry": "[75] William H. Sandholm. Population Games and Evolutionary Dynamics. The MIT Press, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sandholm%2C%20William%20H.%20Population%20Games%20and%20Evolutionary%20Dynamics%202010"
        },
        {
            "id": "76",
            "entry": "[76] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1502.05477"
        },
        {
            "id": "77",
            "entry": "[77] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "78",
            "entry": "[78] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. CoRR, abs/1610.03295, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.03295"
        },
        {
            "id": "79",
            "entry": "[79] Y. Shoham and K. Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. Cambridge University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shoham%2C%20Y.%20Leyton-Brown%2C%20K.%20Multiagent%20Systems%3A%20Algorithmic%2C%20Game-Theoretic%2C%20and%20Logical%20Foundations%202009"
        },
        {
            "id": "80",
            "entry": "[80] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529:484\u2014-489, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Huang%2C%20Aja%20Maddison%2C%20Chris%20J.%20Guez%2C%20Arthur%20Thore%20Graepel%2C%20and%20Demis%20Hassabis.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016"
        },
        {
            "id": "81",
            "entry": "[81] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.01815"
        },
        {
            "id": "82",
            "entry": "[82] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 530:354\u2013359, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silver%2C%20David%20Schrittwieser%2C%20Julian%20Simonyan%2C%20Karen%20Antonoglou%2C%20Ioannis%20Mastering%20the%20game%20of%20go%20without%20human%20knowledge%202017"
        },
        {
            "id": "83",
            "entry": "[83] Satinder P. Singh, Michael J. Kearns, and Yishay Mansour. Nash convergence of gradient dynamics in general-sum games. In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence, UAI \u201900, pages 541\u2013548, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Satinder%20P.%20Kearns%2C%20Michael%20J.%20Mansour%2C%20Yishay%20Nash%20convergence%20of%20gradient%20dynamics%20in%20general-sum%20games%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Satinder%20P.%20Kearns%2C%20Michael%20J.%20Mansour%2C%20Yishay%20Nash%20convergence%20of%20gradient%20dynamics%20in%20general-sum%20games%202000"
        },
        {
            "id": "84",
            "entry": "[84] Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P\u00e9rolat, Karl Tuyls, R\u00e9mi Munos, and Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments. CoRR, abs/1810.09026, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.09026"
        },
        {
            "id": "85",
            "entry": "[85] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 2nd edition, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.%20Barto%2C%20A.%20Reinforcement%20Learning%3A%20An%20Introduction%202018"
        },
        {
            "id": "86",
            "entry": "[86] Richard S. Sutton, Satinder Singh, and David McAllester. Comparing policy-gradient algorithms, 2001. Unpublished.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20Richard%20S.%20Singh%2C%20Satinder%20McAllester%2C%20David%20Comparing%20policy-gradient%20algorithms%202001"
        },
        {
            "id": "87",
            "entry": "[87] Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit Texas Hold\u2019em. In Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tammelin%2C%20Oskari%20Burch%2C%20Neil%20Johanson%2C%20Michael%20Bowling%2C%20Michael%20Solving%20heads-up%20limit%20Texas%20Hold%E2%80%99em%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tammelin%2C%20Oskari%20Burch%2C%20Neil%20Johanson%2C%20Michael%20Bowling%2C%20Michael%20Solving%20heads-up%20limit%20Texas%20Hold%E2%80%99em%202015"
        },
        {
            "id": "88",
            "entry": "[88] Taylor and Jonker. Evolutionarily stable strategies and game dynamics. Mathematical Biosciences, 40:145\u2013156, 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Taylor%20Jonker%20Evolutionarily%20stable%20strategies%20and%20game%20dynamics%201978",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Taylor%20Jonker%20Evolutionarily%20stable%20strategies%20and%20game%20dynamics%201978"
        },
        {
            "id": "89",
            "entry": "[89] Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Z Leibo, and Thore Graepel. A Generalised Method for Empirical Game Theoretic Analysis . In AAMAS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tuyls%2C%20Karl%20Perolat%2C%20Julien%20Lanctot%2C%20Marc%20Leibo%2C%20Joel%20Z.%20A%20Generalised%20Method%20for%20Empirical%20Game%20Theoretic%20Analysis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tuyls%2C%20Karl%20Perolat%2C%20Julien%20Lanctot%2C%20Marc%20Leibo%2C%20Joel%20Z.%20A%20Generalised%20Method%20for%20Empirical%20Game%20Theoretic%20Analysis%202018"
        },
        {
            "id": "90",
            "entry": "[90] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software, 11(1):37\u201357.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vitter%2C%20Jeffrey%20S.%20Random%20sampling%20with%20a%20reservoir",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vitter%2C%20Jeffrey%20S.%20Random%20sampling%20with%20a%20reservoir"
        },
        {
            "id": "91",
            "entry": "[91] W. E. Walsh, D. C. Parkes, and R. Das. Choosing samples to compute heuristic-strategy Nash equilibrium. In Proceedings of the Fifth Workshop on Agent-Mediated Electronic Commerce, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walsh%2C%20W.E.%20Parkes%2C%20D.C.%20Das%2C%20R.%20Choosing%20samples%20to%20compute%20heuristic-strategy%20Nash%20equilibrium%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walsh%2C%20W.E.%20Parkes%2C%20D.C.%20Das%2C%20R.%20Choosing%20samples%20to%20compute%20heuristic-strategy%20Nash%20equilibrium%202003"
        },
        {
            "id": "92",
            "entry": "[92] William E Walsh, Rajarshi Das, Gerald Tesauro, and Jeffrey O Kephart. Analyzing Complex Strategic Interactions in Multi-Agent Systems. In AAAI, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walsh%2C%20William%20E.%20Das%2C%20Rajarshi%20Tesauro%2C%20Gerald%20Kephart%2C%20Jeffrey%20O.%20Analyzing%20Complex%20Strategic%20Interactions%20in%20Multi-Agent%20Systems%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walsh%2C%20William%20E.%20Das%2C%20Rajarshi%20Tesauro%2C%20Gerald%20Kephart%2C%20Jeffrey%20O.%20Analyzing%20Complex%20Strategic%20Interactions%20in%20Multi-Agent%20Systems%202002"
        },
        {
            "id": "93",
            "entry": "[93] Kevin Waugh, Dustin Morrill, J. Andrew Bagnell, and Michael Bowling. Solving games with functional regret estimation. In Proceedongs of the AAAI Conference on Artificial Intelligence, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Waugh%2C%20Kevin%20Dustin%20Morrill%2C%20J.Andrew%20Bagnell%20Bowling%2C%20Michael%20Solving%20games%20with%20functional%20regret%20estimation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Waugh%2C%20Kevin%20Dustin%20Morrill%2C%20J.Andrew%20Bagnell%20Bowling%2C%20Michael%20Solving%20games%20with%20functional%20regret%20estimation%202015"
        },
        {
            "id": "94",
            "entry": "[94] Michael P. Wellman. Methods for empirical game-theoretic analysis. In Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, pages 1552\u20131556, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wellman%2C%20Michael%20P.%20Methods%20for%20empirical%20game-theoretic%20analysis%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wellman%2C%20Michael%20P.%20Methods%20for%20empirical%20game-theoretic%20analysis%202006"
        },
        {
            "id": "95",
            "entry": "[95] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229\u2013256, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20R.J.%20Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning%201992"
        },
        {
            "id": "96",
            "entry": "[96] Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M. Bayen, Sham Kakade, Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with actiondependent factorized baselines. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Cathy%20Rajeswaran%2C%20Aravind%20Duan%2C%20Yan%20Kumar%2C%20Vikash%20Variance%20reduction%20for%20policy%20gradient%20with%20actiondependent%20factorized%20baselines%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Cathy%20Rajeswaran%2C%20Aravind%20Duan%2C%20Yan%20Kumar%2C%20Vikash%20Variance%20reduction%20for%20policy%20gradient%20with%20actiondependent%20factorized%20baselines%202018"
        },
        {
            "id": "97",
            "entry": "[97] Yuxin Wu and Yuandong Tian. Training agent for first-person shooter game with actorcritic curriculum learning. In Proceedings of the International Conference on Representation Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuxin%20Tian%2C%20Yuandong%20Training%20agent%20for%20first-person%20shooter%20game%20with%20actorcritic%20curriculum%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuxin%20Tian%2C%20Yuandong%20Training%20agent%20for%20first-person%20shooter%20game%20with%20actorcritic%20curriculum%20learning%202017"
        },
        {
            "id": "98",
            "entry": "[98] Michael Wunder, Michael Littman, and Monica Babes. Classes of multiagent q-learning dynamics with -greedy exploration. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML\u201910, pages 1167\u20131174, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wunder%2C%20Michael%20Littman%2C%20Michael%20Babes%2C%20Monica%20Classes%20of%20multiagent%20q-learning%20dynamics%20with%20-greedy%20exploration%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wunder%2C%20Michael%20Littman%2C%20Michael%20Babes%2C%20Monica%20Classes%20of%20multiagent%20q-learning%20dynamics%20with%20-greedy%20exploration%202010"
        },
        {
            "id": "99",
            "entry": "[99] Chongjie Zhang and Victor Lesser. Multi-agent learning with policy prediction. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, pages 927\u2013934, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Chongjie%20Lesser%2C%20Victor%20Multi-agent%20learning%20with%20policy%20prediction%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Chongjie%20Lesser%2C%20Victor%20Multi-agent%20learning%20with%20policy%20prediction%202010"
        },
        {
            "id": "100",
            "entry": "[100] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of Twentieth International Conference on Machine Learning (ICML-2003), 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        },
        {
            "id": "101",
            "entry": "[101] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. Technical Report CMU-CS-03-110, Carnegie Mellon University, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        },
        {
            "id": "102",
            "entry": "[102] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. Regret minimization in games with incomplete information. In Advances in Neural Information Processing Systems 20 (NIPS 2007), 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20M.%20Johanson%2C%20M.%20Bowling%2C%20M.%20Piccione%2C%20C.%20Regret%20minimization%20in%20games%20with%20incomplete%20information%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20M.%20Johanson%2C%20M.%20Bowling%2C%20M.%20Piccione%2C%20C.%20Regret%20minimization%20in%20games%20with%20incomplete%20information%202007"
        },
        {
            "id": "103",
            "entry": "[103] Martin Zinkevich, Amy Greenwald, and Michael L. Littman. Cyclic equilibria in markov games. In Proceedings of the 18th International Conference on Neural Information Processing Systems, NIPS\u201905, pages 1641\u20131648, Cambridge, MA, USA, 2005. MIT Press. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Greenwald%2C%20Amy%20Littman%2C%20Michael%20L.%20Cyclic%20equilibria%20in%20markov%20games%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Greenwald%2C%20Amy%20Littman%2C%20Michael%20L.%20Cyclic%20equilibria%20in%20markov%20games%202005"
        }
    ]
}
