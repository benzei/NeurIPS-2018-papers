{
    "filename": "7730-non-ergodic-alternating-proximal-augmented-lagrangian-algorithms-with-optimal-rates.pdf",
    "metadata": {
        "title": "Non-Ergodic Alternating Proximal  Augmented Lagrangian Algorithms with Optimal Rates",
        "author": "Quoc Tran Dinh",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7730-non-ergodic-alternating-proximal-augmented-lagrangian-algorithms-with-optimal-rates.pdf"
        },
        "abstract": "We develop two new non-ergodic alternating proximal augmented Lagrangian algorithms\n(NEAPAL) to solve a class of nonsmooth constrained convex optimization problems. Our approach relies on a novel combination of the augmented Lagrangian framework, alternating/linearization scheme, Nesterov\u2019s acceleration techniques, and adaptive strategy for parameters. Our algorithms have several new features compared to existing methods. Firstly, they have a Nesterov\u2019s acceleration step on the primal variables compared to the dual one in several methods in the literature. Secondly, they achieve non-ergodic optimal convergence rates under standard assumptions, i.e. an O"
    },
    "keywords": [
        {
            "term": "ADMM",
            "url": "https://en.wikipedia.org/wiki/ADMM"
        },
        {
            "term": "convex optimization problem",
            "url": "https://en.wikipedia.org/wiki/convex_optimization_problem"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "convex problem",
            "url": "https://en.wikipedia.org/wiki/convex_problem"
        }
    ],
    "highlights": [
        "This per-iteration complexity is essentially the same as in primal-dual methods [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] when applying to solve composite convex problems with linear operators",
        "Our contribution: Our contribution can be summarized as follows: (a) We propose a novel algorithm called Non-Ergodic Alternating Proximal Augmented Lagrangian, Algorithm 1, to solve (1) under only convexity and strong duality assumptions",
        "(c) When the problem (1) is semi-strongly convex, i.e. f is non-strongly convex and g is strongly convex, we develop a new Non-Ergodic Alternating Proximal Augmented Lagrangian variant, Algorithm 2, that achieves an optimal",
        "Note that our algorithms can be cast into a primal-dual method such as [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] rather than ADMM when solving composite problems with linear operators",
        "Most of existing results have shown an ergodic convergence rate of O\n1 k in either gap function or in both objective residual and constraint violation [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]. This rate has been shown to be optimal for ADMM-type methods under only convexity and strong duality in recent work [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "We have proposed two novel primal-dual algorithms to solve a broad class of nonsmooth constrained convex optimization problems that have the following features"
    ],
    "key_statements": [
        "This per-iteration complexity is essentially the same as in primal-dual methods [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] when applying to solve composite convex problems with linear operators",
        "Our contribution: Our contribution can be summarized as follows: (a) We propose a novel algorithm called Non-Ergodic Alternating Proximal Augmented Lagrangian, Algorithm 1, to solve (1) under only convexity and strong duality assumptions",
        "(c) When the problem (1) is semi-strongly convex, i.e. f is non-strongly convex and g is strongly convex, we develop a new Non-Ergodic Alternating Proximal Augmented Lagrangian variant, Algorithm 2, that achieves an optimal",
        "Note that our algorithms can be cast into a primal-dual method such as [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] rather than ADMM when solving composite problems with linear operators",
        "Most of existing results have shown an ergodic convergence rate of O\n1 k in either gap function or in both objective residual and constraint violation [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]. This rate has been shown to be optimal for ADMM-type methods under only convexity and strong duality in recent work [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>]",
        "We have proposed two novel primal-dual algorithms to solve a broad class of nonsmooth constrained convex optimization problems that have the following features"
    ],
    "summary": [
        "This per-iteration complexity is essentially the same as in primal-dual methods [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] when applying to solve composite convex problems with linear operators.",
        "Optimal rates at the last iterate have not been known yet in primal-dual methods such as in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].2",
        "-rate is achieved in [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>] for accelerated ADMM, but Algorithm 2 remains essentially different from [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "Note that our algorithms can be cast into a primal-dual method such as [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] rather than ADMM when solving composite problems with linear operators.",
        "This rate has been shown to be optimal for ADMM-type methods under only convexity and strong duality in recent work [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>].",
        "A non-ergodic optimal rate of first-order methods for solving (1) was perhaps first proved in [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>].",
        "We first propose a new primal-dual algorithm to solve nonsmooth and nonstrongly convex problems in (1).",
        "Our algorithm is presented in Algorithm 1, which we call a Non-Ergodic Alternating Proximal Augmented Lagrangian (NEAPAL) method.",
        "Combining Step 4 and Step 7, we can show that the per-iteration complexity of Algorithm 1 is dominated by the subproblem (7) at Step 5, one proximal operator of g, one matrix vector-multiplication (Ax, By), and one adjoint B> .",
        "This dual update collapses to the one in classical AL methods such as AMA and ADMM, and their variants when \u2327k = 1 is fixed in all iterations.",
        "To exploit the strong convexity of g, we apply Tseng\u2019s accelerated scheme in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] to the y-subproblem, while using Nesterov\u2019s momentum idea [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] for the x-subproblem to keep the non-ergodic convergence on xk .",
        "This step relies on Tseng\u2019s accelerated variant in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] instead of Nesterov\u2019s optimal scheme [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] as in Algorithm 1.",
        "Since this problem is fully nonsmooth and non-strongly convex, we implement three candidates to compare: ASGARD [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] and its restarting variant, and Chambolle-",
        "Algorithm 2 and Chambolle-Pock\u2019s method with strong convexity are used.",
        "The results of these algorithms and non-strongly convex variants are plotted in Figure 2.",
        "We have proposed two novel primal-dual algorithms to solve a broad class of nonsmooth constrained convex optimization problems that have the following features.",
        "They achieve optimal convergence rates in non-ergodic sense on the objective residual and feasibility violation, which are important in sparse and low-rank optimization as well as in image processing.",
        "The dual update step in Algorithms 1 and 2 can be viewed as the dual step in relaxed augmented Lagrangian-based methods, where the step-size is different from the penalty parameter.",
        "We plan to investigate connection of our methods to primal-dual first-order methods such as primal-dual hybrid gradient and projective and other splitting methods"
    ],
    "headline": "We develop two new non-ergodic alternating proximal augmented Lagrangian algorithms",
    "reference_links": [
        {
            "id": "1",
            "entry": "1. Dimitri P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Athena Scientific, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsekas%2C%20Dimitri%20P.%20Constrained%20Optimization%20and%20Lagrange%20Multiplier%20Methods%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsekas%2C%20Dimitri%20P.%20Constrained%20Optimization%20and%20Lagrange%20Multiplier%20Methods%201996"
        },
        {
            "id": "2",
            "entry": "2. S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. and Trends in Machine Learning, 3(1):1\u2013122, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Parikh%2C%20N.%20Chu%2C%20E.%20Peleato%2C%20B.%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers.%20Found%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boyd%2C%20S.%20Parikh%2C%20N.%20Chu%2C%20E.%20Peleato%2C%20B.%20Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers.%20Found%202011"
        },
        {
            "id": "3",
            "entry": "3. A. Chambolle and T. Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. J. Math. Imaging Vis., 40(1):120\u2013145, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011"
        },
        {
            "id": "4",
            "entry": "4. C. Chen, B. He, Y. Ye, and X. Yuan. The direct extension of ADMM for multi-block convex minimization problems is not necessarily convergent. Math. Program., 155(1-2):57\u201379, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20C.%20He%2C%20B.%20Ye%2C%20Y.%20Yuan%2C%20X.%20The%20direct%20extension%20of%20ADMM%20for%20multi-block%20convex%20minimization%20problems%20is%20not%20necessarily%20convergent%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20C.%20He%2C%20B.%20Ye%2C%20Y.%20Yuan%2C%20X.%20The%20direct%20extension%20of%20ADMM%20for%20multi-block%20convex%20minimization%20problems%20is%20not%20necessarily%20convergent%202016"
        },
        {
            "id": "5",
            "entry": "5. D. Davis. Convergence rate analysis of the forward-Douglas-Rachford splitting scheme. SIAM J. Optim., 25(3):1760\u20131786, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davis%2C%20D.%20Convergence%20rate%20analysis%20of%20the%20forward-Douglas-Rachford%20splitting%20scheme%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davis%2C%20D.%20Convergence%20rate%20analysis%20of%20the%20forward-Douglas-Rachford%20splitting%20scheme%202015"
        },
        {
            "id": "6",
            "entry": "6. D. Davis and W. Yin. Faster convergence rates of relaxed Peaceman-Rachford and ADMM under regularity assumptions. Math. Oper. Res., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davis%2C%20D.%20Yin%2C%20W.%20Faster%20convergence%20rates%20of%20relaxed%20Peaceman-Rachford%20and%20ADMM%20under%20regularity%20assumptions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Davis%2C%20D.%20Yin%2C%20W.%20Faster%20convergence%20rates%20of%20relaxed%20Peaceman-Rachford%20and%20ADMM%20under%20regularity%20assumptions%202014"
        },
        {
            "id": "7",
            "entry": "7. W. Deng, M.-J. Lai, Z. Peng, and W. Yin. Parallel multi-block ADMM with o(1/k) convergence. J. Scientific Computing, 71(2): 712\u2013736, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Deng%2C%20W.%20Lai%2C%20M.-J.%20Peng%2C%20Z.%20Yin%2C%20W.%20Parallel%20multi-block%20ADMM%20with%20o%281/k%29%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Deng%2C%20W.%20Lai%2C%20M.-J.%20Peng%2C%20Z.%20Yin%2C%20W.%20Parallel%20multi-block%20ADMM%20with%20o%281/k%29%20convergence%202017"
        },
        {
            "id": "8",
            "entry": "8. J. Eckstein and D. Bertsekas. On the Douglas - Rachford splitting method and the proximal point algorithm for maximal monotone operators. Math. Program., 55:293\u2013318, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Eckstein%2C%20J.%20Bertsekas%2C%20D.%20On%20the%20Douglas%20-%20Rachford%20splitting%20method%20and%20the%20proximal%20point%20algorithm%20for%20maximal%20monotone%20operators%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Eckstein%2C%20J.%20Bertsekas%2C%20D.%20On%20the%20Douglas%20-%20Rachford%20splitting%20method%20and%20the%20proximal%20point%20algorithm%20for%20maximal%20monotone%20operators%201992"
        },
        {
            "id": "9",
            "entry": "9. J. E. Esser. Primal-dual algorithm for convex models and applications to image restoration, registration and nonlocal inpainting. PhD Thesis, University of California, Los Angeles, Los Angeles, USA, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Esser%2C%20J.E.%20Primal-dual%20algorithm%20for%20convex%20models%20and%20applications%20to%20image%20restoration%2C%20registration%20and%20nonlocal%20inpainting%202010"
        },
        {
            "id": "10",
            "entry": "10. E. Ghadimi, A. Teixeira, I. Shames, and M. Johansson. Optimal parameter selection for the alternating direction method of multipliers: quadratic problems. IEEE Trans. Automat. Contr., 60(3):644\u2013658, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ghadimi%2C%20E.%20Teixeira%2C%20A.%20Shames%2C%20I.%20Johansson%2C%20M.%20Optimal%20parameter%20selection%20for%20the%20alternating%20direction%20method%20of%20multipliers%3A%20quadratic%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ghadimi%2C%20E.%20Teixeira%2C%20A.%20Shames%2C%20I.%20Johansson%2C%20M.%20Optimal%20parameter%20selection%20for%20the%20alternating%20direction%20method%20of%20multipliers%3A%20quadratic%20problems%202015"
        },
        {
            "id": "11",
            "entry": "11. T. Goldstein, B. O\u2019Donoghue, and S. Setzer. Fast Alternating Direction Optimization Methods. SIAM J. Imaging Sci., 7(3):1588\u20131623, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldstein%2C%20T.%20O%E2%80%99Donoghue%2C%20B.%20Setzer%2C%20S.%20Fast%20Alternating%20Direction%20Optimization%20Methods%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldstein%2C%20T.%20O%E2%80%99Donoghue%2C%20B.%20Setzer%2C%20S.%20Fast%20Alternating%20Direction%20Optimization%20Methods%202012"
        },
        {
            "id": "12",
            "entry": "12. B. He and X. Yuan. On non-ergodic convergence rate of Douglas\u2013Rachford alternating direction method of multipliers. Numerische Mathematik, 130(3):567\u2013577, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20B.%20Yuan%2C%20X.%20On%20non-ergodic%20convergence%20rate%20of%20Douglas%E2%80%93Rachford%20alternating%20direction%20method%20of%20multipliers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20B.%20Yuan%2C%20X.%20On%20non-ergodic%20convergence%20rate%20of%20Douglas%E2%80%93Rachford%20alternating%20direction%20method%20of%20multipliers%202012"
        },
        {
            "id": "13",
            "entry": "13. B.S. He, M. Tao, M.H. Xu, and X.M. Yuan. Alternating directions based contraction method for generally separable linearly constrained convex programming problems. Optimization, (to appear), 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20B.S.%20Tao%2C%20M.%20Xu%2C%20M.H.%20Yuan%2C%20X.M.%20Alternating%20directions%20based%20contraction%20method%20for%20generally%20separable%20linearly%20constrained%20convex%20programming%20problems%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20B.S.%20Tao%2C%20M.%20Xu%2C%20M.H.%20Yuan%2C%20X.M.%20Alternating%20directions%20based%20contraction%20method%20for%20generally%20separable%20linearly%20constrained%20convex%20programming%20problems%202011"
        },
        {
            "id": "14",
            "entry": "14. H. Li and Z. Lin. Accelerated Alternating Direction Method of Multipliers: an Optimal O(1/k) Nonergodic Analysis. arXiv preprint arXiv:1608.06366, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.06366"
        },
        {
            "id": "15",
            "entry": "15. P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM J. Num. Anal., 16:964\u2013979, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lions%2C%20P.L.%20Mercier%2C%20B.%20Splitting%20algorithms%20for%20the%20sum%20of%20two%20nonlinear%20operators%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lions%2C%20P.L.%20Mercier%2C%20B.%20Splitting%20algorithms%20for%20the%20sum%20of%20two%20nonlinear%20operators%201979"
        },
        {
            "id": "16",
            "entry": "16. R.D.C. Monteiro and B.F. Svaiter. Iteration-complexity of block-decomposition algorithms and the alternating minimization augmented Lagrangian method. SIAM J. Optim., 23(1):475\u2013507, 2013. O(1/k2). Doklady AN SSSR, 269:543\u2013547, 1983. Translated as Soviet Math. Dokl.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Monteiro%2C%20R.D.C.%20Svaiter%2C%20B.F.%20Iteration-complexity%20of%20block-decomposition%20algorithms%20and%20the%20alternating%20minimization%20augmented%20Lagrangian%20method%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Monteiro%2C%20R.D.C.%20Svaiter%2C%20B.F.%20Iteration-complexity%20of%20block-decomposition%20algorithms%20and%20the%20alternating%20minimization%20augmented%20Lagrangian%20method%202013"
        },
        {
            "id": "18",
            "entry": "18. Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87 of Applied Optimization. Kluwer Academic Publishers, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%20of%20Applied%20Optimization%202004"
        },
        {
            "id": "19",
            "entry": "19. R. Nishihara, L. Lessard, B. Recht, A. Packard, and M. Jordan. A general analysis of the convergence of ADMM. In ICML, Lille, France, pages 343\u2013352, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nishihara%2C%20R.%20Lessard%2C%20L.%20Recht%2C%20B.%20Packard%2C%20A.%20A%20general%20analysis%20of%20the%20convergence%20of%20ADMM%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nishihara%2C%20R.%20Lessard%2C%20L.%20Recht%2C%20B.%20Packard%2C%20A.%20A%20general%20analysis%20of%20the%20convergence%20of%20ADMM%202015"
        },
        {
            "id": "20",
            "entry": "20. Y. Ouyang, Y. Chen, G. Lan, and E. JR. Pasiliao. An accelerated linearized alternating direction method of multiplier. SIAM J. Imaging Sci., 8(1):644\u2013681, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y.%20Ouyang%2C%20Y.%20Chen%2C%20G.%20Lan%20Pasiliao%2C%20E.%20JR.%20An%20accelerated%20linearized%20alternating%20direction%20method%20of%20multiplier%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y.%20Ouyang%2C%20Y.%20Chen%2C%20G.%20Lan%20Pasiliao%2C%20E.%20JR.%20An%20accelerated%20linearized%20alternating%20direction%20method%20of%20multiplier%202015"
        },
        {
            "id": "21",
            "entry": "21. B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471\u2013501, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20B.%20Fazel%2C%20M.%20Parrilo%2C%20P.A.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20B.%20Fazel%2C%20M.%20Parrilo%2C%20P.A.%20Guaranteed%20minimum-rank%20solutions%20of%20linear%20matrix%20equations%20via%20nuclear%20norm%20minimization%202010"
        },
        {
            "id": "22",
            "entry": "22. R. Shefi and M. Teboulle. On the rate of convergence of the proximal alternating linearized minimization algorithm for convex problems. EURO J. Comput. Optim., 4(1):27\u201346, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shefi%2C%20R.%20Teboulle%2C%20M.%20On%20the%20rate%20of%20convergence%20of%20the%20proximal%20alternating%20linearized%20minimization%20algorithm%20for%20convex%20problems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shefi%2C%20R.%20Teboulle%2C%20M.%20On%20the%20rate%20of%20convergence%20of%20the%20proximal%20alternating%20linearized%20minimization%20algorithm%20for%20convex%20problems%202016"
        },
        {
            "id": "23",
            "entry": "23. Q. Tran-Dinh, O. Fercoq, and V. Cevher. A smooth primal-dual optimization framework for nonsmooth composite convex minimization. SIAM J. Optim., pages 1\u201335, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran-Dinh%2C%20Q.%20Fercoq%2C%20O.%20Cevher%2C%20V.%20A%20smooth%20primal-dual%20optimization%20framework%20for%20nonsmooth%20composite%20convex%20minimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran-Dinh%2C%20Q.%20Fercoq%2C%20O.%20Cevher%2C%20V.%20A%20smooth%20primal-dual%20optimization%20framework%20for%20nonsmooth%20composite%20convex%20minimization%202018"
        },
        {
            "id": "24",
            "entry": "24. Q. Tran-Dinh, C. Savorgnan, and M. Diehl. Combining Lagrangian decomposition and excessive gap smoothing technique for solving large-scale separable convex optimization problems. Compt. Optim. Appl., 55(1):75\u2013111, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tran-Dinh%2C%20Q.%20Savorgnan%2C%20C.%20Diehl%2C%20M.%20Combining%20Lagrangian%20decomposition%20and%20excessive%20gap%20smoothing%20technique%20for%20solving%20large-scale%20separable%20convex%20optimization%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tran-Dinh%2C%20Q.%20Savorgnan%2C%20C.%20Diehl%2C%20M.%20Combining%20Lagrangian%20decomposition%20and%20excessive%20gap%20smoothing%20technique%20for%20solving%20large-scale%20separable%20convex%20optimization%20problems%202013"
        },
        {
            "id": "25",
            "entry": "25. P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Submitted to SIAM J. Optim, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tseng%2C%20P.%20On%20accelerated%20proximal%20gradient%20methods%20for%20convex-concave%20optimization%202008"
        },
        {
            "id": "26",
            "entry": "26. H. Wang and A. Banerjee. Bregman Alternating Direction Method of Multipliers. Advances in Neural Information Processing Systems (NIPS), pages 1\u201318, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20H.%20Banerjee%2C%20A.%20Bregman%20Alternating%20Direction%20Method%20of%20Multipliers%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20H.%20Banerjee%2C%20A.%20Bregman%20Alternating%20Direction%20Method%20of%20Multipliers%202013"
        },
        {
            "id": "27",
            "entry": "27. E. Wei, A. Ozdaglar, and A.Jadbabaie. A Distributed Newton Method for Network Utility Maximization. IEEE Trans. Automat. Contr., 58(9):2162 \u2013 2175, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wei%2C%20E.%20Ozdaglar%2C%20A.%20Jadbabaie%2C%20A.%20A%20Distributed%20Newton%20Method%20for%20Network%20Utility%20Maximization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wei%2C%20E.%20Ozdaglar%2C%20A.%20Jadbabaie%2C%20A.%20A%20Distributed%20Newton%20Method%20for%20Network%20Utility%20Maximization%202011"
        },
        {
            "id": "28",
            "entry": "28. B. E. Woodworth and N. Srebro. Tight complexity bounds for optimizing composite objectives. In Advances in neural information processing systems (NIPS), pages 3639\u20133647, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Woodworth%2C%20B.E.%20Srebro%2C%20N.%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Woodworth%2C%20B.E.%20Srebro%2C%20N.%20Tight%20complexity%20bounds%20for%20optimizing%20composite%20objectives%202016"
        },
        {
            "id": "29",
            "entry": "29. Y. Xu. Accelerated first-order primal-dual proximal methods for linearly constrained composite convex programming. SIAM J. Optim., 27(3):1459\u20131484, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Y.%20Accelerated%20first-order%20primal-dual%20proximal%20methods%20for%20linearly%20constrained%20composite%20convex%20programming%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Y.%20Accelerated%20first-order%20primal-dual%20proximal%20methods%20for%20linearly%20constrained%20composite%20convex%20programming%202017"
        },
        {
            "id": "30",
            "entry": "30. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B, 67(2):301\u2013320, 2005. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20H.%20Hastie%2C%20T.%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20H.%20Hastie%2C%20T.%20Regularization%20and%20variable%20selection%20via%20the%20elastic%20net%202005"
        }
    ]
}
