{
    "filename": "7499-breaking-the-span-assumption-yields-fast-finite-sum-minimization.pdf",
    "metadata": {
        "title": "Breaking the Span Assumption Yields Fast Finite-Sum Minimization",
        "author": "Robert Hannah, Yanli Liu, Daniel O'Connor, Wotao Yin",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7499-breaking-the-span-assumption-yields-fast-finite-sum-minimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of n smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality. Most finite sum algorithms follow what we call the \u201cspan assumption\u201d: Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime, where the condition number \u03ba = O(n), the span assumption prevents algorithms from converging to an approximate solution of accuracy \u01eb in less than n ln(1/\u01eb) iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this, they can be up to \u03a9(1 + (ln(n/\u03ba))+) times faster. In particular, to obtain an accuracy \u01eb = 1/n\u03b1 for \u03ba = n\u03b2 and \u03b1, \u03b2 \u2208 (0, 1), modified SVRG requires O(n) iterations, whereas algorithms that follow the span assumption require O(n ln(n)) iterations. Moreover, we present lower bound results that show this speedup is optimal, and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms, future work may purposefully exploit this to yield faster algorithms in the big data regime."
    },
    "keywords": [
        {
            "term": "SARAH",
            "url": "https://en.wikipedia.org/wiki/SARAH"
        },
        {
            "term": "SAGA",
            "url": "https://en.wikipedia.org/wiki/SAGA"
        },
        {
            "term": "first order",
            "url": "https://en.wikipedia.org/wiki/first_order"
        },
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "SVRG",
            "url": "https://en.wikipedia.org/wiki/SVRG"
        },
        {
            "term": "big data",
            "url": "https://en.wikipedia.org/wiki/big_data"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        }
    ],
    "highlights": [
        "Finite sum minimization is an important class of optimization problem that appears in many applications in machine learning and other areas",
        "[<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] extend lower bound results to algorithms that do not follow the span assumption: SD\u221aCA, SVRG, SARAH, accelerated SAGA, etc.; but with a weaker lower bound of \u03a9(n + n\u03ba ln(1/\u01eb))",
        "We show a surprising result in Section 2, that SVRG, and SARAH can be fundamentally faster than methods that satisfy the span assumption, with the full gradient steps playing a critical role in their speedup",
        "Lower complexity bound of Prox-SVRG and SARAH",
        "In the following proposition, which we prove in Appendix C, we show that SDCA has a complexity lower bound of \u03a9(n ln(1/\u01eb))"
    ],
    "key_statements": [
        "Finite sum minimization is an important class of optimization problem that appears in many applications in machine learning and other areas",
        "[<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] extend lower bound results to algorithms that do not follow the span assumption: SD\u221aCA, SVRG, SARAH, accelerated SAGA, etc.; but with a weaker lower bound of \u03a9(n + n\u03ba ln(1/\u01eb))",
        "We show a surprising result in Section 2, that SVRG, and SARAH can be fundamentally faster than methods that satisfy the span assumption, with the full gradient steps playing a critical role in their speedup",
        "Lower complexity bound of Prox-SVRG and SARAH",
        "In the following proposition, which we prove in Appendix C, we show that SDCA has a complexity lower bound of \u03a9(n ln(1/\u01eb))",
        "We explain why SVRG and SARAH, which are a hybrid between full-gradient and VR methods, are fundamentally faster than other VR algorithms",
        "The key insight is that the span condition makes this adversarial example hard to minimize, but that the full gradient steps of SVRG and SARAH make it easy when n \u226b \u03ba",
        "For m = min{n, 2}, in order to obtain an \u01eb-optimal solution in terms of function value, the SVRG in Algorithm 1 needs at most"
    ],
    "summary": [
        "Finite sum minimization is an important class of optimization problem that appears in many applications in machine learning and other areas.",
        "[<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] extend lower bound results to algorithms that do not follow the span assumption: SD\u221aCA, SVRG, SARAH, accelerated SAGA, etc.; but with a weaker lower bound of \u03a9(n + n\u03ba ln(1/\u01eb)).",
        "We show a surprising result in Section 2, that SVRG, and SARAH can be fundamentally faster than methods that satisfy the span assumption, with the full gradient steps playing a critical role in their speedup.",
        "With the knowledge that this assumption is a point of weakness for VR algorithms, future work may more purposefully exploit this to yield better speedups than SVRG and SARAH can currently attain.",
        "To obtain accuracy \u01eb = 1/n\u03b1 for \u03ba = n\u03b2 and some \u03b1, \u03b2 \u2208 (0, 1), modified SVRG requires O(n) iterations, whereas algorithms that follow the span assumption require O(n ln(n)) iterations [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] for sufficiently large problem dimension d.",
        "For every \u01eb and randomized algorithm on (1.1) that follows the span assumption, there are a dimension d, and L-sm\u221aooth, \u03bc-strongly convex functions fi on Rd such that the algorithm takes at least \u03a9((n + \u03ban) ln(1/\u01eb)) steps to reach sub-optimality Ef xk \u2212 f (x\u2217) < \u01eb.",
        "The above algorithms that satisfy the span condition all have known upper complexity bounds of O((n + \u03ba) ln(1/\u01eb)), and for \u03ba = O(n) we have a sharp convergence rate.",
        "We improve10 the analysis of [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], to show that the complexity of SVRG obtained in Corollary 2 is optimal to within a constant factor without fundamentally different assumptions on the class of algorithms that are allowed.",
        "Lower complexity bound of Prox-SVRG and SARAH.",
        "Since SDCA doesn\u2019t follow the span assumption, and the number of iterations k is much greater than the dual problem dimension n, different arguments to the ones used in [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] must be used.",
        "We explain why SVRG and SARAH, which are a hybrid between full-gradient and VR methods, are fundamentally faster than other VR algorithms.",
        "The key insight is that the span condition makes this adversarial example hard to minimize, but that the full gradient steps of SVRG and SARAH make it easy when n \u226b \u03ba.",
        "4 Prox-SVRG for strongly convex sum of smooth nonconvex function",
        "For m = min{n, 2}, in order to obtain an \u01eb-optimal solution in terms of function value, the SVRG in Algorithm 1 needs at most"
    ],
    "headline": "We show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of n smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Defazio, F. Bach, and S. Lacoste-Julien, \u201cSAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives,\u201d in Advances in Neural Information Processing Systems 27, 2014, pp. 1646\u20131654. [Online]. Available: http://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient method-with-support-for-non-strongly-convex-composite-objectives.pdf.",
            "url": "http://papers.nips.cc/paper/5258-saga-a-fast-incremental-gradient"
        },
        {
            "id": "2",
            "entry": "[2] N. L. Roux, M. Schmidt, and F. R. Bach, \u201cA Stochastic Gradient Method with an Exponential Convergence _Rate for Finite Training Sets,\u201d in Advances in Neural Information Processing Systems 25, Curran Associates, Inc., 2012, pp. 2663\u20132671. [Online]. Available: http://papers.nips.cc/paper/4633-a-stochastic-gradientmethod-with-an-exponential-convergence-_rate-for-finite-training-sets.pdf.",
            "url": "http://papers.nips.cc/paper/4633-a-stochastic-gradientmethod-with-an-exponential-convergence-_rate-for-finite-training-sets.pdf"
        },
        {
            "id": "3",
            "entry": "[3] A. Defazio, J. Domke, and Caetano, \u201cFinito: A faster, permutable incremental gradient method for big data problems,\u201d in International Conference on Machine Learning, Jan. 2014, pp. 1125\u20131133. [Online]. Available: http://proceedings.mlr.press/v32/defazio14.html.",
            "url": "http://proceedings.mlr.press/v32/defazio14.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Domke%2C%20J.%20Caetano%20Finito%3A%20A%20faster%2C%20permutable%20incremental%20gradient%20method%20for%20big%20data%20problems%2C%202014-01"
        },
        {
            "id": "4",
            "entry": "[4] J. Mairal, \u201cOptimization with First-Order Surrogate Functions,\u201d in International Conference on Machine Learning, Feb. 2013, pp. 783\u2013791. [Online]. Available: http://proceedings.mlr.press/v28/mairal13.html.",
            "url": "http://proceedings.mlr.press/v28/mairal13.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mairal%2C%20J.%20Optimization%20with%20First-Order%20Surrogate%20Functions%2C%202013-02"
        },
        {
            "id": "5",
            "entry": "[5] R. Johnson and T. Zhang, \u201cAccelerating stochastic gradient descent using predictive variance reduction,\u201d in Advances in Neural Information Processing Systems 26, 2013, pp. 315\u2013323. [Online]. Available: http://papers.nips.cc/paper/4937 accelerating-stochastic-gradient-descent-using-predictive-variance reduction.pdf.",
            "url": "http://papers.nips.cc/paper/4937"
        },
        {
            "id": "6",
            "entry": "[6] L. M. Nguyen, J. Liu, K. Scheinberg, and M. Tak\u00e1\u010d, \u201cSARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient,\u201d arXiv:1703.00102 [cs, math, stat], Feb. 2017. [Online]. Available: http://arxiv.org/abs/1703.00102.",
            "url": "http://arxiv.org/abs/1703.00102",
            "arxiv_url": "https://arxiv.org/pdf/1703.00102"
        },
        {
            "id": "7",
            "entry": "[7] S. Shalev-Shwartz and T. Zhang, \u201cStochastic dual coordinate ascent methods for regularized loss,\u201d J. Mach. Learn. Res., vol. 14, no. 1, pp. 567\u2013599, Feb. 2013. [Online]. Available: http://dl.acm.org/citation.cfm?id=2502581.2502598.",
            "url": "http://dl.acm.org/citation.cfm?id=2502581.2502598",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%2C%202013-02"
        },
        {
            "id": "8",
            "entry": "[8] S. Shalev-Shwartz, \u201cSDCA without Duality,\u201d arXiv:1502.06177, Feb. 2015. [Online]. Available: http://arxiv.org/abs/1502.06177.",
            "url": "http://arxiv.org/abs/1502.06177",
            "arxiv_url": "https://arxiv.org/pdf/1502.06177"
        },
        {
            "id": "9",
            "entry": "[9] Z. Allen-Zhu, \u201cKatyusha: The First Direct Acceleration of Stochastic Gradient Methods,\u201d in Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, ser. STOC 2017, New York, NY, USA: ACM, 2017, pp. 1200\u20131205. [Online]. Available: http://doi.acm.org/10.1145/3055399.3055448.",
            "url": "http://doi.acm.org/10.1145/3055399.3055448",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Z.%20Katyusha%3A%20The%20First%20Direct%20Acceleration%20of%20Stochastic%20Gradient%20Methods%2C%202017"
        },
        {
            "id": "10",
            "entry": "[10] A. Defazio, \u201cA simple practical accelerated method for finite sums,\u201d in Advances in Neural Information Processing Systems, 2016, pp. 676\u2013684.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20A%20simple%20practical%20accelerated%20method%20for%20finite%20sums%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20A%20simple%20practical%20accelerated%20method%20for%20finite%20sums%2C%202016"
        },
        {
            "id": "11",
            "entry": "[11] G. Lan and Y. Zhou, \u201cAn optimal randomized incremental gradient method,\u201d Mathematical Programming, pp. 1\u201349, Jun. 2017. [Online]. Available: https://link.springer.com/article/10.1007/s10107-017-1173-0.",
            "url": "https://link.springer.com/article/10.1007/s10107-017-1173-0"
        },
        {
            "id": "12",
            "entry": "[12] H. Lin, J. Mairal, and Z. Harchaoui, \u201cA Universal Catalyst for First-Order Optimization,\u201d arXiv:1506.02186, Jun. 2015. [Online]. Available: http://arxiv.org/abs/1506.02186.",
            "url": "http://arxiv.org/abs/1506.02186",
            "arxiv_url": "https://arxiv.org/pdf/1506.02186"
        },
        {
            "id": "13",
            "entry": "[13] Q. Lin, Z. Lu, and L. Xiao, \u201cAn Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization,\u201d arXiv:1407.1296, Jul. 2014. [Online]. Available: http://arxiv.org/abs/1407.1296.",
            "url": "http://arxiv.org/abs/1407.1296",
            "arxiv_url": "https://arxiv.org/pdf/1407.1296"
        },
        {
            "id": "14",
            "entry": "[14] S. Shalev-Shwartz and T. Zhang, \u201cAccelerated proximal stochastic dual coordinate ascent for regularized loss minimization,\u201d Mathematical Programming, vol. 155, no. 1-2, pp. 105\u2013145, Jan. 2016. [Online]. Available: http://link.springer.com/article/10.1007/s10107-014-0839-0.",
            "url": "http://link.springer.com/article/10.1007/s10107-014-0839-0",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%2C%202016-01"
        },
        {
            "id": "15",
            "entry": "[15] G. Lan and Y. Zhou, \u201cAn optimal randomized incremental gradient method,\u201d arXiv:1507.02000, Jul. 2015. [Online]. Available: http://arxiv.org/abs/1507.02000.",
            "url": "http://arxiv.org/abs/1507.02000",
            "arxiv_url": "https://arxiv.org/pdf/1507.02000"
        },
        {
            "id": "16",
            "entry": "[16] B. Woodworth and N. Srebro, \u201cTight Complexity Bounds for Optimizing Composite Objectives,\u201d arXiv:1605.08003, May 2016. [Online]. Available: http://arxiv.org/abs/1605.08003.",
            "url": "http://arxiv.org/abs/1605.08003",
            "arxiv_url": "https://arxiv.org/pdf/1605.08003"
        },
        {
            "id": "17",
            "entry": "[17] Y. Arjevani and O. Shamir, \u201cDimension-Free Iteration Complexity of Finite Sum Optimization Problems,\u201d arXiv:1606.09333 [cs, math], Jun. 2016. [Online]. Available: http://arxiv.org/abs/1606.09333.",
            "url": "http://arxiv.org/abs/1606.09333",
            "arxiv_url": "https://arxiv.org/pdf/1606.09333"
        },
        {
            "id": "18",
            "entry": "[18] K. Sridharan, S. Shalev-shwartz, and N. Srebro, \u201cFast Rates for Regularized Objectives,\u201d in Advances in Neural Information Processing Systems 21, Curran Associates, Inc., 2009, pp. 1545\u20131552. [Online]. Available: http://papers.nips.cc/paper/3400-fastrates-for-regularized-objectives.pdf.",
            "url": "http://papers.nips.cc/paper/3400-fastrates-for-regularized-objectives.pdf"
        },
        {
            "id": "19",
            "entry": "[19] M. Eberts and I. Steinwart, \u201cOptimal learning rates for least squares SVMs using Gaussian kernels,\u201d in Advances in Neural Information Processing Systems 24, Curran Associates, Inc., 2011, pp. 1539\u20131547. [Online]. Available: http://papers.nips.cc/paper/4216-optimal-learning-rates-for-least-squares-svms-using gaussian-kernels.pdf.",
            "url": "http://papers.nips.cc/paper/4216-optimal-learning-rates-for-least-squares-svms-using"
        },
        {
            "id": "20",
            "entry": "[20] Z. Allen-Zhu, \u201cKatyusha X: Practical Momentum Method for Stochastic Sum-ofNonconvex Optimization,\u201d arXiv:1802.03866, Feb. 2018. [Online]. Available: http://arxiv.org/abs/1802.03866.",
            "url": "http://arxiv.org/abs/1802.03866",
            "arxiv_url": "https://arxiv.org/pdf/1802.03866"
        },
        {
            "id": "21",
            "entry": "[21] L. Xiao and T. Zhang, \u201cA Proximal Stochastic Gradient Method with Progressive Variance Reduction,\u201d arXiv:1403.4699, Mar. 2014. [Online]. Available: http://arxiv.org/abs/1403.4699.",
            "url": "http://arxiv.org/abs/1403.4699",
            "arxiv_url": "https://arxiv.org/pdf/1403.4699"
        },
        {
            "id": "22",
            "entry": "[22] Y. Arjevani and O. Shamir, \u201cOn the Iteration Complexity of Oblivious First-Order Optimization Algorithms,\u201d arXiv:1605.03529 [cs, math], May 2016. [Online]. Available: http://arxiv.org/abs/1605.03529.",
            "url": "http://arxiv.org/abs/1605.03529",
            "arxiv_url": "https://arxiv.org/pdf/1605.03529"
        },
        {
            "id": "23",
            "entry": "[23] Y. Arjevani, \u201cOn Lower and Upper Bounds in Smooth Strongly Convex Optimization - A Unified Approach via Linear Iterative Methods,\u201d arXiv:1410.6387, Oct. 2014. [Online]. Available: http://arxiv.org/abs/1410.6387.",
            "url": "http://arxiv.org/abs/1410.6387",
            "arxiv_url": "https://arxiv.org/pdf/1410.6387"
        },
        {
            "id": "24",
            "entry": "[24] Y. Nesterov, Introductory Lectures on Convex Optimization: A Basic Course. Springer Science & Business Media, Dec. 2013. [Online]. Available: https://dl.acm.org/citation.cfm?id=2670022. ",
            "url": "https://dl.acm.org/citation.cfm?id=2670022"
        }
    ]
}
