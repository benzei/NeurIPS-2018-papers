{
    "filename": "7887-on-the-local-hessian-in-back-propagation.pdf",
    "metadata": {
        "title": "On the Local Hessian in Back-propagation",
        "author": "Huishuai Zhang, Wei Chen, Tie-Yan Liu",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7887-on-the-local-hessian-in-back-propagation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Back-propagation (BP) is the foundation for successfully training deep neural networks. However, BP sometimes has difficulties in propagating a learning signal deep enough effectively, e.g., the vanishing gradient phenomenon. Meanwhile, BP often works well when combining with \u201cdesigning tricks\u201d like orthogonal initialization, batch normalization and skip connection. There is no clear understanding on what is essential to the efficiency of BP. In this paper, we take one step towards clarifying this problem. We view BP as a solution of back-matching propagation which minimizes a sequence of back-matching losses each corresponding to one block of the network. We study the Hessian of the local back-matching loss (local Hessian) and connect it to the efficiency of BP. It turns out that those designing tricks facilitate BP by improving the spectrum of local Hessian. In addition, we can utilize the local Hessian to balance the training pace of each block and design new training algorithms. Based on a scalar approximation of local Hessian, we propose a scale-amended SGD algorithm. We apply it to train neural networks with batch normalization, and achieve favorable results over vanilla SGD. This corroborates the importance of local Hessian from another side."
    },
    "keywords": [
        {
            "term": "back propagation",
            "url": "https://en.wikipedia.org/wiki/back_propagation"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "Batch normalization",
            "url": "https://en.wikipedia.org/wiki/Batch_normalization"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Deep neural networks have been advancing the state-of-the-art performance over a number of tasks in artificial intelligence, from speech recognition [<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>], computer vision [<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016a</a></a></a>] to natural language understanding [<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a></a>]",
        "We propose a scale-amended stochastic gradient descent algorithm to balance the training pace of each block by considering the scaling effect of local Hessian",
        "We show how skip connection and batch normalization improve the spectrum of the local Hessian.\n3 Explain the efficiency of BP via local Hessian",
        "Because the condition of local Hessian determines how efficiently the back-matching loss is minimized by updating the parameters of current block and how accurately the error signal propagates back to the lower layer, we evaluate how good a block is via analyzing its local Hessian",
        "In this paper we view BP as a solution of back-matching propagation which minimizes a sequence of back-matching losses",
        "Scale-amended stochastic gradient descent achieves favorable results over vanilla stochastic gradient descent empirically for training feed-forward networks with Batch normalization, which corroborates the importance of local Hessian"
    ],
    "key_statements": [
        "Deep neural networks have been advancing the state-of-the-art performance over a number of tasks in artificial intelligence, from speech recognition [<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>], computer vision [<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016a</a></a></a>] to natural language understanding [<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a></a>]",
        "We propose a scale-amended stochastic gradient descent algorithm to balance the training pace of each block by considering the scaling effect of local Hessian",
        "We show how skip connection and batch normalization improve the spectrum of the local Hessian.\n3 Explain the efficiency of BP via local Hessian",
        "Because the condition of local Hessian determines how efficiently the back-matching loss is minimized by updating the parameters of current block and how accurately the error signal propagates back to the lower layer, we evaluate how good a block is via analyzing its local Hessian",
        "We show how Batch normalization facilitates BP for training deep networks",
        "Another direction is to utilize the local Hessian to design new algorithms to improve the training of existing neural networks",
        "We propose a way to employ the information of the local Hessian to facilitate the training of deep neural networks",
        "We propose an algorithm scale-amended stochastic gradient descent to take the effect of mb,W and mb,z into account to balance the training pace of each block",
        "We reduce the learning rate by half once the validation accuracy is on plateau (ReduceLROnPlateau in PyTorch with patience=10), which works well for both vanilla-stochastic gradient descent and scale-amended stochastic gradient descent",
        "In this paper we view BP as a solution of back-matching propagation which minimizes a sequence of back-matching losses",
        "Scale-amended stochastic gradient descent achieves favorable results over vanilla stochastic gradient descent empirically for training feed-forward networks with Batch normalization, which corroborates the importance of local Hessian"
    ],
    "summary": [
        "Deep neural networks have been advancing the state-of-the-art performance over a number of tasks in artificial intelligence, from speech recognition [<a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\"><a class=\"ref-link\" id=\"cHinton_et+al_2012_a\" href=\"#rHinton_et+al_2012_a\">Hinton et al, 2012</a></a>], computer vision [<a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\"><a class=\"ref-link\" id=\"cHe_et+al_2016_a\" href=\"#rHe_et+al_2016_a\">He et al, 2016a</a></a></a>] to natural language understanding [<a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\"><a class=\"ref-link\" id=\"cHochreiter_1997_a\" href=\"#rHochreiter_1997_a\">Hochreiter and Schmidhuber, 1997</a></a>].",
        "If the local Hessian is badly-conditioned, the one-step gradient update does not minimize the back-matching loss sufficiently and the target signal distorts gradually when backward through layers.",
        "We first introduce the quadratic penalty formulation of the loss of neural network and the back-matching propagation procedure which minimizes a sequence of local back-matching losses following the backward order.",
        "We connect BP to the one-step gradient update solution of (3) and (4) and argue that the conditions of the subproblems (3) and (4) affect the efficiency of BP given the explanation of back matching loss.",
        "Because the condition of local Hessian determines how efficiently the back-matching loss is minimized by updating the parameters of current block and how accurately the error signal propagates back to the lower layer, we evaluate how good a block is via analyzing its local Hessian.",
        "As back propagating to lower blocks, the update zbk\u2212+t21 \u2212 zbk\u2212t gets far from the direction of minimizing the back-matching loss b for t = 1, 2, ..., b.",
        "We study the local Hessian of blocks with skip connection and batch normalization and show that these designing tricks can improve the spectrum of Hz and HW to some extent and make the training deep neural networks easier.",
        "We can see that (14) the Hessian of local matching loss for residual block with respect to W b is the same as the case without skip connection.",
        "Another direction is to utilize the local Hessian to design new algorithms to improve the training of existing neural networks.",
        "We propose a way to employ the information of the local Hessian to facilitate the training of deep neural networks.",
        "The scalars to approximate the local Hessians (17) and (18) of the fully connected layer with BN are computed as follows, mb,z :=",
        "This is because scale-amended SGD only modifies the magnitude of each block gradient as a whole and does not involve any further information and hyper-parameters are the same for both algorithms.",
        "By studying the Hessian of the local back-matching loss, we interpret the benefits of practical designing tricks, e.g., batch normalization and skip connection, in a unified way: improving the spectrum of local Hessian.",
        "We propose scale-amended SGD algorithm by employing the information of local Hessian via a scalar approximation.",
        "Scale-amended SGD achieves favorable results over vanilla SGD empirically for training feed-forward networks with BN, which corroborates the importance of local Hessian."
    ],
    "headline": "We study the Hessian of the local back-matching loss  and connect it to the efficiency of BP",
    "reference_links": [
        {
            "id": "Abadi_et+al_2016_a",
            "entry": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. TensorFlow: A system for large-scale machine learning. In OSDI, volume 16, pages 265\u2013283, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20M.%20Barham%2C%20P.%20Chen%2C%20J.%20Chen%2C%20Z.%20TensorFlow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "Amari_1998_a",
            "entry": "S.-I. Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.-I.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "Ba_et+al_2016_a",
            "entry": "J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06450"
        },
        {
            "id": "Bastien_et+al_2012_a",
            "entry": "F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio. Theano: new features and speed improvements. arXiv preprint arXiv:1211.5590, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1211.5590"
        },
        {
            "id": "Carreira-Perpinan_2014_a",
            "entry": "M. Carreira-Perpinan and W. Wang. Distributed optimization of deeply nested systems. In Artificial Intelligence and Statistics, pages 10\u201319, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carreira-Perpinan%2C%20M.%20Wang%2C%20W.%20Distributed%20optimization%20of%20deeply%20nested%20systems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carreira-Perpinan%2C%20M.%20Wang%2C%20W.%20Distributed%20optimization%20of%20deeply%20nested%20systems%202014"
        },
        {
            "id": "Chen_2011_a",
            "entry": "P. Chen. Hessian matrix vs. Gauss\u2013Newton matrix. SIAM Journal on Numerical Analysis, 49(4):1417\u20131435, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20P.%20Hessian%20matrix%20vs.%20Gauss%E2%80%93Newton%20matrix%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20P.%20Hessian%20matrix%20vs.%20Gauss%E2%80%93Newton%20matrix%202011"
        },
        {
            "id": "Cho_2017_a",
            "entry": "M. Cho and J. Lee. Riemannian approach to batch normalization. In Advances in Neural Information Processing Systems (NIPS), pages 5231\u20135241, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20M.%20Lee%2C%20J.%20Riemannian%20approach%20to%20batch%20normalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20M.%20Lee%2C%20J.%20Riemannian%20approach%20to%20batch%20normalization%202017"
        },
        {
            "id": "Dauphin_et+al_2014_a",
            "entry": "Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems (NIPS), pages 2933\u20132941, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dauphin%2C%20Y.N.%20Pascanu%2C%20R.%20Gulcehre%2C%20C.%20Cho%2C%20K.%20Identifying%20and%20attacking%20the%20saddle%20point%20problem%20in%20high-dimensional%20non-convex%20optimization%202014"
        },
        {
            "id": "Frerix_et+al_2018_a",
            "entry": "T. Frerix, T. M\u00f6llenhoff, M. Moeller, and D. Cremers. Proximal backpropagation. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frerix%2C%20T.%20M%C3%B6llenhoff%2C%20T.%20Moeller%2C%20M.%20Cremers%2C%20D.%20Proximal%20backpropagation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frerix%2C%20T.%20M%C3%B6llenhoff%2C%20T.%20Moeller%2C%20M.%20Cremers%2C%20D.%20Proximal%20backpropagation%202018"
        },
        {
            "id": "Grosse_2016_a",
            "entry": "R. Grosse and J. Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In International Conference on Machine Learning (ICML), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grosse%2C%20R.%20Martens%2C%20J.%20A%20Kronecker-factored%20approximate%20Fisher%20matrix%20for%20convolution%20layers%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grosse%2C%20R.%20Martens%2C%20J.%20A%20Kronecker-factored%20approximate%20Fisher%20matrix%20for%20convolution%20layers%202016"
        },
        {
            "id": "He_et+al_2016_a",
            "entry": "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016-06"
        },
        {
            "id": "Springer,_2016_a",
            "entry": "Springer, 2016b.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springer%202016b"
        },
        {
            "id": "Hinton_et+al_2012_a",
            "entry": "G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%20of%20four%20research%20groups%202012"
        },
        {
            "id": "Hochreiter_1991_a",
            "entry": "S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universit\u00e4t M\u00fcnchen, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Untersuchungen%20zu%20dynamischen%20neuronalen%20netzen%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Untersuchungen%20zu%20dynamischen%20neuronalen%20netzen%201991"
        },
        {
            "id": "Hochreiter_1997_a",
            "entry": "S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hochreiter%2C%20S.%20Schmidhuber%2C%20J.%20Long%20short-term%20memory%201997"
        },
        {
            "id": "Hochreiter_et+al_2001_a",
            "entry": "S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber, et al. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hochreiter%2C%20S.%20Bengio%2C%20Y.%20Frasconi%2C%20P.%20Schmidhuber%2C%20J.%20Gradient%20flow%20in%20recurrent%20nets%3A%20the%20difficulty%20of%20learning%20long-term%20dependencies%202001"
        },
        {
            "id": "Huang_et+al_0000_a",
            "entry": "G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten. Densely connected convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, page 3, 2017a.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20G.%20Liu%2C%20Z.%20Weinberger%2C%20K.Q.%20van%20der%20Maaten%2C%20L.%20Densely%20connected%20convolutional%20networks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20G.%20Liu%2C%20Z.%20Weinberger%2C%20K.Q.%20van%20der%20Maaten%2C%20L.%20Densely%20connected%20convolutional%20networks"
        },
        {
            "id": "Huang_et+al_0000_b",
            "entry": "L. Huang, X. Liu, B. Lang, and B. Li. Projection based weight normalization for deep neural networks. arXiv preprint arXiv:1710.02338, 2017b.",
            "arxiv_url": "https://arxiv.org/pdf/1710.02338"
        },
        {
            "id": "Ioffe_2015_a",
            "entry": "S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pages 448\u2013456, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ioffe%2C%20S.%20Szegedy%2C%20C.%20Batch%20normalization%3A%20Accelerating%20deep%20network%20training%20by%20reducing%20internal%20covariate%20shift%202015"
        },
        {
            "id": "Jastrzebski_et+al_2018_a",
            "entry": "S. Jastrzebski, D. Arpit, N. Ballas, V. Verma, T. Che, and Y. Bengio. Residual connections encourage iterative inference. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jastrzebski%2C%20S.%20Arpit%2C%20D.%20Ballas%2C%20N.%20Verma%2C%20V.%20Residual%20connections%20encourage%20iterative%20inference%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jastrzebski%2C%20S.%20Arpit%2C%20D.%20Ballas%2C%20N.%20Verma%2C%20V.%20Residual%20connections%20encourage%20iterative%20inference%202018"
        },
        {
            "id": "Jastrzebski_et+al_2018_b",
            "entry": "S. Jastrzebski, Z. Kenton, N. Ballas, A. Fischer, A. Storkey, and Y. Bengio. SGD smooths the sharpest directions. 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jastrzebski%2C%20S.%20Kenton%2C%20Z.%20Ballas%2C%20N.%20Fischer%2C%20A.%20SGD%20smooths%20the%20sharpest%20directions%202018"
        },
        {
            "id": "Krizhevsky_2009_a",
            "entry": "A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20A.%20Hinton%2C%20G.%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "Lafond_et+al_2017_a",
            "entry": "J. Lafond, N. Vasilache, and L. Bottou. Diagonal rescaling for neural networks. arXiv preprint arXiv:1705.09319, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09319"
        },
        {
            "id": "Cun_et+al_1991_a",
            "entry": "Y. Le Cun, I. Kanter, and S. A. Solla. Eigenvalues of covariance matrices: Application to neural-network learning. Physical Review Letters, 66(18):2396, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cun%2C%20Y.Le%20Kanter%2C%20I.%20Solla%2C%20S.A.%20Eigenvalues%20of%20covariance%20matrices%3A%20Application%20to%20neural-network%20learning%201991",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cun%2C%20Y.Le%20Kanter%2C%20I.%20Solla%2C%20S.A.%20Eigenvalues%20of%20covariance%20matrices%3A%20Application%20to%20neural-network%20learning%201991"
        },
        {
            "id": "Li_et+al_2016_a",
            "entry": "S. Li, J. Jiao, Y. Han, and T. Weissman. Demystifying resnet. arXiv preprint arXiv:1611.01186, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01186"
        },
        {
            "id": "Marcenko_1967_a",
            "entry": "V. A. Marcenko and L. A. Pastur. Distribution of eigenvalues for some sets of random matrices. Mathematics of the USSR-Sbornik, 1(4):457, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Marcenko%2C%20V.A.%20Pastur%2C%20L.A.%20Distribution%20of%20eigenvalues%20for%20some%20sets%20of%20random%20matrices%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Marcenko%2C%20V.A.%20Pastur%2C%20L.A.%20Distribution%20of%20eigenvalues%20for%20some%20sets%20of%20random%20matrices%201967"
        },
        {
            "id": "Martens_2010_a",
            "entry": "J. Martens. Deep learning via Hessian-free optimization. In International Conference on Machine Learning (ICML), pages 735\u2013742, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Deep%20learning%20via%20Hessian-free%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20J.%20Deep%20learning%20via%20Hessian-free%20optimization%202010"
        },
        {
            "id": "Martens_2016_a",
            "entry": "J. Martens. Second-order optimization for neural networks. PhD thesis, University of Toronto, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Second-order%20optimization%20for%20neural%20networks%202016"
        },
        {
            "id": "Martens_2015_a",
            "entry": "J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International Conference on Machine Learning (ICML), pages 2408\u20132417, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20neural%20networks%20with%20Kronecker-factored%20approximate%20curvature%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Martens%2C%20J.%20Grosse%2C%20R.%20Optimizing%20neural%20networks%20with%20Kronecker-factored%20approximate%20curvature%202015"
        },
        {
            "id": "Mishkin_2016_a",
            "entry": "D. Mishkin and J. Matas. All you need is a good init. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishkin%2C%20D.%20Matas%2C%20J.%20All%20you%20need%20is%20a%20good%20init%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishkin%2C%20D.%20Matas%2C%20J.%20All%20you%20need%20is%20a%20good%20init%202016"
        },
        {
            "id": "Nocedal_2006_a",
            "entry": "J. Nocedal and S. J. Wright. Sequential quadratic programming. Springer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nocedal%2C%20J.%20Wright%2C%20S.J.%20Sequential%20quadratic%20programming%202006"
        },
        {
            "id": "Ollivier_2015_a",
            "entry": "Y. Ollivier. Riemannian metrics for neural networks I: feedforward networks. Information and Inference: A Journal of the IMA, 4(2):108\u2013153, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ollivier%2C%20Y.%20Riemannian%20metrics%20for%20neural%20networks%20I%3A%20feedforward%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ollivier%2C%20Y.%20Riemannian%20metrics%20for%20neural%20networks%20I%3A%20feedforward%20networks%202015"
        },
        {
            "id": "Orhan_2018_a",
            "entry": "A. E. Orhan and X. Pitkow. Skip connections eliminate singularities. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orhan%2C%20A.E.%20Pitkow%2C%20X.%20Skip%20connections%20eliminate%20singularities%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orhan%2C%20A.E.%20Pitkow%2C%20X.%20Skip%20connections%20eliminate%20singularities%202018"
        },
        {
            "id": "Pascanu_2014_a",
            "entry": "R. Pascanu and Y. Bengio. Revisiting natural gradient for deep networks. In International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pascanu%2C%20R.%20Bengio%2C%20Y.%20Revisiting%20natural%20gradient%20for%20deep%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pascanu%2C%20R.%20Bengio%2C%20Y.%20Revisiting%20natural%20gradient%20for%20deep%20networks%202014"
        },
        {
            "id": "Paszke_et+al_2017_a",
            "entry": "A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20A.%20Gross%2C%20S.%20Chintala%2C%20S.%20Chanan%2C%20G.%20Automatic%20differentiation%20in%20PyTorch%202017"
        },
        {
            "id": "Rumelhart_et+al_1986_a",
            "entry": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20D.E.%20Hinton%2C%20G.E.%20Williams%2C%20R.J.%20Learning%20representations%20by%20back-propagating%20errors%201986"
        },
        {
            "id": "Sagun_et+al_2017_a",
            "entry": "L. Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04454"
        },
        {
            "id": "Saxe_et+al_2014_a",
            "entry": "A. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saxe%2C%20A.%20McClelland%2C%20J.L.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saxe%2C%20A.%20McClelland%2C%20J.L.%20Ganguli%2C%20S.%20Exact%20solutions%20to%20the%20nonlinear%20dynamics%20of%20learning%20in%20deep%20linear%20neural%20networks%202014"
        },
        {
            "id": "Seide_2016_a",
            "entry": "F. Seide and A. Agarwal. CNTK: Microsoft\u2019s open-source deep-learning toolkit. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2135\u20132135. ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seide%2C%20F.%20Agarwal%2C%20A.%20CNTK%3A%20Microsoft%E2%80%99s%20open-source%20deep-learning%20toolkit%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seide%2C%20F.%20Agarwal%2C%20A.%20CNTK%3A%20Microsoft%E2%80%99s%20open-source%20deep-learning%20toolkit%202016"
        },
        {
            "id": "Simonyan_2015_a",
            "entry": "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Simonyan%2C%20K.%20Zisserman%2C%20A.%20Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition%202015"
        },
        {
            "id": "Singh_et+al_2015_a",
            "entry": "B. Singh, S. De, Y. Zhang, T. Goldstein, and G. Taylor. Layer-specific adaptive learning rates for deep networks. In IEEE 14th International Conference on Machine Learning and Applications (ICMLA), pages 364\u2013368, Dec 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20B.%20De%2C%20S.%20Zhang%2C%20Y.%20Goldstein%2C%20T.%20Layer-specific%20adaptive%20learning%20rates%20for%20deep%20networks%202015-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20B.%20De%2C%20S.%20Zhang%2C%20Y.%20Goldstein%2C%20T.%20Layer-specific%20adaptive%20learning%20rates%20for%20deep%20networks%202015-12"
        },
        {
            "id": "Veit_et+al_2016_a",
            "entry": "A. Veit, M. J. Wilber, and S. Belongie. Residual networks behave like ensembles of relatively shallow networks. In Advances in Neural Information Processing Systems (NIPS), pages 550\u2013558, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Veit%2C%20A.%20Wilber%2C%20M.J.%20Belongie%2C%20S.%20Residual%20networks%20behave%20like%20ensembles%20of%20relatively%20shallow%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Veit%2C%20A.%20Wilber%2C%20M.J.%20Belongie%2C%20S.%20Residual%20networks%20behave%20like%20ensembles%20of%20relatively%20shallow%20networks%202016"
        },
        {
            "id": "Wiseman_et+al_2017_a",
            "entry": "S. Wiseman, S. Chopra, M. Ranzato, A. Szlam, R. Sun, S. Chintala, and N. Vasilache. Training language models using target-propagation. arXiv preprint arXiv:1702.04770, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.04770"
        },
        {
            "id": "Ye_et+al_2017_a",
            "entry": "C. Ye, Y. Yang, C. Fermuller, and Y. Aloimonos. On the importance of consistency in training deep neural networks. arXiv preprint arXiv:1708.00631, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.00631"
        },
        {
            "id": "You_et+al_2017_a",
            "entry": "Y. You, I. Gitman, and B. Ginsburg. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888v3, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.03888v3"
        },
        {
            "id": "Zhang_et+al_2017_a",
            "entry": "H. Zhang, C. Xiong, J. Bradbury, and R. Socher. Block-diagonal hessian-free optimization for training neural networks. arXiv preprint arXiv:1712.07296, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.07296"
        },
        {
            "id": "Zoph_2016_a",
            "entry": "B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%2C%20B.%20Le%2C%20Q.V.%20Neural%20architecture%20search%20with%20reinforcement%20learning%202016"
        }
    ]
}
