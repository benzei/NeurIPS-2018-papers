{
    "filename": "7635-sublinear-time-low-rank-approximation-of-distance-matrices.pdf",
    "metadata": {
        "title": "Sublinear Time Low-Rank Approximation of Distance Matrices",
        "author": "Ainesh Bakshi, David Woodruff",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7635-sublinear-time-low-rank-approximation-of-distance-matrices.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Let P = {p1, p2, . . . pn} and Q = {q1, q2 . . . qm} be two point sets in an arbitrary metric space. Let A represent the m \u00d7 n pairwise distance matrix with Ai,j = d(pi, qj). Such distance matrices are commonly computed in software packages and have applications to learning image manifolds, handwriting recognition, and multi-dimensional unfolding, among other things. In an attempt to reduce their description size, we study low rank approximation of such matrices. Our main result is to show that for any underlying distance metric d, it is possible to achieve an additive error low rank approximation in sublinear time. We note that it is provably impossible to achieve such a guarantee in sublinear time for arbitrary matrices A, and our proof exploits special properties of distance matrices. We develop a recursive algorithm based on additive projection-cost preserving sampling. We then show that in general, relative error approximation in sublinear time is impossible for distance matrices, even if one allows for bicriteria solutions. Additionally, we show that if P = Q and d is the squared Euclidean distance, which is not a metric but rather the square of a metric, then a relative error bicriteria solution can be found in sublinear time. Finally, we empirically compare our algorithm with the singular value decomposition (SVD) and input sparsity time algorithms. Our algorithm is several hundred times faster than the SVD, and about 8-20 times faster than input sparsity methods on real-world and and synthetic datasets of size 108. Accuracy-wise, our algorithm is only slightly worse than that of the SVD (optimal) and input-sparsity time algorithms."
    },
    "keywords": [
        {
            "term": "positive semidefinite",
            "url": "https://en.wikipedia.org/wiki/positive_semidefinite"
        },
        {
            "term": "metric space",
            "url": "https://en.wikipedia.org/wiki/metric_space"
        },
        {
            "term": "software package",
            "url": "https://en.wikipedia.org/wiki/software_package"
        },
        {
            "term": "euclidean distance",
            "url": "https://en.wikipedia.org/wiki/euclidean_distance"
        },
        {
            "term": "handwriting recognition",
            "url": "https://en.wikipedia.org/wiki/handwriting_recognition"
        },
        {
            "term": "low rank approximation",
            "url": "https://en.wikipedia.org/wiki/low_rank_approximation"
        },
        {
            "term": "nuclear magnetic resonance",
            "url": "https://en.wikipedia.org/wiki/nuclear_magnetic_resonance"
        },
        {
            "term": "relative error",
            "url": "https://en.wikipedia.org/wiki/relative_error"
        },
        {
            "term": "singular value decomposition",
            "url": "https://en.wikipedia.org/wiki/singular_value_decomposition"
        },
        {
            "term": "distance matrix",
            "url": "https://en.wikipedia.org/wiki/distance_matrix"
        }
    ],
    "highlights": [
        "(Row Norm Estimation.) Let A be a m \u00d7 n matrix such that A satisfies approximate triangle inequality",
        "We study low rank approximation of matrices A formed by the pairwise distances between two sets of points or observations P = {p1, . . . , pm} and Q = {q1, . . . , qn} in an arbitrary underlying metric space",
        "Large entries in a distance matrix A cannot hide, we show it is still impossible to achieve the relative error guarantee in less than mn time for distance matrices",
        "For any > 0, integer k and a small constant \u03b3 > 0, there exists an algorithm that runs in time O m1+\u03b3 + n1+\u03b3 poly( k ) to output matrices M \u2208 Rm\u00d7k and N \u2208 Rn\u00d7k such that with probability at least 9/10, 2 F\n5 Relative Error Guarantees",
        "We begin by showing a lower bound for any relative error approximation for distance matrices and preclude the possibility of a sublinear bi-criteria algorithm outputting a rank-poly(k) matrix satisfying the rank-k relative error guarantee",
        "We show that in the special case of Euclidean distances, when the entries correspond to squared distances, there exists a bi-criteria algorithm that outputs a rank-(k + 4)"
    ],
    "key_statements": [
        "(Row Norm Estimation.) Let A be a m \u00d7 n matrix such that A satisfies approximate triangle inequality",
        "We study low rank approximation of matrices A formed by the pairwise distances between two sets of points or observations P = {p1, . . . , pm} and Q = {q1, . . . , qn} in an arbitrary underlying metric space",
        "Large entries in a distance matrix A cannot hide, we show it is still impossible to achieve the relative error guarantee in less than mn time for distance matrices",
        "For any > 0, integer k and a small constant \u03b3 > 0, there exists an algorithm that runs in time O m1+\u03b3 + n1+\u03b3 poly( k ) to output matrices M \u2208 Rm\u00d7k and N \u2208 Rn\u00d7k such that with probability at least 9/10, 2 F\n5 Relative Error Guarantees",
        "We begin by showing a lower bound for any relative error approximation for distance matrices and preclude the possibility of a sublinear bi-criteria algorithm outputting a rank-poly(k) matrix satisfying the rank-k relative error guarantee",
        "We show that in the special case of Euclidean distances, when the entries correspond to squared distances, there exists a bi-criteria algorithm that outputs a rank-(k + 4)",
        "We show that there exists an algorithm that outputs the description of a rank-(k + 4) matrix AWWT in sublinear time such it satisfies the relative-error rank-k low rank approximation guarantee"
    ],
    "summary": [
        "(Row Norm Estimation.) Let A be a m \u00d7 n matrix such that A satisfies approximate triangle inequality.",
        "Sketch the problem using the the leverage scores of P following Theorem 4.3 to obtain a sampling matrix E with poly( k ) rows.",
        "We run an input-sparsity time algorithm ([<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]) on the small matrix to obtain a low-rank approximation.",
        "Let AS|Wg denote the columns of AS restricted to the set of indices in Wg. Observe that all entries in AS|Wg are scaled to within a (1 + )-factor of each other and satisfy approximate triangle inequality.",
        "Running the input-sparsity time algorithm with the above guarantee on the matrix TAS, we obtain a rank-k matrix LDWT , such that",
        "If we do not consider running time, we could construct a low-rank approximation to A as follows: since projecting TAS onto WT is approximately optimal, it follows from Lemma A.1 that with probability 98/100, AS \u2212 ASWWT",
        "Even approximately computing a column space P for (AS)k using an input-sparsity time algorithm is no longer sublinear.",
        "The input-sparsity time algorithm low-rank approximation runs in O s1s2 + (s1 + s2)poly( k ) and constructing a solution for A is dominated by",
        "The algorithm is to recursively sub-sample columns and rows of A such that we obtain projection-cost preserving sketches at each step.",
        "For any > 0, integer k and a small constant \u03b3 > 0, there exists an algorithm that runs in time O m1+\u03b3 + n1+\u03b3 poly( k ) to output matrices M \u2208 Rm\u00d7k and N \u2208 Rn\u00d7k such that with probability at least 9/10, 2 F",
        "We begin by showing a lower bound for any relative error approximation for distance matrices and preclude the possibility of a sublinear bi-criteria algorithm outputting a rank-poly(k) matrix satisfying the rank-k relative error guarantee.",
        "We show that there exists an algorithm that outputs the description of a rank-(k + 4) matrix AWWT in sublinear time such it satisfies the relative-error rank-k low rank approximation guarantee.",
        "For any > 0 and integer k, there exists an algorithm that with probability at least 9/10, outputs a rank (k + 4) matrix WWT such that A \u2212 AWWT F \u2264 (1 + ) A \u2212 Ak F , where Ak is the best rank-k approximation to A and runs in O).",
        "We implement the input-sparsity time low-rank approximation algorithm from [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] using a count-sketch matrix [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]."
    ],
    "headline": "In an attempt to reduce their description size, we study low rank approximation of such matrices",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Jean Bourgain, Sjoerd Dirksen, and Jelani Nelson. Toward a unified theory of sparse dimensionality reduction in euclidean space. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 499\u2013508, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bourgain%2C%20Jean%20Dirksen%2C%20Sjoerd%20Nelson%2C%20Jelani%20Toward%20a%20unified%20theory%20of%20sparse%20dimensionality%20reduction%20in%20euclidean%20space%202015-06-14",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bourgain%2C%20Jean%20Dirksen%2C%20Sjoerd%20Nelson%2C%20Jelani%20Toward%20a%20unified%20theory%20of%20sparse%20dimensionality%20reduction%20in%20euclidean%20space%202015-06-14"
        },
        {
            "id": "2",
            "entry": "[2] Robert Cattral, Franz Oppacher, and Dwight Deugo. Evolutionary data mining with automatic rule generalization. Recent Advances in Computers, Computing and Communications, 1(1):296\u2013 300, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cattral%2C%20Robert%20Oppacher%2C%20Franz%20Deugo%2C%20Dwight%20Evolutionary%20data%20mining%20with%20automatic%20rule%20generalization%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cattral%2C%20Robert%20Oppacher%2C%20Franz%20Deugo%2C%20Dwight%20Evolutionary%20data%20mining%20with%20automatic%20rule%20generalization%202002"
        },
        {
            "id": "3",
            "entry": "[3] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pages 693\u2013703.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Charikar%2C%20Moses%20Chen%2C%20Kevin%20Farach-Colton%2C%20Martin%20Finding%20frequent%20items%20in%20data%20streams",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Charikar%2C%20Moses%20Chen%2C%20Kevin%20Farach-Colton%2C%20Martin%20Finding%20frequent%20items%20in%20data%20streams"
        },
        {
            "id": "4",
            "entry": "[4] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input sparsity time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 81\u201390. ACM, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Clarkson%2C%20Kenneth%20L.%20Woodruff%2C%20David%20P.%20Low%20rank%20approximation%20and%20regression%20in%20input%20sparsity%20time%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Clarkson%2C%20Kenneth%20L.%20Woodruff%2C%20David%20P.%20Low%20rank%20approximation%20and%20regression%20in%20input%20sparsity%20time%202013"
        },
        {
            "id": "5",
            "entry": "[5] Michael B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2016, Arlington, VA, USA, January 10-12, 2016, pages 278\u2013287, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Michael%20B.%20Nearly%20tight%20oblivious%20subspace%20embeddings%20by%20trace%20inequalities%202016-01-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Michael%20B.%20Nearly%20tight%20oblivious%20subspace%20embeddings%20by%20trace%20inequalities%202016-01-10"
        },
        {
            "id": "6",
            "entry": "[6] Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, pages 1758\u20131777, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cohen%2C%20Michael%20B.%20Musco%2C%20Cameron%20Musco%2C%20Christopher%20Input%20sparsity%20time%20low-rank%20approximation%20via%20ridge%20leverage%20score%20sampling%202017-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cohen%2C%20Michael%20B.%20Musco%2C%20Cameron%20Musco%2C%20Christopher%20Input%20sparsity%20time%20low-rank%20approximation%20via%20ridge%20leverage%20score%20sampling%202017-01"
        },
        {
            "id": "7",
            "entry": "[7] Erik D. Demaine, Francisco Gomez-Martin, Henk Meijer, David Rappaport, Perouz Taslakian, Godfried T. Toussaint, Terry Winograd, and David R. Wood. The distance geometry of music. Comput. Geom., 42(5):429\u2013454, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Demaine%2C%20Erik%20D.%20Gomez-Martin%2C%20Francisco%20Meijer%2C%20Henk%20Rappaport%2C%20David%20The%20distance%20geometry%20of%20music%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Demaine%2C%20Erik%20D.%20Gomez-Martin%2C%20Francisco%20Meijer%2C%20Henk%20Rappaport%2C%20David%20The%20distance%20geometry%20of%20music%202009"
        },
        {
            "id": "8",
            "entry": "[8] Ivan Dokmanic, Reza Parhizkar, Juri Ranieri, and Martin Vetterli. Euclidean distance matrices: essential theory, algorithms, and applications. IEEE Signal Processing Magazine, 32(6):12\u201330, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dokmanic%2C%20Ivan%20Parhizkar%2C%20Reza%20Ranieri%2C%20Juri%20Vetterli%2C%20Martin%20Euclidean%20distance%20matrices%3A%20essential%20theory%2C%20algorithms%2C%20and%20applications%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dokmanic%2C%20Ivan%20Parhizkar%2C%20Reza%20Ranieri%2C%20Juri%20Vetterli%2C%20Martin%20Euclidean%20distance%20matrices%3A%20essential%20theory%2C%20algorithms%2C%20and%20applications%202015"
        },
        {
            "id": "9",
            "entry": "[9] Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error cur matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844\u2013881, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20Muthukrishnan%2C%20S.%20Relative-error%20cur%20matrix%20decompositions%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Drineas%2C%20Petros%20Mahoney%2C%20Michael%20W.%20Muthukrishnan%2C%20S.%20Relative-error%20cur%20matrix%20decompositions%202008"
        },
        {
            "id": "10",
            "entry": "[10] Alan M. Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank approximations. J. ACM, 51(6):1025\u20131041, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frieze%2C%20Alan%20M.%20Kannan%2C%20Ravi%20Vempala%2C%20Santosh%20Fast%20monte-carlo%20algorithms%20for%20finding%20low-rank%20approximations%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frieze%2C%20Alan%20M.%20Kannan%2C%20Ravi%20Vempala%2C%20Santosh%20Fast%20monte-carlo%20algorithms%20for%20finding%20low-rank%20approximations%202004"
        },
        {
            "id": "11",
            "entry": "[11] Isabelle Guyon, Steve Gunn, Asa Ben-Hur, and Gideon Dror. Result analysis of the nips 2003 feature selection challenge. In Advances in neural information processing systems, pages 545\u2013552, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isabelle%20Guyon%20Steve%20Gunn%20Asa%20BenHur%20and%20Gideon%20Dror%20Result%20analysis%20of%20the%20nips%202003%20feature%20selection%20challenge%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%20545552%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isabelle%20Guyon%20Steve%20Gunn%20Asa%20BenHur%20and%20Gideon%20Dror%20Result%20analysis%20of%20the%20nips%202003%20feature%20selection%20challenge%20In%20Advances%20in%20neural%20information%20processing%20systems%20pages%20545552%202005"
        },
        {
            "id": "12",
            "entry": "[12] Viren Jain and L.K. Saul. Exploratory analysis and visualization of speech and music by locally linear embedding. In Departmental Papers (CIS), pages iii \u2013 984, 06 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jain%2C%20Viren%20Saul%2C%20L.K.%20Exploratory%20analysis%20and%20visualization%20of%20speech%20and%20music%20by%20locally%20linear%20embedding%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jain%2C%20Viren%20Saul%2C%20L.K.%20Exploratory%20analysis%20and%20visualization%20of%20speech%20and%20music%20by%20locally%20linear%20embedding%202004"
        },
        {
            "id": "13",
            "entry": "[13] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression. In Symposium on Theory of Computing Conference, STOC\u201913, Palo Alto, CA, USA, June 1-4, 2013, pages 91\u2013100, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meng%2C%20Xiangrui%20Mahoney%2C%20Michael%20W.%20Low-distortion%20subspace%20embeddings%20in%20inputsparsity%20time%20and%20applications%20to%20robust%20linear%20regression%202013-06-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meng%2C%20Xiangrui%20Mahoney%2C%20Michael%20W.%20Low-distortion%20subspace%20embeddings%20in%20inputsparsity%20time%20and%20applications%20to%20robust%20linear%20regression%202013-06-01"
        },
        {
            "id": "14",
            "entry": "[14] Cameron Musco and David P. Woodruff. Sublinear time low-rank approximation of positive semidefinite matrices. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pages 672\u2013683, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Musco%2C%20Cameron%20Woodruff%2C%20David%20P.%20Sublinear%20time%20low-rank%20approximation%20of%20positive%20semidefinite%20matrices%202017-10-15",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Musco%2C%20Cameron%20Woodruff%2C%20David%20P.%20Sublinear%20time%20low-rank%20approximation%20of%20positive%20semidefinite%20matrices%202017-10-15"
        },
        {
            "id": "15",
            "entry": "[15] Jelani Nelson and Huy L. Nguyen. OSNAP: faster numerical linear algebra algorithms via sparser subspace embeddings. In 54th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA, pages 117\u2013126, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nelson%2C%20Jelani%20Nguyen%2C%20Huy%20L.%20OSNAP%3A%20faster%20numerical%20linear%20algebra%20algorithms%20via%20sparser%20subspace%20embeddings%202013-10-26",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nelson%2C%20Jelani%20Nguyen%2C%20Huy%20L.%20OSNAP%3A%20faster%20numerical%20linear%20algebra%20algorithms%20via%20sparser%20subspace%20embeddings%202013-10-26"
        },
        {
            "id": "16",
            "entry": "[16] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In FOCS, pages 143\u2013152, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sarlos%2C%20Tamas%20Improved%20approximation%20algorithms%20for%20large%20matrices%20via%20random%20projections%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sarlos%2C%20Tamas%20Improved%20approximation%20algorithms%20for%20large%20matrices%20via%20random%20projections%202006"
        },
        {
            "id": "17",
            "entry": "[17] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tenenbaum%2C%20Joshua%20B.%20Silva%2C%20Vin%20De%20Langford%2C%20John%20C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tenenbaum%2C%20Joshua%20B.%20Silva%2C%20Vin%20De%20Langford%2C%20John%20C.%20A%20global%20geometric%20framework%20for%20nonlinear%20dimensionality%20reduction%202000"
        },
        {
            "id": "18",
            "entry": "[18] Kilian Q. Weinberger and Lawrence K. Saul. Unsupervised learning of image manifolds by semidefinite programming. In 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2004), with CD-ROM, 27 June - 2 July 2004, Washington, DC, USA, pages 988\u2013995, 2004. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Weinberger%2C%20Kilian%20Q.%20Saul%2C%20Lawrence%20K.%20Unsupervised%20learning%20of%20image%20manifolds%20by%20semidefinite%20programming%202004-06-27",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Weinberger%2C%20Kilian%20Q.%20Saul%2C%20Lawrence%20K.%20Unsupervised%20learning%20of%20image%20manifolds%20by%20semidefinite%20programming%202004-06-27"
        }
    ]
}
