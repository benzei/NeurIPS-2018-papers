{
    "filename": "8084-estimators-for-multivariate-information-measures-in-general-probability-spaces.pdf",
    "metadata": {
        "title": "Estimators for Multivariate Information Measures in General Probability Spaces",
        "author": "Arman Rahimzamani, Himanshu Asnani, Pramod Viswanath, Sreeram Kannan",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8084-estimators-for-multivariate-information-measures-in-general-probability-spaces.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Information theoretic quantities play an important role in various settings in machine learning, including causality testing, structure inference in graphical models, time-series problems, feature selection as well as in providing privacy guarantees. A key quantity of interest is the mutual information and generalizations thereof, including conditional mutual information, multivariate mutual information, total correlation and directed information. While the aforementioned information quantities are well defined in arbitrary probability spaces, existing estimators add or subtract entropies (we term them \u03a3H methods). These methods work only in purely discrete space or purely continuous case since entropy (or differential entropy) is well defined only in that regime. In this paper, we define a general graph divergence measure (GDM),as a measure of incompatibility between the observed distribution and a given graphical model structure. This generalizes the aforementioned information measures and we construct a novel estimator via a coupling trick that directly estimates these multivariate information measures using the Radon-Nikodym derivative. These estimators are proven to be consistent in a general setting which includes several cases where the existing estimators fail, thus providing the only known estimators for the following settings: (1) the data has some discrete and some continuous valued components (2) some (or all) of the components themselves are discrete-continuous mixtures (3) the data is real-valued but does not have a joint density on the entire space, rather is supported on a low-dimensional manifold. We show that our proposed estimators significantly outperform known estimators on synthetic and real datasets."
    },
    "keywords": [
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "Additive White Gaussian Noise",
            "url": "https://en.wikipedia.org/wiki/Additive_White_Gaussian_Noise"
        },
        {
            "term": "directed acyclic graph",
            "url": "https://en.wikipedia.org/wiki/directed_acyclic_graph"
        },
        {
            "term": "feature selection",
            "url": "https://en.wikipedia.org/wiki/feature_selection"
        },
        {
            "term": "time series",
            "url": "https://en.wikipedia.org/wiki/time_series"
        },
        {
            "term": "Mutual Information",
            "url": "https://en.wikipedia.org/wiki/Mutual_Information"
        },
        {
            "term": "conditional mutual information",
            "url": "https://en.wikipedia.org/wiki/conditional_mutual_information"
        }
    ],
    "highlights": [
        "Such as mutual information and its generalizations, play an important role in various settings in machine learning and statistical estimation and inference",
        "Conditional mutual information measures the amount of information between two variables X and Y given a third variable Z and is zero iff X is independent of Y given Z",
        "Consistency proofs (Section 4): We prove that the proposed estimators converge to the true value of the corresponding graph divergence measures as the number of observed samples increases in a general setting which includes several cases: (1) the data has some discrete and some continuous valued components (2) some of the components themselves are discrete-continuous mixtures (3) the data is real-valued but does not have a joint density on the entire space but is supported on a low-dimensional manifold.\n4",
        "For a random variable X, a directed acyclic graph G, let \u03a0(G) be the set of measures QX defined on the Bayesian Network G, graph divergence measure(X, G) = infQX\u2208\u03a0(G) D(PX QX )",
        "Gao et al [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] proposed a mutual-information estimator based on kth nearest neighbor principle, which can handle such continuous-discrete mixture cases, and the consistency was demonstrated",
        "For the second scheme of our experiments, we considered an Additive White Gaussian Noise (AWGN) Channel in parallel with a Binary Symmetric Channel (BSC) where only one of the two can be activated at a time"
    ],
    "key_statements": [
        "Such as mutual information and its generalizations, play an important role in various settings in machine learning and statistical estimation and inference",
        "Conditional mutual information measures the amount of information between two variables X and Y given a third variable Z and is zero iff X is independent of Y given Z",
        "Consistency proofs (Section 4): We prove that the proposed estimators converge to the true value of the corresponding graph divergence measures as the number of observed samples increases in a general setting which includes several cases: (1) the data has some discrete and some continuous valued components (2) some of the components themselves are discrete-continuous mixtures (3) the data is real-valued but does not have a joint density on the entire space but is supported on a low-dimensional manifold.\n4",
        "For a random variable X, a directed acyclic graph G, let \u03a0(G) be the set of measures QX defined on the Bayesian Network G, graph divergence measure(X, G) = infQX\u2208\u03a0(G) D(PX QX )",
        "Gao et al [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] proposed a mutual-information estimator based on kth nearest neighbor principle, which can handle such continuous-discrete mixture cases, and the consistency was demonstrated",
        "For the second scheme of our experiments, we considered an Additive White Gaussian Noise (AWGN) Channel in parallel with a Binary Symmetric Channel (BSC) where only one of the two can be activated at a time"
    ],
    "summary": [
        "Such as mutual information and its generalizations, play an important role in various settings in machine learning and statistical estimation and inference.",
        "The underlying information measures themselves are well defined in very general probability spaces, for example, involving mixtures of discrete and continuous variables; no known estimators exist.",
        "When calculating mutual information between input and output of a neural network, some of the neurons are deterministic functions of the input variables and they will have a joint density supported on a low-dimensional manifold rather than the entire space.",
        "These graph divergence measures are defined using the Radon Nikodym derivatives which are well-defined for general probability spaces.",
        "3. Consistency proofs (Section 4): We prove that the proposed estimators converge to the true value of the corresponding graph divergence measures as the number of observed samples increases in a general setting which includes several cases: (1) the data has some discrete and some continuous valued components (2) some of the components themselves are discrete-continuous mixtures (3) the data is real-valued but does not have a joint density on the entire space but is supported on a low-dimensional manifold.",
        "For a given set of random variables X and a Bayesian Network G, we define Graph Divergence",
        "For a random variable X, a DAG G, let \u03a0(G) be the set of measures QX defined on the Bayesian Network G, GDM(X, G) = infQX\u2208\u03a0(G) D(PX QX ).",
        "There have been some extensions to the KSG estimator for other information measures such as conditional mutual information [<a class=\"ref-link\" id=\"c48\" href=\"#r48\">48</a>, <a class=\"ref-link\" id=\"c49\" href=\"#r49\">49</a>], directed information [<a class=\"ref-link\" id=\"c50\" href=\"#r50\">50</a>] but none of them show theoretical guarantees on consistency of the estimators, they fail completely in mixture distributions.",
        "Gao et al [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] proposed a mutual-information estimator based on KNN principle, which can handle such continuous-discrete mixture cases, and the consistency was demonstrated.",
        "For the total correlation experiments 3 and 4, similar to that of conditional mutual information in experiments 1 and 2, only the GDM estimator can best estimate the true value.",
        "In this experiment we use different estimators to do Gene Regulatory Network inference based on the conditional Restricted Directed Information [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "A general paradigm of graph divergence measures and novel estimators thereof, for general probability spaces are proposed, which estimate several generalizations of mutual information.",
        "Recent literature including [<a class=\"ref-link\" id=\"c54\" href=\"#r54\">54</a>] provide a new methodology to estimate mutual information in a computationally efficient manner and leveraging these ideas for the generalized measures and general proabability distributions can be a promising direction ahead."
    ],
    "headline": "We show that our proposed estimators significantly outperform known estimators on synthetic and real datasets",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. P. Dawid, \u201cConditional independence in statistical theory,\u201d Journal of the Royal Statistical Society. Series B (Methodological), pp. 1\u201331, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dawid%2C%20A.P.%20Conditional%20independence%20in%20statistical%20theory%2C%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dawid%2C%20A.P.%20Conditional%20independence%20in%20statistical%20theory%2C%201979"
        },
        {
            "id": "2",
            "entry": "[2] K. Zhang, J. Peters, D. Janzing, and B. Sch\u00f6lkopf, \u201cKernel-based conditional independence test and application in causal discovery,\u201d arXiv preprint arXiv:1202.3775, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1202.3775"
        },
        {
            "id": "3",
            "entry": "[3] J. Whittaker, Graphical models in applied multivariate statistics. Wiley Publishing, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Whittaker%2C%20J.%20Graphical%20models%20in%20applied%20multivariate%20statistics%202009"
        },
        {
            "id": "4",
            "entry": "[4] F. Fleuret, \u201cFast binary feature selection with conditional mutual information,\u201d Journal of Machine Learning Research, vol. 5, no. Nov, pp. 1531\u20131555, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fleuret%2C%20F.%20Fast%20binary%20feature%20selection%20with%20conditional%20mutual%20information%2C%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fleuret%2C%20F.%20Fast%20binary%20feature%20selection%20with%20conditional%20mutual%20information%2C%202004"
        },
        {
            "id": "5",
            "entry": "[5] P. Cuff and L. Yu, \u201cDifferential privacy as a mutual information constraint,\u201d in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 43\u201354, ACM, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cuff%2C%20P.%20Yu%2C%20L.%20Differential%20privacy%20as%20a%20mutual%20information%20constraint%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cuff%2C%20P.%20Yu%2C%20L.%20Differential%20privacy%20as%20a%20mutual%20information%20constraint%2C%202016"
        },
        {
            "id": "6",
            "entry": "[6] A. Hyv\u00e4rinen and E. Oja, \u201cIndependent component analysis: algorithms and applications,\u201d Neural networks, vol. 13, no. 4-5, pp. 411\u2013430, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyv%C3%A4rinen%2C%20A.%20Oja%2C%20E.%20Independent%20component%20analysis%3A%20algorithms%20and%20applications%2C%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyv%C3%A4rinen%2C%20A.%20Oja%2C%20E.%20Independent%20component%20analysis%3A%20algorithms%20and%20applications%2C%202000"
        },
        {
            "id": "7",
            "entry": "[7] J. R. Vergara and P. A. Est\u00e9vez, \u201cA review of feature selection methods based on mutual information,\u201d Neural computing and applications, vol. 24, no. 1, pp. 175\u2013186, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vergara%2C%20J.R.%20Est%C3%A9vez%2C%20P.A.%20A%20review%20of%20feature%20selection%20methods%20based%20on%20mutual%20information%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vergara%2C%20J.R.%20Est%C3%A9vez%2C%20P.A.%20A%20review%20of%20feature%20selection%20methods%20based%20on%20mutual%20information%2C%202014"
        },
        {
            "id": "8",
            "entry": "[8] P. E. Meyer, C. Schretter, and G. Bontempi, \u201cInformation-theoretic feature selection in microarray data using variable complementarity,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 2, no. 3, pp. 261\u2013274, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meyer%2C%20P.E.%20Schretter%2C%20C.%20Bontempi%2C%20G.%20Information-theoretic%20feature%20selection%20in%20microarray%20data%20using%20variable%20complementarity%2C%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meyer%2C%20P.E.%20Schretter%2C%20C.%20Bontempi%2C%20G.%20Information-theoretic%20feature%20selection%20in%20microarray%20data%20using%20variable%20complementarity%2C%202008"
        },
        {
            "id": "9",
            "entry": "[9] W. McGill, \u201cMultivariate information transmission,\u201d Transactions of the IRE Professional Group on Information Theory, vol. 4, no. 4, pp. 93\u2013111, 1954.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McGill%2C%20W.%20%E2%80%9CMultivariate%20information%20transmission%2C%E2%80%9D%20Transactions%20of%20the%20IRE%20Professional%20Group%20on%201954",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McGill%2C%20W.%20%E2%80%9CMultivariate%20information%20transmission%2C%E2%80%9D%20Transactions%20of%20the%20IRE%20Professional%20Group%20on%201954"
        },
        {
            "id": "10",
            "entry": "[10] C. Chan, A. Al-Bashabsheh, J. B. Ebrahimi, T. Kaced, and T. Liu, \u201cMultivariate mutual information inspired by secret-key agreement,\u201d Proceedings of the IEEE, vol. 103, no. 10, pp. 1883\u20131913, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20C.%20Al-Bashabsheh%2C%20A.%20Ebrahimi%2C%20J.B.%20Kaced%2C%20T.%20Multivariate%20mutual%20information%20inspired%20by%20secret-key%20agreement%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20C.%20Al-Bashabsheh%2C%20A.%20Ebrahimi%2C%20J.B.%20Kaced%2C%20T.%20Multivariate%20mutual%20information%20inspired%20by%20secret-key%20agreement%2C%202015"
        },
        {
            "id": "11",
            "entry": "[11] J. Lee and D.-W. Kim, \u201cFeature selection for multi-label classification using multivariate mutual information,\u201d Pattern Recognition Letters, vol. 34, no. 3, pp. 349\u2013357, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20J.%20Kim%2C%20D.-W.%20Feature%20selection%20for%20multi-label%20classification%20using%20multivariate%20mutual%20information%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20J.%20Kim%2C%20D.-W.%20Feature%20selection%20for%20multi-label%20classification%20using%20multivariate%20mutual%20information%2C%202013"
        },
        {
            "id": "12",
            "entry": "[12] G. Brown, \u201cA new perspective for information theoretic feature selection,\u201d in Artificial Intelligence and Statistics, pp. 49\u201356, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brown%2C%20G.%20A%20new%20perspective%20for%20information%20theoretic%20feature%20selection%2C%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brown%2C%20G.%20A%20new%20perspective%20for%20information%20theoretic%20feature%20selection%2C%202009"
        },
        {
            "id": "13",
            "entry": "[13] C. Chan, A. Al-Bashabsheh, Q. Zhou, T. Kaced, and T. Liu, \u201cInfo-clustering: A mathematical theory for data clustering,\u201d IEEE Transactions on Molecular, Biological and Multi-Scale Communications, vol. 2, no. 1, pp. 64\u201391, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chan%2C%20C.%20Al-Bashabsheh%2C%20A.%20Zhou%2C%20Q.%20Kaced%2C%20T.%20Info-clustering%3A%20A%20mathematical%20theory%20for%20data%20clustering%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chan%2C%20C.%20Al-Bashabsheh%2C%20A.%20Zhou%2C%20Q.%20Kaced%2C%20T.%20Info-clustering%3A%20A%20mathematical%20theory%20for%20data%20clustering%2C%202016"
        },
        {
            "id": "14",
            "entry": "[14] S. Watanabe, \u201cInformation theoretical analysis of multivariate correlation,\u201d IBM Journal of research and development, vol. 4, no. 1, pp. 66\u201382, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Watanabe%2C%20S.%20Information%20theoretical%20analysis%20of%20multivariate%20correlation%2C%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Watanabe%2C%20S.%20Information%20theoretical%20analysis%20of%20multivariate%20correlation%2C%201960"
        },
        {
            "id": "15",
            "entry": "[15] H. H. Permuter, Y.-H. Kim, and T. Weissman, \u201cInterpretations of directed information in portfolio theory, data compression, and hypothesis testing,\u201d IEEE Transactions on Information Theory, vol. 57, no. 6, pp. 3248\u20133259, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Permuter%2C%20H.H.%20Kim%2C%20Y.-H.%20Weissman%2C%20T.%20Interpretations%20of%20directed%20information%20in%20portfolio%20theory%2C%20data%20compression%2C%20and%20hypothesis%20testing%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Permuter%2C%20H.H.%20Kim%2C%20Y.-H.%20Weissman%2C%20T.%20Interpretations%20of%20directed%20information%20in%20portfolio%20theory%2C%20data%20compression%2C%20and%20hypothesis%20testing%2C%202011"
        },
        {
            "id": "16",
            "entry": "[16] C. J. Quinn, N. Kiyavash, and T. P. Coleman, \u201cDirected information graphs,\u201d IEEE Transactions on information theory, vol. 61, no. 12, pp. 6887\u20136909, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Quinn%2C%20C.J.%20Kiyavash%2C%20N.%20Coleman%2C%20T.P.%20Directed%20information%20graphs%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Quinn%2C%20C.J.%20Kiyavash%2C%20N.%20Coleman%2C%20T.P.%20Directed%20information%20graphs%2C%202015"
        },
        {
            "id": "17",
            "entry": "[17] J. Sun, D. Taylor, and E. M. Bollt, \u201cCausal network inference by optimal causation entropy,\u201d SIAM Journal on Applied Dynamical Systems, vol. 14, no. 1, pp. 73\u2013106, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sun%2C%20J.%20Taylor%2C%20D.%20Bollt%2C%20E.M.%20Causal%20network%20inference%20by%20optimal%20causation%20entropy%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sun%2C%20J.%20Taylor%2C%20D.%20Bollt%2C%20E.M.%20Causal%20network%20inference%20by%20optimal%20causation%20entropy%2C%202015"
        },
        {
            "id": "18",
            "entry": "[18] K. Hlav\u00e1ckov\u00e1-Schindler, M. Palu\u0161, M. Vejmelka, and J. Bhattacharya, \u201cCausality detection based on information-theoretic approaches in time series analysis,\u201d Physics Reports, vol. 441, no. 1, pp. 1\u201346, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K.%20Hlav%C3%A1ckov%C3%A1-Schindler%2C%20M.%20Palu%C5%A1%2C%20M.%20Vejmelka%20Bhattacharya%2C%20J.%20Causality%20detection%20based%20on%20information-theoretic%20approaches%20in%20time%20series%20analysis%2C%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K.%20Hlav%C3%A1ckov%C3%A1-Schindler%2C%20M.%20Palu%C5%A1%2C%20M.%20Vejmelka%20Bhattacharya%2C%20J.%20Causality%20detection%20based%20on%20information-theoretic%20approaches%20in%20time%20series%20analysis%2C%202007"
        },
        {
            "id": "19",
            "entry": "[19] P.-O. Amblard and O. J. Michel, \u201cOn directed information theory and granger causality graphs,\u201d Journal of computational neuroscience, vol. 30, no. 1, pp. 7\u201316, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amblard%2C%20P.-O.%20Michel%2C%20O.J.%20On%20directed%20information%20theory%20and%20granger%20causality%20graphs%2C%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amblard%2C%20P.-O.%20Michel%2C%20O.J.%20On%20directed%20information%20theory%20and%20granger%20causality%20graphs%2C%202011"
        },
        {
            "id": "20",
            "entry": "[20] A. Rahimzamani and S. Kannan, \u201cNetwork inference using directed information: The deterministic limit,\u201d in Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on, pp. 156\u2013163, IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimzamani%2C%20A.%20Kannan%2C%20S.%20%E2%80%9CNetwork%20inference%20using%20directed%20information%3A%20The%20deterministic%20limit%2C%E2%80%9D%20in%20Communication%2C%20Control%2C%20and%20Computing%202016"
        },
        {
            "id": "21",
            "entry": "[21] A. Rahimzamani and S. Kannan, \u201cPotential conditional mutual information: Estimators and properties,\u201d in Communication, Control, and Computing (Allerton), 2017 55th Annual Allerton Conference on, pp. 1228\u20131235, IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimzamani%2C%20A.%20Kannan%2C%20S.%20Potential%20conditional%20mutual%20information%3A%20Estimators%20and%20properties%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimzamani%2C%20A.%20Kannan%2C%20S.%20Potential%20conditional%20mutual%20information%3A%20Estimators%20and%20properties%2C%202017"
        },
        {
            "id": "22",
            "entry": "[22] L. Kozachenko and N. N. Leonenko, \u201cSample estimate of the entropy of a random vector,\u201d Problemy Peredachi Informatsii, vol. 23, no. 2, pp. 9\u201316, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kozachenko%2C%20L.%20Leonenko%2C%20N.N.%20Sample%20estimate%20of%20the%20entropy%20of%20a%20random%20vector%2C%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kozachenko%2C%20L.%20Leonenko%2C%20N.N.%20Sample%20estimate%20of%20the%20entropy%20of%20a%20random%20vector%2C%201987"
        },
        {
            "id": "23",
            "entry": "[23] J. Beirlant, E. J. Dudewicz, L. Gy\u00f6rfi, and E. C. Van der Meulen, \u201cNonparametric entropy estimation: An overview,\u201d International Journal of Mathematical and Statistical Sciences, vol. 6, no. 1, pp. 17\u201339, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beirlant%2C%20J.%20Dudewicz%2C%20E.J.%20Gy%C3%B6rfi%2C%20L.%20der%20Meulen%2C%20E.C.Van%20Nonparametric%20entropy%20estimation%3A%20An%20overview%2C%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beirlant%2C%20J.%20Dudewicz%2C%20E.J.%20Gy%C3%B6rfi%2C%20L.%20der%20Meulen%2C%20E.C.Van%20Nonparametric%20entropy%20estimation%3A%20An%20overview%2C%201997"
        },
        {
            "id": "24",
            "entry": "[24] R. Wieczorkowski and P. Grzegorzewski, \u201cEntropy estimators-improvements and comparisons,\u201d Communications in Statistics-Simulation and Computation, vol. 28, no. 2, pp. 541\u2013567, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wieczorkowski%2C%20R.%20Grzegorzewski%2C%20P.%20Entropy%20estimators-improvements%20and%20comparisons%2C%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wieczorkowski%2C%20R.%20Grzegorzewski%2C%20P.%20Entropy%20estimators-improvements%20and%20comparisons%2C%201999"
        },
        {
            "id": "25",
            "entry": "[25] E. G. Miller, \u201cA new class of entropy estimators for multi-dimensional densities,\u201d in Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, vol. 3, pp. III\u2013297, IEEE, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miller%2C%20E.G.%20%E2%80%9CA%20new%20class%20of%20entropy%20estimators%20for%20multi-dimensional%20densities%2C%E2%80%9D%20in%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miller%2C%20E.G.%20%E2%80%9CA%20new%20class%20of%20entropy%20estimators%20for%20multi-dimensional%20densities%2C%E2%80%9D%20in%202003"
        },
        {
            "id": "26",
            "entry": "[26] I. Lee, \u201cSample-spacings-based density and entropy estimators for spherically invariant multidimensional data,\u201d Neural Computation, vol. 22, no. 8, pp. 2208\u20132227, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20I.%20Sample-spacings-based%20density%20and%20entropy%20estimators%20for%20spherically%20invariant%20multidimensional%20data%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20I.%20Sample-spacings-based%20density%20and%20entropy%20estimators%20for%20spherically%20invariant%20multidimensional%20data%2C%202010"
        },
        {
            "id": "27",
            "entry": "[27] K. Sricharan, D. Wei, and A. O. Hero, \u201cEnsemble estimators for multivariate entropy estimation,\u201d IEEE transactions on information theory, vol. 59, no. 7, pp. 4374\u20134388, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sricharan%2C%20K.%20Wei%2C%20D.%20Hero%2C%20A.O.%20Ensemble%20estimators%20for%20multivariate%20entropy%20estimation%2C%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sricharan%2C%20K.%20Wei%2C%20D.%20Hero%2C%20A.O.%20Ensemble%20estimators%20for%20multivariate%20entropy%20estimation%2C%202013"
        },
        {
            "id": "28",
            "entry": "[28] Y. Han, J. Jiao, and T. Weissman, \u201cAdaptive estimation of shannon entropy,\u201d in Information Theory (ISIT), 2015 IEEE International Symposium on, pp. 1372\u20131376, IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Han%2C%20Y.%20Jiao%2C%20J.%20Weissman%2C%20T.%20Adaptive%20estimation%20of%20shannon%20entropy%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Han%2C%20Y.%20Jiao%2C%20J.%20Weissman%2C%20T.%20Adaptive%20estimation%20of%20shannon%20entropy%2C%202015"
        },
        {
            "id": "29",
            "entry": "[29] N. Tishby and N. Zaslavsky, \u201cDeep learning and the information bottleneck principle,\u201d in Information Theory Workshop (ITW), 2015 IEEE, pp. 1\u20135, IEEE, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tishby%2C%20N.%20Zaslavsky%2C%20N.%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tishby%2C%20N.%20Zaslavsky%2C%20N.%20Deep%20learning%20and%20the%20information%20bottleneck%20principle%2C%202015"
        },
        {
            "id": "30",
            "entry": "[30] S. Liu and C. Trapnell, \u201cSingle-cell transcriptome sequencing: recent advances and remaining challenges,\u201d F1000Research, vol. 5, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20S.%20Trapnell%2C%20C.%20Single-cell%20transcriptome%20sequencing%3A%20recent%20advances%20and%20remaining%20challenges%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20S.%20Trapnell%2C%20C.%20Single-cell%20transcriptome%20sequencing%3A%20recent%20advances%20and%20remaining%20challenges%2C%202016"
        },
        {
            "id": "31",
            "entry": "[31] J. Massey, \u201cCausality, feedback and directed information,\u201d in Proc. Int. Symp. Inf. Theory Applic.(ISITA-90), pp. 303\u2013305, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Massey%2C%20J.%20%E2%80%9CCausality%2C%20feedback%20and%20directed%20information%2C%E2%80%9D%20in%20Proc%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Massey%2C%20J.%20%E2%80%9CCausality%2C%20feedback%20and%20directed%20information%2C%E2%80%9D%20in%20Proc%201990"
        },
        {
            "id": "32",
            "entry": "[32] C. E. Shannon, \u201cPrediction and entropy of printed english,\u201d Bell Labs Technical Journal, vol. 30, no. 1, pp. 50\u201364, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shannon%2C%20C.E.%20Prediction%20and%20entropy%20of%20printed%20english%2C%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shannon%2C%20C.E.%20Prediction%20and%20entropy%20of%20printed%20english%2C%201951"
        },
        {
            "id": "33",
            "entry": "[33] L. Paninski, \u201cEstimation of entropy and mutual information,\u201d Neural computation, vol. 15, no. 6, pp. 1191\u20131253, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paninski%2C%20L.%20Estimation%20of%20entropy%20and%20mutual%20information%2C%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paninski%2C%20L.%20Estimation%20of%20entropy%20and%20mutual%20information%2C%202003"
        },
        {
            "id": "34",
            "entry": "[34] J. Jiao, K. Venkat, Y. Han, and T. Weissman, \u201cMinimax estimation of functionals of discrete distributions,\u201d IEEE Transactions on Information Theory, vol. 61, no. 5, pp. 2835\u20132885, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jiao%2C%20J.%20Venkat%2C%20K.%20Han%2C%20Y.%20Weissman%2C%20T.%20Minimax%20estimation%20of%20functionals%20of%20discrete%20distributions%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jiao%2C%20J.%20Venkat%2C%20K.%20Han%2C%20Y.%20Weissman%2C%20T.%20Minimax%20estimation%20of%20functionals%20of%20discrete%20distributions%2C%202015"
        },
        {
            "id": "35",
            "entry": "[35] Y. Wu and P. Yang, \u201cMinimax rates of entropy estimation on large alphabets via best polynomial approximation,\u201d IEEE Transactions on Information Theory, vol. 62, no. 6, pp. 3702\u20133720, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Y.%20Yang%2C%20P.%20Minimax%20rates%20of%20entropy%20estimation%20on%20large%20alphabets%20via%20best%20polynomial%20approximation%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Y.%20Yang%2C%20P.%20Minimax%20rates%20of%20entropy%20estimation%20on%20large%20alphabets%20via%20best%20polynomial%20approximation%2C%202016"
        },
        {
            "id": "36",
            "entry": "[36] I. Nemenman, F. Shafee, and W. Bialek, \u201cEntropy and inference, revisited,\u201d in Advances in neural information processing systems, pp. 471\u2013478, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemenman%2C%20I.%20Shafee%2C%20F.%20Bialek%2C%20W.%20%E2%80%9CEntropy%20and%20inference%2C%20revisited%2C%E2%80%9D%20in%20Advances%20in%20neural%20information%20processing%20systems%202002"
        },
        {
            "id": "37",
            "entry": "[37] M. Lesniewicz, \u201cExpected entropy as a measure and criterion of randomness of binary sequences,\u201d Przeglad Elektrotechniczny, vol. 90, no. 1, pp. 42\u201346, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lesniewicz%2C%20M.%20Expected%20entropy%20as%20a%20measure%20and%20criterion%20of%20randomness%20of%20binary%20sequences%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lesniewicz%2C%20M.%20Expected%20entropy%20as%20a%20measure%20and%20criterion%20of%20randomness%20of%20binary%20sequences%2C%202014"
        },
        {
            "id": "38",
            "entry": "[38] K. Sricharan, R. Raich, and A. O. Hero, \u201cEstimation of nonlinear functionals of densities with confidence,\u201d IEEE Transactions on Information Theory, vol. 58, no. 7, pp. 4135\u20134159, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sricharan%2C%20K.%20Raich%2C%20R.%20Hero%2C%20A.O.%20Estimation%20of%20nonlinear%20functionals%20of%20densities%20with%20confidence%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sricharan%2C%20K.%20Raich%2C%20R.%20Hero%2C%20A.O.%20Estimation%20of%20nonlinear%20functionals%20of%20densities%20with%20confidence%2C%202012"
        },
        {
            "id": "39",
            "entry": "[39] S. Singh and B. P\u00f3czos, \u201cExponential concentration of a density functional estimator,\u201d in Advances in Neural Information Processing Systems, pp. 3032\u20133040, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20S.%20P%C3%B3czos%2C%20B.%20Exponential%20concentration%20of%20a%20density%20functional%20estimator%2C%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20S.%20P%C3%B3czos%2C%20B.%20Exponential%20concentration%20of%20a%20density%20functional%20estimator%2C%202014"
        },
        {
            "id": "40",
            "entry": "[40] K. Kandasamy, A. Krishnamurthy, B. Poczos, L. Wasserman, et al., \u201cNonparametric von mises estimators for entropies, divergences and mutual informations,\u201d in Advances in Neural Information Processing Systems, pp. 397\u2013405, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kandasamy%2C%20K.%20Krishnamurthy%2C%20A.%20Poczos%2C%20B.%20Wasserman%2C%20L.%20Nonparametric%20von%20mises%20estimators%20for%20entropies%2C%20divergences%20and%20mutual%20informations%2C%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kandasamy%2C%20K.%20Krishnamurthy%2C%20A.%20Poczos%2C%20B.%20Wasserman%2C%20L.%20Nonparametric%20von%20mises%20estimators%20for%20entropies%2C%20divergences%20and%20mutual%20informations%2C%202015"
        },
        {
            "id": "41",
            "entry": "[41] W. Gao, S. Oh, and P. Viswanath, \u201cBreaking the bandwidth barrier: Geometrical adaptive entropy estimation,\u201d in Advances in Neural Information Processing Systems, pp. 2460\u20132468, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20W.%20Oh%2C%20S.%20Viswanath%2C%20P.%20Breaking%20the%20bandwidth%20barrier%3A%20Geometrical%20adaptive%20entropy%20estimation%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20W.%20Oh%2C%20S.%20Viswanath%2C%20P.%20Breaking%20the%20bandwidth%20barrier%3A%20Geometrical%20adaptive%20entropy%20estimation%2C%202016"
        },
        {
            "id": "42",
            "entry": "[42] J. Jiao, W. Gao, and Y. Han, \u201cThe nearest neighbor information estimator is adaptively near minimax rate-optimal,\u201d arXiv preprint arXiv:1711.08824, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.08824"
        },
        {
            "id": "43",
            "entry": "[43] D. P\u00e1l, B. P\u00f3czos, and C. Szepesv\u00e1ri, \u201cEstimation of r\u00e9nyi entropy and mutual information based on generalized nearest-neighbor graphs,\u201d in Advances in Neural Information Processing Systems, pp. 1849\u20131857, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=P%C3%A1l%2C%20D.%20P%C3%B3czos%2C%20B.%20Szepesv%C3%A1ri%2C%20C.%20Estimation%20of%20r%C3%A9nyi%20entropy%20and%20mutual%20information%20based%20on%20generalized%20nearest-neighbor%20graphs%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=P%C3%A1l%2C%20D.%20P%C3%B3czos%2C%20B.%20Szepesv%C3%A1ri%2C%20C.%20Estimation%20of%20r%C3%A9nyi%20entropy%20and%20mutual%20information%20based%20on%20generalized%20nearest-neighbor%20graphs%2C%202010"
        },
        {
            "id": "44",
            "entry": "[44] H. Singh, N. Misra, V. Hnizdo, A. Fedorowicz, and E. Demchuk, \u201cNearest neighbor estimates of entropy,\u201d American journal of mathematical and management sciences, vol. 23, no. 3-4, pp. 301\u2013321, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20H.%20Misra%2C%20N.%20Hnizdo%2C%20V.%20Fedorowicz%2C%20A.%20Nearest%20neighbor%20estimates%20of%20entropy%2C%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20H.%20Misra%2C%20N.%20Hnizdo%2C%20V.%20Fedorowicz%2C%20A.%20Nearest%20neighbor%20estimates%20of%20entropy%2C%202003"
        },
        {
            "id": "45",
            "entry": "[45] S. Singh and B. P\u00f3czos, \u201cFinite-sample analysis of fixed-k nearest neighbor density functional estimators,\u201d in Advances in Neural Information Processing Systems, pp. 1217\u20131225, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20S.%20P%C3%B3czos%2C%20B.%20Finite-sample%20analysis%20of%20fixed-k%20nearest%20neighbor%20density%20functional%20estimators%2C%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20S.%20P%C3%B3czos%2C%20B.%20Finite-sample%20analysis%20of%20fixed-k%20nearest%20neighbor%20density%20functional%20estimators%2C%202016"
        },
        {
            "id": "46",
            "entry": "[46] A. Kraskov, H. St\u00f6gbauer, and P. Grassberger, \u201cEstimating mutual information,\u201d Physical review E, vol. 69, no. 6, p. 066138, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kraskov%2C%20A.%20St%C3%B6gbauer%2C%20H.%20Grassberger%2C%20P.%20Estimating%20mutual%20information%2C%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kraskov%2C%20A.%20St%C3%B6gbauer%2C%20H.%20Grassberger%2C%20P.%20Estimating%20mutual%20information%2C%202004"
        },
        {
            "id": "47",
            "entry": "[47] W. Gao, S. Oh, and P. Viswanath, \u201cDemystifying fixed k-nearest neighbor information estimators,\u201d IEEE Transactions on Information Theory, pp. 1\u20131, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20W.%20Oh%2C%20S.%20Viswanath%2C%20P.%20Demystifying%20fixed%20k-nearest%20neighbor%20information%20estimators%2C%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20W.%20Oh%2C%20S.%20Viswanath%2C%20P.%20Demystifying%20fixed%20k-nearest%20neighbor%20information%20estimators%2C%202018"
        },
        {
            "id": "48",
            "entry": "[48] J. Runge, \u201cConditional independence testing based on a nearest-neighbor estimator of conditional mutual information,\u201d arXiv preprint arXiv:1709.01447, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.01447"
        },
        {
            "id": "49",
            "entry": "[49] S. Frenzel and B. Pompe, \u201cPartial mutual information for coupling analysis of multivariate time series,\u201d Physical review letters, vol. 99, no. 20, p. 204101, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frenzel%2C%20S.%20Pompe%2C%20B.%20Partial%20mutual%20information%20for%20coupling%20analysis%20of%20multivariate%20time%20series%2C%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frenzel%2C%20S.%20Pompe%2C%20B.%20Partial%20mutual%20information%20for%20coupling%20analysis%20of%20multivariate%20time%20series%2C%202007"
        },
        {
            "id": "50",
            "entry": "[50] M. Vejmelka and M. Palu\u0161, \u201cInferring the directionality of coupling with conditional mutual information,\u201d Physical Review E, vol. 77, no. 2, p. 026214, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vejmelka%2C%20M.%20Palu%C5%A1%2C%20M.%20Inferring%20the%20directionality%20of%20coupling%20with%20conditional%20mutual%20information%2C%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vejmelka%2C%20M.%20Palu%C5%A1%2C%20M.%20Inferring%20the%20directionality%20of%20coupling%20with%20conditional%20mutual%20information%2C%202008"
        },
        {
            "id": "51",
            "entry": "[51] W. Gao, S. Kannan, S. Oh, and P. Viswanath, \u201cEstimating mutual information for discretecontinuous mixtures,\u201d in Advances in Neural Information Processing Systems, pp. 5988\u20135999, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gao%2C%20W.%20Kannan%2C%20S.%20Oh%2C%20S.%20Viswanath%2C%20P.%20Estimating%20mutual%20information%20for%20discretecontinuous%20mixtures%2C%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gao%2C%20W.%20Kannan%2C%20S.%20Oh%2C%20S.%20Viswanath%2C%20P.%20Estimating%20mutual%20information%20for%20discretecontinuous%20mixtures%2C%202017"
        },
        {
            "id": "52",
            "entry": "[52] X. Qiu, S. Ding, and T. Shi, \u201cFrom understanding the development landscape of the canonical fate-switch pair to constructing a dynamic landscape for two-step neural differentiation,\u201d PloS one, vol. 7, no. 12, p. e49271, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qiu%2C%20X.%20Ding%2C%20S.%20Shi%2C%20T.%20From%20understanding%20the%20development%20landscape%20of%20the%20canonical%20fate-switch%20pair%20to%20constructing%20a%20dynamic%20landscape%20for%20two-step%20neural%20differentiation%2C%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qiu%2C%20X.%20Ding%2C%20S.%20Shi%2C%20T.%20From%20understanding%20the%20development%20landscape%20of%20the%20canonical%20fate-switch%20pair%20to%20constructing%20a%20dynamic%20landscape%20for%20two-step%20neural%20differentiation%2C%202012"
        },
        {
            "id": "53",
            "entry": "[53] J. R. Vergara and P. A. Est\u00e9vez, \u201cCmim-2: an enhanced conditional mutual information maximization criterion for feature selection,\u201d Journal of Applied Computer Science Methods, vol. 2, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vergara%2C%20J.R.%20Est%C3%A9vez%2C%20P.A.%20Cmim-%202%3A%20an%20enhanced%20conditional%20mutual%20information%20maximization%20criterion%20for%20feature%20selection%2C%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vergara%2C%20J.R.%20Est%C3%A9vez%2C%20P.A.%20Cmim-%202%3A%20an%20enhanced%20conditional%20mutual%20information%20maximization%20criterion%20for%20feature%20selection%2C%202010"
        },
        {
            "id": "54",
            "entry": "[54] M. Noshad and A. O. Hero III, \u201cScalable hash-based estimation of divergence measures,\u201d arXiv preprint arXiv:1801.00398, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.00398"
        },
        {
            "id": "55",
            "entry": "[55] Y. Wu, \u201cLecture notes in information theory,\u201d www.stat.yale.edu/ yw562/teaching/itlectures.pdf.",
            "url": "http://www.stat.yale.edu/yw562/teaching/itlectures.pdf"
        },
        {
            "id": "56",
            "entry": "[56] J. M. Bernardo, \u201cAlgorithm as 103: Psi (digamma) function,\u201d Journal of the Royal Statistical Society. Series C (Applied Statistics), vol. 25, no. 3, pp. 315\u2013317, 1976.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bernardo%2C%20J.M.%20Algorithm%20as%20103%3A%20Psi%20%28digamma%29%20function%2C%201976",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bernardo%2C%20J.M.%20Algorithm%20as%20103%3A%20Psi%20%28digamma%29%20function%2C%201976"
        },
        {
            "id": "57",
            "entry": "[57] L. Evans, Measure theory and fine properties of functions. Routledge, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Evans%2C%20L.%20Measure%20theory%20and%20fine%20properties%20of%20functions%202018"
        }
    ]
}
