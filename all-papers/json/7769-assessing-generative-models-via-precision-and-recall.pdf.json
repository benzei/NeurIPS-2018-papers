{
    "filename": "7769-assessing-generative-models-via-precision-and-recall.pdf",
    "metadata": {
        "title": "Assessing Generative Models via Precision and Recall",
        "author": "Mehdi S. M. Sajjadi\u2217 MPI for Intelligent Systems,",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7769-assessing-generative-models-via-precision-and-recall.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Fr\u00e9chet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution."
    },
    "keywords": [
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "generative model",
            "url": "https://en.wikipedia.org/wiki/generative_model"
        },
        {
            "term": "density estimation",
            "url": "https://en.wikipedia.org/wiki/density_estimation"
        }
    ],
    "highlights": [
        "The task of evaluating generative models is an active research area",
        "Our contributions: (1) We introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, Inception Score and Fr\u00e9chet Inception Distance, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets, and (5) we compare several types of generative models based on the proposed approach \u2013 to our knowledge, this is the first metric that experimentally confirms the folklore that Generative Adversarial Networks often produce \"sharper\" images, but can suffer from mode collapse, while Variational Autoencoders produce \"blurry\" images, but cover more modes of the distribution.\n2 Background and Related Work",
        "Evaluating generative models is a challenging task of paramount importance",
        "In this work we show that one-dimensional scores are not sufficient to capture different failure cases of current state-of-the-art generative models",
        "We connect these notions to total variation distance as well as Fr\u00e9chet Inception Distance and Inception Score and we develop an efficient algorithm that can be readily applied to evaluate deep generative models based on samples",
        "We find empirical evidence supporting the folklore that Variational Autoencoders produce samples of lower quality, while being less prone to mode collapse than Generative Adversarial Networks"
    ],
    "key_statements": [
        "The task of evaluating generative models is an active research area",
        "All of the metrics commonly applied to evaluating generative models share a crucial weakness: Since they yield a one-dimensional score, they are unable to distinguish between different failure cases",
        "In this work we argue that a single-value summary is not adequate to compare generative models",
        "We present a novel approach which disentangles the divergence between distributions into two components: precision and recall",
        "We propose an elegant algorithm which can compute these quantities based on samples from P and Q",
        "Our contributions: (1) We introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, Inception Score and Fr\u00e9chet Inception Distance, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets, and (5) we compare several types of generative models based on the proposed approach \u2013 to our knowledge, this is the first metric that experimentally confirms the folklore that Generative Adversarial Networks often produce \"sharper\" images, but can suffer from mode collapse, while Variational Autoencoders produce \"blurry\" images, but cover more modes of the distribution.\n2 Background and Related Work",
        "While the likelihood can be approximated in some settings, kernel density estimation in high-dimensional spaces is extremely challenging [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>]",
        "Other failure modes related to density estimation in highdimensional spaces have been elaborated in [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>]",
        "If PS = QS, we are faced with a conundrum: Should the differences in PS and QS be attributed to losses in precision or recall? Is QS inadequately \u201ccovering\u201d PS or is it generating \u201cunnecessary\u201d noise? Inspired by PR curves for binary classification, we propose to resolve this predicament by providing a trade-off between precision and recall instead of a two-number summary for any two distributions P and Q",
        "We introduce an equivalent definition of PRD(Q, P ) in Theorem 2 that does not depend on the distributions \u03bc, \u03bdP and \u03bdQ and that leads to an elegant algorithm to compute practical PRD curves",
        "We show that the algorithm introduced in Section 3.3 can be readily applied to evaluate precision and recall of deep generative models",
        "We stress two key differences between the approaches: Firstly, to compute the quantities in Inception Score one needs a classifier and a labeled data set in contrast to the proposed PRD metric which can be applied on unlabeled data",
        "We evaluate the precision and recall of 7 Generative Adversarial Networks types and the Variational Autoencoders with 100 hyperparameter settings each as provided by [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>]",
        "Evaluating generative models is a challenging task of paramount importance",
        "In this work we show that one-dimensional scores are not sufficient to capture different failure cases of current state-of-the-art generative models",
        "We propose a novel notion of precision and recall for distributions and prove that both notions are theoretically sound and have desirable properties",
        "We connect these notions to total variation distance as well as Fr\u00e9chet Inception Distance and Inception Score and we develop an efficient algorithm that can be readily applied to evaluate deep generative models based on samples",
        "We investigate the properties of the proposed algorithm on real-world data sets, including image and text generation, and show that it captures the precision and recall of generative models",
        "We find empirical evidence supporting the folklore that Variational Autoencoders produce samples of lower quality, while being less prone to mode collapse than Generative Adversarial Networks"
    ],
    "summary": [
        "The task of evaluating generative models is an active research area.",
        "Our contributions: (1) We introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, IS and FID, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets, and (5) we compare several types of generative models based on the proposed approach \u2013 to our knowledge, this is the first metric that experimentally confirms the folklore that GANs often produce \"sharper\" images, but can suffer from mode collapse, while VAEs produce \"blurry\" images, but cover more modes of the distribution.",
        "We show that the algorithm introduced in Section 3.3 can be readily applied to evaluate precision and recall of deep generative models.",
        "Due to the symmetry of commonly used metrics with respect to precision and recall, the only way to assess whether the model is producing low-quality images or dropping modes is by visual inspection.",
        "We consider 5 classes from this data set and fix Pto contain samples from all classes to measure the loss in recall for different Qi. Figure 5 curves successfully demonstrate the sensitivity of recall to mode dropping.",
        "We can naturally interpret this difference via the PRD curves: For a desired recall level of less than \u223c0.6, the model on the left enjoys higher precision \u2013 it generates several digits of high quality.",
        "To further analyze the relationship between the proposed approach and PRD curves, we plot p(y|x) against precision and p(y) against recall in Figure 10 in the appendix.",
        "We stress two key differences between the approaches: Firstly, to compute the quantities in IS one needs a classifier and a labeled data set in contrast to the proposed PRD metric which can be applied on unlabeled data.",
        "We show samples from four models on the extreme ends of the plot for all combinations of high and low precision and recall.",
        "We connect these notions to total variation distance as well as FID and IS and we develop an efficient algorithm that can be readily applied to evaluate deep generative models based on samples.",
        "We investigate the properties of the proposed algorithm on real-world data sets, including image and text generation, and show that it captures the precision and recall of generative models.",
        "We find empirical evidence supporting the folklore that VAEs produce samples of lower quality, while being less prone to mode collapse than GANs"
    ],
    "headline": "We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Olivier Bachem, Mario Lucic, Hamed Hassani, and Andreas Krause. Fast and provably good seedings for k-means. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bachem%2C%20Olivier%20Lucic%2C%20Mario%20Hassani%2C%20Hamed%20Krause%2C%20Andreas%20Fast%20and%20provably%20good%20seedings%20for%20k-means%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bachem%2C%20Olivier%20Lucic%2C%20Mario%20Hassani%2C%20Hamed%20Krause%2C%20Andreas%20Fast%20and%20provably%20good%20seedings%20for%20k-means%202016"
        },
        {
            "id": "2",
            "entry": "[2] Olivier Bachem, Mario Lucic, S Hamed Hassani, and Andreas Krause. Approximate k-means++ in sublinear time. In AAAI, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bachem%2C%20Olivier%20Mario%20Lucic%2C%20S.Hamed%20Hassani%20Krause%2C%20Andreas%20Approximate%20k-means%2B%2B%20in%20sublinear%20time%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bachem%2C%20Olivier%20Mario%20Lucic%2C%20S.Hamed%20Hassani%20Krause%2C%20Andreas%20Approximate%20k-means%2B%2B%20in%20sublinear%20time%202016"
        },
        {
            "id": "3",
            "entry": "[3] Shane Barratt and Rishi Sharma. A Note on the Inception Score. arXiv preprint arXiv:1801.01973, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01973"
        },
        {
            "id": "4",
            "entry": "[4] Miko\u0142aj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miko%C5%82aj%20Binkowski%20Dougal%20J%20Sutherland%20Michael%20Arbel%20and%20Arthur%20Gretton%20Demystifying%20MMD%20GANs%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miko%C5%82aj%20Binkowski%20Dougal%20J%20Sutherland%20Michael%20Arbel%20and%20Arthur%20Gretton%20Demystifying%20MMD%20GANs%20In%20International%20Conference%20on%20Learning%20Representations%20ICLR%202018"
        },
        {
            "id": "5",
            "entry": "[5] Ali Borji. Pros and Cons of GAN Evaluation Measures. arXiv preprint arXiv:1802.03446, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03446"
        },
        {
            "id": "6",
            "entry": "[6] Ondrej C\u00edfka, Aliaksei Severyn, Enrique Alfonseca, and Katja Filippova. Eval all, trust a few, do wrong to none: Comparing sentence generation models. arXiv preprint arXiv:1804.07972, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07972"
        },
        {
            "id": "7",
            "entry": "[7] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. arXiv preprint arXiv:1705.02364, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.02364"
        },
        {
            "id": "8",
            "entry": "[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in Neural Information Processing Systems (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20Adversarial%20Networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20Courville%20and%20Yoshua%20Bengio%20Generative%20Adversarial%20Networks%20In%20Advances%20in%20Neural%20Information%20Processing%20Systems%20NIPS%202014"
        },
        {
            "id": "9",
            "entry": "[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium. In Advances in Neural Information Processing Systems (NIPS), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20Martin%20Ramsauer%2C%20Hubert%20Unterthiner%2C%20Thomas%20Nessler%2C%20Bernhard%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20Nash%20equilibrium%202017"
        },
        {
            "id": "10",
            "entry": "[10] Ferenc Husz\u00e1r. How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary? arXiv preprint arXiv:1511.05101, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05101"
        },
        {
            "id": "11",
            "entry": "[11] Daniel Jiwoong Im, He Ma, Graham Taylor, and Kristin Branson. Quantitatively evaluating GANs with divergences proposed for training. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Im%2C%20Daniel%20Jiwoong%20Ma%2C%20He%20Taylor%2C%20Graham%20Branson%2C%20Kristin%20Quantitatively%20evaluating%20GANs%20with%20divergences%20proposed%20for%20training%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Im%2C%20Daniel%20Jiwoong%20Ma%2C%20He%20Taylor%2C%20Graham%20Branson%2C%20Kristin%20Quantitatively%20evaluating%20GANs%20with%20divergences%20proposed%20for%20training%202018"
        },
        {
            "id": "12",
            "entry": "[12] Diederik P Kingma and Max Welling. Auto-encoding Variational Bayes. In International Conference on Learning Representations (ICLR), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20Variational%20Bayes%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Welling%2C%20Max%20Auto-encoding%20Variational%20Bayes%202014"
        },
        {
            "id": "13",
            "entry": "[13] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Hinton%2C%20Geoffrey%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009"
        },
        {
            "id": "14",
            "entry": "[14] Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The GAN Landscape: Losses, architectures, regularization, and normalization. arXiv preprint arXiv:1807.04720, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04720"
        },
        {
            "id": "15",
            "entry": "[15] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In IEEE, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20L%C3%A9on%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "16",
            "entry": "[16] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015"
        },
        {
            "id": "17",
            "entry": "[17] David Lopez-Paz and Maxime Oquab. Revisiting Classifier Two-Sample Tests. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lopez-Paz%2C%20David%20Oquab%2C%20Maxime%20Revisiting%20Classifier%20Two-Sample%20Tests%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lopez-Paz%2C%20David%20Oquab%2C%20Maxime%20Revisiting%20Classifier%20Two-Sample%20Tests%202016"
        },
        {
            "id": "18",
            "entry": "[18] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs Created Equal? A Large-Scale Study. In Advances in Neural Information Processing Systems (NIPS), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lucic%2C%20Mario%20Kurach%2C%20Karol%20Michalski%2C%20Marcin%20Gelly%2C%20Sylvain%20Are%20GANs%20Created%20Equal%3F%20A%20Large-Scale%20Study%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucic%2C%20Mario%20Kurach%2C%20Karol%20Michalski%2C%20Marcin%20Gelly%2C%20Sylvain%20Are%20GANs%20Created%20Equal%3F%20A%20Large-Scale%20Study%202018"
        },
        {
            "id": "19",
            "entry": "[19] Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. Distill, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Odena%2C%20Augustus%20Dumoulin%2C%20Vincent%20Olah%2C%20Chris%20Deconvolution%20and%20checkerboard%20artifacts%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Odena%2C%20Augustus%20Dumoulin%2C%20Vincent%20Olah%2C%20Chris%20Deconvolution%20and%20checkerboard%20artifacts%202016"
        },
        {
            "id": "20",
            "entry": "[20] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved Techniques for Training GANs. In Advances in Neural Information Processing Systems (NIPS), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20Techniques%20for%20Training%20GANs%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Goodfellow%2C%20Ian%20Zaremba%2C%20Wojciech%20Cheung%2C%20Vicki%20Improved%20Techniques%20for%20Training%20GANs%202016"
        },
        {
            "id": "21",
            "entry": "[21] David Sculley. Web-scale k-means clustering. In International Conference on World Wide Web (WWW), 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sculley%2C%20David%20Web-scale%20k-means%20clustering%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sculley%2C%20David%20Web-scale%20k-means%20clustering%202010"
        },
        {
            "id": "22",
            "entry": "[22] Lucas Theis, A\u00e4ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20Lucas%20van%20den%20Oord%2C%20A%C3%A4ron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20Lucas%20van%20den%20Oord%2C%20A%C3%A4ron%20Bethge%2C%20Matthias%20A%20note%20on%20the%20evaluation%20of%20generative%20models%202016"
        },
        {
            "id": "23",
            "entry": "[23] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05426"
        },
        {
            "id": "24",
            "entry": "[24] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of decoder-based generative models. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wu%2C%20Yuhuai%20Burda%2C%20Yuri%20Salakhutdinov%2C%20Ruslan%20Grosse%2C%20Roger%20On%20the%20quantitative%20analysis%20of%20decoder-based%20generative%20models%202017"
        },
        {
            "id": "25",
            "entry": "[25] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv preprint arXiv:1708.07747, 2017. ",
            "arxiv_url": "https://arxiv.org/pdf/1708.07747"
        }
    ]
}
