{
    "filename": "7289-hogwild-gibbs-can-be-panaccurate.pdf",
    "metadata": {
        "title": "HOGWILD!-Gibbs can be PanAccurate",
        "author": "Constantinos Daskalakis, Nishanth Dikkala, Siddhartha Jayanti",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7289-hogwild-gibbs-can-be-panaccurate.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an accurate method for estimating probabilities of events on a small number of variables of a graphical model satisfying Dobrushin\u2019s condition [DSOR16]. We investigate whether it can be used to accurately estimate expectations of functions of all the variables of the model. Under the same condition, we show that the synchronous (sequential) and asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between their (multivariate) samples remains bounded by O(\u03c4 log n), where n is the number of variables in the graphical model, and \u03c4 is a measure of the asynchronicity. A similar bound holds for any constant power of the Hamming distance. Hence, the expectation of any function that is Lipschitz with respect to a power of the Hamming distance, can be estimated with a bias that grows logarithmically in n. Going beyond Lipschitz functions, we consider the bias arising from asynchronicity in estimating the expectation of polynomial functions of all variables in the model. Using recent concentration of measure results [DDK17, GLP17, GSS18], we show that the bias introduced by the asynchronicity is of smaller order than the standard deviation of the function value already present in the true model. We perform experiments on a multiprocessor machine to empirically illustrate our theoretical findings."
    },
    "keywords": [
        {
            "term": "Department of Defense",
            "url": "https://en.wikipedia.org/wiki/Department_of_Defense"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "coordinate descent",
            "url": "https://en.wikipedia.org/wiki/coordinate_descent"
        },
        {
            "term": "hamming distance",
            "url": "https://en.wikipedia.org/wiki/hamming_distance"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "gibbs sampling",
            "url": "https://en.wikipedia.org/wiki/gibbs_sampling"
        },
        {
            "term": "ising model",
            "url": "https://en.wikipedia.org/wiki/ising_model"
        }
    ],
    "highlights": [
        "The increasingly ambitious applications of data analysis, and the corresponding growth in the size of the data that needs to processed has brought important scalability challenges to machine learning algorithms",
        "Fundamental methods such as Gradient Descent and Gibbs sampling, which were designed with a sequential computational model in mind, are to be applied on datasets of increasingly larger size",
        "Our goal in this paper is to push the theoretical understanding of HOGWILD!-Gibbs to estimate functions of all the variables in a graphical model",
        "Starting at the same initial configuration, the executions of the sequential and the asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between the multivariate samples that the two samplers maintain is bounded by O(\u03c4 log n), where n is the number of variables in the graphical model, and \u03c4 is a measure of the average contention in the asynchronicity model of Section 2.1",
        "It follows from Lemmas 2 and 3 that, if a function f of the variables of a graphical model is K-Lipschitz with respect to the d-th power of the Hamming distance, the bias in the expectation of f introduced by HOGWILD!-Gibbs under the asynchronicity model of Section 2.1 is bounded by K \u00b7 C(d, \u03c4 ) logd n",
        "For simplicity we show these improvements for the Ising model, but our results are extendible to general graphical models"
    ],
    "key_statements": [
        "The increasingly ambitious applications of data analysis, and the corresponding growth in the size of the data that needs to processed has brought important scalability challenges to machine learning algorithms",
        "Fundamental methods such as Gradient Descent and Gibbs sampling, which were designed with a sequential computational model in mind, are to be applied on datasets of increasingly larger size",
        "Our goal in this paper is to push the theoretical understanding of HOGWILD!-Gibbs to estimate functions of all the variables in a graphical model",
        "Starting at the same initial configuration, the executions of the sequential and the asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between the multivariate samples that the two samplers maintain is bounded by O(\u03c4 log n), where n is the number of variables in the graphical model, and \u03c4 is a measure of the average contention in the asynchronicity model of Section 2.1",
        "It follows from Lemmas 2 and 3 that, if a function f of the variables of a graphical model is K-Lipschitz with respect to the d-th power of the Hamming distance, the bias in the expectation of f introduced by HOGWILD!-Gibbs under the asynchronicity model of Section 2.1 is bounded by K \u00b7 C(d, \u03c4 ) logd n",
        "For simplicity we show these improvements for the Ising model, but our results are extendible to general graphical models",
        "In Theorem 4, that the bias introduced by HOGWILD!-Gibbs in the expectation of a degree-d polynomial of the Ising model is bounded by O((n log n)(d\u22121)/2)",
        "The bias of O((n log n)(d\u22121)/2) that we show is introduced by the asynchronicity is of a lower order of magnitude than the standard deviation of degree-d polynomials of the Ising model, which is O((n)d/2)\u2014see Theorem 2, and which is already experienced by the sequential sampler",
        "We show that the expected change is negative whenever the Hamming distance between the two chains was above",
        "We observe that our Hamming moment bounds from Section 3 imply that we can accurately estimate functions or events of the graphical model if they are Lipschitz",
        "Let \u03c0 denote the distribution associated with a graphical model over the set of variables V (|V | = n) taking values in a discrete space Sn",
        "Theorem 3 states the bound we show for the bias in computation of degree 2 polynomials of the Ising model",
        "The main intuition behind the proof is that we can improve upon the bound implied by the Lipschitz constant by appealing to strong concentration of measure results about functions of graphical models under Dobrushin\u2019s condition [DDK17, GLP17, GSS18]",
        "We extend the ideas in the above proof to bound the bias introduced by the HOGWILD! Gibbs algorithm when computing the expected values of a degree d polynomial of the Ising model in high temperature",
        "We presented results for accurate estimation of polynomial functions over the Ising model",
        "We focused on two Ising models\u2014Curie-Weiss and the Grid"
    ],
    "summary": [
        "The increasingly ambitious applications of data analysis, and the corresponding growth in the size of the data that needs to processed has brought important scalability challenges to machine learning algorithms.",
        "Whenever the graphical model satisfies Dobrushin\u2019s condition, they show that the mixing time of the asynchronous Gibbs sampler is similar to that of the sequential one.",
        "Starting at the same initial configuration, the executions of the sequential and the asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between the multivariate samples that the two samplers maintain is bounded by O(\u03c4 log n), where n is the number of variables in the graphical model, and \u03c4 is a measure of the average contention in the asynchronicity model of Section 2.1.",
        "It follows from Lemmas 2 and 3 that, if a function f of the variables of a graphical model is K-Lipschitz with respect to the d-th power of the Hamming distance, the bias in the expectation of f introduced by HOGWILD!-Gibbs under the asynchronicity model of Section 2.1 is bounded by K \u00b7 C(d, \u03c4 ) logd n.",
        "In Theorem 4, that the bias introduced by HOGWILD!-Gibbs in the expectation of a degree-d polynomial of the Ising model is bounded by O((n log n)(d\u22121)/2).",
        "We have the following result from [DSOR16] about mixing time of Gibbs sampler for a model satisfying Dobrushin\u2019s condition.",
        "We state a known result about concentration of measure for polynomial functions on Ising models satisfying Dobrushin\u2019s condition.",
        "We observe that our Hamming moment bounds from Section 3 imply that we can accurately estimate functions or events of the graphical model if they are Lipschitz.",
        "Let \u03c0 denote the distribution associated with a graphical model over the set of variables V (|V | = n) taking values in a discrete space Sn. Assume that the model satisfies Dobrushin\u2019s condition with Dobrushin parameter \u03b1 < 1.",
        "Our goal will be to accurately estimate the expected values of constant degree polynomials over the Ising model.",
        "Using the bounds from Lemmas 2 and 3, we proceed to bound the bias in computing polynomial functions of the Ising model using HOGWILD!",
        "Theorem 3 (Bias in Quadratic functions of Ising Model computed using HOGWILD!-Gibbs).",
        "The main intuition behind the proof is that we can improve upon the bound implied by the Lipschitz constant by appealing to strong concentration of measure results about functions of graphical models under Dobrushin\u2019s condition [DDK17, GLP17, GSS18].",
        "Gibbs algorithm when computing the expected values of a degree d polynomial of the Ising model in high temperature."
    ],
    "headline": "We investigate whether it can be used to accurately estimate expectations of functions of all the variables of the model",
    "reference_links": [
        {
            "id": "Daskalakis_et+al_2017_a",
            "entry": "[DDK17] Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Concentration of multilinear functions of the Ising model with applications to network data. In Advances in Neural Information Processing Systems 30, NIPS \u201917. Curran Associates, Inc., 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20Constantinos%20Dikkala%2C%20Nishanth%20Kamath%2C%20Gautam%20Concentration%20of%20multilinear%20functions%20of%20the%20Ising%20model%20with%20applications%20to%20network%20data%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20Constantinos%20Dikkala%2C%20Nishanth%20Kamath%2C%20Gautam%20Concentration%20of%20multilinear%20functions%20of%20the%20Ising%20model%20with%20applications%20to%20network%20data%202017"
        },
        {
            "id": "Daskalakis_et+al_2018_a",
            "entry": "[DDK18] Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing Ising models. In Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201918, Philadelphia, PA, USA, 2018. SIAM.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daskalakis%2C%20Constantinos%20Dikkala%2C%20Nishanth%20Kamath%2C%20Gautam%20Testing%20Ising%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daskalakis%2C%20Constantinos%20Dikkala%2C%20Nishanth%20Kamath%2C%20Gautam%20Testing%20Ising%20models%202018"
        },
        {
            "id": "Constantinos_2011_a",
            "entry": "[DMR11] Constantinos Daskalakis, Elchanan Mossel, and S\u00e9bastien Roch. Evolutionary trees and the Ising model on the Bethe lattice: A proof of Steel\u2019s conjecture. Probability Theory and Related Fields, 149(1):149\u2013189, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Constantinos%20Daskalakis%2C%20Elchanan%20Mossel%20Roch%2C%20S%C3%A9bastien%20Evolutionary%20trees%20and%20the%20Ising%20model%20on%20the%20Bethe%20lattice%3A%20A%20proof%20of%20Steel%E2%80%99s%20conjecture%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Constantinos%20Daskalakis%2C%20Elchanan%20Mossel%20Roch%2C%20S%C3%A9bastien%20Evolutionary%20trees%20and%20the%20Ising%20model%20on%20the%20Bethe%20lattice%3A%20A%20proof%20of%20Steel%E2%80%99s%20conjecture%202011"
        },
        {
            "id": "Sa_et+al_2016_a",
            "entry": "[DSOR16] Christopher De Sa, Kunle Olukotun, and Christopher R\u00e9. Ensuring rapid mixing and low bias for asynchronous gibbs sampling. In JMLR workshop and conference proceedings, volume 48, page 1567. NIH Public Access, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20Christopher%20De%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Ensuring%20rapid%20mixing%20and%20low%20bias%20for%20asynchronous%20gibbs%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20Christopher%20De%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Ensuring%20rapid%20mixing%20and%20low%20bias%20for%20asynchronous%20gibbs%20sampling%202016"
        },
        {
            "id": "Sa_et+al_2015_a",
            "entry": "[DSZOR15] Christopher M De Sa, Ce Zhang, Kunle Olukotun, and Christopher R\u00e9. Taming the wild: A unified analysis of hogwild-style algorithms. In Advances in neural information processing systems, pages 2674\u20132682, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sa%2C%20Christopher%20M.De%20Zhang%2C%20Ce%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20hogwild-style%20algorithms%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sa%2C%20Christopher%20M.De%20Zhang%2C%20Ce%20Olukotun%2C%20Kunle%20R%C3%A9%2C%20Christopher%20Taming%20the%20wild%3A%20A%20unified%20analysis%20of%20hogwild-style%20algorithms%202015"
        },
        {
            "id": "Ellison_1993_a",
            "entry": "[Ell93] Glenn Ellison. Learning, local interaction, and coordination. Econometrica, 61(5):1047\u20131071, 1993.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ellison%2C%20Glenn%20Learning%2C%20local%20interaction%2C%20and%20coordination%201993",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ellison%2C%20Glenn%20Learning%2C%20local%20interaction%2C%20and%20coordination%201993"
        },
        {
            "id": "Felsenstein_2004_a",
            "entry": "[Fel04] Joseph Felsenstein. Inferring Phylogenies. Sinauer Associates Sunderland, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Felsenstein%2C%20Joseph%20Inferring%20Phylogenies%202004"
        },
        {
            "id": "Geman_1986_a",
            "entry": "[GG86] Stuart Geman and Christine Graffigne. Markov random field image models and their applications to computer vision. In Proceedings of the International Congress of Mathematicians, pages 1496\u20131517. American Mathematical Society, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geman%2C%20Stuart%20Graffigne%2C%20Christine%20Markov%20random%20field%20image%20models%20and%20their%20applications%20to%20computer%20vision%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geman%2C%20Stuart%20Graffigne%2C%20Christine%20Markov%20random%20field%20image%20models%20and%20their%20applications%20to%20computer%20vision%201986"
        },
        {
            "id": "Gheissari_et+al_2017_a",
            "entry": "[GLP17] Reza Gheissari, Eyal Lubetzky, and Yuval Peres. Concentration inequalities for polynomials of contracting Ising models. arXiv preprint arXiv:1706.00121, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.00121"
        },
        {
            "id": "Goetze_et+al_2018_a",
            "entry": "[GSS18] Friedrich G\u00f6tze, Holger Sambale, and Arthur Sinulis. Higher order concentration for functions of weakly dependent random variables. arXiv preprint arXiv:1801.06348, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.06348"
        },
        {
            "id": "Johnson_et+al_2013_a",
            "entry": "[JSW13] Matthew Johnson, James Saunderson, and Alan Willsky. Analyzing hogwild parallel gaussian gibbs sampling. In Advances in Neural Information Processing Systems, pages 2715\u20132723, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Matthew%20Saunderson%2C%20James%20Willsky%2C%20Alan%20Analyzing%20hogwild%20parallel%20gaussian%20gibbs%20sampling%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Matthew%20Saunderson%2C%20James%20Willsky%2C%20Alan%20Analyzing%20hogwild%20parallel%20gaussian%20gibbs%20sampling%202013"
        },
        {
            "id": "Levin_et+al_2009_a",
            "entry": "[LPW09] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. American Mathematical Society, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LPW09%20David%20A%20Levin%20Yuval%20Peres%20and%20Elizabeth%20L%20Wilmer%20Markov%20Chains%20and%20Mixing%20Times%20American%20Mathematical%20Society%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LPW09%20David%20A%20Levin%20Yuval%20Peres%20and%20Elizabeth%20L%20Wilmer%20Markov%20Chains%20and%20Mixing%20Times%20American%20Mathematical%20Society%202009"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "[LWR+15] Ji Liu, Stephen J Wright, Christopher R\u00e9, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. The Journal of Machine Learning Research, 16(1):285\u2013322, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ji%20Wright%2C%20Stephen%20J.%20R%C3%A9%2C%20Christopher%20Bittorf%2C%20Victor%20An%20asynchronous%20parallel%20stochastic%20coordinate%20descent%20algorithm%202015"
        },
        {
            "id": "Mitliagkas_et+al_2015_a",
            "entry": "[MBDC15] Ioannis Mitliagkas, Michael Borokhovich, Alexandros G Dimakis, and Constantine Caramanis. Frogwild!: fast pagerank approximations on graph engines. Proceedings of the VLDB Endowment, 8(8):874\u2013885, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mitliagkas%2C%20Ioannis%20Borokhovich%2C%20Michael%20Dimakis%2C%20Alexandros%20G.%20Caramanis%2C%20Constantine%20Frogwild%21%3A%20fast%20pagerank%20approximations%20on%20graph%20engines%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mitliagkas%2C%20Ioannis%20Borokhovich%2C%20Michael%20Dimakis%2C%20Alexandros%20G.%20Caramanis%2C%20Constantine%20Frogwild%21%3A%20fast%20pagerank%20approximations%20on%20graph%20engines%202015"
        },
        {
            "id": "Mania_et+al_2015_a",
            "entry": "[MPP+15] Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. arXiv preprint arXiv:1507.06970, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.06970"
        },
        {
            "id": "Montanari_2010_a",
            "entry": "[MS10] Andrea Montanari and Amin Saberi. The spread of innovations in social networks. Proceedings of the National Academy of Sciences, 107(47):20196\u201320201, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montanari%2C%20Andrea%20Saberi%2C%20Amin%20The%20spread%20of%20innovations%20in%20social%20networks%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montanari%2C%20Andrea%20Saberi%2C%20Amin%20The%20spread%20of%20innovations%20in%20social%20networks%202010"
        },
        {
            "id": "Noel_2014_a",
            "entry": "[NO14] Cyprien Noel and Simon Osindero. Dogwild!-distributed hogwild for cpu & gpu. In NIPS Workshop on Distributed Machine Learning and Matrix Computations, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Noel%2C%20Cyprien%20Osindero%2C%20Simon%20Dogwild%21-distributed%20hogwild%20for%20cpu%20%26%20gpu%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Noel%2C%20Cyprien%20Osindero%2C%20Simon%20Dogwild%21-distributed%20hogwild%20for%20cpu%20%26%20gpu%202014"
        },
        {
            "id": "Niu_et+al_2011_a",
            "entry": "[NRRW11] Feng Niu, Benjamin Recht, Christopher Re, and Stephen Wright. Hogwild: A lockfree approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693\u2013701, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niu%2C%20Feng%20Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Hogwild%3A%20A%20lockfree%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niu%2C%20Feng%20Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Hogwild%3A%20A%20lockfree%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "Recht_et+al_2011_a",
            "entry": "[RRWN11] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lockfree approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693\u2013701, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lockfree%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Recht%2C%20Benjamin%20Re%2C%20Christopher%20Wright%2C%20Stephen%20Niu%2C%20Feng%20Hogwild%3A%20A%20lockfree%20approach%20to%20parallelizing%20stochastic%20gradient%20descent%202011"
        },
        {
            "id": "Smola_2010_a",
            "entry": "[SN10] Alexander Smola and Shravan Narayanamurthy. An architecture for parallel topic models. Proceedings of the VLDB Endowment, 3(1-2):703\u2013710, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20Alexander%20Narayanamurthy%2C%20Shravan%20An%20architecture%20for%20parallel%20topic%20models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smola%2C%20Alexander%20Narayanamurthy%2C%20Shravan%20An%20architecture%20for%20parallel%20topic%20models%202010"
        },
        {
            "id": "Terenin_et+al_2015_a",
            "entry": "[TSD15] Alexander Terenin, Daniel Simpson, and David Draper. Asynchronous gibbs sampling. arXiv preprint arXiv:1509.08999, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.08999"
        },
        {
            "id": "Yu_et+al_2012_a",
            "entry": "[YHSD12] Hsiang-Fu Yu, Cho-Jui Hsieh, Si Si, and Inderjit Dhillon. Scalable coordinate descent approaches to parallel matrix factorization for recommender systems. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 765\u2013774. IEEE, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20Hsiang-Fu%20Hsieh%2C%20Cho-Jui%20Si%2C%20Si%20Dhillon%2C%20Inderjit%20Scalable%20coordinate%20descent%20approaches%20to%20parallel%20matrix%20factorization%20for%20recommender%20systems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20Hsiang-Fu%20Hsieh%2C%20Cho-Jui%20Si%2C%20Si%20Dhillon%2C%20Inderjit%20Scalable%20coordinate%20descent%20approaches%20to%20parallel%20matrix%20factorization%20for%20recommender%20systems%202012"
        },
        {
            "id": "Zhang_2014_a",
            "entry": "[ZR14] Ce Zhang and Christopher R\u00e9. Dimmwitted: A study of main-memory statistical analytics. Proceedings of the VLDB Endowment, 7(12):1283\u20131294, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Ce%20R%C3%A9%2C%20Christopher%20Dimmwitted%3A%20A%20study%20of%20main-memory%20statistical%20analytics%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Ce%20R%C3%A9%2C%20Christopher%20Dimmwitted%3A%20A%20study%20of%20main-memory%20statistical%20analytics%202014"
        }
    ]
}
