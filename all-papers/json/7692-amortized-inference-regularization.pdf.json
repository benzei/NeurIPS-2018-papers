{
    "filename": "7692-amortized-inference-regularization.pdf",
    "metadata": {
        "title": "Amortized Inference Regularization",
        "author": "Rui Shu, Hung H. Bui, Shengjia Zhao, Mykel J. Kochenderfer, Stefano Ermon",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7692-amortized-inference-regularization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs."
    },
    "keywords": [
        {
            "term": "evidence lower bound",
            "url": "https://en.wikipedia.org/wiki/evidence_lower_bound"
        },
        {
            "term": "latent variable",
            "url": "https://en.wikipedia.org/wiki/latent_variable"
        },
        {
            "term": "variational inference",
            "url": "https://en.wikipedia.org/wiki/variational_inference"
        },
        {
            "term": "variational principle",
            "url": "https://en.wikipedia.org/wiki/variational_principle"
        }
    ],
    "highlights": [
        "Consider a joint distribution p\u2713(x, z) parameterized by \u2713, where x 2 X is observed and z 2 Z is latent",
        "Through amortized inference regularization (AIR), we show that it is possible to reduce the inference gap and increase the log-likelihood performance on the test set",
        "We challenged the conventional role that amortized inference plays in training deep generative models",
        "We considered a special case of amortized inference regularization (AIR) where the inference model must learn a smoothed mapping from X ! Q and showed that the denoising variational autoencoder (DVAE) and weight-normalized inference (WNI) are effective instantiations of amortized inference regularization",
        "Promising directions for future work include replacing denoising with adversarial training [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] and weight normalization with spectral normalization [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We demonstrated that amortized inference regularization plays a crucial role in the regularization of IWAE, and that higher levels of regularization may be necessary due to the attenuating effects of importance sampling on amortized inference regularization"
    ],
    "key_statements": [
        "Consider a joint distribution p\u2713(x, z) parameterized by \u2713, where x 2 X is observed and z 2 Z is latent",
        "Many techniques have since been proposed to expand the expressivity of the amortized inference model in order to better approximate maximum likelihood training [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>]",
        "For datasets where the generative model is prone to overfitting, we show that having an amortized inference model provides a new and effective way to regularize maximum likelihood training",
        "Through amortized inference regularization (AIR), we show that it is possible to reduce the inference gap and increase the log-likelihood performance on the test set",
        "We propose several techniques for amortized inference regularization and provide extensive theoretical and empirical analyses of our proposed techniques when applied to the variational autoencoder and the\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada",
        "We found that [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] incorrectly stated the tightness of the denoising variational autoencoder variational lower bound",
        "We show that 1) the optimal denoising variational autoencoder amortized inference model is a kernel regression model and that 2) the variance of the noise \" controls the smoothness of the optimal inference model",
        "In addition to denoising variational autoencoder, we propose an alternative method that directly restricts F to the set of smooth functions",
        "To approximate the log-likelihood, we proposed to use importance-weighted stochastic variational inference (IW-SVI), an extension of SVI [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] which we describe in detail in Appendix C",
        "Our results demonstrate that amortized inference regularization is a highly effective regularizer even when a large number of importance samples are used",
        "The robustness of importance sampling when paired with amortized inference regularization makes amortized inference regularization an effective and practical way to regularize IWAE.\n4.3",
        "We challenged the conventional role that amortized inference plays in training deep generative models",
        "We considered a special case of amortized inference regularization (AIR) where the inference model must learn a smoothed mapping from X ! Q and showed that the denoising variational autoencoder (DVAE) and weight-normalized inference (WNI) are effective instantiations of amortized inference regularization",
        "Promising directions for future work include replacing denoising with adversarial training [<a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] and weight normalization with spectral normalization [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]",
        "We demonstrated that amortized inference regularization plays a crucial role in the regularization of IWAE, and that higher levels of regularization may be necessary due to the attenuating effects of importance sampling on amortized inference regularization"
    ],
    "summary": [
        "Consider a joint distribution p\u2713(x, z) parameterized by \u2713, where x 2 X is observed and z 2 Z is latent.",
        "Many techniques have since been proposed to expand the expressivity of the amortized inference model in order to better approximate maximum likelihood training [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>].",
        "For datasets where the generative model is prone to overfitting, we show that having an amortized inference model provides a new and effective way to regularize maximum likelihood training.",
        "Through amortized inference regularization (AIR), we show that it is possible to reduce the inference gap and increase the log-likelihood performance on the test set.",
        "Many methods have been proposed to expand the variational and amortization families in order to better approximate maximum likelihood training [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "We observe that the evidence lower bound in Eq (2) admits a natural interpretation as implicitly regularizing maximum likelihood training z log-lik}e|lihood { max Ep(x) [ln p\u2713(x)]",
        "We propose two models that encourage inference model smoothness and demonstrate that they can reduce the inference gap and increase log-likelihood on the test set.",
        "Our analysis demonstrates that the denoising objective smooths the inference model and necessarily lower bounds the original variational autoencoder objective.",
        "To approximate the log-likelihood, we proposed to use importance-weighted stochastic variational inference (IW-SVI), an extension of SVI [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] which we describe in detail in Appendix C.",
        "Regularizing the inference model consistently improved the test set log-likelihood performance.",
        "We observed differences in the performance of DVAE versus WNI-VAE on the Caltech 101 Silhouettes dataset, suggesting a difference in how denoising and weight normalization regularizes the inference model; an interesting consideration would be to combine DVAE and WNI.",
        "We see that the denoising IWAE (DIWAE) and weight-normalized inference IWAE (WNI-IWAE) consistently out-perform the standard IWAE on test set log-likelihood evaluations.",
        "Our main experimental contribution is the verification that increasing the number of importance samples results in less underfitting when the inference model is over-regularized.",
        "The robustness of importance sampling when paired with amortized inference regularization makes AIR an effective and practical way to regularize IWAE.",
        "In addition to expediting variational inference, amortized inference introduces new ways to regularize maximum likelihood training.",
        "We considered a special case of amortized inference regularization (AIR) where the inference model must learn a smoothed mapping from X !",
        "Q and showed that the denoising variational autoencoder (DVAE) and weight-normalized inference (WNI) are effective instantiations of AIR.",
        "We believe that variational family expansion by Monte Carlo methods [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] may exhibit the same attenuating effect on AIR and recommend this as an additional research direction"
    ],
    "headline": "By applying amortized inference regularization, it is possible to improve variational autoencoder generalization on both inference and generative performance",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1312.6114"
        },
        {
            "id": "2",
            "entry": "[2] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. SemiSupervised Learning With Deep Generative Models. In Advances In Neural Information Processing Systems, pages 3581\u20133589, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20SemiSupervised%20Learning%20With%20Deep%20Generative%20Models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20SemiSupervised%20Learning%20With%20Deep%20Generative%20Models%202014"
        },
        {
            "id": "3",
            "entry": "[3] Hyunjik Kim and Andriy Mnih. Disentangling By Factorising. arXiv preprint arXiv:1802.05983, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05983"
        },
        {
            "id": "4",
            "entry": "[4] Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating Sources Of Disentanglement In Variational Autoencoders. arXiv preprint arXiv:1802.04942, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04942"
        },
        {
            "id": "5",
            "entry": "[5] Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. SemiAmortized Variational Autoencoders. arXiv preprint arXiv:1802.02550, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.02550"
        },
        {
            "id": "6",
            "entry": "[6] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved Variational Inference With Inverse Autoregressive Flow. In Advances In Neural Information Processing Systems, pages 4743\u20134751, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20Variational%20Inference%20With%20Inverse%20Autoregressive%20Flow%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Salimans%2C%20Tim%20Jozefowicz%2C%20Rafal%20Chen%2C%20Xi%20Improved%20Variational%20Inference%20With%20Inverse%20Autoregressive%20Flow%202016"
        },
        {
            "id": "7",
            "entry": "[7] Casper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Ladder Variational Autoencoders. In Advances In Neural Information Processing Systems, pages 3738\u20133746, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Casper%20Kaae%20S%C3%B8nderby%20Tapani%20Raiko%20Lars%20Maal%C3%B8e%20S%C3%B8ren%20Kaae%20S%C3%B8nderby%20and%20Ole%20Winther%20Ladder%20Variational%20Autoencoders%20In%20Advances%20In%20Neural%20Information%20Processing%20Systems%20pages%2037383746%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Casper%20Kaae%20S%C3%B8nderby%20Tapani%20Raiko%20Lars%20Maal%C3%B8e%20S%C3%B8ren%20Kaae%20S%C3%B8nderby%20and%20Ole%20Winther%20Ladder%20Variational%20Autoencoders%20In%20Advances%20In%20Neural%20Information%20Processing%20Systems%20pages%2037383746%202016"
        },
        {
            "id": "8",
            "entry": "[8] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted Autoencoders. arXiv preprint arXiv:1509.00519, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00519"
        },
        {
            "id": "9",
            "entry": "[9] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic Backpropagation And Approximate Inference In Deep Generative Models. arXiv preprint arXiv:1401.4082, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "10",
            "entry": "[10] Chris Cremer, Xuechen Li, and David Duvenaud. Inference Suboptimality In Variational Autoencoders. arXiv preprint arXiv:1801.03558, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.03558"
        },
        {
            "id": "11",
            "entry": "[11] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On The Quantitative Analysis Of Decoder-Based Generative Models. arXiv preprint arXiv:1611.04273, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.04273"
        },
        {
            "id": "12",
            "entry": "[12] Rahul G Krishnan, Dawen Liang, and Matthew Hoffman. On the challenges of learning with inference networks on sparse, high-dimensional data. arXiv preprint arXiv:1710.06085, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1710.06085"
        },
        {
            "id": "13",
            "entry": "[13] Lars Maal\u00f8e, Casper Kaae S\u00f8nderby, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Auxiliary Deep Generative Models. arXiv preprint arXiv:1602.05473, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1602.05473"
        },
        {
            "id": "14",
            "entry": "[14] Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical Variational Models. In International Conference On Machine Learning, pages 324\u2013333, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ranganath%2C%20Rajesh%20Tran%2C%20Dustin%20Blei%2C%20David%20Hierarchical%20Variational%20Models%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ranganath%2C%20Rajesh%20Tran%2C%20Dustin%20Blei%2C%20David%20Hierarchical%20Variational%20Models%202016"
        },
        {
            "id": "15",
            "entry": "[15] Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, et al. Posterior Regularization For Structured Latent Variable Models. Journal of Machine Learning Research, 11(Jul):2001\u20132049, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ganchev%2C%20Kuzman%20Gillenwater%2C%20Jennifer%20Taskar%2C%20Ben%20Posterior%20Regularization%20For%20Structured%20Latent%20Variable%20Models%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ganchev%2C%20Kuzman%20Gillenwater%2C%20Jennifer%20Taskar%2C%20Ben%20Posterior%20Regularization%20For%20Structured%20Latent%20Variable%20Models%202010"
        },
        {
            "id": "16",
            "entry": "[16] Jun Zhu, Ning Chen, and Eric P Xing. Bayesian Inference With Posterior Regularization And Applications To Infinite Latent Svms. The Journal of Machine Learning Research, 15(1):1799\u2013 1847, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun%20Chen%2C%20Ning%20and%20Eric%20P%20Xing.%20Bayesian%20Inference%20With%20Posterior%20Regularization%20And%20Applications%20To%20Infinite%20Latent%20Svms",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun%20Chen%2C%20Ning%20and%20Eric%20P%20Xing.%20Bayesian%20Inference%20With%20Posterior%20Regularization%20And%20Applications%20To%20Infinite%20Latent%20Svms"
        },
        {
            "id": "17",
            "entry": "[17] Daniel Jiwoong Im, Sungjin Ahn, Roland Memisevic, Yoshua Bengio, et al. Denoising Criterion For Variational Auto-Encoding Framework. In AAAI, pages 2059\u20132065, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Im%2C%20Daniel%20Jiwoong%20Ahn%2C%20Sungjin%20Memisevic%2C%20Roland%20Bengio%2C%20Yoshua%20Denoising%20Criterion%20For%20Variational%20Auto-Encoding%20Framework%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Im%2C%20Daniel%20Jiwoong%20Ahn%2C%20Sungjin%20Memisevic%2C%20Roland%20Bengio%2C%20Yoshua%20Denoising%20Criterion%20For%20Variational%20Auto-Encoding%20Framework%202017"
        },
        {
            "id": "18",
            "entry": "[18] Tim Salimans and Diederik P Kingma. Weight Normalization: A Simple Reparameterization To Accelerate Training Of Deep Neural Networks. In Advances In Neural Information Processing Systems, pages 901\u2013909, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20Normalization%3A%20A%20Simple%20Reparameterization%20To%20Accelerate%20Training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20Tim%20Kingma%2C%20Diederik%20P.%20Weight%20Normalization%3A%20A%20Simple%20Reparameterization%20To%20Accelerate%20Training%202016"
        },
        {
            "id": "19",
            "entry": "[19] Diederik P Kingma and Jimmy Ba. Adam: A method For Stochastic Optimization. arXiv preprint arXiv:1412.6980, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        },
        {
            "id": "20",
            "entry": "[20] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic Variational Inference. The Journal of Machine Learning Research, 14(1):1303\u20131347, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20Variational%20Inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Blei%2C%20David%20M.%20Wang%2C%20Chong%20Paisley%2C%20John%20Stochastic%20Variational%20Inference%202013"
        },
        {
            "id": "21",
            "entry": "[21] Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, and Bernhard Schoelkopf. From Optimal Transport To Generative Modeling: The VEGAN Cookbook. arXiv preprint arXiv:1705.07642, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07642"
        },
        {
            "id": "22",
            "entry": "[22] Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting Importance-Weighted Autoencoders. arXiv preprint arXiv:1704.02916, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.02916"
        },
        {
            "id": "23",
            "entry": "[23] Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank Wood, and Yee Whye Teh. Tighter Variational Bounds Are Not Necessarily Better. arXiv preprint arXiv:1802.04537, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04537"
        },
        {
            "id": "24",
            "entry": "[24] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining And Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1412.6572"
        },
        {
            "id": "25",
            "entry": "[25] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral Normalization For Generative Adversarial Networks. arXiv preprint arXiv:1802.05957, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05957"
        },
        {
            "id": "26",
            "entry": "[26] Matthew D Hoffman. Learning Deep Latent Gaussian Models With Markov Chain Monte Carlo. In International Conference On Machine Learning, pages 1510\u20131519, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoffman%2C%20Matthew%20D.%20Learning%20Deep%20Latent%20Gaussian%20Models%20With%20Markov%20Chain%20Monte%20Carlo%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoffman%2C%20Matthew%20D.%20Learning%20Deep%20Latent%20Gaussian%20Models%20With%20Markov%20Chain%20Monte%20Carlo%202017"
        },
        {
            "id": "27",
            "entry": "[27] Yingzhen Li and Richard E Turner. R\u00e9nyi Divergence Variational Inference. In Advances In Neural Information Processing Systems, pages 1073\u20131081, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yingzhen%20Turner%2C%20Richard%20E.%20R%C3%A9nyi%20Divergence%20Variational%20Inference%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yingzhen%20Turner%2C%20Richard%20E.%20R%C3%A9nyi%20Divergence%20Variational%20Inference%202016"
        },
        {
            "id": "28",
            "entry": "[28] Jakub M Tomczak and Max Welling. VAE With A Vampprior. arXiv preprint arXiv:1705.07120, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07120"
        },
        {
            "id": "29",
            "entry": "[29] Samuel L. Smith and Quoc V. Le. A bayesian Perspective On Generalization And Stochastic Gradient Descent. In International Conference On Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smith%2C%20Samuel%20L.%20Le%2C%20Quoc%20V.%20A%20bayesian%20Perspective%20On%20Generalization%20And%20Stochastic%20Gradient%20Descent%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smith%2C%20Samuel%20L.%20Le%2C%20Quoc%20V.%20A%20bayesian%20Perspective%20On%20Generalization%20And%20Stochastic%20Gradient%20Descent%202018"
        },
        {
            "id": "30",
            "entry": "[30] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp Minima Can Generalize For Deep Nets. arXiv preprint arXiv:1703.04933, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.04933"
        },
        {
            "id": "31",
            "entry": "[31] Dominic Masters and Carlo Luschi. Revisiting Small Batch Training For Deep Neural Networks. arXiv preprint arXiv:1804.07612, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1804.07612"
        },
        {
            "id": "32",
            "entry": "[32] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding Deep Learning Requires Rethinking Generalization. arXiv preprint arXiv:1611.03530, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.03530"
        },
        {
            "id": "33",
            "entry": "[33] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering With Bregman Divergences. Journal of machine learning research, 6(Oct):1705\u20131749, 2005. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Banerjee%2C%20Arindam%20Merugu%2C%20Srujana%20Dhillon%2C%20Inderjit%20S.%20Ghosh%2C%20Joydeep%20Clustering%20With%20Bregman%20Divergences%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Banerjee%2C%20Arindam%20Merugu%2C%20Srujana%20Dhillon%2C%20Inderjit%20S.%20Ghosh%2C%20Joydeep%20Clustering%20With%20Bregman%20Divergences%202005"
        }
    ]
}
