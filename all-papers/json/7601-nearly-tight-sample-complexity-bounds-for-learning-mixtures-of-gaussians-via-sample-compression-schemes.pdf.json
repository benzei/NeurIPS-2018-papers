{
    "filename": "7601-nearly-tight-sample-complexity-bounds-for-learning-mixtures-of-gaussians-via-sample-compression-schemes.pdf",
    "metadata": {
        "title": "Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes",
        "author": "Hassan Ashtiani School of Computer Science University of Waterloo, and Vector Institute, ON, Canada zokaeiam@mcmaster.ca",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7601-nearly-tight-sample-complexity-bounds-for-learning-mixtures-of-gaussians-via-sample-compression-schemes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We prove that \u0398(kd2/\u03b52) samples are necessary and sufficient for learning a mixture of k Gaussians in Rd, up to error \u03b5 in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that O(kd/\u03b52) samples suffice, matching a known lower bound. The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in Rd has an efficient sample compression."
    },
    "keywords": [
        {
            "term": "upper bound",
            "url": "https://en.wikipedia.org/wiki/upper_bound"
        },
        {
            "term": "total variation",
            "url": "https://en.wikipedia.org/wiki/total_variation"
        },
        {
            "term": "density estimation",
            "url": "https://en.wikipedia.org/wiki/density_estimation"
        },
        {
            "term": "intrinsic dimension",
            "url": "https://en.wikipedia.org/wiki/intrinsic_dimension"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        }
    ],
    "highlights": [
        "Estimating distributions from observed data is a fundamental task in statistics that has been studied for over a century",
        "Distribution learning is a vast topic and many approaches have been considered in the literature; here we only review approaches that are most relevant to our problem",
        "The minimum distance estimate [7, Section 6.8] is another approach for deriving sample complexity upper bounds for distribution learning",
        "An insight from supervised learning theory is that the sample complexity of learning a class is typically proportional to intrinsic dimension of the class divided by \u03b52, where \u03b5 is the error tolerance",
        "For the case of agnostic binary classification, the intrinsic dimension is captured by the VC-dimension of the concept class",
        "For the case of distribution learning with respect to \u2018natural\u2019 parametric classes, we expect this dimension to be equal to the number of parameters"
    ],
    "key_statements": [
        "Estimating distributions from observed data is a fundamental task in statistics that has been studied for over a century",
        "This paper develops a general technique for distribution learning, employs this technique in the important setting of mixtures of Gaussians",
        "Distribution learning is a vast topic and many approaches have been considered in the literature; here we only review approaches that are most relevant to our problem",
        "The minimum distance estimate [7, Section 6.8] is another approach for deriving sample complexity upper bounds for distribution learning",
        "For the class of single Gaussians in d dimensions, this approach leads to the optimal sample complexity upper bound of O(d2/\u03b52)",
        "If a learning algorithm with entrywise approximation guarantee is used to learn the distribution in KL divergence or total variation distance, the approximation parameter must depend on the condition number",
        "The compression framework we introduce is quite flexible, and can be used to prove sample complexity upper bounds for other distribution classes as well",
        "In the full version of this paper [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] we relax this requirement and give similar sample complexity bounds for the setting where the target is close to some distribution in the class",
        "To lower bound the total variation, we show that for every pair of these distributions, there is some subspace for which a vector drawn from one Gaussian will have slightly larger projection than a vector drawn from the other Gaussian",
        "An insight from supervised learning theory is that the sample complexity of learning a class is typically proportional to intrinsic dimension of the class divided by \u03b52, where \u03b5 is the error tolerance",
        "For the case of agnostic binary classification, the intrinsic dimension is captured by the VC-dimension of the concept class",
        "For the case of distribution learning with respect to \u2018natural\u2019 parametric classes, we expect this dimension to be equal to the number of parameters",
        "We showed that the new but related notion of distribution compression is sufficient for distribution learning"
    ],
    "summary": [
        "Estimating distributions from observed data is a fundamental task in statistics that has been studied for over a century.",
        "By constructing compression schemes for mixtures of axisaligned Gaussians and general Gaussians, we obtain new upper bounds on the sample complexity of learning with respect to these classes, which we prove to be optimal up to logarithmic factors.",
        "Theorem 1.1 The class of k-mixtures of d-dimensional Gaussians can be learned using O samples.",
        "Theorem 1.2 Any method for learning the class of k-mixtures of d-dimensional Gaussians has sample complexity \u03a9) = \u03a9.",
        "Theorem 1.3 The class of k-mixtures of axis-aligned d-dimensional Gaussians can be learned using O samples.",
        "The minimum distance estimate [7, Section 6.8] is another approach for deriving sample complexity upper bounds for distribution learning.",
        "[<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] used this approach to bound the sample complexity of learning high-dimensional log-concave distributions.",
        "For the class of single Gaussians in d dimensions, this approach leads to the optimal sample complexity upper bound of O(d2/\u03b52).",
        "For density estimation of mixtures of Gaussians, the current best sample complexity upper bounds are O for general Gaussians and O for axis-aligned Gaussians, both due to [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>].",
        "As for lower bounds on the sample complexity, much fewer results are known for learning mixtures of Gaussians.",
        "Suppose there exists a fixed decoder for the class, such that given the compressed set of samples and a sequence of bits, it approximately recovers the original distribution.",
        "If the size of the compressed set and the number of bits is guaranteed to be small, we show that the sample complexity of learning that class is small as well.",
        "If a class admits (\u03c4, t, m) compression, the sample complexity of learning with respect to this class is bounded by O(m + (\u03c4 + t)/\u03b52) (Theorem 3.5).",
        "We prove that the class of d-dimensional Gaussian distributions admits (O(d), O(d2), O(d)) compression (Lemma 4.1).",
        "The above results together imply tight upper bounds of O for mixtures of k Gaussians, and O for mixtures of k axis-aligned Gaussians over Rd. The compression framework we introduce is quite flexible, and can be used to prove sample complexity upper bounds for other distribution classes as well.",
        "In the full version of this paper [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] we relax this requirement and give similar sample complexity bounds for the setting where the target is close to some distribution in the class.",
        "Applying Theorem 3.5 implies that the class of k-mixtures of axis-aligned Gaussians in Rd can be learned using O samples."
    ],
    "headline": "We prove that \u0398 samples are necessary and sufficient for learning a mixture of k Gaussians in Rd, up to error \u03b5 in total variation distance",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Martin Anthony and Peter Bartlett. Neural network learning: theoretical foundations. Cambridge University Press, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20Martin%20Bartlett%2C%20Peter%20Neural%20network%20learning%3A%20theoretical%20foundations%201999"
        },
        {
            "id": "2",
            "entry": "[2] Hassan Ashtiani, Shai Ben-David, Nicholas J. A. Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of Gaussians mixtures via compression schemes. arXiv preprint. URL https://arxiv.org/abs/1710.05209.",
            "url": "https://arxiv.org/abs/1710.05209",
            "arxiv_url": "https://arxiv.org/pdf/1710.05209"
        },
        {
            "id": "3",
            "entry": "[3] Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI\u201918, pages 2679\u20132686. AAAI Publications, 2018. URL https://arxiv.org/abs/1706.01596.",
            "url": "https://arxiv.org/abs/1706.01596",
            "arxiv_url": "https://arxiv.org/pdf/1706.01596"
        },
        {
            "id": "4",
            "entry": "[4] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. J. ACM, 36(4):929\u2013965, October 1989. ISSN 00045411. doi: 10.1145/76359.76371. URL http://doi.acm.org/10.1145/76359.76371.",
            "crossref": "https://dx.doi.org/10.1145/76359.76371",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/76359.76371"
        },
        {
            "id": "5",
            "entry": "[5] Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Efficient density estimation via piecewise polynomial approximation. In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing, STOC \u201914, pages 604\u2013613, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-2710-7. doi: 10.1145/2591796.2591848.",
            "crossref": "https://dx.doi.org/10.1145/2591796.2591848",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/2591796.2591848"
        },
        {
            "id": "6",
            "entry": "[6] Luc Devroye. A course in density estimation, volume 14 of Progress in Probability and Statistics. Birkh\u00e4user Boston, Inc., Boston, MA, 1987. ISBN 0-8176-3365-0.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devroye%2C%20Luc%20A%20course%20in%20density%20estimation%2C%20volume%2014%20of%20Progress%20in%20Probability%20and%20Statistics%201987"
        },
        {
            "id": "7",
            "entry": "[7] Luc Devroye and G\u00e1bor Lugosi. Combinatorial methods in density estimation. Springer Series in Statistics. Springer-Verlag, New York, 2001. ISBN 0-387-95117-2. doi: 10.1007/ 978-1-4613-0125-7.",
            "crossref": "https://dx.doi.org/10.1007/978-1-4613-0125-7"
        },
        {
            "id": "8",
            "entry": "[8] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The minimax learning rate of normal and Ising undirected graphical models. arXiv preprint arXiv:1806.06887, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.06887"
        },
        {
            "id": "9",
            "entry": "[9] Ilias Diakonikolas. Learning Structured Distributions. In Peter B\u00fchlmann, Petros Drineas, Michael Kane, and Mark van der Laan, editors, Handbook of Big Data, chapter 15, pages 267\u2013 283. Chapman and Hall/CRC, 2016. URL http://www.crcnetbase.com/doi/pdfplus/10.1201/b19567-21.",
            "url": "http://www.crcnetbase.com/doi/pdfplus/10.1201/b19567-21",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20Ilias%20Learning%20Structured%20Distributions%202016"
        },
        {
            "id": "10",
            "entry": "[10] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional Gaussians and Gaussian mixtures. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 73\u201384, Oct 2017. doi: 10.1109/FOCS.2017.16. Available on arXiv:1611.03473 [cs.LG].",
            "crossref": "https://dx.doi.org/10.1109/FOCS.2017.16",
            "arxiv_url": "https://arxiv.org/pdf/1611.03473"
        },
        {
            "id": "11",
            "entry": "[11] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Learning multivariate log-concave distributions. In Proceedings of Machine Learning Research, volume 65 of COLT\u201917, pages 1\u2014-17, 2017. ISBN 3-540-35294-5, 978-3-540-35294-5. URL http://proceedings.mlr.press/v65/diakonikolas17a/diakonikolas17a.pdf.",
            "url": "http://proceedings.mlr.press/v65/diakonikolas17a/diakonikolas17a.pdf",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Diakonikolas%2C%20Ilias%20Kane%2C%20Daniel%20M.%20Stewart%2C%20Alistair%20Learning%20multivariate%20log-concave%20distributions%202017"
        },
        {
            "id": "12",
            "entry": "[12] Ildar Ibragimov. Estimation of analytic functions. In State of the art in probability and statistics (Leiden, 1999), volume 36 of IMS Lecture Notes Monogr. Ser., pages 359\u2013383. Inst. Math. Statist., Beachwood, OH, 2001. doi: 10.1214/lnms/1215090078. URL https://doi.org/10.1214/lnms/1215090078.",
            "crossref": "https://dx.doi.org/10.1214/lnms/1215090078",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1214/lnms/1215090078"
        },
        {
            "id": "13",
            "entry": "[13] Adam Kalai, Ankur Moitra, and Gregory Valiant. Disentangling Gaussians. Communications of the ACM, 55(2), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalai%2C%20Adam%20Moitra%2C%20Ankur%20Valiant%2C%20Gregory%20Disentangling%20Gaussians%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalai%2C%20Adam%20Moitra%2C%20Ankur%20Valiant%2C%20Gregory%20Disentangling%20Gaussians%202012"
        },
        {
            "id": "14",
            "entry": "[14] Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. On the learnability of discrete distributions. In Proceedings of the Twentysixth Annual ACM Symposium on Theory of Computing, STOC \u201994, pages 273\u2013282, New York, NY, USA, 1994. ACM. ISBN 0-89791-663-8. doi: 10.1145/195058.195155. URL http://doi.acm.org/10.1145/195058.195155.",
            "crossref": "https://dx.doi.org/10.1145/195058.195155",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1145/195058.195155"
        },
        {
            "id": "15",
            "entry": "[15] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical report, Technical report, University of California, Santa Cruz, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Littlestone%2C%20Nick%20Warmuth%2C%20Manfred%20Relating%20data%20compression%20and%20learnability%201986"
        },
        {
            "id": "16",
            "entry": "[16] Alexander E. Litvak, Alain Pajor, Mark Rudelson, and Nicole Tomczak-Jaegermann. Smallest singular value of random matrices and geometry of random polytopes. Adv. Math., 195(2): 491\u2013523, 2005. ISSN 0001-8708. URL https://doi.org/10.1016/j.aim.2004.08.004.",
            "crossref": "https://dx.doi.org/10.1016/j.aim.2004.08.004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1016/j.aim.2004.08.004"
        },
        {
            "id": "17",
            "entry": "[17] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training Gaussian mixture models at scale via coresets. Journal of Machine Learning Research, 18(160):1\u201325, 2018. URL http://jmlr.org/papers/v18/15-506.html.",
            "url": "http://jmlr.org/papers/v18/15-506.html",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lucic%2C%20Mario%20Faulkner%2C%20Matthew%20Krause%2C%20Andreas%20Feldman%2C%20Dan%20Training%20Gaussian%20mixture%20models%20at%20scale%20via%20coresets%202018"
        },
        {
            "id": "18",
            "entry": "[18] Shay Moran and Amir Yehudayoff. Sample compression schemes for VC classes. Journal of the ACM (JACM), 63(3):21, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moran%2C%20Shay%20Yehudayoff%2C%20Amir%20Sample%20compression%20schemes%20for%20VC%20classes%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moran%2C%20Shay%20Yehudayoff%2C%20Amir%20Sample%20compression%20schemes%20for%20VC%20classes%202016"
        },
        {
            "id": "19",
            "entry": "[19] Bernard W. Silverman. Density estimation for statistics and data analysis. Monographs on Statistics and Applied Probability. Chapman & Hall, London, 1986. ISBN 0-412-24620-1.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silverman%2C%20Bernard%20W.%20Density%20estimation%20for%20statistics%20and%20data%20analysis.%20Monographs%20on%20Statistics%20and%20Applied%20Probability%201986"
        },
        {
            "id": "20",
            "entry": "[20] Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, pages 1395\u2013 1403. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/5251-near-optimal-sample-estimators-for-spherical-gaussian-mixtures.",
            "url": "http://papers.nips.cc/paper/5251-near-optimal-sample-estimators-for-spherical-gaussian-mixtures",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Suresh%2C%20Ananda%20Theertha%20Orlitsky%2C%20Alon%20Acharya%2C%20Jayadev%20Jafarpour%2C%20Ashkan%20Near-optimal-sample%20estimators%20for%20spherical%20gaussian%20mixtures%202014"
        },
        {
            "id": "21",
            "entry": "[21] Vladimir N. Vapnik and Alexey Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2): 264\u2013280, 1971. doi: 10.1137/1116025. ",
            "crossref": "https://dx.doi.org/10.1137/1116025",
            "oa_query": "https://api.scholarcy.com/oa_version?query=https%3A//dx.doi.org/10.1137/1116025"
        }
    ]
}
