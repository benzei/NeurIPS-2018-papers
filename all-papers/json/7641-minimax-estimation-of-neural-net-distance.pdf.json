{
    "filename": "7641-minimax-estimation-of-neural-net-distance.pdf",
    "metadata": {
        "title": "Minimax Estimation of Neural Net Distance",
        "author": "Kaiyi Ji, Yingbin Liang",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7641-minimax-estimation-of-neural-net-distance.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "An important class of distance metrics proposed for training generative adversarial networks (GANs) is the integral probability metric (IPM), in which the neural net distance captures the practical GAN training via two neural networks. This paper investigates the minimax estimation problem of the neural net distance based on samples drawn from the distributions. We develop the first known minimax lower bound on the estimation error of the neural net distance, and an upper bound tighter than an existing bound on the estimator error for the empirical neural net distance. Our lower and upper bounds match not only in the order of the sample size but also in terms of the norm of the parameter matrices of neural networks, which justifies the empirical neural net distance as a good approximation of the true neural net distance for training GANs in practice."
    },
    "keywords": [
        {
            "term": "upper bound",
            "url": "https://en.wikipedia.org/wiki/upper_bound"
        },
        {
            "term": "distance metric",
            "url": "https://en.wikipedia.org/wiki/distance_metric"
        },
        {
            "term": "sample size",
            "url": "https://en.wikipedia.org/wiki/sample_size"
        },
        {
            "term": "cumulative distribution function",
            "url": "https://en.wikipedia.org/wiki/cumulative_distribution_function"
        },
        {
            "term": "Generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/Generative_adversarial_networks"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        }
    ],
    "highlights": [
        "Generative adversarial networks (GANs), first introduced by [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], have become an important technique for learning generative models from complicated real-life data",
        "Training Generative adversarial networks is performed via a minmax optimization with the maximum and minimum respectively taken over a class of discriminators and a class of generators, where both discriminator and generators are modeled by neural networks",
        "We investigate the minimax estimation of the neural net distance dFnn (\u03bc, \u03bd), where the major challenge in analysis lies in dealing with complicated neural network functions",
        "We focus on the minimax estimation error of the neural net distance, and the quantity |dFnn (\u03bc, \u03bd) \u2212 dFnn| is of our interest, on which our bound is tighter than the earlier study in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "We provide an upper bound on |dFnn (\u03bc, \u03bd) \u2212 dFnn|, which serves as an upper bound on the minimax estimation error",
        "We developed both the lower and upper bounds for the minimax estimation of the neural net distance based on finite samples"
    ],
    "key_statements": [
        "Generative adversarial networks (GANs), first introduced by [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], have become an important technique for learning generative models from complicated real-life data",
        "Training Generative adversarial networks is performed via a minmax optimization with the maximum and minimum respectively taken over a class of discriminators and a class of generators, where both discriminator and generators are modeled by neural networks",
        "We investigate the minimax estimation of the neural net distance dFnn (\u03bc, \u03bd), where the major challenge in analysis lies in dealing with complicated neural network functions",
        "In Section 3.1, we provide the first known lower bound on the minimax estimation error of dFnn (\u03bc, \u03bd) based on finite samples, which takes the form as cl max n\u22121/2, m\u22121/2 where the constant cl depends only on the parameters of neural networks",
        "We focus on the minimax estimation error of the neural net distance, and the quantity |dFnn (\u03bc, \u03bd) \u2212 dFnn| is of our interest, on which our bound is tighter than the earlier study in [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>]",
        "Let Fn\u221an be the set of neural networks defined by (4)",
        "Let Fnn be the set of neural networks defined by (4)",
        "We provide an upper bound on |dFnn (\u03bc, \u03bd) \u2212 dFnn|, which serves as an upper bound on the minimax estimation error",
        "For the unbounded-support sub-Gaussian class PuB, comparison of Theorems 1 and 3 indicates that the empirical estimator dFnn achieves the optimal minimax estimation rate max{n\u22121/2, m\u22121/2} as the sample size goes to infinity",
        "[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] provided upper bounds on the data-dependent Rademacher complexity of neural networks defined by Rn(Fnn) = E\n1 n n i=1 if",
        "We developed both the lower and upper bounds for the minimax estimation of the neural net distance based on finite samples"
    ],
    "summary": [
        "Generative adversarial networks (GANs), first introduced by [<a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], have become an important technique for learning generative models from complicated real-life data.",
        "In Section 3.1, we provide the first known lower bound on the minimax estimation error of dFnn (\u03bc, \u03bd) based on finite samples, which takes the form as cl max n\u22121/2, m\u22121/2 where the constant cl depends only on the parameters of neural networks.",
        "In Section 3.3, comparison of the lower and upper bounds indicates that the empirical neural net distance achieves the optimal minimax estimation rate in terms of n\u22121/2 + m\u22121/2.",
        "Part of our analysis of the minimax estimation error of the neural net distance requires to upper-bound the average Rademacher complexity of neural networks.",
        "We first develop the following minimax lower bound for the sub-Gaussian distribution class PuB with unbounded support.",
        "We compare the lower and upper bounds and make the following remarks on the optimality of minimax estimation of the neural net distance.",
        "For the unbounded-support sub-Gaussian class PuB, comparison of Theorems 1 and 3 indicates that the empirical estimator dFnn achieves the optimal minimax estimation rate max{n\u22121/2, m\u22121/2} as the sample size goes to infinity.",
        "To establish an upper bound on |dFnn (\u03bc, \u03bd) \u2212 dFnn|, the standard McDiarmid\u2019s inequality [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] that requires the bounded difference condition is not applicable here, because the input data has unbounded support so that the functions in Fnn can be unbounded, e.g., ReLU neural networks.",
        "We further generalize the result in [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] and establish the following new McDiarmid\u2019s type of concentration inequality for unbounded sub-Gaussian random vectors and Lipschitz functions.",
        "[<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] provided upper bounds on the data-dependent Rademacher complexity of neural networks defined by Rn(Fnn) = E",
        "Let Fnn be the set of neural networks defined by (4), and let x1, ..., xn \u2208 Rh be i.i.d. random samples generated by an unbounded-supported sub-Gaussian distribution \u03bc \u2208 PuB.",
        "We note that [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] provided an upper bound on the Rademacher complexity for one-hidden-layer neural networks for Gaussian inputs.",
        "We developed both the lower and upper bounds for the minimax estimation of the neural net distance based on finite samples.",
        "Our results established the minimax optimality of the empirical estimator in terms of not only the sample size but the norm of the parameter matrices of neural networks, which justifies its usage for training GANs. The work was supported in part by U.S National Science Foundation under the grant CCF-1801855.",
        "Our results established the minimax optimality of the empirical estimator in terms of not only the sample size but the norm of the parameter matrices of neural networks, which justifies its usage for training GANs"
    ],
    "headline": "This paper investigates the minimax estimation problem of the neural net distance based on samples drawn from the distributions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Anthony%2C%20M.%20Bartlett%2C%20P.L.%20Neural%20Network%20Learning%3A%20Theoretical%20Foundations%202009"
        },
        {
            "id": "2",
            "entry": "[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In Proc. International Conference on Machine Learning (ICML), pages 214\u2013223, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "3",
            "entry": "[3] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative adversarial nets (GANs). In Proc. International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arora%2C%20S.%20Ge%2C%20R.%20Liang%2C%20Y.%20Ma%2C%20T.%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arora%2C%20S.%20Ge%2C%20R.%20Liang%2C%20Y.%20Ma%2C%20T.%20Generalization%20and%20equilibrium%20in%20generative%20adversarial%20nets%20%28GANs%29%202017"
        },
        {
            "id": "4",
            "entry": "[4] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural networks. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 6241\u20136250, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Foster%2C%20D.J.%20Telgarsky%2C%20M.J.%20Spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463\u2013482, Nov 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20Gaussian%20complexities%3A%20risk%20bounds%20and%20structural%20results%202002-11",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bartlett%2C%20P.L.%20Mendelson%2C%20S.%20Rademacher%20and%20Gaussian%20complexities%3A%20risk%20bounds%20and%20structural%20results%202002-11"
        },
        {
            "id": "6",
            "entry": "[6] S. S. Du and J. D. Lee. On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01206"
        },
        {
            "id": "7",
            "entry": "[7] G. Dziugaite, D. Roy, and Z. Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Proc. of the 31st Conference on Uncertainty in Artificial Intelligence (UAI), pages 258\u2013267, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dziugaite%2C%20G.%20Roy%2C%20D.%20Ghahramani%2C%20Z.%20Training%20generative%20neural%20networks%20via%20maximum%20mean%20discrepancy%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dziugaite%2C%20G.%20Roy%2C%20D.%20Ghahramani%2C%20Z.%20Training%20generative%20neural%20networks%20via%20maximum%20mean%20discrepancy%20optimization%202015"
        },
        {
            "id": "8",
            "entry": "[8] N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. In Proc. Conference on Learning Theory (COLT), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golowich%2C%20N.%20Rakhlin%2C%20A.%20Shamir%2C%20O.%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golowich%2C%20N.%20Rakhlin%2C%20A.%20Shamir%2C%20O.%20Size-independent%20sample%20complexity%20of%20neural%20networks%202018"
        },
        {
            "id": "9",
            "entry": "[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 2672\u20132680, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20I.%20Pouget-Abadie%2C%20J.%20Mirza%2C%20M.%20Xu%2C%20B.%20Generative%20adversarial%20nets%202014"
        },
        {
            "id": "10",
            "entry": "[10] D. Hsu, S. Kakade, and T. Zhang. A tail inequality for quadratic forms of subGaussian random vectors. Electronic Communications in Probability, 17, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hsu%2C%20D.%20Kakade%2C%20S.%20Zhang%2C%20T.%20A%20tail%20inequality%20for%20quadratic%20forms%20of%20subGaussian%20random%20vectors%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hsu%2C%20D.%20Kakade%2C%20S.%20Zhang%2C%20T.%20A%20tail%20inequality%20for%20quadratic%20forms%20of%20subGaussian%20random%20vectors%202012"
        },
        {
            "id": "11",
            "entry": "[11] A. Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. In Proc. International Conference on Machine Learning (ICML), pages 28\u201336, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kontorovich%2C%20A.%20Concentration%20in%20unbounded%20metric%20spaces%20and%20algorithmic%20stability%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kontorovich%2C%20A.%20Concentration%20in%20unbounded%20metric%20spaces%20and%20algorithmic%20stability%202014"
        },
        {
            "id": "12",
            "entry": "[12] M. Ledoux and M. Talagrand. Probability in Banach Spaces. Springer, 1991.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ledoux%2C%20M.%20Talagrand%2C%20M.%20Probability%20in%20Banach%20Spaces%201991"
        },
        {
            "id": "13",
            "entry": "[13] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In Proc. International Conference on Machine Learning (ICML), pages 1718\u20131727, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Y.%20Swersky%2C%20K.%20Zemel%2C%20R.%20Generative%20moment%20matching%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Y.%20Swersky%2C%20K.%20Zemel%2C%20R.%20Generative%20moment%20matching%20networks%202015"
        },
        {
            "id": "14",
            "entry": "[14] T. Liang. How well can generative adversarial networks (GAN) learn densities: a nonparametric view. arXiv preprint arXiv:1712.08244, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.08244"
        },
        {
            "id": "15",
            "entry": "[15] S. Liu, O. Bousquet, and K. Chaudhuri. Approximation and convergence properties of generative adversarial learning. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 5551\u20135559, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20S.%20Bousquet%2C%20O.%20Chaudhuri%2C%20K.%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20S.%20Bousquet%2C%20O.%20Chaudhuri%2C%20K.%20Approximation%20and%20convergence%20properties%20of%20generative%20adversarial%20learning%202017"
        },
        {
            "id": "16",
            "entry": "[16] C. McDiarmid. On the method of bounded differences. Surveys in Combinatorics, 141(1):148\u2013 188, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McDiarmid%2C%20C.%20On%20the%20method%20of%20bounded%20differences%201989"
        },
        {
            "id": "17",
            "entry": "[17] S. J. Montgomery-Smith. The distribution of Rademacher sums. Proceedings of the American Mathematical Society, 109(2):517\u2013522, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Montgomery-Smith%2C%20S.J.%20The%20distribution%20of%20Rademacher%20sums%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Montgomery-Smith%2C%20S.J.%20The%20distribution%20of%20Rademacher%20sums%201990"
        },
        {
            "id": "18",
            "entry": "[18] K. Muandet, K. Fukumizu, F. Dinuzzo, and B. Sch\u00f6lkopf. Learning from distributions via support measure machines. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 10\u201318, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Muandet%2C%20K.%20Fukumizu%2C%20K.%20Dinuzzo%2C%20F.%20Sch%C3%B6lkopf%2C%20B.%20Learning%20from%20distributions%20via%20support%20measure%20machines%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Muandet%2C%20K.%20Fukumizu%2C%20K.%20Dinuzzo%2C%20F.%20Sch%C3%B6lkopf%2C%20B.%20Learning%20from%20distributions%20via%20support%20measure%20machines%202012"
        },
        {
            "id": "19",
            "entry": "[19] A. M\u00fcller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429\u2013443, 1997.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C3%BCller%2C%20A.%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C3%BCller%2C%20A.%20Integral%20probability%20metrics%20and%20their%20generating%20classes%20of%20functions%201997"
        },
        {
            "id": "20",
            "entry": "[20] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep learning. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 5949\u20135958, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20Exploring%20generalization%20in%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20Exploring%20generalization%20in%20deep%20learning%202017"
        },
        {
            "id": "21",
            "entry": "[21] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. In Proc. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20A%20pac-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20B.%20Bhojanapalli%2C%20S.%20McAllester%2C%20D.%20Srebro%2C%20N.%20A%20pac-bayesian%20approach%20to%20spectrally-normalized%20margin%20bounds%20for%20neural%20networks%202018"
        },
        {
            "id": "22",
            "entry": "[22] B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In Proc. Conference on Learning Theory (COLT), pages 1376\u20131401, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Neyshabur%2C%20B.%20Tomioka%2C%20R.%20Srebro%2C%20N.%20Norm-based%20capacity%20control%20in%20neural%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Neyshabur%2C%20B.%20Tomioka%2C%20R.%20Srebro%2C%20N.%20Norm-based%20capacity%20control%20in%20neural%20networks%202015"
        },
        {
            "id": "23",
            "entry": "[23] S. Oymak. Learning compact neural networks with regularization. arXiv preprint, arXiv:1802.01223, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01223"
        },
        {
            "id": "24",
            "entry": "[24] T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 901\u2013909, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salimans%2C%20T.%20Kingma%2C%20D.P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salimans%2C%20T.%20Kingma%2C%20D.P.%20Weight%20normalization%3A%20A%20simple%20reparameterization%20to%20accelerate%20training%20of%20deep%20neural%20networks%202016"
        },
        {
            "id": "25",
            "entry": "[25] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Sch\u00f6lkopf, and G. R. Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal of Statistics, 6:1550\u2013 1599, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sriperumbudur%2C%20B.K.%20Fukumizu%2C%20K.%20Gretton%2C%20A.%20Sch%C3%B6lkopf%2C%20B.%20On%20the%20empirical%20estimation%20of%20integral%20probability%20metrics%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sriperumbudur%2C%20B.K.%20Fukumizu%2C%20K.%20Gretton%2C%20A.%20Sch%C3%B6lkopf%2C%20B.%20On%20the%20empirical%20estimation%20of%20integral%20probability%20metrics%202012"
        },
        {
            "id": "26",
            "entry": "[26] I. O. Tolstikhin, B. K. Sriperumbudur, and B. Sch\u00f6lkopf. Minimax estimation of maximum mean discrepancy with radial kernels. In Proc. Advances in Neural Information Processing Systems (NIPS), pages 1930\u20131938, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tolstikhin%2C%20I.O.%20Sriperumbudur%2C%20B.K.%20Sch%C3%B6lkopf%2C%20B.%20Minimax%20estimation%20of%20maximum%20mean%20discrepancy%20with%20radial%20kernels%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tolstikhin%2C%20I.O.%20Sriperumbudur%2C%20B.K.%20Sch%C3%B6lkopf%2C%20B.%20Minimax%20estimation%20of%20maximum%20mean%20discrepancy%20with%20radial%20kernels%202016"
        },
        {
            "id": "27",
            "entry": "[27] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In Proc. International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20C.%20Bengio%2C%20S.%20Hardt%2C%20M.%20Recht%2C%20B.%20Understanding%20deep%20learning%20requires%20rethinking%20generalization%202017"
        },
        {
            "id": "28",
            "entry": "[28] P. Zhang, Q. Liu, D. Zhou, T. Xu, and X. He. On the discrimination-generalization tradeoff in GANs. In Proc. International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20P.%20Liu%2C%20Q.%20Zhou%2C%20D.%20Xu%2C%20T.%20On%20the%20discrimination-generalization%20tradeoff%20in%20GANs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20P.%20Liu%2C%20Q.%20Zhou%2C%20D.%20Xu%2C%20T.%20On%20the%20discrimination-generalization%20tradeoff%20in%20GANs%202018"
        },
        {
            "id": "29",
            "entry": "[29] S. Zou, Y. Liang, and H. V. Poor. Nonparametric detection of geometric structures over networks. IEEE Transactions on Signal Processing, 65(19):5034\u20135046, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zou%2C%20S.%20Liang%2C%20Y.%20Poor%2C%20H.V.%20Nonparametric%20detection%20of%20geometric%20structures%20over%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zou%2C%20S.%20Liang%2C%20Y.%20Poor%2C%20H.V.%20Nonparametric%20detection%20of%20geometric%20structures%20over%20networks%202015"
        }
    ]
}
