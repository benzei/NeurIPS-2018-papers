{
    "filename": "7486-dual-principal-component-pursuit-improved-analysis-and-efficient-algorithms.pdf",
    "metadata": {
        "title": "Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms",
        "author": "Zhihui Zhu, Yifan Wang, Daniel Robinson, Daniel Naiman, Rene Vidal, Manolis Tsakiris",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7486-dual-principal-component-pursuit-improved-analysis-and-efficient-algorithms.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Recent methods for learning a linear subspace from data corrupted by outliers are based on convex 1 and nuclear norm optimization and require the dimension of the subspace and the number of outliers to be sufficiently small [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]. In sharp contrast, the recently proposed Dual Principal Component Pursuit (DPCP) method [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] can provably handle subspaces of high dimension by solving a non-convex 1 optimization problem on the sphere. However, its geometric analysis is based on quantities that are difficult to interpret and are not amenable to statistical analysis. In this paper we provide a refined geometric analysis and a new statistical analysis that show that DPCP can tolerate as many outliers as the square of the number of inliers, thus improving upon other provably correct robust PCA methods. We also propose a scalable Projected Sub-Gradient Method (DPCP-PSGM) for solving the DPCP problem and show that it achieves linear convergence even though the underlying optimization problem is non-convex and non-smooth. Experiments on road plane detection from 3D point cloud data demonstrate that DPCP-PSGM can be more efficient than the traditional RANSAC algorithm, which is one of the most popular methods for such computer vision applications."
    },
    "keywords": [
        {
            "term": "linear programs",
            "url": "https://en.wikipedia.org/wiki/linear_programs"
        },
        {
            "term": "geometric analysis",
            "url": "https://en.wikipedia.org/wiki/geometric_analysis"
        },
        {
            "term": "robust principal component analysis",
            "url": "https://en.wikipedia.org/wiki/robust_principal_component_analysis"
        },
        {
            "term": "DPCP",
            "url": "https://en.wikipedia.org/wiki/DPCP"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "statistical analysis",
            "url": "https://en.wikipedia.org/wiki/statistical_analysis"
        },
        {
            "term": "linear subspace",
            "url": "https://en.wikipedia.org/wiki/linear_subspace"
        },
        {
            "term": "Principal Component Analysis",
            "url": "https://en.wikipedia.org/wiki/Principal_Component_Analysis"
        }
    ],
    "highlights": [
        "Fitting a linear subspace to a dataset corrupted by outliers is a fundamental problem in machine learning and statistics, primarily known as (Robust) Principal Component Analysis (PCA) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "Inspired by Lemma 1, which states that any critical point that has principal angle larger than arcsin (M \u03b7O/N cX ,min) must be a normal vector of S, we consider solving (2) with a first-order method, specifically Projected Sub-Gradient Method (DPCP-PSGM), which is stated in Algorithm 1",
        "We provided an improved analysis for the global optimality of the Dual Principal Component Pursuit method that suggests that Dual Principal Component Pursuit can handle O((#inliers)2) outliers",
        "We presented a scalable first-order method for solving the Dual Principal Component Pursuit problem that only uses matrix-vector multiplications, for which we established global convergence guarantees for various step size selection schemes, regardless of the non-convexity and non-smoothness of the Dual Principal Component Pursuit problem",
        "Experiments on 3D point cloud road data demonstrate that the proposed method is able to outperform Random Sampling And Consensus even when Random Sampling And Consensus is allowed to use 100 times the computational budget of the proposed method",
        "Extensions to allow for corrupted data and multiple subspaces, and further applications in computer vision are the subject of ongoing work"
    ],
    "key_statements": [
        "Fitting a linear subspace to a dataset corrupted by outliers is a fundamental problem in machine learning and statistics, primarily known as (Robust) Principal Component Analysis (PCA) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "Inspired by Lemma 1, which states that any critical point that has principal angle larger than arcsin (M \u03b7O/N cX ,min) must be a normal vector of S, we consider solving (2) with a first-order method, specifically Projected Sub-Gradient Method (DPCP-PSGM), which is stated in Algorithm 1",
        "We provided an improved analysis for the global optimality of the Dual Principal Component Pursuit method that suggests that Dual Principal Component Pursuit can handle O((#inliers)2) outliers",
        "We presented a scalable first-order method for solving the Dual Principal Component Pursuit problem that only uses matrix-vector multiplications, for which we established global convergence guarantees for various step size selection schemes, regardless of the non-convexity and non-smoothness of the Dual Principal Component Pursuit problem",
        "Experiments on 3D point cloud road data demonstrate that the proposed method is able to outperform Random Sampling And Consensus even when Random Sampling And Consensus is allowed to use 100 times the computational budget of the proposed method",
        "Extensions to allow for corrupted data and multiple subspaces, and further applications in computer vision are the subject of ongoing work"
    ],
    "summary": [
        "Fitting a linear subspace to a dataset corrupted by outliers is a fundamental problem in machine learning and statistics, primarily known as (Robust) Principal Component Analysis (PCA) [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>].",
        "As shown in [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>], as long as the points are well distributed in a certain deterministic sense, any global minimizer of this non-convex problem is guaranteed to be a vector orthogonal to the subspace, regardless of the outlier/inlier ratio and the subspace dimension; a result that agrees with the earlier findings of [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>].",
        "Algorithms: A scalable Projected Sub-Gradient Method algorithm with piecewise geometrically diminishing step sizes (DPCP-PSGM), which is proven to solve the non-convex DPCP problem with linear convergence and using only matrix-vector multiplications.",
        "We see that according to Lemma 1, any critical point of (2) is either orthogonal to the inlier subspace S, or very close to S, with its principal angle \u03c6 from S being smaller for well distributed points and smaller outlier to inlier ratios M/N .",
        "Theorem 1 implies that regardless of the outlier/inlier ratio M/N , as we have more and more inliers and outliers while keeping D and M/N fixed, and assuming the points are well-distributed, condition (8) will eventually be satisfied and any global minimizer must be orthogonal to the inlier subspace S.",
        "Inspired by Lemma 1, which states that any critical point that has principal angle larger than arcsin (M \u03b7O/N cX ,min) must be a normal vector of S, we consider solving (2) with a first-order method, specifically Projected Sub-Gradient Method (DPCP-PSGM), which is stated in Algorithm 1.",
        "Our result provides performance guarantees for Algorithm 1 for various choices of step sizes ranging from constant to geometrically diminishing step sizes, the latter one giving an R-linear convergence of the sequence of principal angles to zero.",
        "First note that with the choice of constant step size \u03bc, PSGM is not guaranteed to find a normal vector, (11) ensures that after K (\u03bc) iterations, bk is close to S\u22a5 in the sense that \u03b8k \u2264 \u03b8 (\u03bc), which can be much smaller than \u03b80 for a sufficiently small \u03bc.",
        "Theorem 3 guarantees a sub-linear convergence of tan for step size diminishing at the rate of 1/k.",
        "We presented a scalable first-order method for solving the DPCP problem that only uses matrix-vector multiplications, for which we established global convergence guarantees for various step size selection schemes, regardless of the non-convexity and non-smoothness of the DPCP problem.",
        "Extensions to allow for corrupted data and multiple subspaces, and further applications in computer vision are the subject of ongoing work"
    ],
    "headline": "In this paper we provide a refined geometric analysis and a new statistical analysis that show that Dual Principal Component Pursuit can tolerate as many outliers as the square of the number of inliers, improving upon other provably correct robust Principal Component Analysis methods",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] S. Boyd, L. Xiao, and A. Mutapcic. Subgradient methods. Lecture Notes of EE392o, Stanford University, Autumn Quarter, 2004:2004\u20132005, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boyd%2C%20S.%20Xiao%2C%20L.%20Mutapcic%2C%20A.%20Subgradient%20methods%202004"
        },
        {
            "id": "2",
            "entry": "[2] E. Cand\u00e8s, X. Li, Y. Ma, and J. Wright. Robust principal component analysis. Journal of the ACM, 58, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cand%C3%A8s%2C%20E.%20Li%2C%20X.%20Ma%2C%20Y.%20Wright%2C%20J.%20Robust%20principal%20component%20analysis%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cand%C3%A8s%2C%20E.%20Li%2C%20X.%20Ma%2C%20Y.%20Wright%2C%20J.%20Robust%20principal%20component%20analysis%202011"
        },
        {
            "id": "3",
            "entry": "[3] Y. Cherapanamjeri, P. Jain, and P. Netrapalli. Thresholding based efficient outlier robust pca. arXiv preprint arXiv:1702.05571, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05571"
        },
        {
            "id": "4",
            "entry": "[4] D. Davis, D. Drusvyatskiy, K. J. MacPhee, and C. Paquette. Subgradient methods for sharp weakly convex functions. arXiv preprint arXiv:1803.02461, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.02461"
        },
        {
            "id": "5",
            "entry": "[5] M. A. Fischler and R. C. Bolles. RANSAC random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 26:381\u2013395, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fischler%2C%20M.A.%20Bolles%2C%20R.C.%20RANSAC%20random%20sample%20consensus%3A%20A%20paradigm%20for%20model%20fitting%20with%20applications%20to%20image%20analysis%20and%20automated%20cartography%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fischler%2C%20M.A.%20Bolles%2C%20R.C.%20RANSAC%20random%20sample%20consensus%3A%20A%20paradigm%20for%20model%20fitting%20with%20applications%20to%20image%20analysis%20and%20automated%20cartography%201981"
        },
        {
            "id": "6",
            "entry": "[6] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Geiger%2C%20A.%20Lenz%2C%20P.%20Stiller%2C%20C.%20Urtasun%2C%20R.%20Vision%20meets%20robotics%3A%20The%20kitti%20dataset%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Geiger%2C%20A.%20Lenz%2C%20P.%20Stiller%2C%20C.%20Urtasun%2C%20R.%20Vision%20meets%20robotics%3A%20The%20kitti%20dataset%202013"
        },
        {
            "id": "7",
            "entry": "[7] J.-L. Goffin. On convergence rates of subgradient optimization methods. Mathematical Programming, 13(1):329\u2013347, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goffin%2C%20J.-L.%20On%20convergence%20rates%20of%20subgradient%20optimization%20methods%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goffin%2C%20J.-L.%20On%20convergence%20rates%20of%20subgradient%20optimization%20methods%201977"
        },
        {
            "id": "8",
            "entry": "[8] I. Gurobi Optimization. Gurobi optimizer reference manual, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I.%20Gurobi%20Optimization%202015"
        },
        {
            "id": "9",
            "entry": "[9] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge, 2nd edition, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hartley%2C%20R.%20Zisserman%2C%20A.%20Multiple%20View%20Geometry%20in%20Computer%20Vision%202004"
        },
        {
            "id": "10",
            "entry": "[10] I. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jolliffe%2C%20I.%20Principal%20Component%20Analysis%201986"
        },
        {
            "id": "11",
            "entry": "[11] G. Lerman and T. Maunu. Fast, robust and non-convex subspace recovery. Information and Inference: A Journal of the IMA, 7(2):277\u2013336, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lerman%2C%20G.%20Fast%2C%20T.Maunu%20robust%20and%20non-convex%20subspace%20recovery%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lerman%2C%20G.%20Fast%2C%20T.Maunu%20robust%20and%20non-convex%20subspace%20recovery%202017"
        },
        {
            "id": "12",
            "entry": "[12] G. Lerman and T. Maunu. An overview of robust subspace recovery. arXiv:1803.01013 [cs.LG], 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.01013"
        },
        {
            "id": "13",
            "entry": "[13] G. Lerman, M. B. McCoy, J. A. Tropp, and T. Zhang. Robust computation of linear models by convex relaxation. Foundations of Computational Mathematics, 15(2):363\u2013410, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lerman%2C%20G.%20McCoy%2C%20M.B.%20Tropp%2C%20J.A.%20Zhang%2C%20T.%20Robust%20computation%20of%20linear%20models%20by%20convex%20relaxation%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lerman%2C%20G.%20McCoy%2C%20M.B.%20Tropp%2C%20J.A.%20Zhang%2C%20T.%20Robust%20computation%20of%20linear%20models%20by%20convex%20relaxation%202015"
        },
        {
            "id": "14",
            "entry": "[14] G. Lerman and T. Zhang. p-recovery of the most significant subspace among multiple subspaces with outliers. Constructive Approximation, 40(3):329\u2013385, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lerman%2C%20G.%20Zhang%2C%20T.%20p-recovery%20of%20the%20most%20significant%20subspace%20among%20multiple%20subspaces%20with%20outliers%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lerman%2C%20G.%20Zhang%2C%20T.%20p-recovery%20of%20the%20most%20significant%20subspace%20among%20multiple%20subspaces%20with%20outliers%202014"
        },
        {
            "id": "15",
            "entry": "[15] X. Li, Z. Zhu, A. M.-C. So, and R. Vidal. Nonconvex robust low-rank matrix recovery. arXiv preprint arXiv:1809.09237, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1809.09237"
        },
        {
            "id": "16",
            "entry": "[16] T. Maunu and G. Lerman. A well-tempered landscape for non-convex robust subspace recovery. arXiv:1706.03896 [cs.LG], 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03896"
        },
        {
            "id": "17",
            "entry": "[17] J. Nocedal and S. J. Wright. Numerical Optimization, second edition. World Scientific, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J%20Nocedal%20and%20S%20J%20Wright%20Numerical%20Optimization%20second%20edition%20World%20Scientific%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J%20Nocedal%20and%20S%20J%20Wright%20Numerical%20Optimization%20second%20edition%20World%20Scientific%202006"
        },
        {
            "id": "18",
            "entry": "[18] Q. Qu, J. Sun, and J. Wright. Finding a sparse vector in a subspace: Linear sparsity using alternating directions. In Advances in Neural Information Processing Systems, pages 3401\u20133409, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Qu%2C%20Q.%20Sun%2C%20J.%20Wright%2C%20J.%20Finding%20a%20sparse%20vector%20in%20a%20subspace%3A%20Linear%20sparsity%20using%20alternating%20directions%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Qu%2C%20Q.%20Sun%2C%20J.%20Wright%2C%20J.%20Finding%20a%20sparse%20vector%20in%20a%20subspace%3A%20Linear%20sparsity%20using%20alternating%20directions%202014"
        },
        {
            "id": "19",
            "entry": "[19] M. Rahmani and G. Atia. Coherence pursuit: Fast, simple, and robust principal component analysis. arXiv preprint arXiv:1609.04789, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.04789"
        },
        {
            "id": "20",
            "entry": "[20] M. Soltanolkotabi, E. Elhamifar, and E. J. Cand\u00e8s. Robust subspace clustering. Annals of Statistics, 42(2):669\u2013699, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Soltanolkotabi%2C%20M.%20Elhamifar%2C%20E.%20Cand%C3%A8s%2C%20E.J.%20Robust%20subspace%20clustering%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Soltanolkotabi%2C%20M.%20Elhamifar%2C%20E.%20Cand%C3%A8s%2C%20E.J.%20Robust%20subspace%20clustering%202014"
        },
        {
            "id": "21",
            "entry": "[21] H. Sp\u00e4th and G. Watson. On orthogonal linear 1 approximation. Numerische Mathematik, 51(5):531\u2013543, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sp%C3%A4th%2C%20H.%20Watson%2C%20G.%20On%20orthogonal%20linear%201%20approximation%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sp%C3%A4th%2C%20H.%20Watson%2C%20G.%20On%20orthogonal%20linear%201%20approximation%201987"
        },
        {
            "id": "22",
            "entry": "[22] M. Tsakiris and R. Vidal. Dual principal component pursuit. In ICCV Workshop on Robust Subspace Learning and Computer Vision, pages 10\u201318, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsakiris%2C%20M.%20Vidal%2C%20R.%20Dual%20principal%20component%20pursuit%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsakiris%2C%20M.%20Vidal%2C%20R.%20Dual%20principal%20component%20pursuit%202015"
        },
        {
            "id": "23",
            "entry": "[23] M. C. Tsakiris and R. Vidal. Hyperplane clustering via dual principal component pursuit. In International Conference on Machine Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsakiris%2C%20M.C.%20Vidal%2C%20R.%20Hyperplane%20clustering%20via%20dual%20principal%20component%20pursuit%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsakiris%2C%20M.C.%20Vidal%2C%20R.%20Hyperplane%20clustering%20via%20dual%20principal%20component%20pursuit%202017"
        },
        {
            "id": "24",
            "entry": "[24] M. C. Tsakiris and R. Vidal. Dual principal component pursuit. Journal of Machine Learning Research, 19(18):1\u201350, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsakiris%2C%20M.C.%20Vidal%2C%20R.%20Dual%20principal%20component%20pursuit%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tsakiris%2C%20M.C.%20Vidal%2C%20R.%20Dual%20principal%20component%20pursuit%202018"
        },
        {
            "id": "25",
            "entry": "[25] D. E. Tyler. A distribution-free m-estimator of multivariate scatter. The Annals of Statistics, 15(1):234\u2013251, 1987.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tyler%2C%20D.E.%20A%20distribution-free%20m-estimator%20of%20multivariate%20scatter%201987",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tyler%2C%20D.E.%20A%20distribution-free%20m-estimator%20of%20multivariate%20scatter%201987"
        },
        {
            "id": "26",
            "entry": "[26] Y. Wang, C. Dicle, M. Sznaier, and O. Camps. Self scaled regularized robust regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3261\u20133269, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Y.%20Dicle%2C%20C.%20Sznaier%2C%20M.%20Camps%2C%20O.%20Self%20scaled%20regularized%20robust%20regression%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Y.%20Dicle%2C%20C.%20Sznaier%2C%20M.%20Camps%2C%20O.%20Self%20scaled%20regularized%20robust%20regression%202015"
        },
        {
            "id": "27",
            "entry": "[27] H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. In Advances in Neural Information Processing Systems, pages 2496\u20132504, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20H.%20Caramanis%2C%20C.%20Sanghavi%2C%20S.%20Robust%20pca%20via%20outlier%20pursuit%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20H.%20Caramanis%2C%20C.%20Sanghavi%2C%20S.%20Robust%20pca%20via%20outlier%20pursuit%202010"
        },
        {
            "id": "28",
            "entry": "[28] C. You, D. Robinson, and R. Vidal. Provable self-representation based outlier detection in a union of subspaces. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=You%2C%20C.%20Robinson%2C%20D.%20Vidal%2C%20R.%20Provable%20self-representation%20based%20outlier%20detection%20in%20a%20union%20of%20subspaces%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=You%2C%20C.%20Robinson%2C%20D.%20Vidal%2C%20R.%20Provable%20self-representation%20based%20outlier%20detection%20in%20a%20union%20of%20subspaces%202017"
        },
        {
            "id": "29",
            "entry": "[29] T. Zhang. Robust subspace recovery by tyler\u2019s m-estimator. Information and Inference: A Journal of the IMA, 5(1):1\u201321, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20T.%20Robust%20subspace%20recovery%20by%20tyler%E2%80%99s%20m-estimator.%20Information%20and%20Inference%3A%20A%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20T.%20Robust%20subspace%20recovery%20by%20tyler%E2%80%99s%20m-estimator.%20Information%20and%20Inference%3A%20A%202016"
        },
        {
            "id": "30",
            "entry": "[30] T. Zhang and G. Lerman. A novel m-estimator for robust pca. The Journal of Machine Learning Research, 15(1):749\u2013808, 2014. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20T.%20Lerman%2C%20G.%20A%20novel%20m-estimator%20for%20robust%20pca%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20T.%20Lerman%2C%20G.%20A%20novel%20m-estimator%20for%20robust%20pca%202014"
        }
    ]
}
