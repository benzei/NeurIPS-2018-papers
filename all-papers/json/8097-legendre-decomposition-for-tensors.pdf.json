{
    "filename": "8097-legendre-decomposition-for-tensors.pdf",
    "metadata": {
        "title": "Legendre Decomposition for Tensors",
        "author": "Mahito Sugiyama, Hiroyuki Nakahara, Koji Tsuda",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8097-legendre-decomposition-for-tensors.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods."
    },
    "keywords": [
        {
            "term": "restricted Boltzmann machines",
            "url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_Machine"
        },
        {
            "term": "boltzmann machine",
            "url": "https://en.wikipedia.org/wiki/boltzmann_machine"
        },
        {
            "term": "information geometry",
            "url": "https://en.wikipedia.org/wiki/information_geometry"
        },
        {
            "term": "root mean squared error",
            "url": "https://en.wikipedia.org/wiki/root_mean_squared_error"
        },
        {
            "term": "signal processing",
            "url": "https://en.wikipedia.org/wiki/signal_processing"
        },
        {
            "term": "computer vision",
            "url": "https://en.wikipedia.org/wiki/computer_vision"
        },
        {
            "term": "tensor decomposition",
            "url": "https://en.wikipedia.org/wiki/tensor_decomposition"
        },
        {
            "term": "component analysis",
            "url": "https://en.wikipedia.org/wiki/component_analysis"
        },
        {
            "term": "nonnegative matrix factorization",
            "url": "https://en.wikipedia.org/wiki/nonnegative_matrix_factorization"
        },
        {
            "term": "tucker decomposition",
            "url": "https://en.wikipedia.org/wiki/tucker_decomposition"
        },
        {
            "term": "deep boltzmann machine",
            "url": "https://en.wikipedia.org/wiki/Deep_Boltzmann_Machine"
        },
        {
            "term": "MATLAB",
            "url": "https://en.wikipedia.org/wiki/MATLAB"
        },
        {
            "term": "recommender system",
            "url": "https://en.wikipedia.org/wiki/recommender_system"
        },
        {
            "term": "standard error of the mean",
            "url": "https://en.wikipedia.org/wiki/standard_error_of_the_mean"
        },
        {
            "term": "factor analysis",
            "url": "https://en.wikipedia.org/wiki/factor_analysis"
        }
    ],
    "highlights": [
        "Matrix and tensor decomposition is a fundamental technique in machine learning; it is used to analyze data represented in the form of multi-dimensional arrays, and is used in a wide range of applications such as computer vision (<a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\"><a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\">Vasilescu and Terzopoulos, 2002</a></a>, 2007), recommender systems (<a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\"><a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\">Symeonidis, 2016</a></a>), signal processing (<a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\"><a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\">Cichocki et al, 2015</a></a>), and neuroscience (<a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\"><a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\">Beckmann and Smith, 2005</a></a>)",
        "Matrix and tensor decomposition is a fundamental technique in machine learning; it is used to analyze data represented in the form of multi-dimensional arrays, and is used in a wide range of applications such as computer vision (<a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\">Vasilescu and Terzopoulos, 2002</a>, 2007), recommender systems (<a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\">Symeonidis, 2016</a>), signal processing (<a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\">Cichocki et al, 2015</a>), and neuroscience (<a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\">Beckmann and Smith, 2005</a>)",
        "We demonstrate interesting relationships between Legendre decomposition and statistical models, including the exponential family and the Boltzmann (Gibbs) distributions, and show that our decomposition method can be viewed as a generalization of Boltzmann machine learning (<a class=\"ref-link\" id=\"cAckley_et+al_1985_a\" href=\"#rAckley_et+al_1985_a\">Ackley et al, 1985</a>)",
        "In terms of root mean squared error, Legendre decomposition is superior to the other methods if the number of parameters is small, and it is competitive with nonnegative CP decomposition and inferior to CP-Alternating Poisson Regression for a larger number of parameters",
        "We have proposed Legendre decomposition, which incorporates tensor structure into information geometry",
        "We have theoretically shown the desired properties of the Legendre decomposition, namely, that its results are well-defined, unique, and globally optimized, in that it always finds the decomposable tensor that minimizes the Kullback\u2013 Leibler divergence from the input tensor"
    ],
    "key_statements": [
        "Matrix and tensor decomposition is a fundamental technique in machine learning; it is used to analyze data represented in the form of multi-dimensional arrays, and is used in a wide range of applications such as computer vision (<a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\"><a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\">Vasilescu and Terzopoulos, 2002</a></a>, 2007), recommender systems (<a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\"><a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\">Symeonidis, 2016</a></a>), signal processing (<a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\"><a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\">Cichocki et al, 2015</a></a>), and neuroscience (<a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\"><a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\">Beckmann and Smith, 2005</a></a>)",
        "Matrix and tensor decomposition is a fundamental technique in machine learning; it is used to analyze data represented in the form of multi-dimensional arrays, and is used in a wide range of applications such as computer vision (<a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\">Vasilescu and Terzopoulos, 2002</a>, 2007), recommender systems (<a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\">Symeonidis, 2016</a>), signal processing (<a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\">Cichocki et al, 2015</a>), and neuroscience (<a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\">Beckmann and Smith, 2005</a>)",
        "We demonstrate interesting relationships between Legendre decomposition and statistical models, including the exponential family and the Boltzmann (Gibbs) distributions, and show that our decomposition method can be viewed as a generalization of Boltzmann machine learning (<a class=\"ref-link\" id=\"cAckley_et+al_1985_a\" href=\"#rAckley_et+al_1985_a\">Ackley et al, 1985</a>)",
        "The connection between tensor decomposition and graphical models has been analyzed by <a class=\"ref-link\" id=\"cChen_et+al_2018_a\" href=\"#rChen_et+al_2018_a\">Chen et al (2018</a>); Y\u0131lmaz et al (2011); Y\u0131lmaz and Cemgil (2012), our analysis adds a new insight as we focus on not the graphical model itself but the sample space of distributions generated by the model.\n2.4.1",
        "The tensor Q obtained by Legendre decomposition with B = B(V )\u222aB(E) coincides with the distribution learned by the Boltzmann machine G = (V, E)",
        "We evaluated the quality of decomposition by the root mean squared error (RMSE) between the input and the reconstructed tensors",
        "First we examine Legendre decomposition and three competing methods on the face image dataset2",
        "In terms of root mean squared error, Legendre decomposition is superior to the other methods if the number of parameters is small, and it is competitive with nonnegative CP decomposition and inferior to CP-Alternating Poisson Regression for a larger number of parameters",
        "The reason is that Legendre decomposition uses the information of the index order that is based on the structure of the face images; that is, rows or columns cannot be replaced with each other in the data",
        "We have proposed Legendre decomposition, which incorporates tensor structure into information geometry",
        "We have theoretically shown the desired properties of the Legendre decomposition, namely, that its results are well-defined, unique, and globally optimized, in that it always finds the decomposable tensor that minimizes the Kullback\u2013 Leibler divergence from the input tensor",
        "We have shown the connection between Legendre decomposition and Boltzmann machine learning",
        "We have experimentally shown that Legendre decomposition can more accurately reconstruct input tensors than three standard tensor decomposition methods using the same number of parameters"
    ],
    "summary": [
        "Matrix and tensor decomposition is a fundamental technique in machine learning; it is used to analyze data represented in the form of multi-dimensional arrays, and is used in a wide range of applications such as computer vision (<a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\"><a class=\"ref-link\" id=\"cVasilescu_2002_a\" href=\"#rVasilescu_2002_a\">Vasilescu and Terzopoulos, 2002</a></a>, 2007), recommender systems (<a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\"><a class=\"ref-link\" id=\"cSymeonidis_2016_a\" href=\"#rSymeonidis_2016_a\">Symeonidis, 2016</a></a>), signal processing (<a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\"><a class=\"ref-link\" id=\"cCichocki_et+al_2015_a\" href=\"#rCichocki_et+al_2015_a\">Cichocki et al, 2015</a></a>), and neuroscience (<a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\"><a class=\"ref-link\" id=\"cBeckmann_2005_a\" href=\"#rBeckmann_2005_a\">Beckmann and Smith, 2005</a></a>).",
        "We define Legendre decomposition as follows: Given an input tensor P \u2208 R\u2265I10\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , the sample space \u03a9 \u2286 [I1] \u00d7 [I2] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [IN ], and a parameter basis B \u2286 \u03a9+, Legendre decomposition finds the fully decomposable tensor Q \u2211\u2208 RI\u226510\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN with a B that minimizes the Kullback\u2013 Leibler (KL) divergence DKL(P, Q) = v\u2208\u03a9 pv log (Figure 1[a]).",
        "Legendre decomposition for second-order tensors can be viewed as a low rank approximation not of an input matrix P but of its entry-wise logarithm log P.",
        "To reduce the number of iterations to gain efficiency, we propose to use a natural gradient (<a class=\"ref-link\" id=\"cAmari_1998_a\" href=\"#rAmari_1998_a\">Amari, 1998</a>), which is the second-order optimization method, shown in Algorithm 2.",
        "We demonstrate interesting relationships between Legendre decomposition and statistical models, including the exponential family and the Boltzmann (Gibbs) distributions, and show that our decomposition method can be viewed as a generalization of Boltzmann machine learning (<a class=\"ref-link\" id=\"cAckley_et+al_1985_a\" href=\"#rAckley_et+al_1985_a\">Ackley et al, 1985</a>).",
        "It is clear that the set of probability distributions, or Gibbs distributions, that can be represented by the Boltzmann machine G is exactly the same as SB with B = B(V ) \u222a B(E) and exp(\u03c8(\u03b8)) = Z(\u03b8); that is, the set of fully decomposable N th-order tensors defined by Equation (1) with the basis B(V ) \u222a B(E) (Figure 3).",
        "The tensor Q obtained by Legendre decomposition with B = B(V )\u222aB(E) coincides with the distribution learned by the Boltzmann machine G = (V, E).",
        "Results on Real Data we demonstrate the effectiveness of Legendre decomposition on realworld datasets of third-order tensors.",
        "The reason is that Legendre decomposition uses the information of the index order that is based on the structure of the face images; that is, rows or columns cannot be replaced with each other in the data.",
        "Running time shows the same trend as that of the face dataset; that is, Legendre decomposition is slower than other methods when the number of parameters increases.",
        "We have theoretically shown the desired properties of the Legendre decomposition, namely, that its results are well-defined, unique, and globally optimized, in that it always finds the decomposable tensor that minimizes the KL divergence from the input tensor.",
        "We have experimentally shown that Legendre decomposition can more accurately reconstruct input tensors than three standard tensor decomposition methods using the same number of parameters.",
        "We have shown the connection between Legendre decomposition and Boltzmann machine learning"
    ],
    "headline": "We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters",
    "reference_links": [
        {
            "id": "Ackley_et+al_1985_a",
            "entry": "D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines. Cognitive Science, 9(1):147\u2013169, 1985.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ackley%2C%20D.H.%20Hinton%2C%20G.E.%20Sejnowski%2C%20T.J.%20A%20learning%20algorithm%20for%20Boltzmann%20machines%201985",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ackley%2C%20D.H.%20Hinton%2C%20G.E.%20Sejnowski%2C%20T.J.%20A%20learning%20algorithm%20for%20Boltzmann%20machines%201985"
        },
        {
            "id": "Amari_1998_a",
            "entry": "S. Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251\u2013276, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.%20Natural%20gradient%20works%20efficiently%20in%20learning%201998"
        },
        {
            "id": "Amari_2001_a",
            "entry": "S. Amari. Information geometry on hierarchy of probability distributions. IEEE Transactions on Information Theory, 47(5):1701\u20131711, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.%20Information%20geometry%20on%20hierarchy%20of%20probability%20distributions%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.%20Information%20geometry%20on%20hierarchy%20of%20probability%20distributions%202001"
        },
        {
            "id": "Amari_2008_a",
            "entry": "S. Amari. Information geometry and its applications: Convex function and dually flat manifold. In F. Nielsen, editor, Emerging Trends in Visual Computing: LIX Fall Colloquium, ETVC 2008, Revised Invited Papers, pages 75\u2013102.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.%20Information%20geometry%20and%20its%20applications%3A%20Convex%20function%20and%20dually%20flat%20manifold%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Amari%2C%20S.%20Information%20geometry%20and%20its%20applications%3A%20Convex%20function%20and%20dually%20flat%20manifold%202008"
        },
        {
            "id": "Amari_2016_a",
            "entry": "S. Amari. Information Geometry and Its Applications. Springer, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20S.%20Information%20Geometry%20and%20Its%20Applications%202016"
        },
        {
            "id": "Bader_2007_a",
            "entry": "B. W. Bader and T. G. Kolda. Efficient MATLAB computations with sparse and factored tensors. SIAM Journal on Scientific Computing, 30(1):205\u2013231, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bader%2C%20B.W.%20Kolda%2C%20T.G.%20Efficient%20MATLAB%20computations%20with%20sparse%20and%20factored%20tensors%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bader%2C%20B.W.%20Kolda%2C%20T.G.%20Efficient%20MATLAB%20computations%20with%20sparse%20and%20factored%20tensors%202007"
        },
        {
            "id": "Bader_2017_a",
            "entry": "B. W. Bader, T. G. Kolda, et al. MATLAB tensor toolbox version 3.0-dev, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bader%2C%20B.W.%20Kolda%2C%20T.G.%20MATLAB%20tensor%20toolbox%20version%203.0-dev%202017"
        },
        {
            "id": "Beckmann_2005_a",
            "entry": "C. F. Beckmann and S. M. Smith. Tensorial extensions of independent component analysis for multisubject FMRI analysis. NeuroImage, 25(1):294\u2013311, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beckmann%2C%20C.F.%20Smith%2C%20S.M.%20Tensorial%20extensions%20of%20independent%20component%20analysis%20for%20multisubject%20FMRI%20analysis%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beckmann%2C%20C.F.%20Smith%2C%20S.M.%20Tensorial%20extensions%20of%20independent%20component%20analysis%20for%20multisubject%20FMRI%20analysis%202005"
        },
        {
            "id": "Censor_1981_a",
            "entry": "Y. Censor and A. Lent. An iterative row-action method for interval convex programming. Journal of Optimization Theory and Applications, 34(3):321\u2013353, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Censor%2C%20Y.%20Lent%2C%20A.%20An%20iterative%20row-action%20method%20for%20interval%20convex%20programming%201981",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Censor%2C%20Y.%20Lent%2C%20A.%20An%20iterative%20row-action%20method%20for%20interval%20convex%20programming%201981"
        },
        {
            "id": "Chen_et+al_2018_a",
            "entry": "J. Chen, S. Cheng, H. Xie, L. Wang, and T. Xiang. Equivalence of restricted Boltzmann machines and tensor network states. Physical Review B, 97:085104, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20J.%20Cheng%2C%20S.%20Xie%2C%20H.%20Wang%2C%20L.%20Equivalence%20of%20restricted%20Boltzmann%20machines%20and%20tensor%20network%20states%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20J.%20Cheng%2C%20S.%20Xie%2C%20H.%20Wang%2C%20L.%20Equivalence%20of%20restricted%20Boltzmann%20machines%20and%20tensor%20network%20states%202018"
        },
        {
            "id": "Chi_2012_a",
            "entry": "E. C. Chi and T. G. Kolda. On tensors, sparsity, and nonnegative factorizations. SIAM Journal on Matrix Analysis and Applications, 33(4):1272\u20131299, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chi%2C%20E.C.%20Kolda%2C%20T.G.%20On%20tensors%2C%20sparsity%2C%20and%20nonnegative%20factorizations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chi%2C%20E.C.%20Kolda%2C%20T.G.%20On%20tensors%2C%20sparsity%2C%20and%20nonnegative%20factorizations%202012"
        },
        {
            "id": "Cichocki_et+al_2015_a",
            "entry": "A. Cichocki, D. Mandic, L. De Lathauwer, G. Zhou, Q. Zhao, C. Caiafa, and H. A. Phan. Tensor decompositions for signal processing applications: From two-way to multiway component analysis. IEEE Signal Processing Magazine, 32(2):145\u2013163, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cichocki%2C%20A.%20Mandic%2C%20D.%20Lathauwer%2C%20L.De%20Zhou%2C%20G.%20Tensor%20decompositions%20for%20signal%20processing%20applications%3A%20From%20two-way%20to%20multiway%20component%20analysis%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cichocki%2C%20A.%20Mandic%2C%20D.%20Lathauwer%2C%20L.De%20Zhou%2C%20G.%20Tensor%20decompositions%20for%20signal%20processing%20applications%3A%20From%20two-way%20to%20multiway%20component%20analysis%202015"
        },
        {
            "id": "Davey_2002_a",
            "entry": "B. A. Davey and H. A. Priestley. Introduction to Lattices and Order. Cambridge University Press, 2 edition, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Davey%2C%20B.A.%20Priestley%2C%20H.A.%20Introduction%20to%20Lattices%20and%20Order%202002"
        },
        {
            "id": "Gierz_et+al_2003_a",
            "entry": "G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. Mislove, and D. S. Scott. Continuous Lattices and Domains. Cambridge University Press, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gierz%2C%20G.%20Hofmann%2C%20K.H.%20Keimel%2C%20K.%20Lawson%2C%20J.D.%20Continuous%20Lattices%20and%20Domains%202003"
        },
        {
            "id": "Harshman_1970_a",
            "entry": "R. A. Harshman. Foundations of the PARAFAC procedure: Models and conditions for an \u201cexplanatory\u201d multi-modal factor analysis. Technical report, UCLA Working Papers in Phonetics, 1970.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Harshman%2C%20R.A.%20Foundations%20of%20the%20PARAFAC%20procedure%3A%20Models%20and%20conditions%20for%20an%20%E2%80%9Cexplanatory%E2%80%9D%20multi-modal%20factor%20analysis%201970"
        },
        {
            "id": "Hinton_2002_a",
            "entry": "G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771\u20131800, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.E.%20Training%20products%20of%20experts%20by%20minimizing%20contrastive%20divergence%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.E.%20Training%20products%20of%20experts%20by%20minimizing%20contrastive%20divergence%202002"
        },
        {
            "id": "Kim_2007_a",
            "entry": "Y. D. Kim and S. Choi. Nonnegative Tucker decomposition. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Y.D.%20Choi%2C%20S.%20Nonnegative%20Tucker%20decomposition%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Y.D.%20Choi%2C%20S.%20Nonnegative%20Tucker%20decomposition%202007"
        },
        {
            "id": "Kolda_2009_a",
            "entry": "T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455\u2013500, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kolda%2C%20T.G.%20Bader%2C%20B.W.%20Tensor%20decompositions%20and%20applications%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kolda%2C%20T.G.%20Bader%2C%20B.W.%20Tensor%20decompositions%20and%20applications%202009"
        },
        {
            "id": "Kossaifi_et+al_2016_a",
            "entry": "J. Kossaifi, Y. Panagakis, and M. Pantic. TensorLy: Tensor learning in Python. arXiv:1610.09555, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1610.09555"
        },
        {
            "id": "Lecun_et+al_1998_a",
            "entry": "Y. LeCun, C. Cortes, and C. J. C. Burges. The MNIST database of handwritten digits, 1998. URL http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "Lecun_et+al_2007_a",
            "entry": "Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. J. Huang. A tutorial on energy-based learning. In G. Bakir, T. Hofmann, B. Sch\u00f6lkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan, editors, Predicting Structured Data. The MIT Press, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Y.%20Chopra%2C%20S.%20Hadsell%2C%20R.%20Ranzato%2C%20M.%20A%20tutorial%20on%20energy-based%20learning%202007"
        },
        {
            "id": "Lee_1999_a",
            "entry": "D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788\u2013791, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20D.D.%20Seung%2C%20H.S.%20Learning%20the%20parts%20of%20objects%20by%20non-negative%20matrix%20factorization%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20D.D.%20Seung%2C%20H.S.%20Learning%20the%20parts%20of%20objects%20by%20non-negative%20matrix%20factorization%201999"
        },
        {
            "id": "Lee_2001_a",
            "entry": "D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In Advances in Neural Information processing Systems 13, pages 556\u2013562, 2001.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20D.D.%20Seung%2C%20H.S.%20Algorithms%20for%20non-negative%20matrix%20factorization%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20D.D.%20Seung%2C%20H.S.%20Algorithms%20for%20non-negative%20matrix%20factorization%202001"
        },
        {
            "id": "Liu_et+al_2013_a",
            "entry": "J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):208\u2013220, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20J.%20Musialski%2C%20P.%20Wonka%2C%20P.%20Ye%2C%20J.%20Tensor%20completion%20for%20estimating%20missing%20values%20in%20visual%20data%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20J.%20Musialski%2C%20P.%20Wonka%2C%20P.%20Ye%2C%20J.%20Tensor%20completion%20for%20estimating%20missing%20values%20in%20visual%20data%202013"
        },
        {
            "id": "Nakahara_2002_a",
            "entry": "H. Nakahara and S. Amari. Information-geometric measure for neural spikes. Neural Computation, 14(10):2269\u20132316, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nakahara%2C%20H.%20Amari%2C%20S.%20Information-geometric%20measure%20for%20neural%20spikes%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nakahara%2C%20H.%20Amari%2C%20S.%20Information-geometric%20measure%20for%20neural%20spikes%202002"
        },
        {
            "id": "Salakhutdinov_2008_a",
            "entry": "R. Salakhutdinov. Learning and evaluating Boltzmann machines. UTML TR 2008-002, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Learning%20and%20evaluating%20Boltzmann%20machines%202008"
        },
        {
            "id": "Salakhutdinov_2009_a",
            "entry": "R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics, pages 448\u2013455, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Hinton%2C%20G.E.%20Deep%20Boltzmann%20machines%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Hinton%2C%20G.E.%20Deep%20Boltzmann%20machines%202009"
        },
        {
            "id": "Salakhutdinov_2012_a",
            "entry": "R. Salakhutdinov and G. E. Hinton. An efficient learning procedure for deep Boltzmann machines. Neural Computation, 24(8):1967\u20132006, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Hinton%2C%20G.E.%20An%20efficient%20learning%20procedure%20for%20deep%20Boltzmann%20machines%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Hinton%2C%20G.E.%20An%20efficient%20learning%20procedure%20for%20deep%20Boltzmann%20machines%202012"
        },
        {
            "id": "Salakhutdinov_2008_b",
            "entry": "R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In Proceedings of the 25th International Conference on Machine learning, pages 872\u2013879, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20R.%20Murray%2C%20I.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20R.%20Murray%2C%20I.%20On%20the%20quantitative%20analysis%20of%20deep%20belief%20networks%202008"
        },
        {
            "id": "Sejnowski_1986_a",
            "entry": "T. J. Sejnowski. Higher-order Boltzmann machines. In AIP Conference Proceedings, volume 151, pages 398\u2013403, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sejnowski%2C%20T.J.%20Higher-order%20Boltzmann%20machines%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sejnowski%2C%20T.J.%20Higher-order%20Boltzmann%20machines%201986"
        },
        {
            "id": "Shashua_2005_a",
            "entry": "A. Shashua and T. Hazan. Non-negative tensor factorization with applications to statistics and computer vision. In Proceedings of the 22nd International Conference on Machine Learning, pages 792\u2013799, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shashua%2C%20A.%20Hazan%2C%20T.%20Non-negative%20tensor%20factorization%20with%20applications%20to%20statistics%20and%20computer%20vision%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shashua%2C%20A.%20Hazan%2C%20T.%20Non-negative%20tensor%20factorization%20with%20applications%20to%20statistics%20and%20computer%20vision%202005"
        },
        {
            "id": "Smolensky_1986_a",
            "entry": "P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart, J. L. McClelland, and PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1, pages 194\u2013281. MIT Press, 1986.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smolensky%2C%20P.%20Information%20processing%20in%20dynamical%20systems%3A%20Foundations%20of%20harmony%20theory%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smolensky%2C%20P.%20Information%20processing%20in%20dynamical%20systems%3A%20Foundations%20of%20harmony%20theory%201986"
        },
        {
            "id": "Sugiyama_et+al_2016_a",
            "entry": "M. Sugiyama, H. Nakahara, and K. Tsuda. Information decomposition on structured space. In 2016 IEEE International Symposium on Information Theory, pages 575\u2013579, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20M.%20Nakahara%2C%20H.%20Tsuda%2C%20K.%20Information%20decomposition%20on%20structured%20space%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sugiyama%2C%20M.%20Nakahara%2C%20H.%20Tsuda%2C%20K.%20Information%20decomposition%20on%20structured%20space%202016"
        },
        {
            "id": "Sugiyama_et+al_2017_a",
            "entry": "M. Sugiyama, H. Nakahara, and K. Tsuda. Tensor balancing on statistical manifold. In Proceedings of the 34th International Conference on Machine Learning, pages 3270\u20133279, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sugiyama%2C%20M.%20Nakahara%2C%20H.%20Tsuda%2C%20K.%20Tensor%20balancing%20on%20statistical%20manifold%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sugiyama%2C%20M.%20Nakahara%2C%20H.%20Tsuda%2C%20K.%20Tensor%20balancing%20on%20statistical%20manifold%202017"
        },
        {
            "id": "Symeonidis_2016_a",
            "entry": "P. Symeonidis. Matrix and tensor decomposition in recommender systems. In Proceedings of the 10th ACM Conference on Recommender Systems, pages 429\u2013430, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Symeonidis%2C%20P.%20Matrix%20and%20tensor%20decomposition%20in%20recommender%20systems%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Symeonidis%2C%20P.%20Matrix%20and%20tensor%20decomposition%20in%20recommender%20systems%202016"
        },
        {
            "id": "Tomioka_2013_a",
            "entry": "R. Tomioka and T. Suzuki. Convex tensor decomposition via structured schatten norm regularization. In Advances in Neural Information Processing Systems 26, pages 1331\u20131339, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tomioka%2C%20R.%20Suzuki%2C%20T.%20Convex%20tensor%20decomposition%20via%20structured%20schatten%20norm%20regularization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tomioka%2C%20R.%20Suzuki%2C%20T.%20Convex%20tensor%20decomposition%20via%20structured%20schatten%20norm%20regularization%202013"
        },
        {
            "id": "Tucker_1966_a",
            "entry": "L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3): 279\u2013311, 1966.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tucker%2C%20L.R.%20Some%20mathematical%20notes%20on%20three-mode%20factor%20analysis%201966",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tucker%2C%20L.R.%20Some%20mathematical%20notes%20on%20three-mode%20factor%20analysis%201966"
        },
        {
            "id": "Vasilescu_2002_a",
            "entry": "M. A. O. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: TensorFaces. In Proceedings of The 7th European Conference on Computer Vision (ECCV), volume 2350 of LNCS, pages 447\u2013460, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vasilescu%2C%20M.A.O.%20Terzopoulos%2C%20D.%20Multilinear%20analysis%20of%20image%20ensembles%3A%20TensorFaces%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vasilescu%2C%20M.A.O.%20Terzopoulos%2C%20D.%20Multilinear%20analysis%20of%20image%20ensembles%3A%20TensorFaces%202002"
        },
        {
            "id": "Vasilescu_2007_a",
            "entry": "M. A. O. Vasilescu and D. Terzopoulos. Multilinear (tensor) image synthesis, analysis, and recognition [exploratory dsp]. IEEE Signal Processing Magazine, 24(6):118\u2013123, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vasilescu%2C%20M.A.O.%20Terzopoulos%2C%20D.%20Multilinear%20%28tensor%29%20image%20synthesis%2C%20analysis%2C%20and%20recognition%20%5Bexploratory%20dsp%5D%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vasilescu%2C%20M.A.O.%20Terzopoulos%2C%20D.%20Multilinear%20%28tensor%29%20image%20synthesis%2C%20analysis%2C%20and%20recognition%20%5Bexploratory%20dsp%5D%202007"
        },
        {
            "id": "Y_et+al_2011_a",
            "entry": "K. Y. Y\u0131lmaz, A. T. Cemgil, and U. Simsekli. Generalised coupled tensor factorisation. In Advances in Neural Information Processing Systems 24, pages 2151\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%C4%B1lmaz%2C%20K.Y.%20Cemgil%2C%20A.T.%20Simsekli%2C%20U.%20Generalised%20coupled%20tensor%20factorisation%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%C4%B1lmaz%2C%20K.Y.%20Cemgil%2C%20A.T.%20Simsekli%2C%20U.%20Generalised%20coupled%20tensor%20factorisation%202011"
        },
        {
            "id": "Y_2012_a",
            "entry": "Y. K. Y\u0131lmaz and A. T. Cemgil. Algorithms for probabilistic latent tensor factorization. Signal Processing, 92(8):1853\u20131863, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%C4%B1lmaz%2C%20Y.K.%20Cemgil%2C%20A.T.%20Algorithms%20for%20probabilistic%20latent%20tensor%20factorization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%C4%B1lmaz%2C%20Y.K.%20Cemgil%2C%20A.T.%20Algorithms%20for%20probabilistic%20latent%20tensor%20factorization%202012"
        }
    ]
}
