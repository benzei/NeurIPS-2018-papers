{
    "filename": "8144-the-sample-complexity-of-semi-supervised-learning-with-nonparametric-mixture-models.pdf",
    "metadata": {
        "title": "The Sample Complexity of Semi-Supervised Learning with Nonparametric Mixture Models",
        "author": "Chen Dan, Liu Leqi, Bryon Aragam, Pradeep K. Ravikumar, Eric P. Xing",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8144-the-sample-complexity-of-semi-supervised-learning-with-nonparametric-mixture-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We study the sample complexity of semi-supervised learning (SSL) and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the (unknown) class conditional distributions. Under these assumptions, we establish an \u03a9(K log K) labeled sample complexity bound without imposing parametric assumptions, where K is the number of classes. Our results suggest that even in nonparametric settings it is possible to learn a near-optimal classifier using only a few labeled samples. Unlike previous theoretical work which focuses on binary classification, we consider general multiclass classification (K > 2), which requires solving a difficult permutation learning problem. This permutation defines a classifier whose classification error is controlled by the Wasserstein distance between mixing measures, and we provide finite-sample results characterizing the behaviour of the excess risk of this classifier. Finally, we describe three algorithms for computing these estimators based on a connection to bipartite graph matching, and perform experiments to illustrate the superiority of the MLE over the majority vote estimator."
    },
    "keywords": [
        {
            "term": "wasserstein distance",
            "url": "https://en.wikipedia.org/wiki/wasserstein_distance"
        },
        {
            "term": "cluster assumption",
            "url": "https://en.wikipedia.org/wiki/cluster_assumption"
        },
        {
            "term": "sample complexity",
            "url": "https://en.wikipedia.org/wiki/sample_complexity"
        },
        {
            "term": "supervised learning",
            "url": "https://en.wikipedia.org/wiki/supervised_learning"
        },
        {
            "term": "mixture model",
            "url": "https://en.wikipedia.org/wiki/mixture_model"
        }
    ],
    "highlights": [
        "With the rapid growth of modern datasets and increasingly passive collection of data, labeled data is becoming more and more expensive to obtain while unlabeled data remains cheap and plentiful in many applications",
        "Leveraging unlabeled data to improve the predictions of a machine learning system is the problem of semi-supervised learning (SSL), which has been the source of many empirical successes [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] and theoretical inquiries [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "Motivated by recent work on nonparametric mixture models [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], we study the general case where the true mixture model is not known or even learnable from unlabeled data",
        "Our results allow for arbitrary, possibly heuristic estimators of a mixing measure \u039b that is used to approximate the unlabeled data distribution F \u2217. This mixing measure defines decision boundaries that can be used to define a semi-supervised classifier whose classification accuracy is controlled by the Wasserstein distance between \u039b and \u039b\u2217, the true mixing measure corresponding to the class conditional distributions",
        "Our results allow for arbitrary, possibly heuristic estimators of a mixing measure \u039b that is used to approximate the unlabeled data distribution F \u2217. This mixing measure defines decision boundaries that can be used to define a semi-supervised classifier whose classification accuracy is controlled by the Wasserstein distance between \u039b and \u039b\u2217, the true mixing measure corresponding to the class conditional distributions. This draws an explicit connection between the quality of what is learned from the unlabeled data (i.e. \u039b) and the quality of the resulting classifier",
        "Our experiments convey two main takeaway messages: 1) It pays off to use density information as with the MLE, and 2) When the mixture model learned from the unlabeled data is a poor approximation of the true mixing measure, or the true class conditionals have substantial overlap, the MLE can still learn a reasonable semi-supervised classifier"
    ],
    "key_statements": [
        "With the rapid growth of modern datasets and increasingly passive collection of data, labeled data is becoming more and more expensive to obtain while unlabeled data remains cheap and plentiful in many applications",
        "Leveraging unlabeled data to improve the predictions of a machine learning system is the problem of semi-supervised learning (SSL), which has been the source of many empirical successes [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] and theoretical inquiries [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>, <a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "We propose a new type of assumption that loosely combines ideas from both the identifiability and cluster assumption perspectives",
        "We study the sample complexity and rates of convergence for supervised learning and propose simple algorithms to implement the proposed estimators",
        "Motivated by recent work on nonparametric mixture models [<a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>], we study the general case where the true mixture model is not known or even learnable from unlabeled data",
        "Permutation learning problems have gained notoriety recently owing to their applicability to a wide variety of problems, including statistical matching and seriation [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], graphical models [<a class=\"ref-link\" id=\"c18\" href=\"#r18\">18</a>, <a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>], and regression [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>], so these results may be of independent interest. With these goals in mind, we study the MLE and majority voting (MV) rules for learning the unknown class assignment introduced",
        "Contributions A key aspect of our analysis is to establish conditions that connect the mixture model (1) to the true mixture model. Under these conditions we prove nonasymptotic rates of convergence for learning the class assignment (Figure 1a) from labeled data when K > 2, establish an \u03a9(K log K) sample complexity for learning this assignment, and prove that the resulting classifier converges to the Bayes classifier",
        "Use the unlabeled data to learn a K-component mixture model that approximates F \u2217, which is represented by the mixing measure \u039b defined below; 2",
        "Our results have focused on the probability of recovery of the unknown permutation \u03c0\u2217",
        "In order to evaluate the relative performance of the proposed estimators in practice, we implemented each of the three methods described in Section 5 on simulated and real data. These experiments illustrate the gap \u2206MLE(\u039b)) that appears in Theorem 4.1 (resp",
        "Our results allow for arbitrary, possibly heuristic estimators of a mixing measure \u039b that is used to approximate the unlabeled data distribution F \u2217. This mixing measure defines decision boundaries that can be used to define a semi-supervised classifier whose classification accuracy is controlled by the Wasserstein distance between \u039b and \u039b\u2217, the true mixing measure corresponding to the class conditional distributions",
        "Our results allow for arbitrary, possibly heuristic estimators of a mixing measure \u039b that is used to approximate the unlabeled data distribution F \u2217. This mixing measure defines decision boundaries that can be used to define a semi-supervised classifier whose classification accuracy is controlled by the Wasserstein distance between \u039b and \u039b\u2217, the true mixing measure corresponding to the class conditional distributions. This draws an explicit connection between the quality of what is learned from the unlabeled data (i.e. \u039b) and the quality of the resulting classifier",
        "Our experiments convey two main takeaway messages: 1) It pays off to use density information as with the MLE, and 2) When the mixture model learned from the unlabeled data is a poor approximation of the true mixing measure, or the true class conditionals have substantial overlap, the MLE can still learn a reasonable semi-supervised classifier"
    ],
    "summary": [
        "With the rapid growth of modern datasets and increasingly passive collection of data, labeled data is becoming more and more expensive to obtain while unlabeled data remains cheap and plentiful in many applications.",
        "In a follow-up paper, Castelli and Cover [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>] studied the case where F \u2217 is unknown by assuming that K = 2 and the class conditional densities {f1\u2217, f2\u2217} are known up to a permutation.",
        "We are interested in the general case in which a finite amount of unlabeled data is used to estimate both the mixture weights and densities.",
        "Use the unlabeled data to learn a K-component mixture model that approximates F \u2217, which is represented by the mixing measure \u039b defined below; 2.",
        "This formulates SSL as a coupled mixture modeling and permutation learning problem: Given unlabeled and labeled data, learn a pair (\u039b, \u03c0) which yields a classifier g = g\u039b,\u03c0.",
        "The target classifier is the Bayes classifier, which can be written in the form (3): Let \u039b\u2217 denote the true mixing measure that assigns probability \u03bb\u2217k to the density fk\u2217 and note that F \u2217 = m(\u039b\u2217), which is the true mixture model defined previously.",
        "The interpretation of this theorem is as follows: Given \u039b, we learn a permutation \u03c0 = \u03c0n(\u039b) from n labeled samples, e.g. using either the MLE (4) or MV (5).",
        "Corollary 4.8 implies that with high probability, we can learn a Bayes classifier for the problem using finitely many labeled samples.",
        "These experiments illustrate the gap \u2206MLE(\u039b)) that appears in Theorem 4.1: In many of the examples, the learned mixture is badly misspecified and the true class conditionals overlap significantly, it is still possible to recover \u03c0\u2217 with fewer than 100 labeled samples.",
        "These results illustrate how the gaps \u2206MLE and \u2206MV affect learning \u03c0\u2217: Even when W1(\u039b, \u039b\u2217) is large, the labeled sample complexity is relatively small.",
        "Our results allow for arbitrary, possibly heuristic estimators of a mixing measure \u039b that is used to approximate the unlabeled data distribution F \u2217.",
        "This mixing measure defines decision boundaries that can be used to define a semi-supervised classifier whose classification accuracy is controlled by the Wasserstein distance between \u039b and \u039b\u2217, the true mixing measure corresponding to the class conditional distributions.",
        "Our experiments convey two main takeaway messages: 1) It pays off to use density information as with the MLE, and 2) When the mixture model learned from the unlabeled data is a poor approximation of the true mixing measure, or the true class conditionals have substantial overlap, the MLE can still learn a reasonable semi-supervised classifier."
    ],
    "headline": "We study the sample complexity of semi-supervised learning  and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the  class conditional distributions",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92\u2013100. ACM, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Mitchell%2C%20Tom%20Combining%20labeled%20and%20unlabeled%20data%20with%20co-training%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Mitchell%2C%20Tom%20Combining%20labeled%20and%20unlabeled%20data%20with%20co-training%201998"
        },
        {
            "id": "2",
            "entry": "[2] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semisupervised learning with deep generative models. In Advances in Neural Information Processing Systems, pages 3581\u20133589, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semisupervised%20learning%20with%20deep%20generative%20models%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kingma%2C%20Diederik%20P.%20Mohamed%2C%20Shakir%20Rezende%2C%20Danilo%20Jimenez%20Welling%2C%20Max%20Semisupervised%20learning%20with%20deep%20generative%20models%202014"
        },
        {
            "id": "3",
            "entry": "[3] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semi-supervised learning that requires a bad gan. In Advances in Neural Information Processing Systems, pages 6513\u20136523, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Zihang%20Yang%2C%20Zhilin%20Yang%2C%20Fan%20Cohen%2C%20William%20W.%20Good%20semi-supervised%20learning%20that%20requires%20a%20bad%20gan%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Zihang%20Yang%2C%20Zhilin%20Yang%2C%20Fan%20Cohen%2C%20William%20W.%20Good%20semi-supervised%20learning%20that%20requires%20a%20bad%20gan%202017"
        },
        {
            "id": "4",
            "entry": "[4] Martin Azizyan, Aarti Singh, Larry Wasserman, et al. Density-sensitive semisupervised inference. The Annals of Statistics, 41(2):751\u2013771, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Azizyan%2C%20Martin%20Singh%2C%20Aarti%20Wasserman%2C%20Larry%20Density-sensitive%20semisupervised%20inference%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Azizyan%2C%20Martin%20Singh%2C%20Aarti%20Wasserman%2C%20Larry%20Density-sensitive%20semisupervised%20inference%202013"
        },
        {
            "id": "5",
            "entry": "[5] Vittorio Castelli and Thomas M Cover. On the exponential value of labeled samples. Pattern Recognition Letters, 16(1):105\u2013111, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Castelli%2C%20Vittorio%20Cover%2C%20Thomas%20M.%20On%20the%20exponential%20value%20of%20labeled%20samples%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Castelli%2C%20Vittorio%20Cover%2C%20Thomas%20M.%20On%20the%20exponential%20value%20of%20labeled%20samples%201995"
        },
        {
            "id": "6",
            "entry": "[6] Vittorio Castelli and Thomas M Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. IEEE Transactions on information theory, 42(6):2102\u20132117, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Castelli%2C%20Vittorio%20Cover%2C%20Thomas%20M.%20The%20relative%20value%20of%20labeled%20and%20unlabeled%20samples%20in%20pattern%20recognition%20with%20an%20unknown%20mixing%20parameter%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Castelli%2C%20Vittorio%20Cover%2C%20Thomas%20M.%20The%20relative%20value%20of%20labeled%20and%20unlabeled%20samples%20in%20pattern%20recognition%20with%20an%20unknown%20mixing%20parameter%201996"
        },
        {
            "id": "7",
            "entry": "[7] Fabio G Cozman, Ira Cohen, and Marcelo C Cirelo. Semi-supervised learning of mixture models. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 99\u2013106, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cozman%2C%20Fabio%20G.%20Cohen%2C%20Ira%20Cirelo%2C%20Marcelo%20C.%20Semi-supervised%20learning%20of%20mixture%20models%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cozman%2C%20Fabio%20G.%20Cohen%2C%20Ira%20Cirelo%2C%20Marcelo%20C.%20Semi-supervised%20learning%20of%20mixture%20models%202003"
        },
        {
            "id": "8",
            "entry": "[8] Matti K\u00e4\u00e4ri\u00e4inen. Generalization error bounds using unlabeled data. In International Conference on Computational Learning Theory, pages 127\u2013142.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=K%C3%A4%C3%A4ri%C3%A4inen%2C%20Matti%20Generalization%20error%20bounds%20using%20unlabeled%20data",
            "oa_query": "https://api.scholarcy.com/oa_version?query=K%C3%A4%C3%A4ri%C3%A4inen%2C%20Matti%20Generalization%20error%20bounds%20using%20unlabeled%20data"
        },
        {
            "id": "9",
            "entry": "[9] Partha Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses. The Journal of Machine Learning Research, 14(1):1229\u20131250, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Niyogi%2C%20Partha%20Manifold%20regularization%20and%20semi-supervised%20learning%3A%20Some%20theoretical%20analyses%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Niyogi%2C%20Partha%20Manifold%20regularization%20and%20semi-supervised%20learning%3A%20Some%20theoretical%20analyses%202013"
        },
        {
            "id": "10",
            "entry": "[10] Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster assumption. Journal of Machine Learning Research, 8(Jul):1369\u20131392, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rigollet%2C%20Philippe%20Generalization%20error%20bounds%20in%20semi-supervised%20classification%20under%20the%20cluster%20assumption%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rigollet%2C%20Philippe%20Generalization%20error%20bounds%20in%20semi-supervised%20classification%20under%20the%20cluster%20assumption%202007"
        },
        {
            "id": "11",
            "entry": "[11] Aarti Singh, Robert Nowak, and Xiaojin Zhu. Unlabeled data: Now it helps, now it doesn\u2019t. In Advances in neural information processing systems, pages 1513\u20131520, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Singh%2C%20Aarti%20Nowak%2C%20Robert%20Zhu%2C%20Xiaojin%20Unlabeled%20data%3A%20Now%20it%20helps%2C%20now%20it%20doesn%E2%80%99t%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Singh%2C%20Aarti%20Nowak%2C%20Robert%20Zhu%2C%20Xiaojin%20Unlabeled%20data%3A%20Now%20it%20helps%2C%20now%20it%20doesn%E2%80%99t%202009"
        },
        {
            "id": "12",
            "entry": "[12] Larry Wasserman and John D Lafferty. Statistical analysis of semi-supervised regression. In Advances in Neural Information Processing Systems, pages 801\u2013808, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wasserman%2C%20Larry%20Lafferty%2C%20John%20D.%20Statistical%20analysis%20of%20semi-supervised%20regression%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wasserman%2C%20Larry%20Lafferty%2C%20John%20D.%20Statistical%20analysis%20of%20semi-supervised%20regression%202008"
        },
        {
            "id": "13",
            "entry": "[13] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912\u2013919, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Xiaojin%20Ghahramani%2C%20Zoubin%20Lafferty%2C%20John%20D.%20Semi-supervised%20learning%20using%20gaussian%20fields%20and%20harmonic%20functions%202003"
        },
        {
            "id": "14",
            "entry": "[14] Bryon Aragam, Chen Dan, Pradeep Ravikumar, and Eric Xing. Identifiability of nonparametric mixture models and bayes optimal clustering. arXiv preprint, arXiv:1802.04397, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.04397"
        },
        {
            "id": "15",
            "entry": "[15] Olivier Collier and Arnak S Dalalyan. Minimax rates in permutation estimation for feature matching. The Journal of Machine Learning Research, 17(1):162\u2013192, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collier%2C%20Olivier%20Dalalyan%2C%20Arnak%20S.%20Minimax%20rates%20in%20permutation%20estimation%20for%20feature%20matching%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collier%2C%20Olivier%20Dalalyan%2C%20Arnak%20S.%20Minimax%20rates%20in%20permutation%20estimation%20for%20feature%20matching%202016"
        },
        {
            "id": "16",
            "entry": "[16] Fajwel Fogel, Rodolphe Jenatton, Francis Bach, and Alexandre d\u2019Aspremont. Convex relaxations for permutation problems. In Advances in Neural Information Processing Systems, pages 1016\u20131024, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fogel%2C%20Fajwel%20Jenatton%2C%20Rodolphe%20Bach%2C%20Francis%20d%E2%80%99Aspremont%2C%20Alexandre%20Convex%20relaxations%20for%20permutation%20problems%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fogel%2C%20Fajwel%20Jenatton%2C%20Rodolphe%20Bach%2C%20Francis%20d%E2%80%99Aspremont%2C%20Alexandre%20Convex%20relaxations%20for%20permutation%20problems%202013"
        },
        {
            "id": "17",
            "entry": "[17] Cong Han Lim and Stephen Wright. Beyond the birkhoff polytope: Convex relaxations for vector permutation problems. In Advances in Neural Information Processing Systems, pages 2168\u20132176, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lim%2C%20Cong%20Han%20Wright%2C%20Stephen%20Beyond%20the%20birkhoff%20polytope%3A%20Convex%20relaxations%20for%20vector%20permutation%20problems%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lim%2C%20Cong%20Han%20Wright%2C%20Stephen%20Beyond%20the%20birkhoff%20polytope%3A%20Convex%20relaxations%20for%20vector%20permutation%20problems%202014"
        },
        {
            "id": "18",
            "entry": "[18] Sara van de Geer and Peter B\u00fchlmann. 0-penalized maximum likelihood for sparse directed acyclic graphs. Annals of Statistics, 41(2):536\u2013567, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20de%20Geer%2C%20Sara%20B%C3%BChlmann%2C%20Peter%200-penalized%20maximum%20likelihood%20for%20sparse%20directed%20acyclic%20graphs%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20de%20Geer%2C%20Sara%20B%C3%BChlmann%2C%20Peter%200-penalized%20maximum%20likelihood%20for%20sparse%20directed%20acyclic%20graphs%202013"
        },
        {
            "id": "19",
            "entry": "[19] Bryon Aragam, Arash A. Amini, and Qing Zhou. Learning directed acyclic graphs with penalized neighbourhood regression. arXiv:1511.08963, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1511.08963"
        },
        {
            "id": "20",
            "entry": "[20] Ashwin Pananjady, Martin J Wainwright, and Thomas A Courtade. Linear regression with an unknown permutation: Statistical and computational limits. In Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on, pages 417\u2013424. IEEE, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ashwin%20Pananjady%2C%20Martin%20J%20Wainwright%2C%20and%20Thomas%20A%20Courtade.%20Linear%20regression%20with%20an%20unknown%20permutation%3A%20Statistical%20and%20computational%20limits%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ashwin%20Pananjady%2C%20Martin%20J%20Wainwright%2C%20and%20Thomas%20A%20Courtade.%20Linear%20regression%20with%20an%20unknown%20permutation%3A%20Statistical%20and%20computational%20limits%202016"
        },
        {
            "id": "21",
            "entry": "[21] Nicolas Flammarion, Cheng Mao, and Philippe Rigollet. Optimal rates of statistical seriation. arXiv preprint arXiv:1607.02435, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02435"
        },
        {
            "id": "22",
            "entry": "[22] Matthias Seeger. Learning with labeled and unlabeled data. Technical report, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20Matthias%20Learning%20with%20labeled%20and%20unlabeled%20data%202000"
        },
        {
            "id": "23",
            "entry": "[23] XuanLong Nguyen. Convergence of latent mixing measures in finite and infinite mixture models. The Annals of Statistics, 41(1):370\u2013400, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20XuanLong%20Convergence%20of%20latent%20mixing%20measures%20in%20finite%20and%20infinite%20mixture%20models%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20XuanLong%20Convergence%20of%20latent%20mixing%20measures%20in%20finite%20and%20infinite%20mixture%20models%202013"
        },
        {
            "id": "24",
            "entry": "[24] Henry Teicher. Identifiability of mixtures. The annals of Mathematical statistics, 32(1):244\u2013248, 1961.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teicher%2C%20Henry%20Identifiability%20of%20mixtures.%20The%20annals%20of%201961",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teicher%2C%20Henry%20Identifiability%20of%20mixtures.%20The%20annals%20of%201961"
        },
        {
            "id": "25",
            "entry": "[25] Henry Teicher. Identifiability of finite mixtures. The annals of Mathematical statistics, pages 1265\u20131269, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teicher%2C%20Henry%20Identifiability%20of%20finite%20mixtures.%20The%20annals%20of%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teicher%2C%20Henry%20Identifiability%20of%20finite%20mixtures.%20The%20annals%20of%201963"
        },
        {
            "id": "26",
            "entry": "[26] Sidney J Yakowitz and John D Spragins. On the identifiability of finite mixtures. The Annals of Mathematical Statistics, pages 209\u2013214, 1968.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yakowitz%2C%20Sidney%20J.%20Spragins%2C%20John%20D.%20On%20the%20identifiability%20of%20finite%20mixtures%201968",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yakowitz%2C%20Sidney%20J.%20Spragins%2C%20John%20D.%20On%20the%20identifiability%20of%20finite%20mixtures%201968"
        },
        {
            "id": "27",
            "entry": "[27] O Barndorff-Nielsen. Identifiability of mixtures of exponential families. Journal of Mathematical Analysis and Applications, 12(1):115\u2013121, 1965.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Barndorff-Nielsen%2C%20O.%20Identifiability%20of%20mixtures%20of%20exponential%20families%201965",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Barndorff-Nielsen%2C%20O.%20Identifiability%20of%20mixtures%20of%20exponential%20families%201965"
        },
        {
            "id": "28",
            "entry": "[28] Henry Teicher. Identifiability of mixtures of product measures. The Annals of Mathematical Statistics, 38(4):1300\u20131302, 1967.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Teicher%2C%20Henry%20Identifiability%20of%20mixtures%20of%20product%20measures%201967",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Teicher%2C%20Henry%20Identifiability%20of%20mixtures%20of%20product%20measures%201967"
        },
        {
            "id": "29",
            "entry": "[29] Peter Hall and Xiao-Hua Zhou. Nonparametric estimation of component distributions in a multivariate mixture. Annals of Statistics, pages 201\u2013224, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hall%2C%20Peter%20Zhou%2C%20Xiao-Hua%20Nonparametric%20estimation%20of%20component%20distributions%20in%20a%20multivariate%20mixture%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hall%2C%20Peter%20Zhou%2C%20Xiao-Hua%20Nonparametric%20estimation%20of%20component%20distributions%20in%20a%20multivariate%20mixture%202003"
        },
        {
            "id": "30",
            "entry": "[30] Donald J Newman. The double dixie cup problem. The American Mathematical Monthly, 67 (1):58\u201361, 1960.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Newman%2C%20Donald%20J.%20The%20double%20dixie%20cup%20problem%201960",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Newman%2C%20Donald%20J.%20The%20double%20dixie%20cup%20problem%201960"
        },
        {
            "id": "31",
            "entry": "[31] Philippe Flajolet, Daniele Gardy, and Lo\u00ffs Thimonier. Birthday paradox, coupon collectors, caching algorithms and self-organizing search. Discrete Applied Mathematics, 39(3):207\u2013229, 1992.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flajolet%2C%20Philippe%20Gardy%2C%20Daniele%20Thimonier%2C%20Lo%C3%BFs%20Birthday%20paradox%2C%20coupon%20collectors%2C%20caching%20algorithms%20and%20self-organizing%20search%201992",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flajolet%2C%20Philippe%20Gardy%2C%20Daniele%20Thimonier%2C%20Lo%C3%BFs%20Birthday%20paradox%2C%20coupon%20collectors%2C%20caching%20algorithms%20and%20self-organizing%20search%201992"
        },
        {
            "id": "32",
            "entry": "[32] Philippe Heinrich and Jonas Kahn. Minimax rates for finite mixture estimation. arXiv preprint arXiv:1504.03506, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1504.03506"
        },
        {
            "id": "33",
            "entry": "[33] Nhat Ho and XuanLong Nguyen. On strong identifiability and convergence rates of parameter estimation in finite mixtures. Electronic Journal of Statistics, 10(1):271\u2013307, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ho%2C%20Nhat%20Nguyen%2C%20XuanLong%20On%20strong%20identifiability%20and%20convergence%20rates%20of%20parameter%20estimation%20in%20finite%20mixtures%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ho%2C%20Nhat%20Nguyen%2C%20XuanLong%20On%20strong%20identifiability%20and%20convergence%20rates%20of%20parameter%20estimation%20in%20finite%20mixtures%202016"
        },
        {
            "id": "34",
            "entry": "[34] Nhat Ho and XuanLong Nguyen. Singularity structures and impacts on parameter estimation in finite mixtures of distributions. arXiv preprint arXiv:1609.02655, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.02655"
        },
        {
            "id": "35",
            "entry": "[35] Jiahua Chen. Optimal rate of convergence for finite mixture models. Annals of Statistics, pages 221\u2013233, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jiahua%20Optimal%20rate%20of%20convergence%20for%20finite%20mixture%20models%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Jiahua%20Optimal%20rate%20of%20convergence%20for%20finite%20mixture%20models%201995"
        },
        {
            "id": "36",
            "entry": "[36] Harold W Kuhn. The hungarian method for the assignment problem. Naval Research Logistics (NRL), 2(1-2):83\u201397, 1955.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuhn%2C%20Harold%20W.%20The%20hungarian%20method%20for%20the%20assignment%20problem%201955",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuhn%2C%20Harold%20W.%20The%20hungarian%20method%20for%20the%20assignment%20problem%201955"
        },
        {
            "id": "37",
            "entry": "[37] Luc Devroye, L\u00e1szl\u00f3 Gy\u00f6rfi, and G\u00e1bor Lugosi. A probabilistic theory of pattern recognition, volume 31. Springer Science &amp; Business Media, 2013. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luc%20Devroye%2C%20L%C3%A1szl%C3%B3%20Gy%C3%B6rfi%20Lugosi%2C%20G%C3%A1bor%20A%20probabilistic%20theory%20of%20pattern%20recognition%202013"
        }
    ]
}
