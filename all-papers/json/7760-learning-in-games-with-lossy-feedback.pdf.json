{
    "filename": "7760-learning-in-games-with-lossy-feedback.pdf",
    "metadata": {
        "title": "Learning in Games with Lossy Feedback",
        "author": "Zhengyuan Zhou, Panayotis Mertikopoulos, Susan Athey, Nicholas Bambos, Peter W. Glynn, Yinyu Ye",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7760-learning-in-games-with-lossy-feedback.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider a game-theoretical multi-agent learning problem where the feedback information can be lost during the learning process and rewards are given by a broad class of games known as variationally stable games. We propose a simple variant of the classical online gradient descent algorithm, called reweighted online gradient descent (ROGD) and show that in variationally stable games, if each agent adopts ROGD, then almost sure convergence to the set of Nash equilibria is guaranteed, even when the feedback loss is asynchronous and arbitrarily corrrelated among agents. We then extend the framework to deal with unknown feedback loss probabilities by using an estimator (constructed from past data) in its replacement. Finally, we further extend the framework to accomodate both asynchronous loss and stochastic rewards and establish that multi-agent ROGD learning still converges to the set of Nash equilibria in such settings. Together, these results contribute to the broad lanscape of multi-agent online learning by significantly relaxing the feedback information that is required to achieve desirable outcomes."
    },
    "keywords": [
        {
            "term": "imperfect information",
            "url": "https://en.wikipedia.org/wiki/imperfect_information"
        },
        {
            "term": "nash equilibria",
            "url": "https://en.wikipedia.org/wiki/nash_equilibria"
        },
        {
            "term": "NIPS",
            "url": "https://en.wikipedia.org/wiki/NIPS"
        },
        {
            "term": "learning process",
            "url": "https://en.wikipedia.org/wiki/learning_process"
        }
    ],
    "highlights": [
        "In multi-agent online learning [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>], a set of agents repeatedly interact with the environment and each other while accumulating rewards in an online manner",
        "Theorem 4.3 that, in this asynchronous feedback loss setting, the sequence of play induced by joint Reweighted Online Gradient Descent converges almost surely to Nash equilibria in variationally stable games",
        "In the presence of feedback loss, the convergence of multi-agent OGD given in Algorithm 2 to Nash equilibria cannot be guaranteed in variationally stable games",
        "We establish that when agents\u2019 rewards come from a variationally stable game, multiagent Reweighted Online Gradient Descent converges to the set of Nash equilibria almost surely under asynchronous feedback loss",
        "We have provided an algorithmic framework to deal with multi-agent online learning under feedback loss and obtained broad convergence-to-Nash results under fairly general settings",
        "In the presence of a single agent, such problems have been studied in the context of offline policy learning [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] and online bandits [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>]"
    ],
    "key_statements": [
        "In multi-agent online learning [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>], a set of agents repeatedly interact with the environment and each other while accumulating rewards in an online manner",
        "There is a fruitful line of existing literature providing a rich source of online learning algorithms to minimize the standard performance metric known as regret [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>], defined as RegATlg = maxai\u2208A Tt=1{rit \u2212 rit} with the sequence of actions ati generated by some online learning algorithm Alg",
        "The recent paper [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] studied a two-player continuous zero-sum game, and showed that if both players adopt a no-regret learning algorithm, the empirical time-average of the joint action converges to Nash equilibria",
        "Theorem 4.3 that, in this asynchronous feedback loss setting, the sequence of play induced by joint Reweighted Online Gradient Descent converges almost surely to Nash equilibria in variationally stable games",
        "We mention that a very recent work that studies multi-agent online learning under imperfect information at the generality of variationally stable games is [<a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>]",
        "These results not only make meaningful progress towards the challenging open problem of convergence of no-regret algorithms to Nash in general continuous games under perfect information, but more importantly, contribute to the broad lanscape of multi-agent online learning under imperfect information.\n2 Problem setup\n2.1",
        "In the presence of feedback loss, the convergence of multi-agent OGD given in Algorithm 2 to Nash equilibria cannot be guaranteed in variationally stable games",
        "We present a simple modification of the vanilla multi-agent OGD that will later be shown to converge to Nash equilibria, even in the presence of asynchronous delays",
        "We establish that when agents\u2019 rewards come from a variationally stable game, multiagent Reweighted Online Gradient Descent converges to the set of Nash equilibria almost surely under asynchronous feedback loss",
        "We have provided an algorithmic framework to deal with multi-agent online learning under feedback loss and obtained broad convergence-to-Nash results under fairly general settings",
        "In the presence of a single agent, such problems have been studied in the context of offline policy learning [<a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>, <a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>] and online bandits [<a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>]"
    ],
    "summary": [
        "In multi-agent online learning [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>, <a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>], a set of agents repeatedly interact with the environment and each other while accumulating rewards in an online manner.",
        "Establishing joint convergence to Nash equilibria under no-regret learning algorithms (OGD included) for a broad and meaningful subclass of games has been a challenging ongoing effort.",
        "The recent paper [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] studied a two-player continuous zero-sum game, and showed that if both players adopt a no-regret learning algorithm, the empirical time-average of the joint action converges to Nash equilibria.",
        "We go a step further and consider an even more general multi-agent online learning problem, where we allow agents to have asynchronous gradient feedback losses.",
        "Theorem 4.3 that, in this asynchronous feedback loss setting, the sequence of play induced by joint ROGD converges almost surely to Nash equilibria in variationally stable games.",
        "We mention that a very recent work that studies multi-agent online learning under imperfect information at the generality of variationally stable games is [<a class=\"ref-link\" id=\"c53\" href=\"#r53\">53</a>].",
        "These results not only make meaningful progress towards the challenging open problem of convergence of no-regret algorithms to Nash in general continuous games under perfect information, but more importantly, contribute to the broad lanscape of multi-agent online learning under imperfect information.",
        "We consider a general continuous game model for the rewards in multi-agent online learning: Definition 2.1.",
        "In the presence of feedback loss, the convergence of multi-agent OGD given in Algorithm 2 to Nash equilibria cannot be guaranteed in variationally stable games.",
        "We shall see that as a corollary, multi-agent OGD will converge to the set of Nash equilibria if the feedback loss are synchronous.",
        "To deal with asynchronous feedback loss, we give a new algorithm called Reweighted Online Gradient Descent (ROGD) for the multi-agent learning problem.",
        "This results in reweighted online gradient descent: Algorithm 3: Multi-Agent ROGD Learning under Asynchronous Feedback Loss",
        "We establish that when agents\u2019 rewards come from a variationally stable game, multiagent ROGD converges to the set of Nash equilibria almost surely under asynchronous feedback loss.",
        "Under the same setup as in Theorem 4.3, if feedback loss is synchronous on average, multi-agent OGD in Algorithm 2 converges almost surely to A\u2217 in last iterate.",
        "Under the same setup as in Theorem 4.3, if there is no feedback loss, multi-agent OGD in Algorithm 2 converges to A\u2217 in last iterate.",
        "We have provided an algorithmic framework to deal with multi-agent online learning under feedback loss and obtained broad convergence-to-Nash results under fairly general settings.",
        "The multi-agent learning setting is again under-explored; we leave that for future work"
    ],
    "headline": "We propose a simple variant of the classical online gradient descent algorithm, called reweighted online gradient descent  and show that in variationally stable games, if each agent adopts Reweighted Online Gradient Descent, almost sure convergence to the set of Nash equilibria is guaranteed, even when the feedback loss is asynchronous and arbitrarily corrrelated among agents",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. ANAND, A. GROVER, P. SINGLA, ET AL., Asap-uct: Abstraction of state-action pairs in uct, in Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015. , A novel abstraction framework for online planning, in Proceedings of the 2015 International",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ANAND%2C%20A.%20GROVER%2C%20A.%20SINGLA%2C%20P.%20AL%2C%20E.T.%20Asap-uct%3A%20Abstraction%20of%20state-action%20pairs%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ANAND%2C%20A.%20GROVER%2C%20A.%20SINGLA%2C%20P.%20AL%2C%20E.T.%20Asap-uct%3A%20Abstraction%20of%20state-action%20pairs%202015"
        },
        {
            "id": "Agents_2015_a",
            "entry": "Agents and Multiagent Systems, 2015, pp. 1901\u20131902.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agents%20and%20Multiagent%20Systems%202015%20pp%2019011902"
        },
        {
            "id": "3",
            "entry": "[3] S. ARORA, E. HAZAN, AND S. KALE, The multiplicative weights update method: a meta-algorithm and applications., Theory of Computing, 8 (2012), pp. 121\u2013164.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ARORA%2C%20S.%20HAZAN%2C%20E.%20KALE%2C%20S.%20The%20multiplicative%20weights%20update%20method%3A%20a%20meta-algorithm%20and%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ARORA%2C%20S.%20HAZAN%2C%20E.%20KALE%2C%20S.%20The%20multiplicative%20weights%20update%20method%3A%20a%20meta-algorithm%20and%202012"
        },
        {
            "id": "4",
            "entry": "[4] P. AUER, N. CESA-BIANCHI, Y. FREUND, AND R. E. SCHAPIRE, The nonstochastic multiarmed bandit problem, SIAM journal on computing, 32 (2002), pp. 48\u201377.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=AUER%2C%20P.%20CESA-BIANCHI%2C%20N.%20FREUND%2C%20Y.%20SCHAPIRE%2C%20R.E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002",
            "oa_query": "https://api.scholarcy.com/oa_version?query=AUER%2C%20P.%20CESA-BIANCHI%2C%20N.%20FREUND%2C%20Y.%20SCHAPIRE%2C%20R.E.%20The%20nonstochastic%20multiarmed%20bandit%20problem%202002"
        },
        {
            "id": "5",
            "entry": "[5] M. BALANDAT, W. KRICHENE, C. TOMLIN, AND A. BAYEN, Minimizing regret on reflexive banach spaces and learning nash equilibria in continuous zero-sum games, arXiv preprint arXiv:1606.01261, (2016).",
            "arxiv_url": "https://arxiv.org/pdf/1606.01261"
        },
        {
            "id": "6",
            "entry": "[6] D. BERTSIMAS AND N. KALLUS, From predictive to prescriptive analytics, arXiv preprint arXiv:1402.5481, (2014).",
            "arxiv_url": "https://arxiv.org/pdf/1402.5481"
        },
        {
            "id": "7",
            "entry": "[7] S. BERVOETS, M. BRAVO, AND M. FAURE, Learning with minimal information in continuous games. https://arxiv.org/abs/1806.11506, 2018.",
            "url": "https://arxiv.org/abs/1806.11506",
            "arxiv_url": "https://arxiv.org/pdf/1806.11506"
        },
        {
            "id": "8",
            "entry": "[8] D. BLOEMBERGEN, K. TUYLS, D. HENNES, AND M. KAISERS, Evolutionary dynamics of multi-agent learning: a survey, Journal of Artificial Intelligence Research, 53 (2015), pp. 659\u2013697.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BLOEMBERGEN%2C%20D.%20TUYLS%2C%20K.%20HENNES%2C%20D.%20KAISERS%2C%20M.%20Evolutionary%20dynamics%20of%20multi-agent%20learning%3A%20a%20survey%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BLOEMBERGEN%2C%20D.%20TUYLS%2C%20K.%20HENNES%2C%20D.%20KAISERS%2C%20M.%20Evolutionary%20dynamics%20of%20multi-agent%20learning%3A%20a%20survey%202015"
        },
        {
            "id": "9",
            "entry": "[9] A. BLUM, On-line algorithms in machine learning, in Online algorithms, Springer, 1998, pp. 306\u2013325.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BLUM%2C%20A.%20On-line%20algorithms%20in%20machine%20learning%2C%20in%20Online%20algorithms%201998"
        },
        {
            "id": "10",
            "entry": "[10] A. BLUM AND Y. MANSOUR, From external to internal regret, Journal of Machine Learning Research, 8 (2007), pp. 1307\u20131324.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BLUM%2C%20A.%20MANSOUR%2C%20Y.%20From%20external%20to%20internal%20regret%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BLUM%2C%20A.%20MANSOUR%2C%20Y.%20From%20external%20to%20internal%20regret%202007"
        },
        {
            "id": "11",
            "entry": "[11] M. BRAVO, D. S. LESLIE, AND P. MERTIKOPOULOS, Bandit learning in concave N -person games, in NIPS \u201918: Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BRAVO%2C%20M.%20LESLIE%2C%20D.S.%20MERTIKOPOULOS%2C%20P.%20Bandit%20learning%20in%20concave%20N%20-person%20games%2C%20in%20NIPS%E2%80%99%2018%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BRAVO%2C%20M.%20LESLIE%2C%20D.S.%20MERTIKOPOULOS%2C%20P.%20Bandit%20learning%20in%20concave%20N%20-person%20games%2C%20in%20NIPS%E2%80%99%2018%202018"
        },
        {
            "id": "12",
            "entry": "[12] S. BUBECK, N. CESA-BIANCHI, ET AL., Regret analysis of stochastic and nonstochastic multi-armed bandit problems, Foundations and Trends R in Machine Learning, 5 (2012), pp. 1\u2013122.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BUBECK%2C%20S.%20CESA-BIANCHI%2C%20N.%20AL%2C%20E.T.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%2C%20Foundations%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=BUBECK%2C%20S.%20CESA-BIANCHI%2C%20N.%20AL%2C%20E.T.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%2C%20Foundations%202012"
        },
        {
            "id": "13",
            "entry": "[13] L. BUSONIU, R. BABUSKA, AND B. DE SCHUTTER, Multi-agent reinforcement learning: An overview, in Innovations in multi-agent systems and applications-1, Springer, 2010, pp. 183\u2013221.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=BUSONIU%2C%20L.%20BABUSKA%2C%20R.%20SCHUTTER%2C%20B.D.E.%20Multi-agent%20reinforcement%20learning%3A%20An%20overview%2C%20in%20Innovations%20in%20multi-agent%20systems%20and%20applications-1%202010"
        },
        {
            "id": "14",
            "entry": "[14] N. CESA-BIANCHI AND G. LUGOSI, Prediction, learning, and games, Cambridge university press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=CESA-BIANCHI%2C%20N.%20LUGOSI%2C%20G.%20Prediction%2C%20learning%2C%20and%20games%202006"
        },
        {
            "id": "15",
            "entry": "[15] J. COHEN, A. H\u00c9LIOU, AND P. MERTIKOPOULOS, Learning with bandit feedback in potential games, in NIPS \u201917: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=COHEN%2C%20J.%20H%C3%89LIOU%2C%20A.%20MERTIKOPOULOS%2C%20P.%20Learning%20with%20bandit%20feedback%20in%20potential%20games%2C%20in%20NIPS%E2%80%99%2017%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=COHEN%2C%20J.%20H%C3%89LIOU%2C%20A.%20MERTIKOPOULOS%2C%20P.%20Learning%20with%20bandit%20feedback%20in%20potential%20games%2C%20in%20NIPS%E2%80%99%2017%202017"
        },
        {
            "id": "16",
            "entry": "[16] P. DE SOUZA AND J. SILVA, Berkeley Problems in Mathematics, Problem Books in Mathematics, Springer New York, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SOUZA%2C%20P.D.E.%20SILVA%2C%20J.%20Berkeley%20Problems%20in%20Mathematics%2C%20Problem%20Books%20in%20Mathematics%202012"
        },
        {
            "id": "17",
            "entry": "[17] M. DIMAKOPOULOU AND B. VAN ROY, Coordinated exploration in concurrent reinforcement learning, arXiv preprint arXiv:1802.01282, (2018).",
            "arxiv_url": "https://arxiv.org/pdf/1802.01282"
        },
        {
            "id": "18",
            "entry": "[18] D. FUDENBERG AND D. K. LEVINE, The Theory of Learning in Games, vol. 2 of Economic learning and social evolution, MIT Press, Cambridge, MA, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=FUDENBERG%2C%20D.%20LEVINE%2C%20D.K.%20The%20Theory%20of%20Learning%20in%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=FUDENBERG%2C%20D.%20LEVINE%2C%20D.K.%20The%20Theory%20of%20Learning%20in%201998"
        },
        {
            "id": "19",
            "entry": "[19] I. GOODFELLOW, Y. BENGIO, AND A. COURVILLE, Deep Learning, Adaptive computation and machine learning, MIT Press, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GOODFELLOW%2C%20I.%20BENGIO%2C%20Y.%20COURVILLE%2C%20A.%20Learning%2C%20Deep%20Adaptive%20computation%20and%20machine%20learning%202016"
        },
        {
            "id": "20",
            "entry": "[20] A. GROVER, M. AL-SHEDIVAT, J. K. GUPTA, Y. BURDA, AND H. EDWARDS, Evaluating generalization in multiagent systems using agent-interaction graphs, in International Conference on Autonomous Agents and Multiagent Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GROVER%2C%20A.%20AL-SHEDIVAT%2C%20M.%20GUPTA%2C%20J.K.%20BURDA%2C%20Y.%20Evaluating%20generalization%20in%20multiagent%20systems%20using%20agent-interaction%20graphs%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GROVER%2C%20A.%20AL-SHEDIVAT%2C%20M.%20GUPTA%2C%20J.K.%20BURDA%2C%20Y.%20Evaluating%20generalization%20in%20multiagent%20systems%20using%20agent-interaction%20graphs%202018"
        },
        {
            "id": "21",
            "entry": "[21] A. GROVER, M. AL-SHEDIVAT, J. K. GUPTA, Y. BURDA, AND H. EDWARDS, Learning policy representations in multiagent systems, in International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=GROVER%2C%20A.%20AL-SHEDIVAT%2C%20M.%20GUPTA%2C%20J.K.%20BURDA%2C%20Y.%20Learning%20policy%20representations%20in%20multiagent%20systems%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=GROVER%2C%20A.%20AL-SHEDIVAT%2C%20M.%20GUPTA%2C%20J.K.%20BURDA%2C%20Y.%20Learning%20policy%20representations%20in%20multiagent%20systems%202018"
        },
        {
            "id": "22",
            "entry": "[22] A. GROVER, T. MARKOV, P. ATTIA, N. JIN, N. PERKINS, B. CHEONG, M. CHEN, Z. YANG, S. HARRIS, W. CHUEH, ET AL., Best arm identification in multi-armed bandits with delayed feedback, arXiv preprint arXiv:1803.10937, (2018).",
            "arxiv_url": "https://arxiv.org/pdf/1803.10937"
        },
        {
            "id": "23",
            "entry": "[23] E. HAZAN, Introduction to Online Convex Optimization, Foundations and Trends(r) in Optimization Series, Now Publishers, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HAZAN%2C%20E.%20Introduction%20to%20Online%20Convex%20Optimization%2C%20Foundations%20and%20Trends%28r%29%20in%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=HAZAN%2C%20E.%20Introduction%20to%20Online%20Convex%20Optimization%2C%20Foundations%20and%20Trends%28r%29%20in%202016"
        },
        {
            "id": "24",
            "entry": "[24] E. HAZAN, A. AGARWAL, AND S. KALE, Logarithmic regret algorithms for online convex optimization, Machine Learning, 69 (2007), pp. 169\u2013192.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=HAZAN%2C%20E.%20AGARWAL%2C%20A.%20KALE%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=HAZAN%2C%20E.%20AGARWAL%2C%20A.%20KALE%2C%20S.%20Logarithmic%20regret%20algorithms%20for%20online%20convex%20optimization%202007"
        },
        {
            "id": "25",
            "entry": "[25] P. JOULANI, A. GYORGY, AND C. SZEPESV\u00c1RI, Online learning under delayed feedback, in International Conference on Machine Learning, 2013, pp. 1453\u20131461.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=JOULANI%2C%20P.%20GYORGY%2C%20A.%20C.%20SZEPESV%C3%81RI%2C%20Online%20learning%20under%20delayed%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=JOULANI%2C%20P.%20GYORGY%2C%20A.%20C.%20SZEPESV%C3%81RI%2C%20Online%20learning%20under%20delayed%202013"
        },
        {
            "id": "26",
            "entry": "[26] A. KALAI AND S. VEMPALA, Efficient algorithms for online decision problems, Journal of Computer and System Sciences, 71 (2005), pp. 291\u2013307.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KALAI%2C%20A.%20VEMPALA%2C%20S.%20Efficient%20algorithms%20for%20online%20decision%20problems%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=KALAI%2C%20A.%20VEMPALA%2C%20S.%20Efficient%20algorithms%20for%20online%20decision%20problems%202005"
        },
        {
            "id": "27",
            "entry": "[27] S. KRICHENE, W. KRICHENE, R. DONG, AND A. BAYEN, Convergence of heterogeneous distributed learning in stochastic routing games, in Communication, Control, and Computing (Allerton), 2015 53rd Annual Allerton Conference on, IEEE, 2015, pp. 480\u2013487.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=KRICHENE%2C%20S.%20KRICHENE%2C%20W.%20DONG%2C%20R.%20BAYEN%2C%20A.%20Convergence%20of%20heterogeneous%20distributed%20learning%20in%20stochastic%20routing%20games%2C%20in%20Communication%2C%20Control%2C%20and%20Computing%202015"
        },
        {
            "id": "28",
            "entry": "[28] K. LAM, W. KRICHENE, AND A. BAYEN, On learning how players learn: estimation of learning dynamics in the routing game, in Cyber-Physical Systems (ICCPS), 2016 ACM/IEEE 7th International Conference on, IEEE, 2016, pp. 1\u201310.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LAM%2C%20K.%20KRICHENE%2C%20W.%20A.%20BAYEN%2C%20On%20learning%20how%20players%20learn%3A%20estimation%20of%20learning%20dynamics%20in%20the%20routing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LAM%2C%20K.%20KRICHENE%2C%20W.%20A.%20BAYEN%2C%20On%20learning%20how%20players%20learn%3A%20estimation%20of%20learning%20dynamics%20in%20the%20routing%202016"
        },
        {
            "id": "29",
            "entry": "[29] I. MENACHE AND A. OZDAGLAR, Network games: Theory, models, and dynamics, Synthesis Lectures on Communication Networks, 4 (2011), pp. 1\u2013159.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MENACHE%2C%20I.%20OZDAGLAR%2C%20A.%20Network%20games%3A%20Theory%2C%20models%2C%20and%20dynamics%2C%20Synthesis%20Lectures%20on%20Communication%20Networks%2C%204%202011"
        },
        {
            "id": "30",
            "entry": "[30] P. MERTIKOPOULOS, E. V. BELMEGA, R. NEGREL, AND L. SANGUINETTI, Distributed stochastic optimization via matrix exponential learning, IEEE Trans. Signal Process., 65 (2017), pp. 2277\u20132290.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MERTIKOPOULOS%2C%20P.%20BELMEGA%2C%20E.V.%20NEGREL%2C%20R.%20SANGUINETTI%2C%20L.%20Distributed%20stochastic%20optimization%20via%20matrix%20exponential%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MERTIKOPOULOS%2C%20P.%20BELMEGA%2C%20E.V.%20NEGREL%2C%20R.%20SANGUINETTI%2C%20L.%20Distributed%20stochastic%20optimization%20via%20matrix%20exponential%20learning%202017"
        },
        {
            "id": "31",
            "entry": "[31] P. MERTIKOPOULOS, C. PAPADIMITRIOU, AND G. PILIOURAS, Cycles in adversarial regularized learning, in Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, 2018, pp. 2703\u20132717.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MERTIKOPOULOS%2C%20P.%20PAPADIMITRIOU%2C%20C.%20PILIOURAS%2C%20G.%20Cycles%20in%20adversarial%20regularized%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MERTIKOPOULOS%2C%20P.%20PAPADIMITRIOU%2C%20C.%20PILIOURAS%2C%20G.%20Cycles%20in%20adversarial%20regularized%20learning%202018"
        },
        {
            "id": "32",
            "entry": "[32] P. MERTIKOPOULOS AND Z. ZHOU, Learning in games with continuous action sets and unknown payoff functions, Mathematical Programming, (2018), pp. 1\u201343.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MERTIKOPOULOS%2C%20P.%20ZHOU%2C%20Z.%20Learning%20in%20games%20with%20continuous%20action%20sets%20and%20unknown%20payoff%20functions%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MERTIKOPOULOS%2C%20P.%20ZHOU%2C%20Z.%20Learning%20in%20games%20with%20continuous%20action%20sets%20and%20unknown%20payoff%20functions%202018"
        },
        {
            "id": "33",
            "entry": "[33] V. MNIH, K. KAVUKCUOGLU, D. SILVER, A. GRAVES, I. ANTONOGLOU, D. WIERSTRA, AND M. RIEDMILLER, Playing atari with deep reinforcement learning, arXiv preprint arXiv:1312.5602, (2013).",
            "arxiv_url": "https://arxiv.org/pdf/1312.5602"
        },
        {
            "id": "34",
            "entry": "[34] B. MONNOT AND G. PILIOURAS, Limits and limitations of no-regret learning in games, The Knowledge Engineering Review, 32 (2017).",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=MONNOT%2C%20B.%20PILIOURAS%2C%20G.%20Limits%20and%20limitations%20of%20no-regret%20learning%20in%20games%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=MONNOT%2C%20B.%20PILIOURAS%2C%20G.%20Limits%20and%20limitations%20of%20no-regret%20learning%20in%20games%202017"
        },
        {
            "id": "35",
            "entry": "[35] Y. NESTEROV, Primal-dual subgradient methods for convex problems, Mathematical programming, 120 (2009), pp. 221\u2013259.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=NESTEROV%2C%20Y.%20Primal-dual%20subgradient%20methods%20for%20convex%20problems%2C%20Mathematical%20programming%2C%20120%202009"
        },
        {
            "id": "36",
            "entry": "[36] G. PALAIOPANOS, I. PANAGEAS, AND G. PILIOURAS, Multiplicative weights update with constant step-size in congestion games: Convergence, limit cycles and chaos, in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds., Curran Associates, Inc., 2017, pp. 5872\u20135882.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=PALAIOPANOS%2C%20G.%20PANAGEAS%2C%20I.%20PILIOURAS%2C%20G.%20Multiplicative%20weights%20update%20with%20constant%20step-size%20in%20congestion%20games%3A%20Convergence%2C%20limit%20cycles%20and%20chaos%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=PALAIOPANOS%2C%20G.%20PANAGEAS%2C%20I.%20PILIOURAS%2C%20G.%20Multiplicative%20weights%20update%20with%20constant%20step-size%20in%20congestion%20games%3A%20Convergence%2C%20limit%20cycles%20and%20chaos%202017"
        },
        {
            "id": "37",
            "entry": "[37] S. PERKINS, P. MERTIKOPOULOS, AND D. S. LESLIE, Mixed-strategy learning with continuous action sets, IEEE Trans. Autom. Control, 62 (2017), pp. 379\u2013384.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=PERKINS%2C%20S.%20MERTIKOPOULOS%2C%20P.%20LESLIE%2C%20D.S.%20Mixed-strategy%20learning%20with%20continuous%20action%20sets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=PERKINS%2C%20S.%20MERTIKOPOULOS%2C%20P.%20LESLIE%2C%20D.S.%20Mixed-strategy%20learning%20with%20continuous%20action%20sets%202017"
        },
        {
            "id": "38",
            "entry": "[38] C. PIKE-BURKE, S. AGRAWAL, C. SZEPESVARI, AND S. GRUNEWALDER, Bandits with delayed, aggregated anonymous feedback, in International Conference on Machine Learning, 2018, pp. 4102\u20134110.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=PIKE-BURKE%2C%20C.%20AGRAWAL%2C%20S.%20SZEPESVARI%2C%20C.%20S.%20GRUNEWALDER%2C%20Bandits%20with%20delayed%2C%20aggregated%20anonymous%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=PIKE-BURKE%2C%20C.%20AGRAWAL%2C%20S.%20SZEPESVARI%2C%20C.%20S.%20GRUNEWALDER%2C%20Bandits%20with%20delayed%2C%20aggregated%20anonymous%202018"
        },
        {
            "id": "39",
            "entry": "[39] K. QUANRUD AND D. KHASHABI, Online learning with adversarial delays, in Advances in Neural Information Processing Systems, 2015, pp. 1270\u20131278.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=QUANRUD%2C%20K.%20KHASHABI%2C%20D.%20Online%20learning%20with%20adversarial%20delays%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=QUANRUD%2C%20K.%20KHASHABI%2C%20D.%20Online%20learning%20with%20adversarial%20delays%202015"
        },
        {
            "id": "41",
            "entry": "[41] F. SALEHISADAGHIANI AND L. PAVEL, Distributed nash equilibrium seeking via the alternating direction method of multipliers, IFAC-PapersOnLine, 50 (2017), pp. 6166\u20136171. University of Jerusalem, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SALEHISADAGHIANI%2C%20F.%20PAVEL%2C%20L.%20Distributed%20nash%20equilibrium%20seeking%20via%20the%20alternating%20direction%20method%20of%20multipliers%202017"
        },
        {
            "id": "in_2012_a",
            "entry": "in Machine Learning, 4 (2012), pp. 107\u2013194.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=in%20Machine%20Learning%204%202012%20pp%20107194",
            "oa_query": "https://api.scholarcy.com/oa_version?query=in%20Machine%20Learning%204%202012%20pp%20107194"
        },
        {
            "id": "Neural_2007_a",
            "entry": "Neural Information Processing Systems 19, MIT Press, 2007, pp. 1265\u20131272.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Neural%20Information%20Processing%20Systems%2019%202007"
        },
        {
            "id": "45",
            "entry": "[45] Y. SHOHAM AND K. LEYTON-BROWN, Multiagent systems: Algorithmic, game-theoretic, and logical foundations, Cambridge University Press, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=SHOHAM%2C%20Y.%20LEYTON-BROWN%2C%20K.%20Multiagent%20systems%3A%20Algorithmic%2C%20game-theoretic%2C%20and%20logical%20foundations%202008"
        },
        {
            "id": "46",
            "entry": "[46] Y. VIOSSAT AND A. ZAPECHELNYUK, No-regret dynamics and fictitious play, Journal of Economic Theory, 148 (2013), pp. 825\u2013842.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=VIOSSAT%2C%20Y.%20ZAPECHELNYUK%2C%20A.%20No-regret%20dynamics%20and%20fictitious%20play%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=VIOSSAT%2C%20Y.%20ZAPECHELNYUK%2C%20A.%20No-regret%20dynamics%20and%20fictitious%20play%202013"
        },
        {
            "id": "47",
            "entry": "[47] Y. VIOSSAT AND A. ZAPECHELNYUK, No-regret dynamics and fictitious play, Journal of Economic",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=VIOSSAT%2C%20Y.%20ZAPECHELNYUK%2C%20A.%20No-regret%20dynamics%20and%20fictitious%20play",
            "oa_query": "https://api.scholarcy.com/oa_version?query=VIOSSAT%2C%20Y.%20ZAPECHELNYUK%2C%20A.%20No-regret%20dynamics%20and%20fictitious%20play"
        },
        {
            "id": "48",
            "entry": "[48] Z. YIN, K.-H. CHANG, AND R. ZHANG, Deepprobe: Information directed sequence understanding and chatbot design via recurrent neural networks, in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2017, pp. 2131\u20132139.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=YIN%2C%20Z.%20CHANG%2C%20K.-H.%20ZHANG%2C%20R.%20Deepprobe%3A%20Information%20directed%20sequence%20understanding%20and%20chatbot%20design%20via%20recurrent%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=YIN%2C%20Z.%20CHANG%2C%20K.-H.%20ZHANG%2C%20R.%20Deepprobe%3A%20Information%20directed%20sequence%20understanding%20and%20chatbot%20design%20via%20recurrent%20neural%20networks%202017"
        },
        {
            "id": "49",
            "entry": "[49] Z. YIN, V. SACHIDANANDA, AND B. PRABHAKAR, The global anchor method for quantifying linguistic shifts and domain adaptation, in Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=YIN%2C%20Z.%20SACHIDANANDA%2C%20V.%20PRABHAKAR%2C%20B.%20The%20global%20anchor%20method%20for%20quantifying%20linguistic%20shifts%20and%20domain%20adaptation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=YIN%2C%20Z.%20SACHIDANANDA%2C%20V.%20PRABHAKAR%2C%20B.%20The%20global%20anchor%20method%20for%20quantifying%20linguistic%20shifts%20and%20domain%20adaptation%202018"
        },
        {
            "id": "50",
            "entry": "[50] Z. YIN AND Y. SHEN, On the dimensionality of word embedding, in Advances in Neural Information Processing Systems, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=YIN%2C%20Z.%20SHEN%2C%20Y.%20On%20the%20dimensionality%20of%20word%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=YIN%2C%20Z.%20SHEN%2C%20Y.%20On%20the%20dimensionality%20of%20word%202018"
        },
        {
            "id": "51",
            "entry": "[51] Z. ZHOU, S. ATHEY, AND S. WAGER, Offline multi-action policy learning: Generalization and optimization, arXiv preprint arXiv:1810.04778, (2018).",
            "arxiv_url": "https://arxiv.org/pdf/1810.04778"
        },
        {
            "id": "52",
            "entry": "[52] Z. ZHOU, N. BAMBOS, AND P. GLYNN, Dynamics on linear influence network games under stochastic environments, in International Conference on Decision and Game Theory for Security, Springer, 2016, pp. 114\u2013126.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ZHOU%2C%20Z.%20BAMBOS%2C%20N.%20GLYNN%2C%20P.%20Dynamics%20on%20linear%20influence%20network%20games%20under%20stochastic%20environments%2C%20in%20International%20Conference%20on%20Decision%20and%20Game%20Theory%20for%20Security%202016"
        },
        {
            "id": "53",
            "entry": "[53] Z. ZHOU, P. MERTIKOPOULOS, N. BAMBOS, P. W. GLYNN, AND C. TOMLIN, Countering feedback delays in multi-agent learning, in NIPS \u201917: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ZHOU%2C%20Z.%20MERTIKOPOULOS%2C%20P.%20BAMBOS%2C%20N.%20GLYNN%2C%20P.W.%20Countering%20feedback%20delays%20in%20multi-agent%20learning%2C%20in%20NIPS%E2%80%99%2017%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ZHOU%2C%20Z.%20MERTIKOPOULOS%2C%20P.%20BAMBOS%2C%20N.%20GLYNN%2C%20P.W.%20Countering%20feedback%20delays%20in%20multi-agent%20learning%2C%20in%20NIPS%E2%80%99%2017%202017"
        },
        {
            "id": "54",
            "entry": "[54] Z. ZHOU, P. MERTIKOPOULOS, A. L. MOUSTAKAS, N. BAMBOS, AND P. GLYNN, Mirror descent learning in continuous games, in Decision and Control (CDC), 2017 IEEE 56th Annual Conference on, IEEE, 2017, pp. 5776\u20135783.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ZHOU%2C%20Z.%20MERTIKOPOULOS%2C%20P.%20MOUSTAKAS%2C%20A.L.%20BAMBOS%2C%20N.%20Mirror%20descent%20learning%20in%20continuous%20games%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ZHOU%2C%20Z.%20MERTIKOPOULOS%2C%20P.%20MOUSTAKAS%2C%20A.L.%20BAMBOS%2C%20N.%20Mirror%20descent%20learning%20in%20continuous%20games%202017"
        },
        {
            "id": "55",
            "entry": "[55] Z. ZHOU, B. YOLKEN, R. A. MIURA-KO, AND N. BAMBOS, A game-theoretical formulation of influence networks, in American Control Conference (ACC), 2016, IEEE, 2016, pp. 3802\u20133807.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ZHOU%2C%20Z.%20YOLKEN%2C%20B.%20MIURA-KO%2C%20R.A.%20BAMBOS%2C%20N.%20A%20game-theoretical%20formulation%20of%20influence%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ZHOU%2C%20Z.%20YOLKEN%2C%20B.%20MIURA-KO%2C%20R.A.%20BAMBOS%2C%20N.%20A%20game-theoretical%20formulation%20of%20influence%202016"
        },
        {
            "id": "56",
            "entry": "[56] M. ZHU AND E. FRAZZOLI, Distributed robust adaptive equilibrium computation for generalized convex games, Automatica, 63 (2016), pp. 82\u201391.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ZHU%2C%20M.%20FRAZZOLI%2C%20E.%20Distributed%20robust%20adaptive%20equilibrium%20computation%20for%20generalized%20convex%20games%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ZHU%2C%20M.%20FRAZZOLI%2C%20E.%20Distributed%20robust%20adaptive%20equilibrium%20computation%20for%20generalized%20convex%20games%202016"
        },
        {
            "id": "57",
            "entry": "[57] M. ZINKEVICH, Online convex programming and generalized infinitesimal gradient ascent, in ICML \u201903: Proceedings of the 20th International Conference on Machine Learning, 2003, pp. 928\u2013936. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=ZINKEVICH%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%2C%20in%20ICML%E2%80%99%2003%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=ZINKEVICH%2C%20M.%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%2C%20in%20ICML%E2%80%99%2003%202003"
        }
    ]
}
