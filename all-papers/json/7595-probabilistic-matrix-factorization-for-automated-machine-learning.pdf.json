{
    "filename": "7595-probabilistic-matrix-factorization-for-automated-machine-learning.pdf",
    "metadata": {
        "title": "Probabilistic Matrix Factorization for Automated Machine Learning",
        "author": "Nicolo Fusi, Rishit Sheth, Melih Elibol",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7595-probabilistic-matrix-factorization-for-automated-machine-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data preprocessing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible pipelines. In our experiments, we show that our approach quickly identifies highperforming pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art."
    },
    "keywords": [
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "matrix factorization",
            "url": "https://en.wikipedia.org/wiki/matrix_factorization"
        },
        {
            "term": "gaussian process",
            "url": "https://en.wikipedia.org/wiki/gaussian_process"
        },
        {
            "term": "bayesian optimization",
            "url": "https://en.wikipedia.org/wiki/bayesian_optimization"
        },
        {
            "term": "collaborative filtering",
            "url": "https://en.wikipedia.org/wiki/collaborative_filtering"
        }
    ],
    "highlights": [
        "Machine learning models often depend on hyperparameters that require extensive fine-tuning in order to achieve optimal performance",
        "We show that the problem of predicting the performance of ML pipelines on a new dataset can be cast as a collaborative filtering problem that can be solved with probabilistic matrix factorization techniques",
        "In the Bayesian optimization community, most of the work revolves around either casting this problem as an instance of multi-task learning or by selecting the first parameter settings to evaluate on a new dataset by looking at what worked in related datasets",
        "We have presented a new approach to automatically build predictive ML pipelines for a given dataset, automating the selection of data pre-processing method and machine learning model as well as the tuning of their hyperparameters",
        "Our approach combines techniques from collaborative filtering and ideas from Bayesian optimization to intelligently explore the space of ML pipelines, exploiting experiments performed in previous datasets"
    ],
    "key_statements": [
        "Machine learning models often depend on hyperparameters that require extensive fine-tuning in order to achieve optimal performance",
        "The goal of Bayesian optimization is to find the vector of hyperparameters \u03b8 that corresponds to arg min L (M(x; \u03b8), y), \u03b8 where M(x; \u03b8) are the predictions generated by a machine learning model M (e.g. SVM, random forest, etc.) with hyperparameters \u03b8 on some inputs x, y are the targets/labels, and L is a loss function",
        "We show that the problem of predicting the performance of ML pipelines on a new dataset can be cast as a collaborative filtering problem that can be solved with probabilistic matrix factorization techniques",
        "The approach we follow in the rest of this paper, based on Gaussian process latent variable models [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], embeds different pipelines in a latent space based on their performance across different datasets",
        "Since our probabilistic approach produces a full predictive distribution over the performance of the ML pipelines considered, we can use it in conjunction with acquisition functions commonly used in Bayesian optimization to guide the exploration of the ML pipeline space.\n2 Related work",
        "In the Bayesian optimization community, most of the work revolves around either casting this problem as an instance of multi-task learning or by selecting the first parameter settings to evaluate on a new dataset by looking at what worked in related datasets",
        "We have presented a new approach to automatically build predictive ML pipelines for a given dataset, automating the selection of data pre-processing method and machine learning model as well as the tuning of their hyperparameters",
        "Our approach combines techniques from collaborative filtering and ideas from Bayesian optimization to intelligently explore the space of ML pipelines, exploiting experiments performed in previous datasets"
    ],
    "summary": [
        "Machine learning models often depend on hyperparameters that require extensive fine-tuning in order to achieve optimal performance.",
        "The approach we follow in the rest of this paper, based on Gaussian process latent variable models [<a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c9\" href=\"#r9\">9</a>], embeds different pipelines in a latent space based on their performance across different datasets.",
        "Figure 1 shows the first two dimensions of the latent space of ML pipelines identified by our model on OpenML [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] datasets.",
        "Since our probabilistic approach produces a full predictive distribution over the performance of the ML pipelines considered, we can use it in conjunction with acquisition functions commonly used in Bayesian optimization to guide the exploration of the ML pipeline space.",
        "In the Bayesian optimization community, most of the work revolves around either casting this problem as an instance of multi-task learning or by selecting the first parameter settings to evaluate on a new dataset by looking at what worked in related datasets.",
        "Feurer et al (2015) [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] consider learning the dataset similarity function from training data for warm-starting, and Wistuba et al (2015) [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>] extend this by taking into account the performance of hyperparameter configurations evaluated on the new dataset.",
        "Their model performs a joint linear embedding of problem instances and experts based on their meta-features and a sparse matrix containing the results of previous algorithm runs.",
        "Our approach is loosely related to the work of [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], but we perform sequential model based optimization with a non-linear mapping between latent and observed space in an unsupervised model, while they use a supervised linear model trained on ranks for one-shot algorithm selection.",
        "We ran all of the experiments on 564 OpenML [<a class=\"ref-link\" id=\"c28\" href=\"#r28\">28</a>] datasets selected by filtering for binary and multi-class classification problems with no more than 10, 000 samples and no missing values, our method is capable of handling datasets which cause ML pipeline runs to be unsuccessful.",
        "We have presented a new approach to automatically build predictive ML pipelines for a given dataset, automating the selection of data pre-processing method and machine learning model as well as the tuning of their hyperparameters.",
        "Our approach combines techniques from collaborative filtering and ideas from Bayesian optimization to intelligently explore the space of ML pipelines, exploiting experiments performed in previous datasets.",
        "We are interested in using acquisition functions that include a factor representing the computational cost of running a given pipeline [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] to handle instances when datasets have a large number of samples."
    ],
    "headline": "We propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] James Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyperparameter optimization. In NIPS, pages 2546\u20132554, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=James%20Bergstra%2C%20R%C3%A9mi%20Bardenet%2C%20Yoshua%20Bengio%20K%C3%A9gl%2C%20Bal%C3%A1zs%20Algorithms%20for%20hyperparameter%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=James%20Bergstra%2C%20R%C3%A9mi%20Bardenet%2C%20Yoshua%20Bengio%20K%C3%A9gl%2C%20Bal%C3%A1zs%20Algorithms%20for%20hyperparameter%20optimization%202011"
        },
        {
            "id": "2",
            "entry": "[2] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. JMLR, 13:281\u2013305, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20James%20Bengio%2C%20Yoshua%20Random%20search%20for%20hyper-parameter%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20James%20Bengio%2C%20Yoshua%20Random%20search%20for%20hyper-parameter%20optimization%202012"
        },
        {
            "id": "3",
            "entry": "[3] James Bergstra, Daniel Yamins, and David D Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. ICML, pages 115\u2013123, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bergstra%2C%20James%20Yamins%2C%20Daniel%20Cox%2C%20David%20D.%20Making%20a%20science%20of%20model%20search%3A%20Hyperparameter%20optimization%20in%20hundreds%20of%20dimensions%20for%20vision%20architectures%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bergstra%2C%20James%20Yamins%2C%20Daniel%20Cox%2C%20David%20D.%20Making%20a%20science%20of%20model%20search%3A%20Hyperparameter%20optimization%20in%20hundreds%20of%20dimensions%20for%20vision%20architectures%202013"
        },
        {
            "id": "4",
            "entry": "[4] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter. Efficient and robust automated machine learning. In NIPS, pages 2962\u20132970, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feurer%2C%20Matthias%20Klein%2C%20Aaron%20Eggensperger%2C%20Katharina%20Springenberg%2C%20Jost%20Efficient%20and%20robust%20automated%20machine%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feurer%2C%20Matthias%20Klein%2C%20Aaron%20Eggensperger%2C%20Katharina%20Springenberg%2C%20Jost%20Efficient%20and%20robust%20automated%20machine%20learning%202015"
        },
        {
            "id": "5",
            "entry": "[5] Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparameter optimization via meta-learning. In AAAI, pages 1128\u20131135, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feurer%2C%20Matthias%20Springenberg%2C%20Jost%20Tobias%20Hutter%2C%20Frank%20Initializing%20bayesian%20hyperparameter%20optimization%20via%20meta-learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feurer%2C%20Matthias%20Springenberg%2C%20Jost%20Tobias%20Hutter%2C%20Frank%20Initializing%20bayesian%20hyperparameter%20optimization%20via%20meta-learning%202015"
        },
        {
            "id": "6",
            "entry": "[6] Steffen Gr\u00fcnew\u00e4lder, Jean-Yves Audibert, Manfred Opper, and John Shawe-Taylor. Regret bounds for Gaussian process bandit problems. In AISTATS, pages 273\u2013280, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gr%C3%BCnew%C3%A4lder%2C%20Steffen%20Audibert%2C%20Jean-Yves%20Opper%2C%20Manfred%20Shawe-Taylor%2C%20John%20Regret%20bounds%20for%20Gaussian%20process%20bandit%20problems%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gr%C3%BCnew%C3%A4lder%2C%20Steffen%20Audibert%2C%20Jean-Yves%20Opper%2C%20Manfred%20Shawe-Taylor%2C%20John%20Regret%20bounds%20for%20Gaussian%20process%20bandit%20problems%202010"
        },
        {
            "id": "7",
            "entry": "[7] Isabelle Guyon, Imad Chaabane, Hugo Jair Escalante, Sergio Escalera, Damir Jajetic, James Robert Lloyd, N\u00faria Maci\u00e0, Bisakha Ray, Lukasz Romaszko, Mich\u00e8le Sebag, Alexander Statnikov, S\u00e9bastien Treguer, and Evelyne Viegas. A brief review of the ChaLearn AutoML challenge: Any-time any-dataset learning without human intervention. In Proceedings of the Workshop on Automatic Machine Learning, volume 64, pages 21\u201330, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Guyon%2C%20Isabelle%20Chaabane%2C%20Imad%20Escalante%2C%20Hugo%20Jair%20Escalera%2C%20Sergio%20S%C3%A9bastien%20Treguer%2C%20and%20Evelyne%20Viegas.%20A%20brief%20review%20of%20the%20ChaLearn%20AutoML%20challenge%3A%20Any-time%20any-dataset%20learning%20without%20human%20intervention%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Guyon%2C%20Isabelle%20Chaabane%2C%20Imad%20Escalante%2C%20Hugo%20Jair%20Escalera%2C%20Sergio%20S%C3%A9bastien%20Treguer%2C%20and%20Evelyne%20Viegas.%20A%20brief%20review%20of%20the%20ChaLearn%20AutoML%20challenge%3A%20Any-time%20any-dataset%20learning%20without%20human%20intervention%202016"
        },
        {
            "id": "8",
            "entry": "[8] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In International Conference on Learning and Intelligent Optimization, pages 507\u2013523, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hutter%2C%20Frank%20Hoos%2C%20Holger%20H.%20Leyton-Brown%2C%20Kevin%20Sequential%20model-based%20optimization%20for%20general%20algorithm%20configuration%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hutter%2C%20Frank%20Hoos%2C%20Holger%20H.%20Leyton-Brown%2C%20Kevin%20Sequential%20model-based%20optimization%20for%20general%20algorithm%20configuration%202011"
        },
        {
            "id": "9",
            "entry": "[9] Neil Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. JMLR, 6:1783\u20131816, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lawrence%2C%20Neil%20Probabilistic%20non-linear%20principal%20component%20analysis%20with%20Gaussian%20process%20latent%20variable%20models%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lawrence%2C%20Neil%20Probabilistic%20non-linear%20principal%20component%20analysis%20with%20Gaussian%20process%20latent%20variable%20models%202005"
        },
        {
            "id": "10",
            "entry": "[10] Neil Lawrence and Raquel Urtasun. Non-linear matrix factorization with Gaussian processes. ICML, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lawrence%2C%20Neil%20Urtasun%2C%20Raquel%20Non-linear%20matrix%20factorization%20with%20Gaussian%20processes%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lawrence%2C%20Neil%20Urtasun%2C%20Raquel%20Non-linear%20matrix%20factorization%20with%20Gaussian%20processes%202009"
        },
        {
            "id": "11",
            "entry": "[11] Rui Leite, Pavel Brazdil, and Joaquin Vanschoren. Selecting classification algorithms with active testing. In International workshop on machine learning and data mining in pattern recognition, pages 117\u2013131, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Leite%2C%20Rui%20Brazdil%2C%20Pavel%20Vanschoren%2C%20Joaquin%20Selecting%20classification%20algorithms%20with%20active%20testing%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Leite%2C%20Rui%20Brazdil%2C%20Pavel%20Vanschoren%2C%20Joaquin%20Selecting%20classification%20algorithms%20with%20active%20testing%202012"
        },
        {
            "id": "12",
            "entry": "[12] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv:1603.06560, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.06560"
        },
        {
            "id": "13",
            "entry": "[13] Yuri Malitsky and Barry O\u2019Sullivan. Latent features for algorithm selection. In Seventh Annual Symposium on Combinatorial Search, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Malitsky%2C%20Yuri%20O%E2%80%99Sullivan%2C%20Barry%20Latent%20features%20for%20algorithm%20selection%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Malitsky%2C%20Yuri%20O%E2%80%99Sullivan%2C%20Barry%20Latent%20features%20for%20algorithm%20selection%202014"
        },
        {
            "id": "14",
            "entry": "[14] Mustafa M\u0131s\u0131r and Mich\u00e8le Sebag. ALORS: An algorithm recommender system. Artificial Intelligence, 244:291\u2013314, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%C4%B1s%C4%B1r%2C%20Mustafa%20Sebag%2C%20Mich%C3%A8le%20ALORS%3A%20An%20algorithm%20recommender%20system%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%C4%B1s%C4%B1r%2C%20Mustafa%20Sebag%2C%20Mich%C3%A8le%20ALORS%3A%20An%20algorithm%20recommender%20system%202017"
        },
        {
            "id": "15",
            "entry": "[15] J Mockus. On Bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical Conference, pages 400\u2013404, 1975.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mockus%2C%20J.%20On%20Bayesian%20methods%20for%20seeking%20the%20extremum%201975",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mockus%2C%20J.%20On%20Bayesian%20methods%20for%20seeking%20the%20extremum%201975"
        },
        {
            "id": "16",
            "entry": "[16] Michael A Osborne, Roman Garnett, and Stephen J Roberts. Gaussian processes for global optimization. In 3rd International Conference on Learning and Intelligent Optimization (LION3), pages 1\u201315, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osborne%2C%20Michael%20A.%20Garnett%2C%20Roman%20Roberts%2C%20Stephen%20J.%20Gaussian%20processes%20for%20global%20optimization%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osborne%2C%20Michael%20A.%20Garnett%2C%20Roman%20Roberts%2C%20Stephen%20J.%20Gaussian%20processes%20for%20global%20optimization%202009"
        },
        {
            "id": "17",
            "entry": "[17] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and \u00c9douard Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 12:2825\u20132830, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20Fabian%20Varoquaux%2C%20Ga%C3%ABl%20Gramfort%2C%20Alexandre%20Michel%2C%20Vincent%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20Fabian%20Varoquaux%2C%20Ga%C3%ABl%20Gramfort%2C%20Alexandre%20Michel%2C%20Vincent%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011"
        },
        {
            "id": "18",
            "entry": "[18] Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, and Cedric Archambeau. Multiple adaptive Bayesian linear regression for scalable Bayesian optimization with warm start. arXiv:1712.02902, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.02902"
        },
        {
            "id": "19",
            "entry": "[19] Matthias Reif, Faisal Shafait, and Andreas Dengel. Meta-learning for evolutionary parameter optimization of classifiers. Machine learning, 87:357\u2013380, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reif%2C%20Matthias%20Shafait%2C%20Faisal%20Dengel%2C%20Andreas%20Meta-learning%20for%20evolutionary%20parameter%20optimization%20of%20classifiers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reif%2C%20Matthias%20Shafait%2C%20Faisal%20Dengel%2C%20Andreas%20Meta-learning%20for%20evolutionary%20parameter%20optimization%20of%20classifiers%202012"
        },
        {
            "id": "20",
            "entry": "[20] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In ICML, pages 880\u2013887, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Salakhutdinov%2C%20Ruslan%20Mnih%2C%20Andriy%20Bayesian%20probabilistic%20matrix%20factorization%20using%20Markov%20chain%20Monte%20Carlo%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Salakhutdinov%2C%20Ruslan%20Mnih%2C%20Andriy%20Bayesian%20probabilistic%20matrix%20factorization%20using%20Markov%20chain%20Monte%20Carlo%202008"
        },
        {
            "id": "21",
            "entry": "[21] Nicolas Schilling, Martin Wistuba, Lucas Drumond, and Lars Schmidt-Thieme. Hyperparameter optimization with factorized multilayer perceptrons. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 87\u2013103, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schilling%2C%20Nicolas%20Wistuba%2C%20Martin%20Drumond%2C%20Lucas%20Schmidt-Thieme%2C%20Lars%20Hyperparameter%20optimization%20with%20factorized%20multilayer%20perceptrons%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schilling%2C%20Nicolas%20Wistuba%2C%20Martin%20Drumond%2C%20Lucas%20Schmidt-Thieme%2C%20Lars%20Hyperparameter%20optimization%20with%20factorized%20multilayer%20perceptrons%202015"
        },
        {
            "id": "22",
            "entry": "[22] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104:148\u2013175, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shahriari%2C%20Bobak%20Swersky%2C%20Kevin%20Wang%2C%20Ziyu%20Adams%2C%20Ryan%20P.%20Taking%20the%20human%20out%20of%20the%20loop%3A%20A%20review%20of%20Bayesian%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shahriari%2C%20Bobak%20Swersky%2C%20Kevin%20Wang%2C%20Ziyu%20Adams%2C%20Ryan%20P.%20Taking%20the%20human%20out%20of%20the%20loop%3A%20A%20review%20of%20Bayesian%20optimization%202016"
        },
        {
            "id": "23",
            "entry": "[23] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. In NIPS, pages 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "24",
            "entry": "[24] Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust Bayesian neural networks. In NIPS, pages 4134\u20134142. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Springenberg%2C%20Jost%20Tobias%20Klein%2C%20Aaron%20Falkner%2C%20Stefan%20Hutter%2C%20Frank%20Bayesian%20optimization%20with%20robust%20Bayesian%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Springenberg%2C%20Jost%20Tobias%20Klein%2C%20Aaron%20Falkner%2C%20Stefan%20Hutter%2C%20Frank%20Bayesian%20optimization%20with%20robust%20Bayesian%20neural%20networks%202016"
        },
        {
            "id": "25",
            "entry": "[25] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. arXiv:0912.3995, 2009.",
            "arxiv_url": "https://arxiv.org/pdf/0912.3995"
        },
        {
            "id": "26",
            "entry": "[26] David H Stern, Horst Samulowitz, Ralf Herbrich, Thore Graepel, Luca Pulina, and Armando Tacchella. Collaborative expert portfolio management. In AAAI, pages 179\u2013184, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stern%2C%20David%20H.%20Samulowitz%2C%20Horst%20Herbrich%2C%20Ralf%20Graepel%2C%20Thore%20Collaborative%20expert%20portfolio%20management%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stern%2C%20David%20H.%20Samulowitz%2C%20Horst%20Herbrich%2C%20Ralf%20Graepel%2C%20Thore%20Collaborative%20expert%20portfolio%20management%202010"
        },
        {
            "id": "27",
            "entry": "[27] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In NIPS, pages 2004\u20132012, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Swersky%2C%20Kevin%20Snoek%2C%20Jasper%20Adams%2C%20Ryan%20P.%20Multi-task%20Bayesian%20optimization%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Swersky%2C%20Kevin%20Snoek%2C%20Jasper%20Adams%2C%20Ryan%20P.%20Multi-task%20Bayesian%20optimization%202004"
        },
        {
            "id": "28",
            "entry": "[28] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: Networked science in machine learning. SIGKDD Explorations, 15:49\u201360, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vanschoren%2C%20Joaquin%20van%20Rijn%2C%20Jan%20N.%20Bischl%2C%20Bernd%20Torgo%2C%20Luis%20OpenML%3A%20Networked%20science%20in%20machine%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vanschoren%2C%20Joaquin%20van%20Rijn%2C%20Jan%20N.%20Bischl%2C%20Bernd%20Torgo%2C%20Luis%20OpenML%3A%20Networked%20science%20in%20machine%20learning%202013"
        },
        {
            "id": "29",
            "entry": "[29] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning. The MIT Press, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20Christopher%20K.I.%20Rasmussen%2C%20Carl%20Edward%20Gaussian%20processes%20for%20machine%20learning%202006"
        },
        {
            "id": "30",
            "entry": "[30] Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning data set similarities for hyperparameter optimization initializations. In MetaSel@ PKDD/ECML, pages 15\u201326, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wistuba%2C%20Martin%20Schilling%2C%20Nicolas%20Schmidt-Thieme%2C%20Lars%20Learning%20data%20set%20similarities%20for%20hyperparameter%20optimization%20initializations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wistuba%2C%20Martin%20Schilling%2C%20Nicolas%20Schmidt-Thieme%2C%20Lars%20Learning%20data%20set%20similarities%20for%20hyperparameter%20optimization%20initializations%202015"
        }
    ]
}
