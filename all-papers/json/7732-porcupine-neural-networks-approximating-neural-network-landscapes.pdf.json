{
    "filename": "7732-porcupine-neural-networks-approximating-neural-network-landscapes.pdf",
    "metadata": {
        "title": "Porcupine Neural Networks: Approximating Neural Network Landscapes",
        "author": "Soheil Feizi, Hamid Javadi, Jesse Zhang, David Tse",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7732-porcupine-neural-networks-approximating-neural-network-landscapes.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes analyzing their performance challenging. In this paper, we take another approach to this problem by constraining the network such that the corresponding optimization landscape has good theoretical properties without significantly compromising performance. In particular, for two-layer neural networks we introduce Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN."
    },
    "keywords": [
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "mean squared error",
            "url": "https://en.wikipedia.org/wiki/mean_squared_error"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "rectified linear unit",
            "url": "https://en.wikipedia.org/wiki/rectified_linear_unit"
        },
        {
            "term": "local optima",
            "url": "https://en.wikipedia.org/wiki/local_optima"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "Neural networks have been used in several machine learning and statistical inference problems including regression and classification tasks",
        "We study a key question: can an unconstrained neural network be approximated with a constrained one whose optimization landscape has good theoretical properties? For two-layer neural networks, we provide an affirmative answer to this question by introducing a family of constrained neural networks which we refer to as Porcupine Neural Networks (PNNs) (Figure 1-a)",
        "We introduced a family of constrained neural networks, called Porcupine Neural Networks (PNNs), whose population risk landscapes have good theoretical properties, i.e., most local optima of Porcupine Neural Networks optimizations are global while we have a characterization of parameter regions where bad local optimizers may exist",
        "We showed that an unconstrained neural network function can be approximated by a polynomially-large Porcupine Neural Networks",
        "We provided approximation bounds at global optima and bad local optima of the Porcupine Neural Networks optimization. These results may provide a means of explaining the success of local search methods in solving the unconstrained neural network optimization because every bad local optimum of the unconstrained problem can be viewed as a local optimum for a Porcupine Neural Networks constrained problem, which has a bounded loss according to our results",
        "Exploring the design of projection lines for Porcupine Neural Networks can be an interesting direction for future work"
    ],
    "key_statements": [
        "Neural networks have been used in several machine learning and statistical inference problems including regression and classification tasks",
        "We study a key question: can an unconstrained neural network be approximated with a constrained one whose optimization landscape has good theoretical properties? For two-layer neural networks, we provide an affirmative answer to this question by introducing a family of constrained neural networks which we refer to as Porcupine Neural Networks (PNNs) (Figure 1-a)",
        "We show that under some modeling assumptions, most local optima of Porcupine Neural Networks optimizations are global optimizers",
        "We show that under some modeling assumptions, the Porcupine Neural Networks approximation error can be bounded by the spectral norm of the generalized Schur complement of a kernel matrix",
        "We study whether an unconstrained two-layer neural network function can be approximated by a Porcupine Neural Networks",
        "Since these lines will be different than L\u2217, the neural network optimization can be formulated as a mismatched Porcupine Neural Networks optimization as studied in Section 5",
        "We introduced a family of constrained neural networks, called Porcupine Neural Networks (PNNs), whose population risk landscapes have good theoretical properties, i.e., most local optima of Porcupine Neural Networks optimizations are global while we have a characterization of parameter regions where bad local optimizers may exist",
        "We showed that an unconstrained neural network function can be approximated by a polynomially-large Porcupine Neural Networks",
        "We provided approximation bounds at global optima and bad local optima of the Porcupine Neural Networks optimization. These results may provide a means of explaining the success of local search methods in solving the unconstrained neural network optimization because every bad local optimum of the unconstrained problem can be viewed as a local optimum for a Porcupine Neural Networks constrained problem, which has a bounded loss according to our results",
        "A more sophisticated design can have a higher density of projection lines along directions with high variances",
        "Exploring the design of projection lines for Porcupine Neural Networks can be an interesting direction for future work"
    ],
    "summary": [
        "Neural networks have been used in several machine learning and statistical inference problems including regression and classification tasks.",
        "We study whether one can approximate an unconstrained two-layer neural network function with a PNN whose number of neurons is polynomially-large in dimension.",
        "Suppose the output data is generated using an unconstrained two-layer neural network with d = 15 inputs and k\u2217 = 20 hidden neurons.",
        "In Section 6, we study a characterization of the PNN approximation error with respect to the input dimension and the complexity of the unconstrained neural network function.",
        "Similar to the matched PNN case, we define R(s1, ..., sr) as the space of W where si is the vector of sign variables of weight vectors over the line Li. Theorem 4 For a mismatched PNN, in regions R(s1, ..., sr) where at least d of the si\u2019s are not equal to \u00b1(1, 1, ..., 1), every local optimizer of optimization (6) is a global optimizer.",
        "When the condition of Theorem 4 holds, the spectral norm of the matrix \u03c8[K]/\u03c8[KL] provides an upper-bound on the loss value at global optimizers of the mismatched PNN.",
        "We study whether an unconstrained two-layer neural network function can be approximated by a PNN.",
        "This neural network function can be viewed as a PNN whose lines are determined by input weight vectors to neurons.",
        "Since these lines will be different than L\u2217, the neural network optimization can be formulated as a mismatched PNN optimization as studied in Section 5.",
        "Since Theorem 4 provides an upper-bound for the mismatched PNN optimization loss using \u03c8[K]/\u03c8[KL] , intuitively, increasing the number of lines in L should decrease \u03c8[K]/\u03c8[KL] .",
        "We introduced a family of constrained neural networks, called Porcupine Neural Networks (PNNs), whose population risk landscapes have good theoretical properties, i.e., most local optima of PNN optimizations are global while we have a characterization of parameter regions where bad local optimizers may exist.",
        "These results may provide a means of explaining the success of local search methods in solving the unconstrained neural network optimization because every bad local optimum of the unconstrained problem can be viewed as a local optimum for a PNN constrained problem, which has a bounded loss according to our results.",
        "It is possible to extend our results to feed-forward neural networks with more than two layers by constraining weight vectors in every layer, except the last one, to lie on fixed projection lines.",
        "Other extensions of PNNs to network architectures with different activation functions than ReLU, and other types of operations are among interesting directions for future work"
    ],
    "headline": "For two-layer neural networks we introduce Porcupine Neural Networks  whose weight vectors are constrained to lie over a finite set of lines",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Abdel-rahman Mohamed, George E Dahl, and Geoffrey Hinton. Acoustic modeling using deep belief networks. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):14\u201322, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mohamed%2C%20Abdel-rahman%20Dahl%2C%20George%20E.%20Hinton%2C%20Geoffrey%20Acoustic%20modeling%20using%20deep%20belief%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mohamed%2C%20Abdel-rahman%20Dahl%2C%20George%20E.%20Hinton%2C%20Geoffrey%20Acoustic%20modeling%20using%20deep%20belief%20networks%202012"
        },
        {
            "id": "2",
            "entry": "[2] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Collobert%2C%20Ronan%20Weston%2C%20Jason%20A%20unified%20architecture%20for%20natural%20language%20processing%3A%20Deep%20neural%20networks%20with%20multitask%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Collobert%2C%20Ronan%20Weston%2C%20Jason%20A%20unified%20architecture%20for%20natural%20language%20processing%3A%20Deep%20neural%20networks%20with%20multitask%20learning%202008"
        },
        {
            "id": "3",
            "entry": "[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Hinton%2C%20Geoffrey%20E.%20Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks%202012"
        },
        {
            "id": "4",
            "entry": "[4] Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Advances in neural information processing systems, pages 494\u2013501, 1989.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blum%2C%20Avrim%20Rivest%2C%20Ronald%20L.%20Training%20a%203-node%20neural%20network%20is%20np-complete%201989",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blum%2C%20Avrim%20Rivest%2C%20Ronald%20L.%20Training%20a%203-node%20neural%20network%20is%20np-complete%201989"
        },
        {
            "id": "5",
            "entry": "[5] Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.06534"
        },
        {
            "id": "6",
            "entry": "[6] Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems, pages 1594\u20131602, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Shalev-Shwartz%2C%20Shai%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hazan%2C%20Elad%20Levy%2C%20Kfir%20Shalev-Shwartz%2C%20Shai%20Beyond%20convexity%3A%20Stochastic%20quasi-convex%20optimization%202015"
        },
        {
            "id": "7",
            "entry": "[7] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pages 927\u2013935, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kakade%2C%20Sham%20M.%20Kanade%2C%20Varun%20Shamir%2C%20Ohad%20Kalai%2C%20Adam%20Efficient%20learning%20of%20generalized%20linear%20and%20single%20index%20models%20with%20isotonic%20regression%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kakade%2C%20Sham%20M.%20Kanade%2C%20Varun%20Shamir%2C%20Ohad%20Kalai%2C%20Adam%20Efficient%20learning%20of%20generalized%20linear%20and%20single%20index%20models%20with%20isotonic%20regression%202011"
        },
        {
            "id": "8",
            "entry": "[8] Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.04591"
        },
        {
            "id": "9",
            "entry": "[9] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pages 586\u2013594, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kawaguchi%2C%20Kenji%20Deep%20learning%20without%20poor%20local%20minima%202016"
        },
        {
            "id": "10",
            "entry": "[10] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks. arXiv preprint arXiv:1707.02444, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.02444"
        },
        {
            "id": "11",
            "entry": "[11] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.08361"
        },
        {
            "id": "12",
            "entry": "[12] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192\u2013204, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Choromanska%2C%20Anna%20Henaff%2C%20Mikael%20Mathieu%2C%20Michael%20Arous%2C%20G%C3%A9rard%20Ben%20The%20loss%20surfaces%20of%20multilayer%20networks%202015"
        },
        {
            "id": "13",
            "entry": "[13] Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity. 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tian%2C%20Yuandong%20Symmetry-breaking%20convergence%20analysis%20of%20certain%20two-layered%20neural%20networks%20with%20relu%20nonlinearity%202016"
        },
        {
            "id": "14",
            "entry": "[14] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00560"
        },
        {
            "id": "15",
            "entry": "[15] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.03175"
        },
        {
            "id": "16",
            "entry": "[16] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.07966"
        },
        {
            "id": "17",
            "entry": "[17] Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva, and Ali Rahimi. Electron-proton dynamics in deep learning. arXiv preprint arXiv:1702.00458, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.00458"
        },
        {
            "id": "18",
            "entry": "[18] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. arXiv preprint arXiv:1705.09886, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.09886"
        },
        {
            "id": "19",
            "entry": "[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.08473"
        },
        {
            "id": "20",
            "entry": "[20] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04926"
        },
        {
            "id": "21",
            "entry": "[21] Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.08045"
        },
        {
            "id": "22",
            "entry": "[22] Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.09913"
        },
        {
            "id": "23",
            "entry": "[23] Rene Vidal, Joan Bruna, Raja Giryes, and Stefano Soatto. Mathematics of deep learning. arXiv preprint arXiv:1712.04741, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.04741"
        },
        {
            "id": "24",
            "entry": "[24] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00501"
        },
        {
            "id": "25",
            "entry": "[25] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177\u20131184, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "26",
            "entry": "[26] Amit Daniely, Roy Frostig, Vineet Gupta, and Yoram Singer. Random features for compositional kernels. arXiv preprint arXiv:1703.07872, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07872"
        },
        {
            "id": "27",
            "entry": "[27] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pages 2253\u20132261, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daniely%2C%20Amit%20Frostig%2C%20Roy%20Singer%2C%20Yoram%20Toward%20deeper%20understanding%20of%20neural%20networks%3A%20The%20power%20of%20initialization%20and%20a%20dual%20view%20on%20expressivity%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daniely%2C%20Amit%20Frostig%2C%20Roy%20Singer%2C%20Yoram%20Toward%20deeper%20understanding%20of%20neural%20networks%3A%20The%20power%20of%20initialization%20and%20a%20dual%20view%20on%20expressivity%202016"
        },
        {
            "id": "28",
            "entry": "[28] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural information processing systems, pages 342\u2013350, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Youngmin%20Saul%2C%20Lawrence%20K.%20Kernel%20methods%20for%20deep%20learning%202009"
        },
        {
            "id": "29",
            "entry": "[29] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1\u201353, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20Breaking%20the%20curse%20of%20dimensionality%20with%20convex%20neural%20networks%202017"
        },
        {
            "id": "30",
            "entry": "[30] Uri Heinemann, Roi Livni, Elad Eban, Gal Elidan, and Amir Globerson. Improper deep kernels. In Artificial Intelligence and Statistics, pages 1159\u20131167, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heinemann%2C%20Uri%20Livni%2C%20Roi%20Eban%2C%20Elad%20Elidan%2C%20Gal%20Improper%20deep%20kernels%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heinemann%2C%20Uri%20Livni%2C%20Roi%20Eban%2C%20Elad%20Elidan%2C%20Gal%20Improper%20deep%20kernels%202016"
        },
        {
            "id": "31",
            "entry": "[31] Noureddine El Karoui et al. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):1\u201350, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karoui%2C%20Noureddine%20El%20The%20spectrum%20of%20kernel%20random%20matrices%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karoui%2C%20Noureddine%20El%20The%20spectrum%20of%20kernel%20random%20matrices%202010"
        },
        {
            "id": "32",
            "entry": "[32] Y Do and V Vu. The spectrum of random kernel matrices. arXiv preprint arXiv:1206.3763, 2012.",
            "arxiv_url": "https://arxiv.org/pdf/1206.3763"
        },
        {
            "id": "33",
            "entry": "[33] Xiuyuan Cheng and Amit Singer. The spectrum of random inner-product kernel matrices. Random Matrices: Theory and Applications, 2(04):1350010, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cheng%2C%20Xiuyuan%20Singer%2C%20Amit%20The%20spectrum%20of%20random%20inner-product%20kernel%20matrices%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cheng%2C%20Xiuyuan%20Singer%2C%20Amit%20The%20spectrum%20of%20random%20inner-product%20kernel%20matrices%202013"
        },
        {
            "id": "34",
            "entry": "[34] Zhou Fan and Andrea Montanari. The spectral norm of random inner-product kernel matrices. arXiv preprint arXiv:1507.05343, 2015. ",
            "arxiv_url": "https://arxiv.org/pdf/1507.05343"
        }
    ]
}
