{
    "filename": "7627-unsupervised-attention-guided-image-to-image-translation.pdf",
    "metadata": {
        "title": "Unsupervised Attention-guided Image-to-Image Translation",
        "author": "Youssef Alami Mejjati, Christian Richardt, James Tompkin, Darren Cosker, Kwang In Kim",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7627-unsupervised-attention-guided-image-to-image-translation.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Current unsupervised image-to-image translation techniques struggle to focus their attention on individual objects without altering the background or the way multiple objects interact within a scene. Motivated by the important role of attention in human perception, we tackle this limitation by introducing unsupervised attention mechanisms that are jointly adversarially trained with the generators and discriminators. We demonstrate qualitatively and quantitatively that our approach attends to relevant regions in the image without requiring supervision, which creates more realistic mappings when compared to those of recent approaches."
    },
    "keywords": [
        {
            "term": "Recurrent Neural Networks",
            "url": "https://en.wikipedia.org/wiki/Recurrent_Neural_Network"
        },
        {
            "term": "generative adversarial networks",
            "url": "https://en.wikipedia.org/wiki/generative_adversarial_networks"
        }
    ],
    "highlights": [
        "Liu et al.\u2019s unsupervised image-to-image translation networks (UNIT) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] build upon Coupled generative adversarial networks by assuming the existence of a shared low-dimensional latent space between the source and target domains",
        "We demonstrate that our algorithm is able to overcome this limitation by incorporating attention into the image translation framework",
        "If the attention map sa was replaced by all ones, to mark the entire image as relevant, we obtain CycleGAN as a special case of our approach",
        "If sa was all zeros, the generated image would be identical to the input image due to the background term in Equation 1, and the discriminator would never be fooled by the generator",
        "By incorporating an attention mechanism into unsupervised image-to-image translation, we demonstrate significant improvements in the quality of generated images",
        "Our simple algorithm leverages the discriminator to learn accurate attention maps with no additional supervision. This suggests that our learned attention maps reflect where the discriminator looks before deciding whether an image is real or fake, making it an appropriate tool for investigating the behavior of adversarial networks"
    ],
    "key_statements": [
        "Liu et al.\u2019s unsupervised image-to-image translation networks (UNIT) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] build upon Coupled generative adversarial networks by assuming the existence of a shared low-dimensional latent space between the source and target domains",
        "We demonstrate that our algorithm is able to overcome this limitation by incorporating attention into the image translation framework",
        "If the attention map sa was replaced by all ones, to mark the entire image as relevant, we obtain CycleGAN as a special case of our approach",
        "If sa was all zeros, the generated image would be identical to the input image due to the background term in Equation 1, and the discriminator would never be fooled by the generator",
        "We report the mean Kernel Inception Distance value computed between generated samples using both source and target domains in Table 1",
        "Our approach differs in two ways: first, we feed the holistic image to the discriminator for the first 30 epochs, and afterwards show it only the masked image; second, we stop the training of the attention networks after 30 epochs to prevent it from focusing on the background as well",
        "By incorporating an attention mechanism into unsupervised image-to-image translation, we demonstrate significant improvements in the quality of generated images",
        "Our simple algorithm leverages the discriminator to learn accurate attention maps with no additional supervision. This suggests that our learned attention maps reflect where the discriminator looks before deciding whether an image is real or fake, making it an appropriate tool for investigating the behavior of adversarial networks"
    ],
    "summary": [
        "Liu et al.\u2019s unsupervised image-to-image translation networks (UNIT) [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] build upon Coupled GAN by assuming the existence of a shared low-dimensional latent space between the source and target domains.",
        "Chen et al.\u2019s contemporaneous work shares our goal of learning an attention map for image translation [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>]; we will discuss the differences between our methods after explaining our approach.",
        "We feed the input image s \u2208 S into the generator FS\u2192T , which maps s to the target domain T .",
        "The discriminator mechanism which makes GAN generators produce realistic images makes our attention networks find the domain-descriptive objects in the images.",
        "This added loss makes our framework more robust in two ways: (1) it enforces the attended regions in the generated image to conserve content, and (2) it encourages the attention maps to be sharp, as the cycle-consistency loss of unattended areas will always be zero.",
        "Equation 1 constrains the generators to act only on attended regions: as the attention networks train to become more accurate at finding the foreground, the generator improves in translating just the object of interest between domains, e.g., from horse to zebra.",
        "When optimizing the objective in Equation 7 beyond 30 epochs, real image inputs to the discriminator are dependent on the learned attention maps.",
        "Observing our learned attention maps, we can see that our approach is able to learn relevant image regions and ignore the background (Figure 3).",
        "Our algorithm successfully ignores background contents and reproduces the input images.",
        "Overall, our approach of learning attention maps within unsupervised image-to-image translation obtains more realistic results, particularly for datasets containing objects at multiple scales and with different backgrounds.",
        "These approaches are the best performing after our final implementation: AS acts on s, but on t via the inverse mapping, which influences the generators to still only translate relevant regions.",
        "Our approach differs in two ways: first, we feed the holistic image to the discriminator for the first 30 epochs, and afterwards show it only the masked image; second, we stop the training of the attention networks after 30 epochs to prevent it from focusing on the background as well.",
        "This suggests that our learned attention maps reflect where the discriminator looks before deciding whether an image is real or fake, making it an appropriate tool for investigating the behavior of adversarial networks."
    ],
    "headline": "We demonstrate qualitatively and quantitatively that our approach attends to relevant regions in the image without requiring supervision, which creates more realistic mappings when compared to those of recent approaches",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] J. Zhu, T. Park, P. Isola, and A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017"
        },
        {
            "id": "2",
            "entry": "[2] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, and X. Tang. Residual attention network for image classification. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20F.%20Jiang%2C%20M.%20Qian%2C%20C.%20Yang%2C%20S.%20Residual%20attention%20network%20for%20image%20classification%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20F.%20Jiang%2C%20M.%20Qian%2C%20C.%20Yang%2C%20S.%20Residual%20attention%20network%20for%20image%20classification%202017"
        },
        {
            "id": "3",
            "entry": "[3] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to discover cross-domain relations with generative adversarial networks. JMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20T.%20Cha%2C%20M.%20Kim%2C%20H.%20Lee%2C%20J.%20Learning%20to%20discover%20cross-domain%20relations%20with%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20T.%20Cha%2C%20M.%20Kim%2C%20H.%20Lee%2C%20J.%20Learning%20to%20discover%20cross-domain%20relations%20with%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "4",
            "entry": "[4] M. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.%20Breuel%2C%20T.%20Kautz%2C%20J.%20Unsupervised%20image-to-image%20translation%20networks%202017"
        },
        {
            "id": "5",
            "entry": "[5] Z. Yi, H. Zhang, P. Tan, and M. Gong. DualGAN: Unsupervised dual learning for image-to-image translation. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yi%2C%20Z.%20Zhang%2C%20H.%20Tan%2C%20P.%20Gong%2C%20M.%20DualGAN%3A%20Unsupervised%20dual%20learning%20for%20image-to-image%20translation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yi%2C%20Z.%20Zhang%2C%20H.%20Tan%2C%20P.%20Gong%2C%20M.%20DualGAN%3A%20Unsupervised%20dual%20learning%20for%20image-to-image%20translation%202017"
        },
        {
            "id": "6",
            "entry": "[6] Y. Cao, Z. Zhou, W. Zhang, and Y. Yu. Unsupervised diverse colorization via generative adversarial networks. In ECML-PKDD, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cao%2C%20Y.%20Zhou%2C%20Z.%20Zhang%2C%20W.%20Yu%2C%20Y.%20Unsupervised%20diverse%20colorization%20via%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cao%2C%20Y.%20Zhou%2C%20Z.%20Zhang%2C%20W.%20Yu%2C%20Y.%20Unsupervised%20diverse%20colorization%20via%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "7",
            "entry": "[7] C. Ledig, L. Theis, F. Husz\u00e1r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Photo-realistic%20single%20image%20super-resolution%20using%20a%20generative%20adversarial%20network%202017"
        },
        {
            "id": "8",
            "entry": "[8] B. Wu, H. Duan, Z. Liu, and G. Sun. SRPGAN: Perceptual generative adversarial network for single image super resolution. arXiv preprint arXiv:1712.05927, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.05927"
        },
        {
            "id": "9",
            "entry": "[9] P. Isola, J. Zhu, T. Zhou, and A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Isola%2C%20P.%20Zhu%2C%20J.%20Zhou%2C%20T.%20Efros%2C%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Isola%2C%20P.%20Zhu%2C%20J.%20Zhou%2C%20T.%20Efros%2C%20A.%20Image-to-image%20translation%20with%20conditional%20adversarial%20networks%202017"
        },
        {
            "id": "10",
            "entry": "[10] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim. Image to image translation for domain adaptation. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murez%2C%20Z.%20Kolouri%2C%20S.%20Kriegman%2C%20D.%20Ramamoorthi%2C%20R.%20Image%20to%20image%20translation%20for%20domain%20adaptation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murez%2C%20Z.%20Kolouri%2C%20S.%20Kriegman%2C%20D.%20Ramamoorthi%2C%20R.%20Image%20to%20image%20translation%20for%20domain%20adaptation%202018"
        },
        {
            "id": "11",
            "entry": "[11] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, and C. Malossi. BAGAN: Data augmentation with balancing GAN. arXiv preprint arXiv:1803.09655, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.09655"
        },
        {
            "id": "12",
            "entry": "[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=I%20Goodfellow%20J%20PougetAbadie%20M%20Mirza%20B%20Xu%20D%20WardeFarley%20S%20Ozair%20A%20Courville%20and%20Y%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "13",
            "entry": "[13] C. Li and M. Wand. Precomputed real-time texture synthesis with Markovian generative adversarial networks. In ECCV, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Wand%2C%20M.%20Precomputed%20real-time%20texture%20synthesis%20with%20Markovian%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20C.%20Wand%2C%20M.%20Precomputed%20real-time%20texture%20synthesis%20with%20Markovian%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "14",
            "entry": "[14] R. Rensink. The dynamic representation of scenes. Visual Cognition, 7(1\u20133):17\u201342, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rensink%2C%20R.%20The%20dynamic%20representation%20of%20scenes%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rensink%2C%20R.%20The%20dynamic%20representation%20of%20scenes%202000"
        },
        {
            "id": "15",
            "entry": "[15] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu. Recurrent models of visual attention. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mnih%2C%20V.%20Heess%2C%20N.%20Graves%2C%20A.%20Kavukcuoglu%2C%20K.%20Recurrent%20models%20of%20visual%20attention%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mnih%2C%20V.%20Heess%2C%20N.%20Graves%2C%20A.%20Kavukcuoglu%2C%20K.%20Recurrent%20models%20of%20visual%20attention%202014"
        },
        {
            "id": "16",
            "entry": "[16] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20M.-Y.%20Tuzel%2C%20O.%20Coupled%20generative%20adversarial%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20M.-Y.%20Tuzel%2C%20O.%20Coupled%20generative%20adversarial%20networks%202016"
        },
        {
            "id": "17",
            "entry": "[17] X. Huang, M. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Liu%2C%20M.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20X.%20Liu%2C%20M.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-to-image%20translation%202018"
        },
        {
            "id": "18",
            "entry": "[18] S. Ma, J. Fu, C. Wen Chen, and T. Mei. DA-GAN: Instance-level image translation by deep attention generative adversarial networks. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ma%2C%20S.%20Fu%2C%20J.%20Chen%2C%20C.Wen%20Mei%2C%20T.%20DA-GAN%3A%20Instance-level%20image%20translation%20by%20deep%20attention%20generative%20adversarial%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ma%2C%20S.%20Fu%2C%20J.%20Chen%2C%20C.Wen%20Mei%2C%20T.%20DA-GAN%3A%20Instance-level%20image%20translation%20by%20deep%20attention%20generative%20adversarial%20networks%202018"
        },
        {
            "id": "19",
            "entry": "[19] N. Liu, J. Han, and M.-H. Yang. PiCANet: Learning pixel-wise contextual attention for saliency detection. In CVPR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20N.%20Han%2C%20J.%20Yang%2C%20M.-H.%20PiCANet%3A%20Learning%20pixel-wise%20contextual%20attention%20for%20saliency%20detection%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20N.%20Han%2C%20J.%20Yang%2C%20M.-H.%20PiCANet%3A%20Learning%20pixel-wise%20contextual%20attention%20for%20saliency%20detection%202018"
        },
        {
            "id": "20",
            "entry": "[20] J. Kuen, Z. Wang, and G. Wang. Recurrent attentional networks for saliency detection. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kuen%2C%20J.%20Wang%2C%20Z.%20Wang%2C%20G.%20Recurrent%20attentional%20networks%20for%20saliency%20detection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kuen%2C%20J.%20Wang%2C%20Z.%20Wang%2C%20G.%20Recurrent%20attentional%20networks%20for%20saliency%20detection%202016"
        },
        {
            "id": "21",
            "entry": "[21] S. Jetley, N. Lord, N. Lee, and P. Torr. Learn to pay attention. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jetley%2C%20S.%20Lord%2C%20N.%20Lee%2C%20N.%20Torr%2C%20P.%20Learn%20to%20pay%20attention%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jetley%2C%20S.%20Lord%2C%20N.%20Lee%2C%20N.%20Torr%2C%20P.%20Learn%20to%20pay%20attention%202018"
        },
        {
            "id": "22",
            "entry": "[22] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.08318"
        },
        {
            "id": "23",
            "entry": "[23] J. Yang, A. Kannan, D. Batra, and D. Parikh. LR-GAN: Layered recursive generative adversarial networks for image generation. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20J.%20Kannan%2C%20A.%20Batra%2C%20D.%20Parikh%2C%20D.%20LR-GAN%3A%20Layered%20recursive%20generative%20adversarial%20networks%20for%20image%20generation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20J.%20Kannan%2C%20A.%20Batra%2C%20D.%20Parikh%2C%20D.%20LR-GAN%3A%20Layered%20recursive%20generative%20adversarial%20networks%20for%20image%20generation%202017"
        },
        {
            "id": "24",
            "entry": "[24] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vondrick%2C%20C.%20Pirsiavash%2C%20H.%20Torralba%2C%20A.%20Generating%20videos%20with%20scene%20dynamics%202016"
        },
        {
            "id": "25",
            "entry": "[25] X. Chen, C. Xu, X. Yang, and D. Tao. Attention-GAN for object transfiguration in wild images. In ECCV, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Xu%2C%20C.%20Yang%2C%20X.%20Tao%2C%20D.%20Attention-GAN%20for%20object%20transfiguration%20in%20wild%20images%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20X.%20Xu%2C%20C.%20Yang%2C%20X.%20Tao%2C%20D.%20Attention-GAN%20for%20object%20transfiguration%20in%20wild%20images%202018"
        },
        {
            "id": "26",
            "entry": "[26] X. Mao, Q. Li, H. Xie, R. Lau, Z. Wang, and S. P. Smolley. Least squares generative adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.%20Least%20squares%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mao%2C%20X.%20Li%2C%20Q.%20Xie%2C%20H.%20Lau%2C%20R.%20Least%20squares%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "27",
            "entry": "[27] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjovsky%2C%20M.%20Chintala%2C%20S.%20Bottou%2C%20L.%20Wasserstein%20generative%20adversarial%20networks%202017"
        },
        {
            "id": "28",
            "entry": "[28] C. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In CVPR, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lampert%2C%20C.%20Nickisch%2C%20H.%20Harmeling%2C%20S.%20Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lampert%2C%20C.%20Nickisch%2C%20H.%20Harmeling%2C%20S.%20Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer%202009"
        },
        {
            "id": "29",
            "entry": "[29] M. Binkowski, D. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M%20Binkowski%20D%20Sutherland%20M%20Arbel%20and%20A%20Gretton%20Demystifying%20MMD%20GANs%20In%20ICLR%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M%20Binkowski%20D%20Sutherland%20M%20Arbel%20and%20A%20Gretton%20Demystifying%20MMD%20GANs%20In%20ICLR%202018"
        },
        {
            "id": "30",
            "entry": "[30] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception architecture for computer vision. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20Inception%20architecture%20for%20computer%20vision%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Szegedy%2C%20C.%20Vanhoucke%2C%20V.%20Ioffe%2C%20S.%20Shlens%2C%20J.%20Rethinking%20the%20Inception%20architecture%20for%20computer%20vision%202016"
        },
        {
            "id": "31",
            "entry": "[31] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Klambauer. GANs trained by a two time-scale update rule converge to a Nash equilibrium. In NIPS, 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20Nash%20equilibrium%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Heusel%2C%20M.%20Ramsauer%2C%20H.%20Unterthiner%2C%20T.%20Nessler%2C%20B.%20GANs%20trained%20by%20a%20two%20time-scale%20update%20rule%20converge%20to%20a%20Nash%20equilibrium%202017"
        }
    ]
}
