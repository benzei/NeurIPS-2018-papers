{
    "filename": "8249-diversity-driven-exploration-strategy-for-deep-reinforcement-learning.pdf",
    "metadata": {
        "title": "Diversity-Driven Exploration Strategy for Deep Reinforcement Learning",
        "author": "Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, Chun-Yi Lee",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8249-diversity-driven-exploration-strategy-for-deep-reinforcement-learning.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both offand on-policy reinforcement learning algorithms. We show that by simply adding a distance measure regularization to the loss function, the proposed methodology significantly enhances an agent\u2019s exploratory behavior, and thus prevents the policy from being trapped in local optima. We further propose an adaptive scaling strategy to enhance the performance. We demonstrate the effectiveness of our method in huge 2D gridworlds and a variety of benchmark environments, including Atari 2600 and MuJoCo. Experimental results validate that our method outperforms baseline approaches in most tasks in terms of mean scores and exploration efficiency."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "mean square error",
            "url": "https://en.wikipedia.org/wiki/mean_square_error"
        },
        {
            "term": "Ministry of Science and Technology",
            "url": "https://en.wikipedia.org/wiki/Ministry_of_Science_and_Technology"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "state space",
            "url": "https://en.wikipedia.org/wiki/state_space"
        }
    ],
    "highlights": [
        "We review the concept of RL, and the offand on-policy methods used in this paper. 2.1",
        "We propose to use a distance measure to modify the loss function to tackle the problems of large state spaces, deceptiveness and sparsity in reward signals",
        "We presented a diversity-driven exploration strategy, which can be effectively combined with current RL algorithms",
        "We proposed to promote exploration by encouraging an agent to engage in different behaviors from its previous ones, and showed that this can be achieved through the use of an additional distance measure term to the loss function",
        "We performed experiments in various benchmark environments and demonstrated that our method leads to superior performance in most of the settings",
        "We verified that the proposed approach can deal with sparsity and deceptiveness in the reward function, and explore in large state spaces efficiently"
    ],
    "key_statements": [
        "We review the concept of RL, and the offand on-policy methods used in this paper. 2.1",
        "We present a diversity-driven exploration strategy, a methodology that encourages a deep reinforcement learning agent to attempt policies different from its prior policies",
        "We propose to use a distance measure to modify the loss function to tackle the problems of large state spaces, deceptiveness and sparsity in reward signals",
        "We demonstrate that our methodology is complementary and applicable to most offand on-policy deep reinforcement learning algorithms",
        "We further propose an adaptive scaling strategy, which dynamically scales the effect of the distance measure for enhancing the overall performance",
        "We briefly review two representative off-policy methods, namely Deep Q-Network (DQN) [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and Deep Deterministic Policy Gradient (DDPG) [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>]",
        "We show that diversity-driven exploration can be readily applied to existing algorithms for both discrete (DQN) and continuous (DDPG) control tasks, with only a few modifications to the experience replay buffer Z",
        "\u0391 can be linearly annealed over time, we find this solution less than ideal in some cases",
        "While all of the methods are able to succeed in these environments, it can be noticed that Div-Deep Deterministic Policy Gradient learns faster, and achieves higher average rewards",
        "As our diversity-driven exploration strategy relates to several prior works in RL, this section provides a comprehensive comparison with those researches in terms of objectives and implementations",
        "We presented a diversity-driven exploration strategy, which can be effectively combined with current RL algorithms",
        "We proposed to promote exploration by encouraging an agent to engage in different behaviors from its previous ones, and showed that this can be achieved through the use of an additional distance measure term to the loss function",
        "We performed experiments in various benchmark environments and demonstrated that our method leads to superior performance in most of the settings",
        "We verified that the proposed approach can deal with sparsity and deceptiveness in the reward function, and explore in large state spaces efficiently"
    ],
    "summary": [
        "We review the concept of RL, and the offand on-policy methods used in this paper. 2.1.",
        "We propose to use a distance measure to modify the loss function to tackle the problems of large state spaces, deceptiveness and sparsity in reward signals.",
        "To validate the effectiveness of the proposed diversity-driven exploration strategy, we first demonstrate that our method does lead to better exploratory behaviors in 2D gridworlds with deceptive or sparse rewards.",
        "Diversity-driven exploration is an effective way to motivate an agent to examine a richer set of states, as well as provide it with an approach to escape from sub-optimal policies.",
        "Based on the above results, we conclude that the proposed diversity-driven exploration strategy does offer advantages that are favorable in exploring large gridworlds with deceptive or sparse rewards.",
        "We observe that our strategy helps an agent explore a wider area more efficiently compared to the other baselines, which is especially useful when the state spaces become sufficiently large.",
        "To evaluate the proposed methods in continuous control domains, we conduct experiments in MuJoCo environments with (1) deceptive rewards, (2) large state space, and (3) sparse rewards, and plot the in-training median scores in Fig. 5.",
        "We investigate and discuss how diversity-driven loss influences the agents\u2019 behavior in these environments, and demonstrate that our methodology does lead to superior exploration efficiency and performance.",
        "Even though these environments provide well-defined reward functions, it is still challenging for agents to learn feasible policies, as they have to explore the enormous state spaces.",
        "While all of the methods are able to succeed in these environments, it can be noticed that Div-DDPG learns faster, and achieves higher average rewards.",
        "As our diversity-driven exploration strategy relates to several prior works in RL, this section provides a comprehensive comparison with those researches in terms of objectives and implementations.",
        "Our method aims to improve the insufficient exploration problem in deceptive and sparse reward settings, rather than addressing the premature convergence problem in policy learning.",
        "Our method encourages exploration by maximizing the distance measure between the current and prior policies, instead of restraining their state-action distribution or KL-divergence.",
        "We proposed to promote exploration by encouraging an agent to engage in different behaviors from its previous ones, and showed that this can be achieved through the use of an additional distance measure term to the loss function.",
        "We verified that the proposed approach can deal with sparsity and deceptiveness in the reward function, and explore in large state spaces efficiently.",
        "We analyzed the adaptive scaling methods, and validated that the methods do improve the performance"
    ],
    "headline": "We present a diversity-driven approach for exploration, which can be combined with both offand on-policy reinforcement learning algorithms",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] V. et al. Mnih. Human-level control through deep reinforcement learning. Nature, vol. 518, no. 7540, pp. 529-533, February 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=V.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015-02",
            "oa_query": "https://api.scholarcy.com/oa_version?query=V.%20Human-level%20control%20through%20deep%20reinforcement%20learning%202015-02"
        },
        {
            "id": "2",
            "entry": "[2] D. et al. Silver. Mastering the game of Go with deep neural networks and tree search. Nature, vol. 529, no. 7587, pp. 484-489, January 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016-01",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D.%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%202016-01"
        },
        {
            "id": "3",
            "entry": "[3] M. et al. Zhang. Learning deep neural network policies with continuous memory states. In Proc. Int. Conf. Robotics and Automation (ICRA), pages 520\u2013527, May 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M.%20Learning%20deep%20neural%20network%20policies%20with%20continuous%20memory%20states%202016-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M.%20Learning%20deep%20neural%20network%20policies%20with%20continuous%20memory%20states%202016-05"
        },
        {
            "id": "4",
            "entry": "[4] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Proc. Int. Conf. Intelligent Robots and Systems (IROS), pages 5026\u20135033, December 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Todorov%2C%20E.%20Erez%2C%20T.%20Tassa%2C%20Y.%20Mujoco%3A%20A%20physics%20engine%20for%20model-based%20control%202012-12"
        },
        {
            "id": "5",
            "entry": "[5] M. et al. Plappert. Parameter space noise for exploration. In Proc. Int. Conf. Learning Representations (ICLR), May 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M.%20Plappert.%20Parameter%20space%20noise%20for%20exploration%202018-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M.%20Plappert.%20Parameter%20space%20noise%20for%20exploration%202018-05"
        },
        {
            "id": "6",
            "entry": "[6] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutton%2C%20R.S.%20Barto%2C%20A.G.%20Reinforcement%20learning%3A%20An%20introduction%201998"
        },
        {
            "id": "7",
            "entry": "[7] V. et al. Mnih. Asynchronous methods for deep reinforcement learning. In Proc. Int. Conf. Machine Learning (ICML), pages 1928\u20131937, June 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=V.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=V.%20Asynchronous%20methods%20for%20deep%20reinforcement%20learning%202016-06"
        },
        {
            "id": "8",
            "entry": "[8] M. et al. Fortunato. Noisy networks for exploration. In Proc. Int. Conf. Learning Representations (ICLR), May 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M.%20Noisy%20networks%20for%20exploration%202018-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M.%20Noisy%20networks%20for%20exploration%202018-05"
        },
        {
            "id": "9",
            "entry": "[9] I. Osband, D. Russo, Z. Wen, and B. Van Roy. Deep exploration via randomized value functions. arXiv:1703.07608, March 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.07608"
        },
        {
            "id": "10",
            "entry": "[10] R. et al. Houthooft. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems (NIPS), pages 1109\u20131117, December 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=R.%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=R.%20Vime%3A%20Variational%20information%20maximizing%20exploration%202016-12"
        },
        {
            "id": "11",
            "entry": "[11] H. et al. Tang. #Exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pages 2750\u20132759, December 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=H.%20%23Exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=H.%20%23Exploration%3A%20A%20study%20of%20count-based%20exploration%20for%20deep%20reinforcement%20learning%202017-12"
        },
        {
            "id": "12",
            "entry": "[12] A. et al. van den Oord. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems (NIPS), pages 4790\u20134798, December 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=A.%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=A.%20Conditional%20image%20generation%20with%20pixelcnn%20decoders%202016-12"
        },
        {
            "id": "13",
            "entry": "[13] B. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv:1507.00814, November 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.00814"
        },
        {
            "id": "14",
            "entry": "[14] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by selfsupervised prediction. In Proc. Int. Conf. Machine Learning (ICML), pages 2778\u20132787, August 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pathak%2C%20D.%20Agrawal%2C%20P.%20Efros%2C%20A.A.%20Darrell%2C%20T.%20Curiosity-driven%20exploration%20by%20selfsupervised%20prediction%202017-08"
        },
        {
            "id": "15",
            "entry": "[15] I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped DQN. In Advances in Neural Information Processing Systems (NIPS), pages 4026\u20134034, December 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osband%2C%20I.%20Blundell%2C%20C.%20Pritzel%2C%20A.%20Roy%2C%20B.Van%20Deep%20exploration%20via%20bootstrapped%20DQN%202016-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osband%2C%20I.%20Blundell%2C%20C.%20Pritzel%2C%20A.%20Roy%2C%20B.Van%20Deep%20exploration%20via%20bootstrapped%20DQN%202016-12"
        },
        {
            "id": "16",
            "entry": "[16] J. Lehman and K. O. Stanley. Abandoning objectives: Evolution through the search for novelty alone. Evolutionary computation, 19(2):189\u2013223, May 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20J.%20Stanley%2C%20K.O.%20Abandoning%20objectives%3A%20Evolution%20through%20the%20search%20for%20novelty%20alone%202011-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20J.%20Stanley%2C%20K.O.%20Abandoning%20objectives%3A%20Evolution%20through%20the%20search%20for%20novelty%20alone%202011-05"
        },
        {
            "id": "17",
            "entry": "[17] J. Lehman and K. O. Stanley. Evolving a diversity of virtual creatures through novelty search and local competition. In Proc. Conf. Genetic and Evolutionary Computation, pages 211\u2013218, July 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20J.%20Stanley%2C%20K.O.%20Evolving%20a%20diversity%20of%20virtual%20creatures%20through%20novelty%20search%20and%20local%20competition%202011-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lehman%2C%20J.%20Stanley%2C%20K.O.%20Evolving%20a%20diversity%20of%20virtual%20creatures%20through%20novelty%20search%20and%20local%20competition%202011-07"
        },
        {
            "id": "18",
            "entry": "[18] J. Lehman and K. O. Stanley. Novelty search and the problem with objectives. Genetic Programming Theory and Practice IX, pages 37\u201356, October 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lehman%2C%20J.%20Stanley%2C%20K.O.%20Novelty%20search%20and%20the%20problem%20with%20objectives.%20Genetic%20Programming%20Theory%20and%20Practice%20IX%202011-10"
        },
        {
            "id": "19",
            "entry": "[19] E. et al. Conti. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. arXiv:1712.06560, December 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.06560"
        },
        {
            "id": "20",
            "entry": "[20] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research (JAIR), 47: 253\u2013279, May 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-05",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellemare%2C%20M.G.%20Naddaf%2C%20Y.%20Veness%2C%20J.%20Bowling%2C%20M.%20The%20arcade%20learning%20environment%3A%20An%20evaluation%20platform%20for%20general%20agents%202013-05"
        },
        {
            "id": "21",
            "entry": "[21] T. P. et al. Lillicrap. Continuous control with deep reinforcement learning. arXiv:1509.02971, February 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1509.02971"
        },
        {
            "id": "22",
            "entry": "[22] D. et al. Silver. Deterministic policy gradient algorithms. In Proc. Int. Conf. Machine Learning (ICML), pages 387\u2013395, June 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=D.%20Deterministic%20policy%20gradient%20algorithms%202014-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=D.%20Deterministic%20policy%20gradient%20algorithms%202014-06"
        },
        {
            "id": "23",
            "entry": "[23] Y. et al. Wu. Scalable trust-region method for deep reinforcement learning using Kroneckerfactored approximation. In Advances in Neural Information Processing Systems (NIPS), pages 5285\u20135294, December 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y.%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20Kroneckerfactored%20approximation%202017-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y.%20Scalable%20trust-region%20method%20for%20deep%20reinforcement%20learning%20using%20Kroneckerfactored%20approximation%202017-12"
        },
        {
            "id": "24",
            "entry": "[24] M. et al. Bellemare. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems (NIPS), pages 1471\u20131479, December 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=M.%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=M.%20Unifying%20count-based%20exploration%20and%20intrinsic%20motivation%202016-12"
        },
        {
            "id": "25",
            "entry": "[25] Jan Peters, Katharina M\u00fclling, and Yasemin Altun. Relative entropy policy search. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1607\u20131612.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Altun%2C%20Yasemin%20Relative%20entropy%20policy%20search",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Peters%2C%20Jan%20M%C3%BClling%2C%20Katharina%20Altun%2C%20Yasemin%20Relative%20entropy%20policy%20search"
        },
        {
            "id": "26",
            "entry": "[26] J. et al. Schulman. Trust region policy optimization. In Proc. Int. Conf. Machine Learning (ICML), pages 1889\u20131897, July 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=J.%20Trust%20region%20policy%20optimization%202015-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=J.%20Trust%20region%20policy%20optimization%202015-07"
        },
        {
            "id": "27",
            "entry": "[27] J. et al. Schulman. Proximal policy optimization algorithms. arXiv:1707.06347, August 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.06347"
        },
        {
            "id": "28",
            "entry": "[28] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv:1801.01290, January 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01290"
        },
        {
            "id": "29",
            "entry": "[29] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proc. of Machine Learning Research, pages 1861\u20131870, Stockholmsm\u00e4ssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Haarnoja%2C%20Tuomas%20Zhou%2C%20Aurick%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018-07",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Haarnoja%2C%20Tuomas%20Zhou%2C%20Aurick%20Abbeel%2C%20Pieter%20Levine%2C%20Sergey%20Soft%20actor-critic%3A%20Off-policy%20maximum%20entropy%20deep%20reinforcement%20learning%20with%20a%20stochastic%20actor%202018-07"
        }
    ]
}
