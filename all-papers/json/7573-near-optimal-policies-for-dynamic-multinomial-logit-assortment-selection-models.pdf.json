{
    "filename": "7573-near-optimal-policies-for-dynamic-multinomial-logit-assortment-selection-models.pdf",
    "metadata": {
        "title": "Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models",
        "author": "Yining Wang, Xi Chen, Yuan Zhou",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7573-near-optimal-policies-for-dynamic-multinomial-logit-assortment-selection-models.pdf"
        },
        "abstract": "In this paper we consider the dynamic assortment selection problem under an uncapacitated multinomial-logit (MNL) model. By carefully analyzing a revenue potential function, independent regret we show bound of"
    },
    "keywords": [
        "regret analysis",
        "armed bandit problem",
        "multi armed bandit problem",
        "unimodal bandit",
        "multinomial logit choice model",
        "dynamic assortment planning",
        "assortment selection",
        "assortment planning",
        "trisection algorithm",
        "dynamic assortment"
    ],
    "highlights": [
        "Assortment planning has a wide range of applications in e-commerce and online advertising",
        "Given a large number of substitutable products, the assortment planning problem refers to the selection of a subset of products (a.k.a., an assortment) offering to a customer such that the expected revenue is maximized [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]",
        "To enable such an N -independent regret, we provide a refined analysis of a certain unimodal revenue potential function first studied in [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and consider a trisection algorithm on revenue levels, borrowing ideas from literature on unimodal bandits on either discrete or continuous arm domains [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "Most previous results on dynamic assortment selection [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] and unimodal/convex bandit [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] have additional log T terms in regret upper bounds. (The work of [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] derived gap-dependent regret bounds for unimodal bandit, which is not comparable to our bounds.) The improvement from log T to log log T achieved in this paper is done by using a sharper law-of-the-iterated-logarithm (LIL) type concentration inequalities [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and an adaptive confidence strategy similar to the MOSS algorithm for multi-armed bandits [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "For the single assortment S \u201c tiu, because the preference parameter for each item is rather small, no single assortment can produce amrb0nen.yav4elnle6etyhnxt,\u02c6eupSfeeloc1ao\u201crtw5fe{tdShNtoe1rfies,\u02c6\u201cvl2aaae,rrpn0ogpu.ue8renoNnd,peuNxr5mic\u201ca.ue5bt,ee2e\u201ddb5{risep.na,51cgs2atshuo0aesr.1ne5te2md\u0159xqep\u0159\u201cniNe\u201cptic2P10tS0Se.r{4divN2v\u201cir5ieq\u00d1v{\u0105\u00d1ptppe1in0u10`.Pe45.42{2o5rN20fN.\u02c6{STNsh1iq:e5s0{.a\u201cra8NbirNoo1vu\u011b0en\u201c{ddNp0i26s.104c.\u201c272u.5uss,{6NTip.ow17hnq5ee.sraehhF1nfoaoo5dwvrrqees\u0159t,h\u201ct\u0159hNiet\u201chaif0eP1tuS.av4lelirrx2eiap2\u00d1vvspe.iescontF1\u00d1reupitd5e-threshold r P p0.4, 0.5q is mandatory to extract a portion of the items ti P rN s : ri \u011b ru that attain the optimal expected revenue, which is highly non-trivial for a dynamic assortment selection algorithm to identify"
    ],
    "key_statements": [
        "Assortment planning has a wide range of applications in e-commerce and online advertising",
        "Given a large number of substitutable products, the assortment planning problem refers to the selection of a subset of products (a.k.a., an assortment) offering to a customer such that the expected revenue is maximized [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c14\" href=\"#r14\">14</a>]",
        "An important aspect of Theorem 1 is that our regret bound is completely independent of the number of items N , which improves the existing dynamic regret minimization results on the MNL assortment selection problem [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>]",
        "To enable such an N -independent regret, we provide a refined analysis of a certain unimodal revenue potential function first studied in [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>] and consider a trisection algorithm on revenue levels, borrowing ideas from literature on unimodal bandits on either discrete or continuous arm domains [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "Most previous results on dynamic assortment selection [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] and unimodal/convex bandit [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] have additional log T terms in regret upper bounds. (The work of [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] derived gap-dependent regret bounds for unimodal bandit, which is not comparable to our bounds.) The improvement from log T to log log T achieved in this paper is done by using a sharper law-of-the-iterated-logarithm (LIL) type concentration inequalities [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and an adaptive confidence strategy similar to the MOSS algorithm for multi-armed bandits [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>]",
        "For the single assortment S \u201c tiu, because the preference parameter for each item is rather small, no single assortment can produce amrb0nen.yav4elnle6etyhnxt,\u02c6eupSfeeloc1ao\u201crtw5fe{tdShNtoe1rfies,\u02c6\u201cvl2aaae,rrpn0ogpu.ue8renoNnd,peuNxr5mic\u201ca.ue5bt,ee2e\u201ddb5{risep.na,51cgs2atshuo0aesr.1ne5te2md\u0159xqep\u0159\u201cniNe\u201cptic2P10tS0Se.r{4divN2v\u201cir5ieq\u00d1v{\u0105\u00d1ptppe1in0u10`.Pe45.42{2o5rN20fN.\u02c6{STNsh1iq:e5s0{.a\u201cra8NbirNoo1vu\u011b0en\u201c{ddNp0i26s.104c.\u201c272u.5uss,{6NTip.ow17hnq5ee.sraehhF1nfoaoo5dwvrrqees\u0159t,h\u201ct\u0159hNiet\u201chaif0eP1tuS.av4lelirrx2eiap2\u00d1vvspe.iescontF1\u00d1reupitd5e-threshold r P p0.4, 0.5q is mandatory to extract a portion of the items ti P rN s : ri \u011b ru that attain the optimal expected revenue, which is highly non-trivial for a dynamic assortment selection algorithm to identify"
    ],
    "summary": [
        "Assortment planning has a wide range of applications in e-commerce and online advertising.",
        "Trisection based algorithm log log T q, which matches achieves an information itemtheoretical lower bounds up to iterated logarithmic terms.",
        "An important aspect of Theorem 1 is that our regret bound is completely independent of the number of items N , which improves the existing dynamic regret minimization results on the MNL assortment selection problem [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "Most previous results on dynamic assortment selection [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>, <a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] and unimodal/convex bandit [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] have additional log T terms in regret upper bounds.",
        "(The work of [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>] derived gap-dependent regret bounds for unimodal bandit, which is not comparable to our bounds.) The improvement from log T to log log T achieved in this paper is done by using a sharper law-of-the-iterated-logarithm (LIL) type concentration inequalities [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] and an adaptive confidence strategy similar to the MOSS algorithm for multi-armed bandits [<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>].",
        "The work of [<a class=\"ref-link\" id=\"c19\" href=\"#r19\">19</a>] is perhaps the closest to our paper, which analyzed the same revenue potential function and designed a golden-ratio search algorithm whose regret only depends logarithmically on the number of items.",
        "In this work we relax the gap assumption and remove the additional log N dependency by a more refined analysis of properties of the revenue potential function and borrowing \u201ctrisection\u201d ideas from the unimodal bandit literature [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>, <a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "Additional structures such as \u201cinverse Lipschitz continuity\u201d (e.g., |\u03bcpiq\u03bcpjq| \u011b L|ij|) or convexity are imposed to ensure improvement of regret, both of which fail to hold for the potential function F arising from unca?pacitated MNL assortment choice problems.",
        "We propose an algorithm based on trisections of the potential function F in order to locate level \u03b8at which the maximum expected revenue F \u201c F p\u03b8q is attained.",
        "For the single assortment S \u201c tiu, because the preference parameter for each item is rather small, no single assortment can produce amrb0nen.yav4elnle6etyhnxt,\u02c6eupSfeeloc1ao\u201crtw5fe{tdShNtoe1rfies,\u02c6\u201cvl2aaae,rrpn0ogpu.ue8renoNnd,peuNxr5mic\u201ca.ue5bt,ee2e\u201ddb5{risep.na,51cgs2atshuo0aesr.1ne5te2md\u0159xqep\u0159\u201cniNe\u201cptic2P10tS0Se.r{4divN2v\u201cir5ieq\u00d1v{\u0105\u00d1ptppe1in0u10`.Pe45.42{2o5rN20fN.\u02c6{STNsh1iq:e5s0{.a\u201cra8NbirNoo1vu\u011b0en\u201c{ddNp0i26s.104c.\u201c272u.5uss,{6NTip.ow17hnq5ee.sraehhF1nfoaoo5dwvrrqees\u0159t,h\u201ct\u0159hNiet\u201chaif0eP1tuS.av4lelirrx2eiap2\u00d1vvspe.iescontF1\u00d1reupitd5e-threshold r P p0.4, 0.5q is mandatory to extract a portion of the items ti P rN s : ri \u011b ru that attain the optimal expected revenue, which is highly non-trivial for a dynamic assortment selection algorithm to identify.",
        "We observe that as the number of items (N ) becomes large, our algorithms (TRISEC and LIL-TRISEC) achieve smaller mean and maximum regret compared to their competitors, and LILTRISEC consistently outperforms TRISEC in all settings.",
        "One important term in the upper bound in Theorem 2 and open question is to further remove eventually achieve upper and lower regret bounds that match each other up to universal numerical constants."
    ],
    "headline": "By carefully analyzing a revenue potential function, independent regret we show bound of",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] A. Agarwal, D. P. Foster, D. Hsu, S. M. Kakade, and A. Rakhlin. Stochastic convex optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213\u2013240, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Foster%2C%20D.P.%20Hsu%2C%20D.%20Kakade%2C%20S.M.%20Stochastic%20convex%20optimization%20with%20bandit%20feedback%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Foster%2C%20D.P.%20Hsu%2C%20D.%20Kakade%2C%20S.M.%20Stochastic%20convex%20optimization%20with%20bandit%20feedback%202013"
        },
        {
            "id": "2",
            "entry": "[2] S. Agrawal, V. Avandhanula, V. Goyal, and A. Zeevi. An exploration-exploitation approach for assortment selection. In EC, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20S.%20Avandhanula%2C%20V.%20Goyal%2C%20V.%20Zeevi%2C%20A.%20An%20exploration-exploitation%20approach%20for%20assortment%20selection%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20S.%20Avandhanula%2C%20V.%20Goyal%2C%20V.%20Zeevi%2C%20A.%20An%20exploration-exploitation%20approach%20for%20assortment%20selection%202016"
        },
        {
            "id": "3",
            "entry": "[3] S. Agrawal, V. Avandhanula, V. Goyal, and A. Zeevi. Thompson sampling for mnl-bandit. In COLT, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agrawal%2C%20S.%20Avandhanula%2C%20V.%20Goyal%2C%20V.%20Zeevi%2C%20A.%20Thompson%20sampling%20for%20mnl-bandit%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agrawal%2C%20S.%20Avandhanula%2C%20V.%20Goyal%2C%20V.%20Zeevi%2C%20A.%20Thompson%20sampling%20for%20mnl-bandit%202017"
        },
        {
            "id": "4",
            "entry": "[4] J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Audibert%2C%20J.-Y.%20Bubeck%2C%20S.%20Minimax%20policies%20for%20adversarial%20and%20stochastic%20bandits%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Audibert%2C%20J.-Y.%20Bubeck%2C%20S.%20Minimax%20policies%20for%20adversarial%20and%20stochastic%20bandits%202009"
        },
        {
            "id": "5",
            "entry": "[5] A. Borsch-Supan. On the compatibility of nested logit models with utility maximization. Journal of Econometrics, 43(3):373\u2013388, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Borsch-Supan%2C%20A.%20On%20the%20compatibility%20of%20nested%20logit%20models%20with%20utility%20maximization%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Borsch-Supan%2C%20A.%20On%20the%20compatibility%20of%20nested%20logit%20models%20with%20utility%20maximization%201990"
        },
        {
            "id": "6",
            "entry": "[6] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1\u2013122, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Cesa-Bianchi%2C%20N.%20Regret%20analysis%20of%20stochastic%20and%20nonstochastic%20multi-armed%20bandit%20problems%202012"
        },
        {
            "id": "7",
            "entry": "[7] S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In ALT, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20S.%20Munos%2C%20R.%20Stoltz%2C%20G.%20Pure%20exploration%20in%20multi-armed%20bandits%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bubeck%2C%20S.%20Munos%2C%20R.%20Stoltz%2C%20G.%20Pure%20exploration%20in%20multi-armed%20bandits%20problems%202009"
        },
        {
            "id": "8",
            "entry": "[8] F. Caro and J. Gallien. Dynamic Assortment with Demand Learning for Seasonal Consumer Goods. Management Science, 53(2):276\u2013292, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caro%2C%20F.%20Gallien%2C%20J.%20Dynamic%20Assortment%20with%20Demand%20Learning%20for%20Seasonal%20Consumer%20Goods%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caro%2C%20F.%20Gallien%2C%20J.%20Dynamic%20Assortment%20with%20Demand%20Learning%20for%20Seasonal%20Consumer%20Goods%202007"
        },
        {
            "id": "9",
            "entry": "[9] X. Chen and Y. Wang. A note on tight lower bound for mnl-bandit assortment selection models. arXiv preprint: arXiv:1709.06192, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.06192"
        },
        {
            "id": "10",
            "entry": "[10] V. Cohen-Addad and V. Kanade. Online optimization of smoothed piecewise constant functions. arXiv preprint arXiv:1604.01999, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1604.01999"
        },
        {
            "id": "11",
            "entry": "[11] R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In ICML, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Combes%2C%20R.%20Proutiere%2C%20A.%20Unimodal%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Combes%2C%20R.%20Proutiere%2C%20A.%20Unimodal%20bandits%3A%20Regret%20lower%20bounds%20and%20optimal%20algorithms%202014"
        },
        {
            "id": "12",
            "entry": "[12] E. W. Cope. Regret and convergence bounds for a class of continuum-armed bandit problems. IEEE Transactions on Automatic Control, 54(6):1243\u20131253, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cope%2C%20E.W.%20Regret%20and%20convergence%20bounds%20for%20a%20class%20of%20continuum-armed%20bandit%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cope%2C%20E.W.%20Regret%20and%20convergence%20bounds%20for%20a%20class%20of%20continuum-armed%20bandit%20problems%202009"
        },
        {
            "id": "13",
            "entry": "[13] D. Darling and H. Robbins. Iterated logarithm inequalities. In Herbert Robbins Selected Papers, pages 254\u2013258.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Darling%2C%20D.%20Robbins%2C%20H.%20Iterated%20logarithm%20inequalities",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Darling%2C%20D.%20Robbins%2C%20H.%20Iterated%20logarithm%20inequalities"
        },
        {
            "id": "14",
            "entry": "[14] N. Golrezaei, H. Nazerzadeh, and P. Rusmevichientong. Real-time optimization of personalized assortments. Management Science, 60(6):1532\u20131551, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Golrezaei%2C%20N.%20Nazerzadeh%2C%20H.%20Rusmevichientong%2C%20P.%20Real-time%20optimization%20of%20personalized%20assortments%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Golrezaei%2C%20N.%20Nazerzadeh%2C%20H.%20Rusmevichientong%2C%20P.%20Real-time%20optimization%20of%20personalized%20assortments%202014"
        },
        {
            "id": "15",
            "entry": "[15] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 58(301):13\u201330, 1963.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hoeffding%2C%20W.%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hoeffding%2C%20W.%20Probability%20inequalities%20for%20sums%20of%20bounded%20random%20variables%201963"
        },
        {
            "id": "16",
            "entry": "[16] K. Jamieson, M. Malloy, R. Nowak, and S. Bubeck. lilUCB: An optimal exploration algorithm for multi-armed bandits. In COLT, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jamieson%2C%20K.%20Malloy%2C%20M.%20Nowak%2C%20R.%20Bubeck%2C%20S.%20lilUCB%3A%20An%20optimal%20exploration%20algorithm%20for%20multi-armed%20bandits%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jamieson%2C%20K.%20Malloy%2C%20M.%20Nowak%2C%20R.%20Bubeck%2C%20S.%20lilUCB%3A%20An%20optimal%20exploration%20algorithm%20for%20multi-armed%20bandits%202014"
        },
        {
            "id": "17",
            "entry": "[17] A. G. Kok, M. L. Fisher, and R. Vaidyanathan. Assortment planning: Review of literature and industry practice. In Retail supply chain management, pages 99\u2013153.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kok%2C%20A.G.%20Fisher%2C%20M.L.%20Vaidyanathan%2C%20R.%20Assortment%20planning%3A%20Review%20of%20literature%20and%20industry%20practice",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kok%2C%20A.G.%20Fisher%2C%20M.L.%20Vaidyanathan%2C%20R.%20Assortment%20planning%3A%20Review%20of%20literature%20and%20industry%20practice"
        },
        {
            "id": "18",
            "entry": "[18] D. McFadden. Econometric models for probabilistic choice among products. Journal of Business, pages S13\u2013S29, 1980.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McFadden%2C%20D.%20Econometric%20models%20for%20probabilistic%20choice%20among%20products%201980",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McFadden%2C%20D.%20Econometric%20models%20for%20probabilistic%20choice%20among%20products%201980"
        },
        {
            "id": "19",
            "entry": "[19] P. Rusmevichientong, Z.-J. Shen, and D. Shmoys. Dynamic assortment optimization with a multinomial logit choice model and capacity constraint. Operations Research, 58(6):1666\u2013 1680, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rusmevichientong%2C%20P.%20Shen%2C%20Z.-J.%20Shmoys%2C%20D.%20Dynamic%20assortment%20optimization%20with%20a%20multinomial%20logit%20choice%20model%20and%20capacity%20constraint%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rusmevichientong%2C%20P.%20Shen%2C%20Z.-J.%20Shmoys%2C%20D.%20Dynamic%20assortment%20optimization%20with%20a%20multinomial%20logit%20choice%20model%20and%20capacity%20constraint%202010"
        },
        {
            "id": "20",
            "entry": "[20] P. Rusmevichientong and H. Topaloglu. Robust assortment optimization in revenue management under the multinomial logit choice model. Operations Research, 60(4):865\u2013882, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rusmevichientong%2C%20P.%20Topaloglu%2C%20H.%20Robust%20assortment%20optimization%20in%20revenue%20management%20under%20the%20multinomial%20logit%20choice%20model%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rusmevichientong%2C%20P.%20Topaloglu%2C%20H.%20Robust%20assortment%20optimization%20in%20revenue%20management%20under%20the%20multinomial%20logit%20choice%20model%202012"
        },
        {
            "id": "21",
            "entry": "[21] D. Saure and A. Zeevi. Optimal dynamic assortment planning with demand learning. Manufacturing & Service Operations Management, 15(3):387\u2013404, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Saure%2C%20D.%20Zeevi%2C%20A.%20Optimal%20dynamic%20assortment%20planning%20with%20demand%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Saure%2C%20D.%20Zeevi%2C%20A.%20Optimal%20dynamic%20assortment%20planning%20with%20demand%20learning%202013"
        },
        {
            "id": "22",
            "entry": "[22] H. C. W. L. Williams. On the formation of travel demand models and economic evaluation measures of user benefit. Environment and Planning A: Economy and Space, 9:285\u2013344, 1977.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Williams%2C%20H.C.W.L.%20On%20the%20formation%20of%20travel%20demand%20models%20and%20economic%20evaluation%20measures%20of%20user%20benefit%201977",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Williams%2C%20H.C.W.L.%20On%20the%20formation%20of%20travel%20demand%20models%20and%20economic%20evaluation%20measures%20of%20user%20benefit%201977"
        },
        {
            "id": "23",
            "entry": "[23] J. Y. Yu and S. Mannor. Unimodal bandits. In ICML, 2011. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yu%2C%20J.Y.%20Mannor%2C%20S.%20Unimodal%20bandits%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yu%2C%20J.Y.%20Mannor%2C%20S.%20Unimodal%20bandits%202011"
        }
    ]
}
