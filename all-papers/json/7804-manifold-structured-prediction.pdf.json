{
    "filename": "7804-manifold-structured-prediction.pdf",
    "metadata": {
        "title": "Manifold Structured Prediction",
        "author": "Alessandro Rudi, Carlo Ciliberto, GianMaria Marconi, Lorenzo Rosasco",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7804-manifold-structured-prediction.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Structured prediction provides a general framework to deal with supervised problems where the outputs have semantically rich structure. While classical approaches consider finite, albeit potentially huge, output spaces, in this paper we discuss how structured prediction can be extended to a continuous scenario. Specifically, we study a structured prediction approach to manifold valued regression. We characterize a class of problems for which the considered approach is statistically consistent and study how geometric optimization can be used to compute the corresponding estimator. Promising experimental results on both simulated and real data complete our study."
    },
    "keywords": [
        {
            "term": "positive definite matrix",
            "url": "https://en.wikipedia.org/wiki/positive_definite_matrix"
        },
        {
            "term": "area under curve",
            "url": "https://en.wikipedia.org/wiki/area_under_curve"
        },
        {
            "term": "Gradient Descent",
            "url": "https://en.wikipedia.org/wiki/Gradient_Descent"
        },
        {
            "term": "AFOSR",
            "url": "https://en.wikipedia.org/wiki/AFOSR"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "reproducing kernel Hilbert space",
            "url": "https://en.wikipedia.org/wiki/reproducing_kernel_Hilbert_space"
        },
        {
            "term": "riemannian manifold",
            "url": "https://en.wikipedia.org/wiki/riemannian_manifold"
        }
    ],
    "highlights": [
        "Regression and classification are probably the most classical machine learning problems and correspond to estimating a function with scalar and binary values, respectively",
        "We study how these ideas can be applied to non-discrete output spaces",
        "In Section 3 we study how the above assumption applies to manifold structured loss functions, including the squared geodesic distance",
        "In this paper we studied a structured prediction approach for manifold valued learning problems",
        "In particular we characterized a wide class of loss functions for which we proved the considered algorithm to be statistically consistent, providing finite sample bounds under standard regularity assumptions",
        "With the latter we considered applications on fingerprint reconstruction and multi-labeling"
    ],
    "key_statements": [
        "Regression and classification are probably the most classical machine learning problems and correspond to estimating a function with scalar and binary values, respectively",
        "While ad-hoc solutions are available for many specific problems [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>, <a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], structured prediction [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>] provides a unifying framework where a variety of problems can be tackled as special cases",
        "We study how these ideas can be applied to non-discrete output spaces",
        "While in this case ad-hoc methods are available [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], in this paper we adopt and study a structured prediction approach starting from a framework proposed in [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]",
        "To tackle this latter problem, we investigate the application of geometric optimization methods, and in particular Riemannian gradient descent [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "We consider the problem of manifold structured prediction [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], in which output data lies on a manifold M \u2282 Rd",
        "In Section 3 we study how the above assumption applies to manifold structured loss functions, including the squared geodesic distance",
        "It guarantees that the algorithm considered in this work finds a consistent estimator for the manifold structured problem, when the loss function is smooth ( in the case of the squared geodesic distance)",
        "To our knowledge these are the first results characterizing in such generality the generalization properties of an estimator for manifold structured learning with generic smooth loss function",
        "In this paper we studied a structured prediction approach for manifold valued learning problems",
        "In particular we characterized a wide class of loss functions for which we proved the considered algorithm to be statistically consistent, providing finite sample bounds under standard regularity assumptions",
        "With the latter we considered applications on fingerprint reconstruction and multi-labeling",
        "The proposed method leads to some open questions",
        "We gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research",
        "This work was supported in part by EPSRC grant EP/P009069/1"
    ],
    "summary": [
        "Regression and classification are probably the most classical machine learning problems and correspond to estimating a function with scalar and binary values, respectively.",
        "From a computational point of view, the proposed algorithm requires solving a linear system and a minimization problem over the output manifold.",
        "We consider the problem of manifold structured prediction [<a class=\"ref-link\" id=\"c51\" href=\"#r51\">51</a>], in which output data lies on a manifold M \u2282 Rd. In this context, statistical learning corresponds to solving argmin E(f ) with E(f ) =",
        "In Section 3 we study how the above assumption applies to manifold structured loss functions, including the squared geodesic distance.",
        "We discuss the main results of the paper, showing that a large class of loss functions for manifold structured prediction are SELF.",
        "Thm. 1 shows that the SELF estimator proposed in Eq (2) can tackle any manifold valued learning problem in the form of Eq (1) with smooth loss function.",
        "Let X be a compact set and k : X \u00d7 X \u2192 R be a bounded continuous universal kernel2 For any n \u2208 N and any distribution \u03c1 on X \u00d7 Y let fn : X \u2192 Y be the manifold structured estimator in Eq (2) for a learning problem with smooth loss function , within=1 training points independently sampled from \u03c1 and \u03bbn = n\u22121/4.",
        "It guarantees that the algorithm considered in this work finds a consistent estimator for the manifold structured problem, when the loss function is smooth.",
        "For any n \u2208 N, let fn denote the manifold structured estimator in Eq (2) for a learning problem with smooth loss : Y \u00d7 Y \u2192 R and \u03bbn = n\u22121/2.",
        "To our knowledge these are the first results characterizing in such generality the generalization properties of an estimator for manifold structured learning with generic smooth loss function.",
        "Table 1 reports gradients and retraction maps for the geodesic distance of two problems of interest considered in this work: positive definite manifold and the sphere.",
        "For the experiments reported in the following we compared the performance of the manifold structured estimator minimizing the loss PD and a Kernel Regularized Least Squares classifier (KRLS) baseline, both trained using the Gaussian kernel k(x, x ) = exp(\u2212 x \u2212 x 2/2\u03c32).",
        "For the manifold structured estimator, the optimization problem at Eq (2) was performed with the Riemannian Gradient Descent (RGD) algorithm [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "In particular we characterized a wide class of loss functions for which we proved the considered algorithm to be statistically consistent, providing finite sample bounds under standard regularity assumptions.",
        "This work was supported in part by EPSRC grant EP/P009069/1"
    ],
    "headline": "We study a structured prediction approach to manifold valued regression",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Absil%2C%20P.-A.%20Mahony%2C%20Robert%20Sepulchre%2C%20Rodolphe%20Optimization%20algorithms%20on%20matrix%20manifolds%202009"
        },
        {
            "id": "2",
            "entry": "[2] Mauricio A Alvarez, Lorenzo Rosasco, Neil D Lawrence, et al. Kernels for vector-valued functions: A review. Foundations and Trends R in Machine Learning, 4(3):195\u2013266, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alvarez%2C%20Mauricio%20A.%20Rosasco%2C%20Lorenzo%20Lawrence%2C%20Neil%20D.%20Kernels%20for%20vector-valued%20functions%3A%20A%20review%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alvarez%2C%20Mauricio%20A.%20Rosasco%2C%20Lorenzo%20Lawrence%2C%20Neil%20D.%20Kernels%20for%20vector-valued%20functions%3A%20A%20review%202012"
        },
        {
            "id": "3",
            "entry": "[3] Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry, volume 191. American Mathematical Soc., 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Amari%2C%20Shun-ichi%20Nagaoka%2C%20Hiroshi%20Methods%20of%20information%20geometry%2C%20volume%20191%202007"
        },
        {
            "id": "4",
            "entry": "[4] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337\u2013404, 1950.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Aronszajn%2C%20Nachman%20Theory%20of%20reproducing%20kernels.%20Transactions%20of%20the%20American%20mathematical%201950"
        },
        {
            "id": "5",
            "entry": "[5] GH Bakir, T Hofmann, B Sch\u00f6lkopf, AJ Smola, B Taskar, and SVN Vishwanathan. Predicting structured data. neural information processing, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bakir%2C%20G.H.%20Hofmann%2C%20T.%20Sch%C3%B6lkopf%2C%20B.%20Smola%2C%20A.J.%20Predicting%20structured%20data.%20neural%20information%20processing%202007"
        },
        {
            "id": "6",
            "entry": "[6] Rajendra Bhatia. Positive definite matrices. Princeton university press, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bhatia%2C%20Rajendra%20Positive%20definite%20matrices%202009"
        },
        {
            "id": "7",
            "entry": "[7] Veli Bicer, Thanh Tran, and Anna Gossen. Relational kernel machines for learning from graph-structured rdf data. In Extended Semantic Web Conference, pages 47\u201362.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bicer%2C%20Veli%20Tran%2C%20Thanh%20Gossen%2C%20Anna%20Relational%20kernel%20machines%20for%20learning%20from%20graph-structured%20rdf%20data",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bicer%2C%20Veli%20Tran%2C%20Thanh%20Gossen%2C%20Anna%20Relational%20kernel%20machines%20for%20learning%20from%20graph-structured%20rdf%20data"
        },
        {
            "id": "8",
            "entry": "[8] Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58(9):2217\u20132229, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonnabel%2C%20Silvere%20Stochastic%20gradient%20descent%20on%20riemannian%20manifolds%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bonnabel%2C%20Silvere%20Stochastic%20gradient%20descent%20on%20riemannian%20manifolds%202013"
        },
        {
            "id": "9",
            "entry": "[9] Raffaello Camoriano, Tom\u00e1s Angles, Alessandro Rudi, and Lorenzo Rosasco. Nytro: When subsampling meets early stopping. In Artificial Intelligence and Statistics, pages 1403\u20131411, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Camoriano%2C%20Raffaello%20Angles%2C%20Tom%C3%A1s%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Nytro%3A%20When%20subsampling%20meets%20early%20stopping%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Camoriano%2C%20Raffaello%20Angles%2C%20Tom%C3%A1s%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Nytro%3A%20When%20subsampling%20meets%20early%20stopping%202016"
        },
        {
            "id": "10",
            "entry": "[10] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331\u2013368, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Caponnetto%2C%20Andrea%20Vito%2C%20Ernesto%20De%20Optimal%20rates%20for%20the%20regularized%20least-squares%20algorithm%202007"
        },
        {
            "id": "11",
            "entry": "[11] Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with sgd and random features. arXiv preprint arXiv:1807.06343, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.06343"
        },
        {
            "id": "12",
            "entry": "[12] Carlo Ciliberto, Francis Bach, and Alessandro Rudi. Localized structured prediction. arXiv preprint arXiv:1806.02402, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02402"
        },
        {
            "id": "13",
            "entry": "[13] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A consistent regularization approach for structured prediction. Advances in Neural Information Processing Systems 29 (NIPS), pages 4412\u20134420, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rosasco%2C%20Lorenzo%20Rudi%2C%20Alessandro%20A%20consistent%20regularization%20approach%20for%20structured%20prediction%202016"
        },
        {
            "id": "14",
            "entry": "[14] Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, and Massimiliano Pontil. Consistent multitask learning with nonlinear output relations. In Advances in Neural Information Processing Systems, pages 1986\u20131996, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciliberto%2C%20Carlo%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Pontil%2C%20Massimiliano%20Consistent%20multitask%20learning%20with%20nonlinear%20output%20relations%201986"
        },
        {
            "id": "15",
            "entry": "[15] Harold Charles Daume and Daniel Marcu. Practical structured learning techniques for natural language processing. Citeseer, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Daume%2C%20Harold%20Charles%20Marcu%2C%20Daniel%20Practical%20structured%20learning%20techniques%20for%20natural%20language%20processing%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Daume%2C%20Harold%20Charles%20Marcu%2C%20Daniel%20Practical%20structured%20learning%20techniques%20for%20natural%20language%20processing%202006"
        },
        {
            "id": "16",
            "entry": "[16] Joe Diestel and Angela Spalsbury. The joys of Haar measure. American Mathematical Soc., 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Diestel%2C%20Joe%20Spalsbury%2C%20Angela%20The%20joys%20of%20Haar%20measure%202014"
        },
        {
            "id": "17",
            "entry": "[17] Moussab Djerrab, Alexandre Garcia, Maxime Sangnier, and Florence d\u2019Alch\u00e9 Buc. Output fisher embedding regression. Machine Learning, pages 1\u201328, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Djerrab%2C%20Moussab%20Garcia%2C%20Alexandre%20Sangnier%2C%20Maxime%20and%20Florence%20d%E2%80%99Alch%C3%A9%20Buc.%20Output%20fisher%20embedding%20regression%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Djerrab%2C%20Moussab%20Garcia%2C%20Alexandre%20Sangnier%2C%20Maxime%20and%20Florence%20d%E2%80%99Alch%C3%A9%20Buc.%20Output%20fisher%20embedding%20regression%202018"
        },
        {
            "id": "18",
            "entry": "[18] John C Duchi, Lester W Mackey, and Michael I Jordan. On the consistency of ranking algorithms. In ICML, pages 327\u2013334, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20John%20C.%20Mackey%2C%20Lester%20W.%20Jordan%2C%20Michael%20I.%20On%20the%20consistency%20of%20ranking%20algorithms%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20John%20C.%20Mackey%2C%20Lester%20W.%20Jordan%2C%20Michael%20I.%20On%20the%20consistency%20of%20ranking%20algorithms%202010"
        },
        {
            "id": "19",
            "entry": "[19] P Thomas Fletcher. Geodesic regression and the theory of least squares on riemannian manifolds. International journal of computer vision, 105(2):171\u2013185, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fletcher%2C%20P.Thomas%20Geodesic%20regression%20and%20the%20theory%20of%20least%20squares%20on%20riemannian%20manifolds%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fletcher%2C%20P.Thomas%20Geodesic%20regression%20and%20the%20theory%20of%20least%20squares%20on%20riemannian%20manifolds%202013"
        },
        {
            "id": "20",
            "entry": "[20] Sylvestre Gallot, Dominique Hulin, and Jacques Lafontaine. Riemannian geometry, volume 3.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sylvestre%20Gallot%20Dominique%20Hulin%20and%20Jacques%20Lafontaine%20Riemannian%20geometry%20volume%203",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sylvestre%20Gallot%20Dominique%20Hulin%20and%20Jacques%20Lafontaine%20Riemannian%20geometry%20volume%203"
        },
        {
            "id": "21",
            "entry": "[21] Wolfgang H\u00e4rdle and L\u00e9opold Simar. Applied multivariate statistical analysis, volume 22007. Springer, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolfgang%20H%C3%A4rdle%20and%20L%C3%A9opold%20Simar%20Applied%20multivariate%20statistical%20analysis%20volume%2022007%20Springer%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolfgang%20H%C3%A4rdle%20and%20L%C3%A9opold%20Simar%20Applied%20multivariate%20statistical%20analysis%20volume%2022007%20Springer%202007"
        },
        {
            "id": "22",
            "entry": "[22] S\u00f8ren Hauberg, Oren Freifeld, and Michael J Black. A geometric take on metric learning. In Advances in Neural Information Processing Systems, pages 2024\u20132032, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hauberg%2C%20S%C3%B8ren%20Freifeld%2C%20Oren%20Black%2C%20Michael%20J.%20A%20geometric%20take%20on%20metric%20learning%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hauberg%2C%20S%C3%B8ren%20Freifeld%2C%20Oren%20Black%2C%20Michael%20J.%20A%20geometric%20take%20on%20metric%20learning%202012"
        },
        {
            "id": "23",
            "entry": "[23] Emmanuel Hebey. Nonlinear analysis on manifolds: Sobolev spaces and inequalities, volume 5. American Mathematical Soc., 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hebey%2C%20Emmanuel%20Nonlinear%20analysis%20on%20manifolds%3A%20Sobolev%20spaces%20and%20inequalities%2C%20volume%205%202000"
        },
        {
            "id": "24",
            "entry": "[24] Jacob Hinkle, Prasanna Muralidharan, P Thomas Fletcher, and Sarang Joshi. Polynomial regression on riemannian manifolds. In European Conference on Computer Vision, pages 1\u201314.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinkle%2C%20Jacob%20Prasanna%20Muralidharan%2C%20P.Thomas%20Fletcher%20Joshi%2C%20Sarang%20Polynomial%20regression%20on%20riemannian%20manifolds",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinkle%2C%20Jacob%20Prasanna%20Muralidharan%2C%20P.Thomas%20Fletcher%20Joshi%2C%20Sarang%20Polynomial%20regression%20on%20riemannian%20manifolds"
        },
        {
            "id": "25",
            "entry": "[25] Mohammed Waleed Kadous and Claude Sammut. Classification of multivariate time series and structured data using constructive induction. Machine learning, 58(2):179\u2013216, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kadous%2C%20Mohammed%20Waleed%20Sammut%2C%20Claude%20Classification%20of%20multivariate%20time%20series%20and%20structured%20data%20using%20constructive%20induction%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kadous%2C%20Mohammed%20Waleed%20Sammut%2C%20Claude%20Classification%20of%20multivariate%20time%20series%20and%20structured%20data%20using%20constructive%20induction%202005"
        },
        {
            "id": "26",
            "entry": "[26] Anna Korba, Alexandre Garcia, and Florence d\u2019Alch\u00e9 Buc. A structured prediction approach for label ranking. arXiv preprint arXiv:1807.02374, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.02374"
        },
        {
            "id": "27",
            "entry": "[27] Quoc V Le, Tim Sears, and Alexander J Smola. Nonparametric quantile estimation. 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Quoc%20V.%20Sears%2C%20Tim%20Smola%2C%20Alexander%20J.%20Nonparametric%20quantile%20estimation%202005"
        },
        {
            "id": "28",
            "entry": "[28] John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds, pages 1\u201329.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lee%2C%20John%20M.%20Smooth%20manifolds",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lee%2C%20John%20M.%20Smooth%20manifolds"
        },
        {
            "id": "29",
            "entry": "[29] Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral algorithms with least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Junhong%20Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Cevher%2C%20Volkan%20Optimal%20rates%20for%20spectral%20algorithms%20with%20least-squares%20regression%20over%20hilbert%20spaces%202018"
        },
        {
            "id": "30",
            "entry": "[30] Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto. Differential properties of sinkhorn approximation for learning with wasserstein distance. arXiv preprint arXiv:1805.11897, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.11897"
        },
        {
            "id": "31",
            "entry": "[31] Charles A Micchelli and Massimiliano Pontil. On learning vector-valued functions. Neural computation, 17(1):177\u2013204, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Charles%20A%20Micchelli%20and%20Massimiliano%20Pontil.%20On%20learning%20vector-valued%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Charles%20A%20Micchelli%20and%20Massimiliano%20Pontil.%20On%20learning%20vector-valued%20functions%202005"
        },
        {
            "id": "32",
            "entry": "[32] Maher Moakher and Philipp G Batchelor. Symmetric positive-definite matrices: From geometry to applications and visualization. In Visualization and Processing of Tensor Fields, pages 285\u2013298.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moakher%2C%20Maher%20Batchelor%2C%20Philipp%20G.%20Symmetric%20positive-definite%20matrices%3A%20From%20geometry%20to%20applications%20and%20visualization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moakher%2C%20Maher%20Batchelor%2C%20Philipp%20G.%20Symmetric%20positive-definite%20matrices%3A%20From%20geometry%20to%20applications%20and%20visualization"
        },
        {
            "id": "33",
            "entry": "[33] Jeffrey S Morris. Functional regression. Annual Review of Statistics and Its Application, 2:321\u2013359, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Morris%2C%20Jeffrey%20S.%20Functional%20regression%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Morris%2C%20Jeffrey%20S.%20Functional%20regression%202015"
        },
        {
            "id": "34",
            "entry": "[34] Janez Mrcun. On isomorphisms of algebras of smooth functions. Proceedings of the American Mathematical Society, 133(10):3109\u20133113, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mrcun%2C%20Janez%20On%20isomorphisms%20of%20algebras%20of%20smooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mrcun%2C%20Janez%20On%20isomorphisms%20of%20algebras%20of%20smooth%20functions%202005"
        },
        {
            "id": "35",
            "entry": "[35] Jet Nestruev. Smooth manifolds and observables, volume 220. Springer Science & Business Media, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nestruev%2C%20Jet%20Smooth%20manifolds%20and%20observables%2C%20volume%20220%202006"
        },
        {
            "id": "36",
            "entry": "[36] Maximilian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. International Conference on Machine Learning, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickel%2C%20Maximilian%20Kiela%2C%20Douwe%20Learning%20continuous%20hierarchies%20in%20the%20lorentz%20model%20of%20hyperbolic%20geometry%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickel%2C%20Maximilian%20Kiela%2C%20Douwe%20Learning%20continuous%20hierarchies%20in%20the%20lorentz%20model%20of%20hyperbolic%20geometry%202018"
        },
        {
            "id": "37",
            "entry": "[37] Maximillian Nickel and Douwe Kiela. Poincar\u00e9 embeddings for learning hierarchical representations. In Advances in neural information processing systems, pages 6338\u20136347, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nickel%2C%20Maximillian%20Kiela%2C%20Douwe%20Poincar%C3%A9%20embeddings%20for%20learning%20hierarchical%20representations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nickel%2C%20Maximillian%20Kiela%2C%20Douwe%20Poincar%C3%A9%20embeddings%20for%20learning%20hierarchical%20representations%202017"
        },
        {
            "id": "38",
            "entry": "[38] Frank Nielsen and Ke Sun. Clustering in hilbert simplex geometry. arXiv preprint arXiv:1704.00454, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1704.00454"
        },
        {
            "id": "39",
            "entry": "[39] Alex Nowak-Vila, Francis Bach, and Alessandro Rudi. Sharp analysis of learning with discrete losses. arXiv preprint arXiv:1810.06839, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1810.06839"
        },
        {
            "id": "40",
            "entry": "[40] Sebastian Nowozin, Christoph H Lampert, et al. Structured learning and prediction in computer vision. Foundations and Trends R in Computer Graphics and Vision, 6(3\u20134):185\u2013365, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nowozin%2C%20Sebastian%20Lampert%2C%20Christoph%20H.%20Structured%20learning%20and%20prediction%20in%20computer%20vision%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nowozin%2C%20Sebastian%20Lampert%2C%20Christoph%20H.%20Structured%20learning%20and%20prediction%20in%20computer%20vision%202011"
        },
        {
            "id": "41",
            "entry": "[41] Benjamin Paa\u00dfen, Christina G\u00f6pfert, and Barbara Hammer. Time series prediction for graphs in kernel and dissimilarity spaces. Neural Processing Letters, pages 1\u201321, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paa%C3%9Fen%2C%20Benjamin%20G%C3%B6pfert%2C%20Christina%20Hammer%2C%20Barbara%20Time%20series%20prediction%20for%20graphs%20in%20kernel%20and%20dissimilarity%20spaces%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Paa%C3%9Fen%2C%20Benjamin%20G%C3%B6pfert%2C%20Christina%20Hammer%2C%20Barbara%20Time%20series%20prediction%20for%20graphs%20in%20kernel%20and%20dissimilarity%20spaces%202017"
        },
        {
            "id": "42",
            "entry": "[42] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177\u20131184, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202008"
        },
        {
            "id": "43",
            "entry": "[43] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystr\u00f6m computational regularization. In Advances in Neural Information Processing Systems, pages 1657\u20131665, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Camoriano%2C%20Raffaello%20Rosasco%2C%20Lorenzo%20Less%20is%20more%3A%20Nystr%C3%B6m%20computational%20regularization%202015"
        },
        {
            "id": "44",
            "entry": "[44] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel method. In Advances in Neural Information Processing Systems, pages 3888\u20133898, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Carratino%2C%20Luigi%20Rosasco%2C%20Lorenzo%20Falkon%3A%20An%20optimal%20large%20scale%20kernel%20method%202017"
        },
        {
            "id": "45",
            "entry": "[45] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in Neural Information Processing Systems, pages 3215\u20133225, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rudi%2C%20Alessandro%20Rosasco%2C%20Lorenzo%20Generalization%20properties%20of%20learning%20with%20random%20features%202017"
        },
        {
            "id": "46",
            "entry": "[46] Takashi Sakai. Riemannian geometry, volume 149. American Mathematical Soc., 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Takashi%20Sakai%20Riemannian%20geometry%20volume%20149%20American%20Mathematical%20Soc%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Takashi%20Sakai%20Riemannian%20geometry%20volume%20149%20American%20Mathematical%20Soc%201996"
        },
        {
            "id": "47",
            "entry": "[47] John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge university press, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shawe-Taylor%2C%20John%20Cristianini%2C%20Nello%20Kernel%20methods%20for%20pattern%20analysis%202004"
        },
        {
            "id": "48",
            "entry": "[48] Alex J Smola and Bernhard Sch\u00f6lkopf. Sparse greedy matrix approximation for machine learning. 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20Alex%20J.%20Sch%C3%B6lkopf%2C%20Bernhard%20Sparse%20greedy%20matrix%20approximation%20for%20machine%20learning%202000"
        },
        {
            "id": "49",
            "entry": "[49] Ashwin Srinivasan. Note on the location of optimal classifiers in n-dimensional roc space. 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srinivasan%2C%20Ashwin%20Note%20on%20the%20location%20of%20optimal%20classifiers%20in%20n-dimensional%20roc%20space%201999"
        },
        {
            "id": "50",
            "entry": "[50] Florian Steinke and Matthias Hein. Non-parametric regression between manifolds. In Advances in Neural Information Processing Systems, pages 1561\u20131568, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinke%2C%20Florian%20Hein%2C%20Matthias%20Non-parametric%20regression%20between%20manifolds%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinke%2C%20Florian%20Hein%2C%20Matthias%20Non-parametric%20regression%20between%20manifolds%202009"
        },
        {
            "id": "51",
            "entry": "[51] Florian Steinke, Matthias Hein, and Bernhard Sch\u00f6lkopf. Nonparametric regression between general riemannian manifolds. SIAM Journal on Imaging Sciences, 3(3):527\u2013563, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinke%2C%20Florian%20Hein%2C%20Matthias%20Sch%C3%B6lkopf%2C%20Bernhard%20Nonparametric%20regression%20between%20general%20riemannian%20manifolds%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinke%2C%20Florian%20Hein%2C%20Matthias%20Sch%C3%B6lkopf%2C%20Bernhard%20Nonparametric%20regression%20between%20general%20riemannian%20manifolds%202010"
        },
        {
            "id": "52",
            "entry": "[52] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Christmann%2C%20Andreas%20Support%20vector%20machines%202008"
        },
        {
            "id": "53",
            "entry": "[53] Ingo Steinwart, Don R Hush, Clint Scovel, et al. Optimal rates for regularized least squares regression. In",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Steinwart%2C%20Ingo%20Hush%2C%20Don%20R.%20Scovel%2C%20Clint%20Optimal%20rates%20for%20regularized%20least%20squares%20regression"
        },
        {
            "id": "54",
            "entry": "[54] Fran\u00e7ois Treves. Topological Vector Spaces, Distributions and Kernels: Pure and Applied Mathematics, volume 25.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Treves%2C%20Fran%C3%A7ois%20Topological%20Vector%20Spaces%2C%20Distributions%20and%20Kernels%3A%20Pure%20and",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Treves%2C%20Fran%C3%A7ois%20Topological%20Vector%20Spaces%2C%20Distributions%20and%20Kernels%3A%20Pure%20and"
        },
        {
            "id": "55",
            "entry": "[55] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. Mining multi-label data. In Data mining and knowledge discovery handbook, pages 667\u2013685.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tsoumakas%2C%20Grigorios%20Katakis%2C%20Ioannis%20Vlahavas%2C%20Ioannis%20Mining%20multi-label%20data.%20In%20Data%20mining%20and%20knowledge%20discovery%20handbook"
        },
        {
            "id": "56",
            "entry": "[56] C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Villani%2C%20C%C3%A9dric%20Optimal%20transport%3A%20old%20and%20new%2C%20volume%20338%202008"
        },
        {
            "id": "57",
            "entry": "[57] Franz-Erich Wolter. Distance function and cut loci on a complete riemannian manifold. Archiv der Mathematik, 32(1):92\u201396, 1979.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wolter%2C%20Franz-Erich%20Distance%20function%20and%20cut%20loci%20on%20a%20complete%20riemannian%20manifold%201979",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wolter%2C%20Franz-Erich%20Distance%20function%20and%20cut%20loci%20on%20a%20complete%20riemannian%20manifold%201979"
        },
        {
            "id": "58",
            "entry": "[58] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on Learning Theory, pages 1617\u20131638, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Hongyi%20Sra%2C%20Suvrit%20First-order%20methods%20for%20geodesically%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Hongyi%20Sra%2C%20Suvrit%20First-order%20methods%20for%20geodesically%20convex%20optimization%202016"
        }
    ]
}
