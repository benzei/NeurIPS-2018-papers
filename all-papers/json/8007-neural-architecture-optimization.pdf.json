{
    "filename": "8007-neural-architecture-optimization.pdf",
    "metadata": {
        "title": "Neural Architecture Optimization",
        "author": "Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, Tie-Yan Liu",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8007-neural-architecture-optimization.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 2.11% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. The best discovered architectures on both tasks are successfully transferred to other tasks such as CIFAR-100 and WikiText-2. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 3.53%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks."
    },
    "keywords": [
        {
            "term": "reinforcement learning",
            "url": "https://en.wikipedia.org/wiki/reinforcement_learning"
        },
        {
            "term": "architecture search",
            "url": "https://en.wikipedia.org/wiki/architecture_search"
        },
        {
            "term": "Gaussian process",
            "url": "https://en.wikipedia.org/wiki/Gaussian_process"
        },
        {
            "term": "CIFAR",
            "url": "https://en.wikipedia.org/wiki/CIFAR"
        },
        {
            "term": "bayesian optimization",
            "url": "https://en.wikipedia.org/wiki/bayesian_optimization"
        },
        {
            "term": "evolutionary algorithm",
            "url": "https://en.wikipedia.org/wiki/evolutionary_algorithm"
        },
        {
            "term": "neural architecture search",
            "url": "https://en.wikipedia.org/wiki/neural_architecture_search"
        },
        {
            "term": "continuous space",
            "url": "https://en.wikipedia.org/wiki/continuous_space"
        }
    ],
    "highlights": [
        "Automatic design of neural network architecture without human intervention has been the interests of the community from decades ago [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to very recent [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "Combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 and on PTB, with very limited computational resources for both tasks",
        "The latest algorithms for automatic architecture design usually fall into two categories: reinforcement learning (RL) [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] based methods and evolutionary algorithm (EA) based methods [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>]",
        "The detailed results are shown in Table 1, where we demonstrate the performances of best experts designed architectures, the networks discovered by previous Neural Architecture Search algorithm and by neural architecture optimization, which we name as NAONet",
        "We report all the results in Table 3, separated into three blocks, respectively reporting the results of experts designed methods, architectures discovered via previous automatic neural architecture search methods, and our neural architecture optimization",
        "We design a new automatic architecture design algorithm named as neural architecture optimization (NAO), which performs the optimization within continuous space rather than searching discrete decisions"
    ],
    "key_statements": [
        "Automatic design of neural network architecture without human intervention has been the interests of the community from decades ago [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to very recent [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>]",
        "The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy",
        "Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources",
        "Combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 and on PTB, with very limited computational resources for both tasks",
        "The latest algorithms for automatic architecture design usually fall into two categories: reinforcement learning (RL) [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] based methods and evolutionary algorithm (EA) based methods [<a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>]",
        "Using the same architecture space commonly used in previous works [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], the architecture found via neural architecture optimization achieves 2.11% test set error rate on CIFAR-10",
        "We show that equipped with the recent proposed weight sharing mechanism in ENAS [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] to reduce the large complexity in the parameter space of child models, we can achieve improved efficiency in discovering powerful convolutional and recurrent architectures, e.g., both take less than 10 hours on 1 GPU",
        "The detailed results are shown in Table 1, where we demonstrate the performances of best experts designed architectures, the networks discovered by previous Neural Architecture Search algorithm and by neural architecture optimization, which we name as NAONet",
        "In Fig. 2(b) we show the average performances of architectures in Xeval discovered via neural architecture optimization at each optimization iteration",
        "We evaluate the performances of other automatically discovered neural networks on CIFAR-100 by strictly using the reported architectures in previous Neural Architecture Search papers [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "The encoder in neural architecture optimization is an LSTM with embedding size 64 and hidden size 128",
        "We report all the results in Table 3, separated into three blocks, respectively reporting the results of experts designed methods, architectures discovered via previous automatic neural architecture search methods, and our neural architecture optimization",
        "We design a new automatic architecture design algorithm named as neural architecture optimization (NAO), which performs the optimization within continuous space rather than searching discrete decisions"
    ],
    "summary": [
        "Automatic design of neural network architecture without human intervention has been the interests of the community from decades ago [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c22\" href=\"#r22\">22</a>] to very recent [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>, <a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>].",
        "The core of NAO is an encoder model responsible to map a neural network architecture into a continuous representation.",
        "What distinguishes our method is how to leverage the performance predictor: different with previous work [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>] that uses the performance predictor as a heuristic to select the already generated architectures to speed up searching process, we directly optimize the module to obtain the continuous representation of a better network by gradient descent.",
        "The optimized representation is leveraged to produce a new neural network architecture that is predicted to perform better.",
        "Using the same architecture space commonly used in previous works [<a class=\"ref-link\" id=\"c44\" href=\"#r44\">44</a>, <a class=\"ref-link\" id=\"c45\" href=\"#r45\">45</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>], the architecture found via NAO achieves 2.11% test set error rate on CIFAR-10.",
        "When both the encoder and decoder are optimized to convergence, the inference process for better architectures is performed in the continuous space E.",
        "We report the empirical performances of NAO in discovering competitive neural architectures on benchmark datasets of two tasks, the image recognition and the language modeling.",
        "The architecture encoder of NAO is an LSTM model with token embedding size and hidden state size respectively set as and",
        "The detailed results are shown in Table 1, where we demonstrate the performances of best experts designed architectures, the networks discovered by previous NAS algorithm and by NAO, which we name as NAONet. We have several observations.",
        "We evaluate the performances of other automatically discovered neural networks on CIFAR-100 by strictly using the reported architectures in previous NAS papers [<a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>].",
        "Similar to CIFAR-10, we explore the possibility of combining weight sharing with NAO and the resulting architecture is denoted as \u2018NAO-WS\u2019.",
        "NAO successfully discovered an architecture that achieves quite competitive perplexity 56.0, surpassing previous NAS methods and is on par with the best performance from LSTM method with advanced manually designed techniques such as averaged weight drop [<a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>].",
        "We design a new automatic architecture design algorithm named as neural architecture optimization (NAO), which performs the optimization within continuous space rather than searching discrete decisions.",
        "The encoder, performance predictor and decoder together makes it more effective and efficient to discover better architectures and we achieve quite competitive results on both image classification task and language modeling task.",
        "First we would like to try other methods to further improve the performance of the discovered architecture, such as mixture of softmax [<a class=\"ref-link\" id=\"c41\" href=\"#r41\">41</a>] for language modeling.",
        "We plan to design better neural models from the view of teaching and learning to teach [<a class=\"ref-link\" id=\"c13\" href=\"#r13\">13</a>]"
    ],
    "headline": "We propose a simple and efficient method to automatic neural architecture design based on continuous optimization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikel%20Artetxe%20Gorka%20Labaka%20Eneko%20Agirre%20and%20Kyunghyun%20Cho%20Unsupervised%20neural%20machine%20translation%20In%20International%20Conference%20on%20Learning%20Representations%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikel%20Artetxe%20Gorka%20Labaka%20Eneko%20Agirre%20and%20Kyunghyun%20Cho%20Unsupervised%20neural%20machine%20translation%20In%20International%20Conference%20on%20Learning%20Representations%202018"
        },
        {
            "id": "2",
            "entry": "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.0473"
        },
        {
            "id": "3",
            "entry": "[3] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. In International Conference on Learning Representations, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Naik%2C%20Nikhil%20Raskar%2C%20Ramesh%20Designing%20neural%20network%20architectures%20using%20reinforcement%20learning%202017"
        },
        {
            "id": "4",
            "entry": "[4] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In International Conference on Learning Representations, Workshop Track, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Raskar%2C%20Ramesh%20Naik%2C%20Nikhil%20Accelerating%20neural%20architecture%20search%20using%20performance%20prediction%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baker%2C%20Bowen%20Gupta%2C%20Otkrist%20Raskar%2C%20Ramesh%20Naik%2C%20Nikhil%20Accelerating%20neural%20architecture%20search%20using%20performance%20prediction%202018"
        },
        {
            "id": "5",
            "entry": "[5] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and simplifying one-shot architecture search. In International Conference on Machine Learning, pages 549\u2013558, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bender%2C%20Gabriel%20Kindermans%2C%20Pieter-Jan%20Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Understanding%20and%20simplifying%20one-shot%20architecture%20search%202018"
        },
        {
            "id": "6",
            "entry": "[6] Andrew Brock, Theo Lim, J.M. Ritchie, and Nick Weston. SMASH: One-shot model architecture search through hypernetworks. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Brock%2C%20Andrew%20Theo%20Lim%2C%20J.M.Ritchie%20Weston%2C%20Nick%20SMASH%3A%20One-shot%20model%20architecture%20search%20through%20hypernetworks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Brock%2C%20Andrew%20Theo%20Lim%2C%20J.M.Ritchie%20Weston%2C%20Nick%20SMASH%3A%20One-shot%20model%20architecture%20search%20through%20hypernetworks%202018"
        },
        {
            "id": "7",
            "entry": "[7] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Reinforcement learning for architecture search by network transformation. arXiv preprint arXiv:1707.04873, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.04873"
        },
        {
            "id": "8",
            "entry": "[8] Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation for efficient architecture search. arXiv preprint arXiv:1806.02639, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.02639"
        },
        {
            "id": "9",
            "entry": "[9] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar, October 2014. Association for Computational Linguistics.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93%20decoder%20for%20statistical%20machine%20translation%202014-10",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cho%2C%20Kyunghyun%20van%20Merrienboer%2C%20Bart%20Gulcehre%2C%20Caglar%20Bahdanau%2C%20Dzmitry%20Learning%20phrase%20representations%20using%20rnn%20encoder%E2%80%93%20decoder%20for%20statistical%20machine%20translation%202014-10"
        },
        {
            "id": "10",
            "entry": "[10] Boyang Deng, Junjie Yan, and Dahua Lin. Peephole: Predicting network performance before training. arXiv preprint arXiv:1712.03351, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03351"
        },
        {
            "id": "11",
            "entry": "[11] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.04552"
        },
        {
            "id": "12",
            "entry": "[12] Scott E Fahlman and Christian Lebiere. The cascade-correlation learning architecture. In Advances in neural information processing systems, pages 524\u2013532, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Fahlman%20and%20Christian%20Lebiere.%20The%20cascade-correlation%20learning%20architecture%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Fahlman%20and%20Christian%20Lebiere.%20The%20cascade-correlation%20learning%20architecture%201990"
        },
        {
            "id": "13",
            "entry": "[13] Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Fan%2C%20Yang%20Tian%2C%20Fei%20Qin%2C%20Tao%20Li%2C%20Xiang-Yang%20Learning%20to%20teach%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Fan%2C%20Yang%20Tian%2C%20Fei%20Qin%2C%20Tao%20Li%2C%20Xiang-Yang%20Learning%20to%20teach%202018"
        },
        {
            "id": "14",
            "entry": "[14] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pages 1019\u20131027, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gal%2C%20Yarin%20Ghahramani%2C%20Zoubin%20A%20theoretically%20grounded%20application%20of%20dropout%20in%20recurrent%20neural%20networks%202016"
        },
        {
            "id": "15",
            "entry": "[15] Xavier Gastaldi. Shake-shake regularization. CoRR, abs/1705.07485, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.07485"
        },
        {
            "id": "16",
            "entry": "[16] Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. CoRR, abs/1612.04426, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1612.04426"
        },
        {
            "id": "17",
            "entry": "[17] Roger B. Grosse, Ruslan Salakhutdinov, William T. Freeman, and Joshua B. Tenenbaum. Exploiting compositionality to explore a large space of model structures. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI\u201912, pages 306\u2013315, Arlington, Virginia, United States, 2012. AUAI Press.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Grosse%2C%20Roger%20B.%20Salakhutdinov%2C%20Ruslan%20Freeman%2C%20William%20T.%20Tenenbaum%2C%20Joshua%20B.%20Exploiting%20compositionality%20to%20explore%20a%20large%20space%20of%20model%20structures%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Grosse%2C%20Roger%20B.%20Salakhutdinov%2C%20Ruslan%20Freeman%2C%20William%20T.%20Tenenbaum%2C%20Joshua%20B.%20Exploiting%20compositionality%20to%20explore%20a%20large%20space%20of%20model%20structures%202012"
        },
        {
            "id": "18",
            "entry": "[18] Furong Huang, Jordan Ash, John Langford, and Robert Schapire. Learning deep resnet blocks sequentially using boosting theory. arXiv preprint arXiv:1706.04964, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04964"
        },
        {
            "id": "19",
            "entry": "[19] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4700\u20134708, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20Gao%20Liu%2C%20Zhuang%20van%20der%20Maaten%2C%20Laurens%20Weinberger%2C%20Kilian%20Q.%20Densely%20connected%20convolutional%20networks%202017"
        },
        {
            "id": "20",
            "entry": "[20] Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01462"
        },
        {
            "id": "21",
            "entry": "[21] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing. Neural architecture search with bayesian optimisation and optimal transport. arXiv preprint arXiv:1802.07191, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.07191"
        },
        {
            "id": "22",
            "entry": "[22] Hiroaki Kitano. Designing neural networks using genetic algorithms with graph generation system. Complex Systems Journal, 4:461\u2013476, 1990.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kitano%2C%20Hiroaki%20Designing%20neural%20networks%20using%20genetic%20algorithms%20with%20graph%20generation%20system%201990",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kitano%2C%20Hiroaki%20Designing%20neural%20networks%20using%20genetic%20algorithms%20with%20graph%20generation%20system%201990"
        },
        {
            "id": "23",
            "entry": "[23] David Krueger, Tegan Maharaj, J\u00e1nos Kram\u00e1r, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Chris Pal. Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.01305"
        },
        {
            "id": "24",
            "entry": "[24] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lample%2C%20Guillaume%20Conneau%2C%20Alexis%20Denoyer%2C%20Ludovic%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Unsupervised%20machine%20translation%20using%20monolingual%20corpora%20only%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lample%2C%20Guillaume%20Conneau%2C%20Alexis%20Denoyer%2C%20Ludovic%20Ranzato%2C%20Marc%E2%80%99Aurelio%20Unsupervised%20machine%20translation%20using%20monolingual%20corpora%20only%202018"
        },
        {
            "id": "25",
            "entry": "[25] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1188\u20131196, Bejing, China, 22\u201324 Jun 2014. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Quoc%20Le%20and%20Tomas%20Mikolov.%20Distributed%20representations%20of%20sentences%20and%20documents%202014-06",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Quoc%20Le%20and%20Tomas%20Mikolov.%20Distributed%20representations%20of%20sentences%20and%20documents%202014-06"
        },
        {
            "id": "26",
            "entry": "[26] Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00559"
        },
        {
            "id": "27",
            "entry": "[27] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierarchical representations for efficient architecture search. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Vinyals%2C%20Oriol%20Fernando%2C%20Chrisantha%20Hierarchical%20representations%20for%20efficient%20architecture%20search%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Hanxiao%20Simonyan%2C%20Karen%20Vinyals%2C%20Oriol%20Fernando%2C%20Chrisantha%20Hierarchical%20representations%20for%20efficient%20architecture%20search%202018"
        },
        {
            "id": "28",
            "entry": "[28] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1806.09055"
        },
        {
            "id": "29",
            "entry": "[29] G\u00e1bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20state%20of%20the%20art%20of%20evaluation%20in%20neural%20language%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Melis%2C%20G%C3%A1bor%20Dyer%2C%20Chris%20Blunsom%2C%20Phil%20On%20the%20state%20of%20the%20art%20of%20evaluation%20in%20neural%20language%20models%202018"
        },
        {
            "id": "30",
            "entry": "[30] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. CoRR, abs/1708.02182, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.02182"
        },
        {
            "id": "31",
            "entry": "[31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07843"
        },
        {
            "id": "32",
            "entry": "[32] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, et al. Evolving deep neural networks. arXiv preprint arXiv:1703.00548, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00548"
        },
        {
            "id": "33",
            "entry": "[33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "34",
            "entry": "[34] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.03268"
        },
        {
            "id": "35",
            "entry": "[35] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.01548"
        },
        {
            "id": "36",
            "entry": "[36] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning, pages 2902\u20132911, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Real%2C%20Esteban%20Moore%2C%20Sherry%20Selle%2C%20Andrew%20Saxena%2C%20Saurabh%20Large-scale%20evolution%20of%20image%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Real%2C%20Esteban%20Moore%2C%20Sherry%20Selle%2C%20Andrew%20Saxena%2C%20Saurabh%20Large-scale%20evolution%20of%20image%20classifiers%202017"
        },
        {
            "id": "37",
            "entry": "[37] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951\u20132959, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Snoek%2C%20Jasper%20Larochelle%2C%20Hugo%20Adams%2C%20Ryan%20P.%20Practical%20bayesian%20optimization%20of%20machine%20learning%20algorithms%202012"
        },
        {
            "id": "38",
            "entry": "[38] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20Ilya%20Vinyals%2C%20Oriol%20Le%2C%20Quoc%20V.%20Sequence%20to%20sequence%20learning%20with%20neural%20networks%202014"
        },
        {
            "id": "39",
            "entry": "[39] L. Xie and A. Yuille. Genetic cnn. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 1388\u20131397, Oct. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=L%20Xie%20and%20A%20Yuille%20Genetic%20cnn%20In%202017%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%20pages%2013881397%20Oct%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=L%20Xie%20and%20A%20Yuille%20Genetic%20cnn%20In%202017%20IEEE%20International%20Conference%20on%20Computer%20Vision%20ICCV%20pages%2013881397%20Oct%202017"
        },
        {
            "id": "40",
            "entry": "[40] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 5987\u20135995. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xie%2C%20Saining%20Girshick%2C%20Ross%20Doll%C3%A1r%2C%20Piotr%20Tu%2C%20Zhuowen%20Aggregated%20residual%20transformations%20for%20deep%20neural%20networks%202017"
        },
        {
            "id": "41",
            "entry": "[41] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax bottleneck: A high-rank RNN language model. In International Conference on Learning Representations, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20Zhilin%20Dai%2C%20Zihang%20Salakhutdinov%2C%20Ruslan%20Cohen%2C%20William%20W.%20Breaking%20the%20softmax%20bottleneck%3A%20A%20high-rank%20RNN%20language%20model%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20Zhilin%20Dai%2C%20Zihang%20Salakhutdinov%2C%20Ruslan%20Cohen%2C%20William%20W.%20Breaking%20the%20softmax%20bottleneck%3A%20A%20high-rank%20RNN%20language%20model%202018"
        },
        {
            "id": "42",
            "entry": "[42] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1409.2329"
        },
        {
            "id": "43",
            "entry": "[43] Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn\u00edk, and J\u00fcrgen Schmidhuber. Recurrent highway networks. In International Conference on Machine Learning, pages 4189\u20134198, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Julian%20Georg%20Zilly%2C%20Rupesh%20Kumar%20Srivastava%2C%20Jan%20Koutn%C3%ADk%20Schmidhuber%2C%20J%C3%BCrgen%20Recurrent%20highway%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Julian%20Georg%20Zilly%2C%20Rupesh%20Kumar%20Srivastava%2C%20Jan%20Koutn%C3%ADk%20Schmidhuber%2C%20J%C3%BCrgen%20Recurrent%20highway%20networks%202017"
        },
        {
            "id": "44",
            "entry": "[44] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01578"
        },
        {
            "id": "45",
            "entry": "[45] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Shlens%2C%20Jonathon%20Le%2C%20Quoc%20V.%20Learning%20transferable%20architectures%20for%20scalable%20image%20recognition%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoph%2C%20Barret%20Vasudevan%2C%20Vijay%20Shlens%2C%20Jonathon%20Le%2C%20Quoc%20V.%20Learning%20transferable%20architectures%20for%20scalable%20image%20recognition%202018"
        }
    ]
}
