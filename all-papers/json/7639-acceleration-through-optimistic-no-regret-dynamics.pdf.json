{
    "filename": "7639-acceleration-through-optimistic-no-regret-dynamics.pdf",
    "metadata": {
        "title": "Acceleration through Optimistic No-Regret Dynamics",
        "author": "Jun-Kun Wang, Jacob D. Abernethy",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7639-acceleration-through-optimistic-no-regret-dynamics.pdf"
        },
        "abstract": "We consider the problem of minimizing a smooth convex function by reducing the optimization to computing the Nash equilibrium of a particular zero-sum convexconcave game. Zero-sum games can be solved using online learning dynamics, where a classical technique involves simulating two no-regret algorithms that play against each other and, after T rounds, the average iterate is guaranteed to solve the original optimization problem with error decaying as O(log T /T ). In this paper we show that the technique can be enhanced to a rate of O(1/T 2) by extending recent work [<a class=\"ref-link\" id=\"c21\" href=\"#r21\">21</a>, <a class=\"ref-link\" id=\"c24\" href=\"#r24\">24</a>] that leverages optimistic learning to speed up equilibrium computation. The resulting optimization algorithm derived from this analysis coincides exactly with the well-known NESTEROVACCELERATION [<a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>] method, and indeed the same story allows us to recover several variants of the Nesterov\u2019s algorithm via small tweaks. We are also able to establish the accelerated linear rate for a function which is both strongly-convex and smooth. This methodology unifies a number of different iterative optimization methods: we show that the HEAVYBALL algorithm is precisely the non-optimistic variant of NESTEROVACCELERATION, and recent prior work already established a similar perspective on FRANKWOLFE [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>, <a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]."
    },
    "keywords": [
        {
            "term": "online convex optimization",
            "url": "https://en.wikipedia.org/wiki/online_convex_optimization"
        },
        {
            "term": "convex optimization",
            "url": "https://en.wikipedia.org/wiki/convex_optimization"
        },
        {
            "term": "loss function",
            "url": "https://en.wikipedia.org/wiki/loss_function"
        },
        {
            "term": "optimization algorithm",
            "url": "https://en.wikipedia.org/wiki/optimization_algorithm"
        }
    ],
    "highlights": [
        "One of the most successful and broadly useful tools recently developed within the machine learning literature is the no-regret framework, and in particular online convex optimization (OCO)<br/><br/>[<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]",
        "One of the most successful and broadly useful tools recently developed within the machine learning literature is the no-regret framework, and in particular online convex optimization (OCO)",
        "Many simple algorithms have been developed for online convex optimization problems\u2014including MIRRORDESCENT, FOLLOWTHEREGU-",
        "When viewed as an optimization algorithm, this method is identically the original NESTEROVACCELERATION method",
        "The cumulative loss function of the x-player becomes more and more strongly convex over time, which is the key to allowing the exponential growth of the total weight At that leads to the linear rate"
    ],
    "key_statements": [
        "One of the most successful and broadly useful tools recently developed within the machine learning literature is the no-regret framework, and in particular online convex optimization (OCO)<br/><br/>[<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>]",
        "One of the most successful and broadly useful tools recently developed within the machine learning literature is the no-regret framework, and in particular online convex optimization (OCO)",
        "Many simple algorithms have been developed for online convex optimization problems\u2014including MIRRORDESCENT, FOLLOWTHEREGU-",
        "When viewed as an optimization algorithm, this method is identically the original NESTEROVACCELERATION method",
        "The cumulative loss function of the x-player becomes more and more strongly convex over time, which is the key to allowing the exponential growth of the total weight At that leads to the linear rate"
    ],
    "summary": [
        "One of the most successful and broadly useful tools recently developed within the machine learning literature is the no-regret framework, and in particular online convex optimization (OCO)<br/><br/>[<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>].",
        "One can imagine computing the equilibrium of the Fenchel game using no-regret dynamics, and this was the result of recent work [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] establishing the FRANKWOLFE algorithm as precisely an instance of two competing learning algorithms.",
        "5. Under the additional assumption that function f (\u00b7) is strongly convex, we show that an accelerated linear rate can be obtained from the game framework.",
        "We consider Fenchel game with weighted losses depicted in Algorithm 1, following the same setup as [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "We are going to analyze more closely the use of Algorithm 1, with the help of Theorem 1, to establish a fast method to compute an approximate equilibrium of the Fenchel Game.",
        "FOLLOWTHELEADER is known to not perform well against arbitrary loss functions, but for strongly convex t(\u00b7) one can prove an O regret bound in the unweighted case.",
        "We notice that a similar bound is given in [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] for the gradient player using OPTIMISTICFTL, yet the above result is a stict improvement as the previous work relied on the additional assumption that f (\u00b7) is strongly convex.",
        "Let us consider the output of Algorithm 1 under the following conditions: (a) the sequence {\u03b1t} is positive but otherwise arbitrary (b) OAlgy is chosen OPTIMISTICFTL, (c) OAlgx is MIRRORDESCENT with any non-increasing positive sequence {\u03b3t}, and (d) we have a bound Vxt (x\u2217) \u2264 D for all t.",
        "Theorem 1 tells us that the pair derived from Algorithm 1 is controlled by the sum of averaged regrets of both players, 1 AT",
        "The negative terms xt \u2212 xt\u22121 2 in this sum happen to correspond exactly to the positive terms one obtains in the regret bound for the y-player, but this is true only as a result of using the OPTIMISTICFTL algorithm.",
        "We show that our accelerated algorithm to the Fenchel game can generate all his methods with some simple tweaks.",
        "1: In the weighted loss setting of Algorithm 1: 2: y-player uses FOLLOWTHELEADER as OAlgy: yt = \u2207f.",
        "It is natural to ask if the zero-sum game and regret analysis in the present work recovers this faster rate in the same fashion.",
        "The cumulative loss function of the x-player becomes more and more strongly convex over time, which is the key to allowing the exponential growth of the total weight At that leads to the linear rate.",
        "We consider the following two-players zero-sum game, minx maxy{ x, y \u2212f \u2217(y)+\u03c8(x)}."
    ],
    "headline": "In this paper we show that the technique can be enhanced to a rate of O by extending recent work  that leverages optimistic learning to speed up equilibrium computation",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Jacob Abernethy, Kfir Levy, Kevin Lai, and Jun-Kun Wang. Faster rates for convex-concave games. COLT, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abernethy%2C%20Jacob%20Levy%2C%20Kfir%20Lai%2C%20Kevin%20Wang%2C%20Jun-Kun%20Faster%20rates%20for%20convex-concave%20games%202018"
        },
        {
            "id": "2",
            "entry": "[2] Jacob Abernethy and Jun-Kun Wang. Frank-wolfe and equilibrium computation. NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abernethy%2C%20Jacob%20Wang%2C%20Jun-Kun%20Frank-wolfe%20and%20equilibrium%20computation%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abernethy%2C%20Jacob%20Wang%2C%20Jun-Kun%20Frank-wolfe%20and%20equilibrium%20computation%202017"
        },
        {
            "id": "3",
            "entry": "[3] Jacob Abernethy, Manfred K Warmuth, and Joel Yellin. Optimal strategies from random walks. In Proceedings of The 21st Annual Conference on Learning Theory, pages 437\u2013446.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abernethy%2C%20Jacob%20Warmuth%2C%20Manfred%20K.%20Yellin%2C%20Joel%20Optimal%20strategies%20from%20random%20walks",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abernethy%2C%20Jacob%20Warmuth%2C%20Manfred%20K.%20Yellin%2C%20Joel%20Optimal%20strategies%20from%20random%20walks"
        },
        {
            "id": "4",
            "entry": "[4] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. ITCS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Orecchia%2C%20Lorenzo%20Linear%20coupling%3A%20An%20ultimate%20unification%20of%20gradient%20and%20mirror%20descent%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Orecchia%2C%20Lorenzo%20Linear%20coupling%3A%20An%20ultimate%20unification%20of%20gradient%20and%20mirror%20descent%202017"
        },
        {
            "id": "5",
            "entry": "[5] David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.05642"
        },
        {
            "id": "6",
            "entry": "[6] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. on Imaging Sciences, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20Amir%20Teboulle%2C%20Marc%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "7",
            "entry": "[7] Sabastien Bubeck, Yin Tat Lee, and Mohit Singh. A geometric alternative to nesterov\u2019s accelerated gradient descent. 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bubeck%2C%20Sabastien%20Lee%2C%20Yin%20Tat%20Singh%2C%20Mohit%20A%20geometric%20alternative%20to%20nesterov%E2%80%99s%20accelerated%20gradient%20descent%202015"
        },
        {
            "id": "8",
            "entry": "[8] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, , and Shenghuo Zhu. Online optimization with gradual variations. 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Online%20optimization%20with%20gradual%20variations%202012"
        },
        {
            "id": "9",
            "entry": "[9] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. arXiv preprint arXiv:1711.00141, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.00141"
        },
        {
            "id": "10",
            "entry": "[10] Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. COLT, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flammarion%2C%20Nicolas%20Bach%2C%20Francis%20From%20averaging%20to%20acceleration%2C%20there%20is%20only%20a%20step-size%202015"
        },
        {
            "id": "11",
            "entry": "[11] Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Gabriel Huang, Remi Lepriol, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. arXiv preprint arXiv:1807.04740, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1807.04740"
        },
        {
            "id": "12",
            "entry": "[12] Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lessard%2C%20Laurent%20Recht%2C%20Benjamin%20Packard%2C%20Andrew%20Analysis%20and%20design%20of%20optimization%20algorithms%20via%20integral%20quadratic%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lessard%2C%20Laurent%20Recht%2C%20Benjamin%20Packard%2C%20Andrew%20Analysis%20and%20design%20of%20optimization%20algorithms%20via%20integral%20quadratic%20constraints%202016"
        },
        {
            "id": "13",
            "entry": "[13] Brendan McMahan and Jacob Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2724\u20132732. Curran Associates, Inc., 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=McMahan%2C%20Brendan%20Abernethy%2C%20Jacob%20Minimax%20optimal%20algorithms%20for%20unconstrained%20linear%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=McMahan%2C%20Brendan%20Abernethy%2C%20Jacob%20Minimax%20optimal%20algorithms%20for%20unconstrained%20linear%20optimization%202013"
        },
        {
            "id": "14",
            "entry": "[14] Yuri Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady AN USSR, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yuri%20A%20method%20for%20unconstrained%20convex%20minimization%20problem%20with%20the%20rate%20of%20convergence%20o%281/k2%29.%20Doklady%20AN%20USSR%201983"
        },
        {
            "id": "15",
            "entry": "[15] Yuri Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27:372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yuri%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20o%281/k2%29%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yuri%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%20o%281/k2%29%201983"
        },
        {
            "id": "16",
            "entry": "[16] Yuri Nesterov. On an approach to the construction of optimal methods of minimization of smooth convex functions. Ekonom. i. Mat. Metody, 24:509\u2013517, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yuri%20On%20an%20approach%20to%20the%20construction%20of%20optimal%20methods%20of%20minimization%20of%20smooth%20convex%20functions.%20Ekonom.%20i%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yuri%20On%20an%20approach%20to%20the%20construction%20of%20optimal%20methods%20of%20minimization%20of%20smooth%20convex%20functions.%20Ekonom.%20i%201988"
        },
        {
            "id": "17",
            "entry": "[17] Yuri Nesterov. Introductory lectures on convex optimization: A basic course. Springer, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yuri%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%202004"
        },
        {
            "id": "18",
            "entry": "[18] Yuri Nesterov. Smooth minimization of nonsmooth functions. Mathematical programming, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yuri%20Smooth%20minimization%20of%20nonsmooth%20functions%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yuri%20Smooth%20minimization%20of%20nonsmooth%20functions%202005"
        },
        {
            "id": "19",
            "entry": "[19] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parikh%2C%20Neal%20Boyd%2C%20Stephen%20Proximal%20algorithms%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parikh%2C%20Neal%20Boyd%2C%20Stephen%20Proximal%20algorithms%202014"
        },
        {
            "id": "20",
            "entry": "[20] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. COLT, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20Alexander%20Sridharan%2C%20Karthik%20Online%20learning%20with%20predictable%20sequences%202013"
        },
        {
            "id": "21",
            "entry": "[21] Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rakhlin%2C%20Alexander%20Sridharan%2C%20Karthik%20Optimization%2C%20learning%2C%20and%20games%20with%20predictable%20sequences%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rakhlin%2C%20Alexander%20Sridharan%2C%20Karthik%20Optimization%2C%20learning%2C%20and%20games%20with%20predictable%20sequences%202013"
        },
        {
            "id": "22",
            "entry": "[22] Tyrrell Rockafellar. Convex analysis. Princeton University Press, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rockafellar%2C%20Tyrrell%20Convex%20analysis%201996"
        },
        {
            "id": "23",
            "entry": "[23] Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov\u2019s accelerated gradient method: Theory and insights. NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20Weijie%20Boyd%2C%20Stephen%20Candes%2C%20Emmanuel%20A%20differential%20equation%20for%20modeling%20nesterov%E2%80%99s%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Su%2C%20Weijie%20Boyd%2C%20Stephen%20Candes%2C%20Emmanuel%20A%20differential%20equation%20for%20modeling%20nesterov%E2%80%99s%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014"
        },
        {
            "id": "24",
            "entry": "[24] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of regularized learning in games. NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Syrgkanis%2C%20Vasilis%20Agarwal%2C%20Alekh%20Luo%2C%20Haipeng%20Schapire%2C%20Robert%20E.%20Fast%20convergence%20of%20regularized%20learning%20in%20games%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Syrgkanis%2C%20Vasilis%20Agarwal%2C%20Alekh%20Luo%2C%20Haipeng%20Schapire%2C%20Robert%20E.%20Fast%20convergence%20of%20regularized%20learning%20in%20games%202015"
        },
        {
            "id": "25",
            "entry": "[25] Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tseng%2C%20Paul%20On%20accelerated%20proximal%20gradient%20methods%20for%20convex-concave%20optimization%202008"
        },
        {
            "id": "26",
            "entry": "[26] Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351\u2013 E7358, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wibisono%2C%20Andre%20Wilson%2C%20Ashia%20C.%20and%20Michael%20I%20Jordan.%20A%20variational%20perspective%20on%20accelerated%20methods%20in%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wibisono%2C%20Andre%20Wilson%2C%20Ashia%20C.%20and%20Michael%20I%20Jordan.%20A%20variational%20perspective%20on%20accelerated%20methods%20in%20optimization%202016"
        },
        {
            "id": "27",
            "entry": "[27] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. ICML, 2003. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zinkevich%2C%20Martin%20Online%20convex%20programming%20and%20generalized%20infinitesimal%20gradient%20ascent%202003"
        }
    ]
}
