{
    "filename": "7325-rest-katyusha-exploiting-the-solutions-structure-via-scheduled-restart-schemes.pdf",
    "metadata": {
        "title": "Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes",
        "author": "Junqi Tang, Mohammad Golbabaee, Francis Bach, Mike E. davies",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7325-rest-katyusha-exploiting-the-solutions-structure-via-scheduled-restart-schemes.pdf"
        },
        "abstract": "We propose a structure-adaptive variant of the state-of-the-art stochastic variancereduced gradient algorithm Katyusha for regularized empirical risk minimization. The proposed method is able to exploit the intrinsic low-dimensional structure of the solution, such as sparsity or low rank which is enforced by a non-smooth regularization, to achieve even faster convergence rate. This provable algorithmic improvement is done by restarting the Katyusha algorithm according to restricted strong-convexity constants. We demonstrate the effectiveness of our approach via numerical experiments."
    },
    "keywords": [
        {
            "term": "gradient method",
            "url": "https://en.wikipedia.org/wiki/gradient_method"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "strong convexity",
            "url": "https://en.wikipedia.org/wiki/strong_convexity"
        },
        {
            "term": "high dimensional",
            "url": "https://en.wikipedia.org/wiki/high_dimensional"
        },
        {
            "term": "empirical risk minimization",
            "url": "https://en.wikipedia.org/wiki/empirical_risk_minimization"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        }
    ],
    "highlights": [
        "Many applications in supervised machine learning and signal processing share the same goal, which is to estimate the minimizer of a population risk via minimizing the empirical<br/><br/>1 n n i=1 fi, where ai, x \u2208 Rd, each fi is a convex and smooth function [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "The convergence analysis of Rest-Katyusha algorithm is provided, wherein we prove linear convergence up to a statistical accuracy with an accelerated convergence rate characterized by the restricted strong-convexity property",
        "Like all other accelerated gradient methods which require the explicit knowledge of strong-convexity parameter to achieve accelerated linear convergence, the vanilla Rest-Katyusha method need to explicitly know the restricted strong-convexity parameter",
        "We propose to warm start the algorithm prior to the periodic restart stage, by running the Katyusha algorithm for a number of epochs, which in theory should be proportional to the suboptimality of the starting point x0",
        "Motivated by the theory above, we further propose our practical adaptive restart heuristic of RestKatyusha which is able to estimate the effective restricted strong-convexity on the fly",
        "We developed a restart variant of the Katyusha algorithm for regularized empirical risk minimization tasks, which is provably able to actively exploit the intrinsic low-dimensional structure of the solution for the acceleration of convergence"
    ],
    "key_statements": [
        "Many applications in supervised machine learning and signal processing share the same goal, which is to estimate the minimizer of a population risk via minimizing the empirical<br/><br/>1 n n i=1 fi, where ai, x \u2208 Rd, each fi is a convex and smooth function [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "The convergence analysis of Rest-Katyusha algorithm is provided, wherein we prove linear convergence up to a statistical accuracy with an accelerated convergence rate characterized by the restricted strong-convexity property",
        "Like all other accelerated gradient methods which require the explicit knowledge of strong-convexity parameter to achieve accelerated linear convergence, the vanilla Rest-Katyusha method need to explicitly know the restricted strong-convexity parameter",
        "We propose to warm start the algorithm prior to the periodic restart stage, by running the Katyusha algorithm for a number of epochs, which in theory should be proportional to the suboptimality of the starting point x0",
        "Motivated by the theory above, we further propose our practical adaptive restart heuristic of RestKatyusha which is able to estimate the effective restricted strong-convexity on the fly",
        "We developed a restart variant of the Katyusha algorithm for regularized empirical risk minimization tasks, which is provably able to actively exploit the intrinsic low-dimensional structure of the solution for the acceleration of convergence"
    ],
    "summary": [
        "Many applications in supervised machine learning and signal processing share the same goal, which is to estimate the minimizer of a population risk via minimizing the empirical<br/><br/>1 n n i=1 fi, where ai, x \u2208 Rd, each fi is a convex and smooth function [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "In the context of statistical estimation with high-dimensional data where the usual strong-convexity assumption is vacuous, these authors have shown that the proximal gradient descent method is able to achieve global linear convergence up to a point x which satisfies x \u2212 x 2 = o( x \u2212 x\u2020 2), the accuracy level of statistical precision.",
        "Like all other accelerated gradient methods which require the explicit knowledge of strong-convexity parameter to achieve accelerated linear convergence, the vanilla Rest-Katyusha method need to explicitly know the RSC parameter.",
        "Inspired by Nesterov [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>] we first propose the Katyusha method with periodic restarts, and demonstrate that when the restart period is appropriately chosen, the proposed method is able to exploit the restricted strong-convexity property to achieve an accelerated linear convergence, even when the cost function itself is not strongly-convex.",
        "We refer \u03bcc as the effective restricted strong convexity parameter, which will provide us a direct link between the convergence speed of an algorithm and the low-dimensional structure of the solution.",
        "Parameter choices and notations as Theorem 3.4, the total number of stochastic gradient evaluations required by Rest-Katyusha to get an \u03b4 > \u03b5-accuracy is: nL 1",
        "Under the RSC assumption, Theorem 3.4 and Corollary 3.5 demonstrate a local accelerated linear convergence rate of Rest-Katyusha up to a statistical accuracy \u03b4 > \u03b5.",
        "We derive this result based on extending the framework provided by Agarwal et al [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>], by which they established fast structure-dependent linear convergence of proximal gradient descent method up to a statistical accuracy of \u03b4 > \u03b5.",
        "From these experiments we observe that as our theory has predicted, the Rest-Katyusha achieves accelerated linear convergence even when there is no explicit strong-convexity in the cost function (RCV1 and REGED dataset), and the convergence speed has a direct relationship with the sparsity of solution.",
        "We have observe that the adaptive Rest-Katyusha achieves a good estimation of the RSC parameter and properly adapts the choice of restart period automatically on the fly, its performance is often comparable with the best tuned Rest-Katyusha.",
        "We developed a restart variant of the Katyusha algorithm for regularized empirical risk minimization tasks, which is provably able to actively exploit the intrinsic low-dimensional structure of the solution for the acceleration of convergence.",
        "We aim to develop more refined and provably-good adaptive restart schemes for Rest-Katyusha algorithm to further exploit the solution\u2019s structure for acceleration."
    ],
    "headline": "We propose a structure-adaptive variant of the state-of-the-art stochastic variancereduced gradient algorithm Katyusha for regularized empirical risk minimization",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vapnik%2C%20Vladimir%20The%20nature%20of%20statistical%20learning%20theory%202013"
        },
        {
            "id": "2",
            "entry": "[2] Martin J Wainwright. Structured regularizers for high-dimensional problems: Statistical and computational issues. Annual Review of Statistics and Its Application, 1:233\u2013253, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20J.%20Structured%20regularizers%20for%20high-dimensional%20problems%3A%20Statistical%20and%20computational%20issues%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wainwright%2C%20Martin%20J.%20Structured%20regularizers%20for%20high-dimensional%20problems%3A%20Statistical%20and%20computational%20issues%202014"
        },
        {
            "id": "3",
            "entry": "[3] Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, pages 538\u2013557, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Negahban%2C%20Sahand%20N.%20Ravikumar%2C%20Pradeep%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20A%20unified%20framework%20for%20high-dimensional%20analysis%20of%20m-estimators%20with%20decomposable%20regularizers%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Negahban%2C%20Sahand%20N.%20Ravikumar%2C%20Pradeep%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20A%20unified%20framework%20for%20high-dimensional%20analysis%20of%20m-estimators%20with%20decomposable%20regularizers%202012"
        },
        {
            "id": "4",
            "entry": "[4] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "5",
            "entry": "[5] Robert Tibshirani, Martin Wainwright, and Trevor Hastie. Statistical learning with sparsity: the lasso and generalizations. Chapman and Hall/CRC, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Wainwright%2C%20Martin%20Hastie%2C%20Trevor%20Statistical%20learning%20with%20sparsity%3A%20the%20lasso%20and%20generalizations%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Wainwright%2C%20Martin%20Hastie%2C%20Trevor%20Statistical%20learning%20with%20sparsity%3A%20the%20lasso%20and%20generalizations%202015"
        },
        {
            "id": "6",
            "entry": "[6] Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):91\u2013 108, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Saunders%2C%20Michael%20Rosset%2C%20Saharon%20Zhu%2C%20Ji%20Sparsity%20and%20smoothness%20via%20the%20fused%20lasso%202005",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Saunders%2C%20Michael%20Rosset%2C%20Saharon%20Zhu%2C%20Ji%20Sparsity%20and%20smoothness%20via%20the%20fused%20lasso%202005"
        },
        {
            "id": "7",
            "entry": "[7] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the twenty-first international conference on Machine learning, page 116. ACM, 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Tong%20Solving%20large%20scale%20linear%20prediction%20problems%20using%20stochastic%20gradient%20descent%20algorithms%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Tong%20Solving%20large%20scale%20linear%20prediction%20problems%20using%20stochastic%20gradient%20descent%20algorithms%202004"
        },
        {
            "id": "8",
            "entry": "[8] L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010, pages 177\u2013186.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L%C3%A9on%20Large-scale%20machine%20learning%20with%20stochastic%20gradient%20descent"
        },
        {
            "id": "9",
            "entry": "[9] Nicolas L. Roux, Mark Schmidt, and Francis R. Bach. A stochastic gradient method with an exponential convergence _rate for finite training sets. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2663\u20132671. Curran Associates, Inc., 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20_rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20Nicolas%20L.%20Schmidt%2C%20Mark%20Bach%2C%20Francis%20R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20_rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "10",
            "entry": "[10] M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, pages 1\u201330, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schmidt%2C%20M.%20Roux%2C%20N.Le%20Bach%2C%20F.%20Minimizing%20finite%20sums%20with%20the%20stochastic%20average%20gradient%202013"
        },
        {
            "id": "11",
            "entry": "[11] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(Feb):567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "12",
            "entry": "[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in neural information processing systems, pages 315\u2013323, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20Rie%20Zhang%2C%20Tong%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "13",
            "entry": "[13] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20L.%20Zhang%2C%20T.%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20L.%20Zhang%2C%20T.%20A%20proximal%20stochastic%20gradient%20method%20with%20progressive%20variance%20reduction%202014"
        },
        {
            "id": "14",
            "entry": "[14] A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20Saga%3A%20A%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "15",
            "entry": "[15] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Yurii%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%201983"
        },
        {
            "id": "16",
            "entry": "[16] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical report, UCL, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20Gradient%20methods%20for%20minimizing%20composite%20objective%20function%202007"
        },
        {
            "id": "17",
            "entry": "[17] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Yurii%20Introductory%20lectures%20on%20convex%20optimization%3A%20A%20basic%20course%2C%20volume%2087%202013"
        },
        {
            "id": "18",
            "entry": "[18] Qihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient method. In Advances in Neural Information Processing Systems, pages 3059\u20133067, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20Qihang%20Lu%2C%20Zhaosong%20Xiao%2C%20Lin%20An%20accelerated%20proximal%20coordinate%20gradient%20method%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20Qihang%20Lu%2C%20Zhaosong%20Xiao%2C%20Lin%20An%20accelerated%20proximal%20coordinate%20gradient%20method%202014"
        },
        {
            "id": "19",
            "entry": "[19] Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\u00e1rik, and Yang Yuan. Even faster accelerated coordinate descent using non-uniform sampling. In International Conference on Machine Learning, pages 1110\u20131119, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allen-Zhu%2C%20Zeyuan%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Yuan%2C%20Yang%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allen-Zhu%2C%20Zeyuan%20Qu%2C%20Zheng%20Richt%C3%A1rik%2C%20Peter%20Yuan%2C%20Yang%20Even%20faster%20accelerated%20coordinate%20descent%20using%20non-uniform%20sampling%202016"
        },
        {
            "id": "20",
            "entry": "[20] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In International Conference on Machine Learning, pages 64\u201372, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20Shai%20Zhang%2C%20Tong%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014"
        },
        {
            "id": "21",
            "entry": "[21] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint arXiv:1507.02000, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.02000"
        },
        {
            "id": "22",
            "entry": "[22] Yuchen Zhang and Xiao Lin. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 353\u2013361, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20Yuchen%20Lin%2C%20Xiao%20Stochastic%20primal-dual%20coordinate%20method%20for%20regularized%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "23",
            "entry": "[23] Z. Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. arXiv preprint arXiv:1603.05953, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1603.05953"
        },
        {
            "id": "24",
            "entry": "[24] Aaron Defazio. A simple practical accelerated method for finite sums. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 676\u2013684. Curran Associates, Inc., 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20Aaron%20A%20simple%20practical%20accelerated%20method%20for%20finite%20sums%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20Aaron%20A%20simple%20practical%20accelerated%20method%20for%20finite%20sums%202016"
        },
        {
            "id": "25",
            "entry": "[25] Tomoya Murata and Taiji Suzuki. Doubly accelerated stochastic variance reduced dual averaging method for regularized empirical risk minimization. In Advances in Neural Information Processing Systems, pages 608\u2013617, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Murata%2C%20Tomoya%20Suzuki%2C%20Taiji%20Doubly%20accelerated%20stochastic%20variance%20reduced%20dual%20averaging%20method%20for%20regularized%20empirical%20risk%20minimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Murata%2C%20Tomoya%20Suzuki%2C%20Taiji%20Doubly%20accelerated%20stochastic%20variance%20reduced%20dual%20averaging%20method%20for%20regularized%20empirical%20risk%20minimization%202017"
        },
        {
            "id": "26",
            "entry": "[26] Yossi Arjevani. Limitations on variance-reduction and acceleration schemes for finite sums optimization. In Advances in Neural Information Processing Systems, pages 3543\u20133552, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjevani%2C%20Yossi%20Limitations%20on%20variance-reduction%20and%20acceleration%20schemes%20for%20finite%20sums%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjevani%2C%20Yossi%20Limitations%20on%20variance-reduction%20and%20acceleration%20schemes%20for%20finite%20sums%20optimization%202017"
        },
        {
            "id": "27",
            "entry": "[27] B. O\u2019Donoghue and E. Candes. Adaptive restart for accelerated gradient schemes. Foundations of computational mathematics, 15(3):715\u2013732, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=O%E2%80%99Donoghue%2C%20B.%20Candes%2C%20E.%20Adaptive%20restart%20for%20accelerated%20gradient%20schemes.%20Foundations%20of%20computational%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=O%E2%80%99Donoghue%2C%20B.%20Candes%2C%20E.%20Adaptive%20restart%20for%20accelerated%20gradient%20schemes.%20Foundations%20of%20computational%202015"
        },
        {
            "id": "28",
            "entry": "[28] Vincent Roulet and Alexandre d\u2019Aspremont. Sharpness, restart and acceleration. In Advances in Neural Information Processing Systems, pages 1119\u20131129, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roulet%2C%20Vincent%20d%E2%80%99Aspremont%2C%20Alexandre%20Sharpness%2C%20restart%20and%20acceleration%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roulet%2C%20Vincent%20d%E2%80%99Aspremont%2C%20Alexandre%20Sharpness%2C%20restart%20and%20acceleration%202017"
        },
        {
            "id": "29",
            "entry": "[29] Olivier Fercoq and Zheng Qu. Adaptive restart of accelerated gradient methods under local quadratic growth condition. arXiv preprint arXiv:1709.02300, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1709.02300"
        },
        {
            "id": "30",
            "entry": "[30] Olivier Fercoq and Zheng Qu. Restarting the accelerated coordinate descent method with a rough strong convexity estimate. arXiv preprint arXiv:1803.05771, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.05771"
        },
        {
            "id": "31",
            "entry": "[31] Jialei Wang and Lin Xiao. Exploiting strong convexity from data with primal-dual first-order algorithms. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3694\u20133702, International Convention Centre, Sydney, Australia, 06\u201311 Aug 2017. PMLR.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wang%2C%20Jialei%20Xiao%2C%20Lin%20Exploiting%20strong%20convexity%20from%20data%20with%20primal-dual%20first-order%20algorithms%202017-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wang%2C%20Jialei%20Xiao%2C%20Lin%20Exploiting%20strong%20convexity%20from%20data%20with%20primal-dual%20first-order%20algorithms%202017-08"
        },
        {
            "id": "32",
            "entry": "[32] Francis R Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning Research, 9(Jun):1179\u20131225, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bach%2C%20Francis%20R.%20Consistency%20of%20the%20group%20lasso%20and%20multiple%20kernel%20learning%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bach%2C%20Francis%20R.%20Consistency%20of%20the%20group%20lasso%20and%20multiple%20kernel%20learning%202008"
        },
        {
            "id": "33",
            "entry": "[33] Jian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cai%2C%20Jian-Feng%20Cand%C3%A8s%2C%20Emmanuel%20J.%20Shen%2C%20Zuowei%20A%20singular%20value%20thresholding%20algorithm%20for%20matrix%20completion%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cai%2C%20Jian-Feng%20Cand%C3%A8s%2C%20Emmanuel%20J.%20Shen%2C%20Zuowei%20A%20singular%20value%20thresholding%20algorithm%20for%20matrix%20completion%202010"
        },
        {
            "id": "34",
            "entry": "[34] A. Agarwal, S. Negahban, and M. J. Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. The Annals of Statistics, 40(5):2452\u20132482, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Agarwal%2C%20A.%20Negahban%2C%20S.%20Wainwright%2C%20M.J.%20Fast%20global%20convergence%20rates%20of%20gradient%20methods%20for%20high-dimensional%20statistical%20recovery%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Agarwal%2C%20A.%20Negahban%2C%20S.%20Wainwright%2C%20M.J.%20Fast%20global%20convergence%20rates%20of%20gradient%20methods%20for%20high-dimensional%20statistical%20recovery%202012"
        },
        {
            "id": "35",
            "entry": "[35] Chao Qu and Huan Xu. Linear convergence of svrg in statistical estimation. arXiv preprint arXiv:1611.01957, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01957"
        },
        {
            "id": "36",
            "entry": "[36] Junqi Tang, Francis Bach, Mohammad Golbabaee, and Mike Davies. Structure-adaptive, variance-reduced, and accelerated stochastic optimization. arXiv preprint arXiv:1712.03156, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1712.03156"
        },
        {
            "id": "37",
            "entry": "[37] Junqi Tang, Mohammad Golbabaee, Francis Bach, and Mike Davies. Structure-adaptive accelerated coordinate descent. hal.archives hal-01889990, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Junqi%20Golbabaee%2C%20Mohammad%20Bach%2C%20Francis%20Davies%2C%20Mike%20Structure-adaptive%20accelerated%20coordinate%20descent.%20hal.archives%20hal-01889990%202018"
        },
        {
            "id": "38",
            "entry": "[38] M. Pilanci and M. J. Wainwright. Randomized sketches of convex programs with sharp guarantees. Information Theory, IEEE Transactions on, 61(9):5096\u20135115, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilanci%2C%20M.%20Wainwright%2C%20M.J.%20Randomized%20sketches%20of%20convex%20programs%20with%20sharp%20guarantees%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilanci%2C%20M.%20Wainwright%2C%20M.J.%20Randomized%20sketches%20of%20convex%20programs%20with%20sharp%20guarantees%202015"
        },
        {
            "id": "39",
            "entry": "[39] M. Pilanci and M. J. Wainwright. Iterative hessian sketch: Fast and accurate solution approximation for constrained least-squares. Journal of Machine Learning Research, 17(53):1\u201338, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilanci%2C%20M.%20Wainwright%2C%20M.J.%20Iterative%20hessian%20sketch%3A%20Fast%20and%20accurate%20solution%20approximation%20for%20constrained%20least-squares%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilanci%2C%20M.%20Wainwright%2C%20M.J.%20Iterative%20hessian%20sketch%3A%20Fast%20and%20accurate%20solution%20approximation%20for%20constrained%20least-squares%202016"
        },
        {
            "id": "40",
            "entry": "[40] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205\u2013245, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pilanci%2C%20Mert%20Wainwright%2C%20Martin%20J.%20Newton%20sketch%3A%20A%20near%20linear-time%20optimization%20algorithm%20with%20linear-quadratic%20convergence%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pilanci%2C%20Mert%20Wainwright%2C%20Martin%20J.%20Newton%20sketch%3A%20A%20near%20linear-time%20optimization%20algorithm%20with%20linear-quadratic%20convergence%202017"
        },
        {
            "id": "41",
            "entry": "[41] Junqi Tang, Mohammad Golbabaee, and Mike E. Davies. Gradient projection iterative sketch for large-scale constrained least-squares. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3377\u20133386. PMLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Junqi%20Golbabaee%2C%20Mohammad%20Davies%2C%20Mike%20E.%20Gradient%20projection%20iterative%20sketch%20for%20large-scale%20constrained%20least-squares%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Junqi%20Golbabaee%2C%20Mohammad%20Davies%2C%20Mike%20E.%20Gradient%20projection%20iterative%20sketch%20for%20large-scale%20constrained%20least-squares%202017"
        },
        {
            "id": "42",
            "entry": "[42] Junqi Tang, Mohammad Golbabaee, and Mike Davies. Exploiting the structure via sketched gradient algorithms. In Signal and Information Processing (GlobalSIP), 2017 IEEE Global Conference on, pages 1305\u20131309. IEEE, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Junqi%20Golbabaee%2C%20Mohammad%20Davies%2C%20Mike%20Exploiting%20the%20structure%20via%20sketched%20gradient%20algorithms%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Junqi%20Golbabaee%2C%20Mohammad%20Davies%2C%20Mike%20Exploiting%20the%20structure%20via%20sketched%20gradient%20algorithms%202017"
        },
        {
            "id": "43",
            "entry": "[43] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1407.1537"
        },
        {
            "id": "44",
            "entry": "[44] H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In Advances in Neural Information Processing Systems, pages 3384\u20133392, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015"
        },
        {
            "id": "45",
            "entry": "[45] Olivier Fercoq and Zheng Qu. Restarting accelerated gradient methods with a rough strong convexity estimate. arXiv preprint arXiv:1609.07358, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1609.07358"
        },
        {
            "id": "46",
            "entry": "[46] Samuel Vaiter, Mohammad Golbabaee, Jalal Fadili, and Gabriel Peyr\u00e9. Model selection with low complexity priors. Information and Inference: A Journal of the IMA, 4(3):230\u2013287, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vaiter%2C%20Samuel%20Golbabaee%2C%20Mohammad%20Fadili%2C%20Jalal%20Peyr%C3%A9%2C%20Gabriel%20Model%20selection%20with%20low%20complexity%20priors%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vaiter%2C%20Samuel%20Golbabaee%2C%20Mohammad%20Fadili%2C%20Jalal%20Peyr%C3%A9%2C%20Gabriel%20Model%20selection%20with%20low%20complexity%20priors%202015"
        },
        {
            "id": "47",
            "entry": "[47] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795\u2013811.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximal-gradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Karimi%2C%20Hamed%20Nutini%2C%20Julie%20Schmidt%2C%20Mark%20Linear%20convergence%20of%20gradient%20and%20proximal-gradient%20methods%20under%20the%20polyak-%C5%82ojasiewicz%20condition"
        },
        {
            "id": "48",
            "entry": "[48] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. Journal of Machine Learning Research, 11(Aug):2241\u20132259, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Raskutti%2C%20Garvesh%20Wainwright%2C%20Martin%20J.%20Yu%2C%20Bin%20Restricted%20eigenvalue%20properties%20for%20correlated%20gaussian%20designs%202010"
        },
        {
            "id": "49",
            "entry": "[49] M. Lichman. UCI machine learning repository, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lichman%2C%20M.%20UCI%20machine%20learning%20repository%202013"
        },
        {
            "id": "50",
            "entry": "[50] Causality workbench team. A genomics dataset, 09 2008. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Causality%20workbench%20team.%20A%20genomics%20dataset%202008"
        }
    ]
}
