{
    "filename": "8212-blockwise-parallel-decoding-for-deep-autoregressive-models.pdf",
    "metadata": {
        "title": "Blockwise Parallel Decoding for Deep Autoregressive Models",
        "author": "Mitchell Stern, Noam Shazeer, Jakob Uszkoreit",
        "date": 2017,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8212-blockwise-parallel-decoding-for-deep-autoregressive-models.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Deep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While common architecture classes such as recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, generation still remains an inherently sequential process. To overcome this limitation, we propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model. This allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel. We verify our approach empirically through a series of experiments using state-of-the-art self-attention models for machine translation and image super-resolution, achieving iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. In terms of wall-clock time, our fastest models exhibit real-time speedups of up to 4x over standard greedy decoding."
    },
    "keywords": [
        {
            "term": "neural machine translation",
            "url": "https://en.wikipedia.org/wiki/neural_machine_translation"
        },
        {
            "term": "machine translation",
            "url": "https://en.wikipedia.org/wiki/machine_translation"
        },
        {
            "term": "speech synthesis",
            "url": "https://en.wikipedia.org/wiki/speech_synthesis"
        }
    ],
    "highlights": [
        "Neural autoregressive sequence-to-sequence models have become the de facto standard for a wide variety of tasks, including machine translation, summarization, and speech synthesis (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a>; <a class=\"ref-link\" id=\"cRush_et+al_2015_a\" href=\"#rRush_et+al_2015_a\"><a class=\"ref-link\" id=\"cRush_et+al_2015_a\" href=\"#rRush_et+al_2015_a\">Rush et al, 2015</a></a>; van den Oord et al, 2016)",
        "Some general-purpose methods include probability density distillation (Oord et al, 2017), subscaling (<a class=\"ref-link\" id=\"cKalchbrenner_et+al_2018_a\" href=\"#rKalchbrenner_et+al_2018_a\">Kalchbrenner et al, 2018</a>), and decomposing the problem into the autoregressive generation of a short sequence of discrete latent variables followed by a parallel generation step conditioned on the discrete latents (<a class=\"ref-link\" id=\"cKaiser_et+al_2018_a\" href=\"#rKaiser_et+al_2018_a\">Kaiser et al, 2018</a>)",
        "We propose the following blockwise parallel decoding algorithm, which is guaranteed to produce the same prediction ythat would be found under greedy decoding but uses as few as m/k steps",
        "We posit that sequence-level distillation could be especially useful for blockwise parallel decoding, as it tends to result in a training set with greater predictability due to consistent mode breaking from the teacher model",
        "We proposed blockwise parallel decoding as a simple and generic technique for improving decoding performance in deep autoregressive models whose architectures allow for parallelization of scoring across output positions",
        "It is comparatively straightforward to add to existing models and we demonstrate significant improvements in decoding speed on machine translation and a conditional image generation task at no loss or only small losses in quality"
    ],
    "key_statements": [
        "Neural autoregressive sequence-to-sequence models have become the de facto standard for a wide variety of tasks, including machine translation, summarization, and speech synthesis (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a>; <a class=\"ref-link\" id=\"cRush_et+al_2015_a\" href=\"#rRush_et+al_2015_a\"><a class=\"ref-link\" id=\"cRush_et+al_2015_a\" href=\"#rRush_et+al_2015_a\">Rush et al, 2015</a></a>; van den Oord et al, 2016)",
        "Some general-purpose methods include probability density distillation (Oord et al, 2017), subscaling (<a class=\"ref-link\" id=\"cKalchbrenner_et+al_2018_a\" href=\"#rKalchbrenner_et+al_2018_a\">Kalchbrenner et al, 2018</a>), and decomposing the problem into the autoregressive generation of a short sequence of discrete latent variables followed by a parallel generation step conditioned on the discrete latents (<a class=\"ref-link\" id=\"cKaiser_et+al_2018_a\" href=\"#rKaiser_et+al_2018_a\">Kaiser et al, 2018</a>)",
        "We propose the following blockwise parallel decoding algorithm, which is guaranteed to produce the same prediction ythat would be found under greedy decoding but uses as few as m/k steps",
        "We posit that sequence-level distillation could be especially useful for blockwise parallel decoding, as it tends to result in a training set with greater predictability due to consistent mode breaking from the teacher model",
        "We proposed blockwise parallel decoding as a simple and generic technique for improving decoding performance in deep autoregressive models whose architectures allow for parallelization of scoring across output positions",
        "It is comparatively straightforward to add to existing models and we demonstrate significant improvements in decoding speed on machine translation and a conditional image generation task at no loss or only small losses in quality"
    ],
    "summary": [
        "Neural autoregressive sequence-to-sequence models have become the de facto standard for a wide variety of tasks, including machine translation, summarization, and speech synthesis (<a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\"><a class=\"ref-link\" id=\"cVaswani_et+al_2017_a\" href=\"#rVaswani_et+al_2017_a\">Vaswani et al, 2017</a></a></a>; <a class=\"ref-link\" id=\"cRush_et+al_2015_a\" href=\"#rRush_et+al_2015_a\"><a class=\"ref-link\" id=\"cRush_et+al_2015_a\" href=\"#rRush_et+al_2015_a\">Rush et al, 2015</a></a>; van den Oord et al, 2016).",
        "We determine the longest prefix of these predictions that would have generated under greedy decoding by scoring each position in parallel using the base model.",
        "We posit that sequence-level distillation could be especially useful for blockwise parallel decoding, as it tends to result in a training set with greater predictability due to consistent mode breaking from the teacher model.",
        "We train a collection of combined scoring and proposal Transformer models for various block sizes k; see Section 6 for implementation details.",
        "When we instead use distilled data, the BLEU score increases by 0.4 and the mean block size reaches 1.91, showing slight improvements on both metrics.",
        "As with our machine translation experiments, we train a collection of additional models with warm-started parameters for various block sizes k, both with and without fine tuning of the base model\u2019s parameters.",
        "We find that exact-match decoding for the models trained with frozen base parameters is perhaps overly stringent, barely allowing for any speedup for even the largest block size.",
        "The models with fine-tuned parameters fare somewhat better when exact-match decoding is used, achieving a mean block size of slightly over 2.2 in the best case.",
        "We have framed our results in terms of the mean accepted block size, which is reflective of the speedup achieved relative to greedy decoding in terms of number of decoding iterations.",
        "The wall-clock speedup peaks at 3.3x, corresponding to the setting with k = 8 and mean accepted block size 4.7.",
        "For super-resolution, the wall-clock speedup reaches 4.0x, corresponding to the setting with k = 6 and mean accepted block size 5.3.",
        "The left image is the low-resolution input, the middle image is the standard greedy decode, and the right image is the approximate greedy decode using the fine-tuned model with block size k = 10.",
        "We proposed blockwise parallel decoding as a simple and generic technique for improving decoding performance in deep autoregressive models whose architectures allow for parallelization of scoring across output positions.",
        "It is comparatively straightforward to add to existing models and we demonstrate significant improvements in decoding speed on machine translation and a conditional image generation task at no loss or only small losses in quality.",
        "In future work we plan to investigate combinations of this technique with potentially orthogonal approaches such as those based on sequences of discrete latent variables (<a class=\"ref-link\" id=\"cKaiser_et+al_2018_a\" href=\"#rKaiser_et+al_2018_a\"><a class=\"ref-link\" id=\"cKaiser_et+al_2018_a\" href=\"#rKaiser_et+al_2018_a\">Kaiser et al, 2018</a></a>)"
    ],
    "headline": "We propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel back off to the longest prefix validated by a scoring model",
    "reference_links": [
        {
            "id": "Furlanello_et+al_2017_a",
            "entry": "Tommaso Furlanello, Zachary C Lipton, AI Amazon, Laurent Itti, and Anima Anandkumar. Born again neural networks. In NIPS Workshop on Meta Learning, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Furlanello%2C%20Tommaso%20Lipton%2C%20Zachary%20C.%20Amazon%2C%20A.I.%20Itti%2C%20Laurent%20Born%20again%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Furlanello%2C%20Tommaso%20Lipton%2C%20Zachary%20C.%20Amazon%2C%20A.I.%20Itti%2C%20Laurent%20Born%20again%20neural%20networks%202017"
        },
        {
            "id": "Gehring_et+al_2017_a",
            "entry": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. CoRR, abs/1705.03122, 2017. URL http://arxiv.org/abs/1705.03122.",
            "url": "http://arxiv.org/abs/1705.03122",
            "arxiv_url": "https://arxiv.org/pdf/1705.03122"
        },
        {
            "id": "Gu_et+al_2018_a",
            "entry": "Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. Non-autoregressive neural machine translation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1l8BtlCb.",
            "url": "https://openreview.net/forum?id=B1l8BtlCb",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gu%2C%20Jiatao%20Bradbury%2C%20James%20Xiong%2C%20Caiming%20Li%2C%20Victor%20O.K.%20Non-autoregressive%20neural%20machine%20translation%202018"
        },
        {
            "id": "Hinton_et+al_2015_a",
            "entry": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1503.02531"
        },
        {
            "id": "Kaiser_et+al_2018_a",
            "entry": "\u0141ukasz Kaiser, Aurko Roy, Ashish Vaswani, Niki Pamar, Samy Bengio, Jakob Uszkoreit, and Noam Shazeer. Fast decoding in sequence models using discrete latent variables. arXiv preprint arXiv:1803.03382, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1803.03382"
        },
        {
            "id": "Kalchbrenner_et+al_2018_a",
            "entry": "Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. arXiv preprint arXiv:1802.08435, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.08435"
        },
        {
            "id": "Kim_2016_a",
            "entry": "Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprint arXiv:1606.07947, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.07947"
        },
        {
            "id": "Lee_et+al_2018_a",
            "entry": "Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence modeling by iterative refinement. CoRR, abs/1802.06901, 2018. URL http://arxiv.org/abs/1802.06901.",
            "url": "http://arxiv.org/abs/1802.06901",
            "arxiv_url": "https://arxiv.org/pdf/1802.06901"
        },
        {
            "id": "Liu_et+al_2015_a",
            "entry": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Liu%2C%20Ziwei%20Luo%2C%20Ping%20Wang%2C%20Xiaogang%20Tang%2C%20Xiaoou%20Deep%20learning%20face%20attributes%20in%20the%20wild%202015-12"
        },
        {
            "id": "Van_et+al_2017_a",
            "entry": "Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg, et al. Parallel wavenet: Fast high-fidelity speech synthesis. arXiv preprint arXiv:1711.10433, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1711.10433"
        },
        {
            "id": "Parmar_et+al_2018_a",
            "entry": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and Alexander Ku. Image transformer. CoRR, abs/1802.05751, 2018. URL http://arxiv.org/abs/1802.05751.",
            "url": "http://arxiv.org/abs/1802.05751",
            "arxiv_url": "https://arxiv.org/pdf/1802.05751"
        },
        {
            "id": "Rush_et+al_2015_a",
            "entry": "Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1509.00685"
        },
        {
            "id": "Van_et+al_2016_a",
            "entry": "A\u00e4ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alexander Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. CoRR, abs/1609.03499, 2016. URL http://arxiv.org/abs/1609.03499.",
            "url": "http://arxiv.org/abs/1609.03499",
            "arxiv_url": "https://arxiv.org/pdf/1609.03499"
        },
        {
            "id": "Vaswani_et+al_2017_a",
            "entry": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, 2017. URL http://arxiv.org/abs/1706.03762.",
            "url": "http://arxiv.org/abs/1706.03762",
            "arxiv_url": "https://arxiv.org/pdf/1706.03762"
        },
        {
            "id": "Vaswani_et+al_2018_a",
            "entry": "Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.",
            "url": "http://arxiv.org/abs/1803.07416",
            "arxiv_url": "https://arxiv.org/pdf/1803.07416"
        }
    ]
}
