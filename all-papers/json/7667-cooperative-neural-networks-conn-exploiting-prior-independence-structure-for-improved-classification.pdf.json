{
    "filename": "7667-cooperative-neural-networks-conn-exploiting-prior-independence-structure-for-improved-classification.pdf",
    "metadata": {
        "title": "Cooperative neural networks (CoNN): Exploiting prior independence structure for improved classification",
        "author": "Harsh Shrivastava, Eugene Bart, Bob Price, Hanjun Dai, Bo Dai, Srinivas Aluru",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7667-cooperative-neural-networks-conn-exploiting-prior-independence-structure-for-improved-classification.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We propose a new approach, called cooperative neural networks (CoNN), which uses a set of cooperatively trained neural networks to capture latent representations that exploit prior given independence structure. The model is more flexible than traditional graphical models based on exponential family distributions, but incorporates more domain specific prior structure than traditional deep networks or variational autoencoders. The framework is very general and can be used to exploit the independence structure of any graphical model. We illustrate the technique by showing that we can transfer the independence structure of the popular Latent Dirichlet Allocation (LDA) model to a cooperative neural network, CoNNsLDA. Empirical evaluation of CoNN-sLDA on supervised text classification tasks demonstrates that the theoretical advantages of prior independence structure can be realized in practice - we demonstrate a 23% reduction in error on the challenging MultiSent data set compared to state-of-the-art."
    },
    "keywords": [
        {
            "term": "exponential family",
            "url": "https://en.wikipedia.org/wiki/exponential_family"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "problem domain",
            "url": "https://en.wikipedia.org/wiki/problem_domain"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "graphical model",
            "url": "https://en.wikipedia.org/wiki/graphical_model"
        },
        {
            "term": "text classification",
            "url": "https://en.wikipedia.org/wiki/text_classification"
        },
        {
            "term": "latent dirichlet allocation",
            "url": "https://en.wikipedia.org/wiki/latent_dirichlet_allocation"
        }
    ],
    "highlights": [
        "Neural networks offer a low-bias solution for learning complex concepts such as the linguistic knowledge required to separate documents into thematically related classes",
        "We demonstrate empirically that the theoretical advantages of cooperative neural networks are realized in practice by showing that our Cooperative neural networks-sLDA model beats both probabilistic and neural networkbased state-of-the-art alternatives",
        "An advantage of CoNNsLDA over traditional probabilistic graphical models is that we can use low-bias, highly expressive distributions implied by the neural network implementations of update operators.\n4.1",
        "Cooperative neural networks (CoNN) are a new theoretical approach for implementing learning systems which can exploit both prior insights about the independence structure of the problem domain and the universal approximation capability of deep networks",
        "We make the theory concrete with an example, Cooperative neural networks-sLDA, which has superior performance to both prior work based on the probabilistic graphical model latent dirichlet allocation and generic deep networks",
        "While we demonstrated the method on text classification using the structure of latent dirichlet allocation, the approach provides a fully general methodology for computing factored embeddings using a set of highly expressive networks"
    ],
    "key_statements": [
        "Neural networks offer a low-bias solution for learning complex concepts such as the linguistic knowledge required to separate documents into thematically related classes",
        "Cooperative neural networks-sLDA is better than a generic neural network classifier as the factored representation forces a consistent latent feature representation that has a natural relationship between topics, words and documents",
        "We demonstrate empirically that the theoretical advantages of cooperative neural networks are realized in practice by showing that our Cooperative neural networks-sLDA model beats both probabilistic and neural networkbased state-of-the-art alternatives",
        "An advantage of CoNNsLDA over traditional probabilistic graphical models is that we can use low-bias, highly expressive distributions implied by the neural network implementations of update operators.\n4.1",
        "A dropout of \u223c 0.8 was applied to word2vec embeddings",
        "The performance of Cooperative neural networks-sLDA is better than BP-sLDA and at par with 5 layer DUI-sLDA model",
        "Cooperative neural networks (CoNN) are a new theoretical approach for implementing learning systems which can exploit both prior insights about the independence structure of the problem domain and the universal approximation capability of deep networks",
        "We make the theory concrete with an example, Cooperative neural networks-sLDA, which has superior performance to both prior work based on the probabilistic graphical model latent dirichlet allocation and generic deep networks",
        "While we demonstrated the method on text classification using the structure of latent dirichlet allocation, the approach provides a fully general methodology for computing factored embeddings using a set of highly expressive networks"
    ],
    "summary": [
        "Neural networks offer a low-bias solution for learning complex concepts such as the linguistic knowledge required to separate documents into thematically related classes.",
        "This approach works by constructing a set of neural networks, each trained to output an embedding of a probability distribution.",
        "CoNN-sLDA improves over LDA as it admits more complex distributions for document topics and better generalization over word distributions.",
        "CoNN-sLDA is better than a generic neural network classifier as the factored representation forces a consistent latent feature representation that has a natural relationship between topics, words and documents.",
        "We emphasize that our example is based on LDA, the CoNN approach is general and can be used with other graphical models, as well as other sources of independence structure.",
        "The probability distributions involved in the variational approximation, as well as the inference equations, are mapped into a Hilbert space to reduce limitations on their functional form.",
        "The step in the proposed method is to map the probability distributions and the corresponding fixed-point equations into a Hilbert space, where some of these limitations can be relaxed.",
        "Iterating through all values of \u03b8, zi and using the operator view given in equation (9) as reference, we get the following equivalent fixed-point equations in the Hilbert Space: \u03bc\u03b8 = T1 \u25e6 {\u03bczi }",
        "An advantage of CoNNsLDA over traditional probabilistic graphical models is that we can use low-bias, highly expressive distributions implied by the neural network implementations of update operators.",
        "We recommend starting with a small Hilbert space dimension and batch size, try increasing the number of fully connected layers and choose to unroll the model further.",
        "In Figure 3 we see that CoNN-sLDA clearly maps different newsgroups to homogeneous regions of space that help classification accuracy and provide",
        "An interesting extension for the CoNN-sLDA model will be to map the Hilbert space topic embedding \u03bc\u03b8 back to the original topic space distribution.",
        "Cooperative neural networks (CoNN) are a new theoretical approach for implementing learning systems which can exploit both prior insights about the independence structure of the problem domain and the universal approximation capability of deep networks.",
        "We make the theory concrete with an example, CoNN-sLDA, which has superior performance to both prior work based on the probabilistic graphical model LDA and generic deep networks.",
        "While we demonstrated the method on text classification using the structure of LDA, the approach provides a fully general methodology for computing factored embeddings using a set of highly expressive networks.",
        "Cooperative neural networks expand the design space of deep learning machines in new and promising ways"
    ],
    "headline": "We propose a new approach, called cooperative neural networks , which uses a set of cooperatively trained neural networks to capture latent representations that exploit prior given independence structure",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Yoshua Bengio, Eric Laufer, Guillaume Alain, and Jason Yosinski. Deep generative stochastic networks trainable by backprop. In International Conference on Machine Learning, pages 226\u2013234, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bengio%2C%20Yoshua%20Laufer%2C%20Eric%20Alain%2C%20Guillaume%20Yosinski%2C%20Jason%20Deep%20generative%20stochastic%20networks%20trainable%20by%20backprop%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bengio%2C%20Yoshua%20Laufer%2C%20Eric%20Alain%2C%20Guillaume%20Yosinski%2C%20Jason%20Deep%20generative%20stochastic%20networks%20trainable%20by%20backprop%202014"
        },
        {
            "id": "2",
            "entry": "[2] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993\u20131022, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Andrew%20Y%20Ng%2C%20and%20Michael%20I%20Jordan.%20Latent%20dirichlet%20allocation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Andrew%20Y%20Ng%2C%20and%20Michael%20I%20Jordan.%20Latent%20dirichlet%20allocation%202003"
        },
        {
            "id": "3",
            "entry": "[3] John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th annual meeting of the association of computational linguistics, pages 440\u2013447, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blitzer%2C%20John%20Dredze%2C%20Mark%20Biographies%2C%20Fernando%20Pereira%20bollywood%20boom-boxes%20and%20blenders%3A%20Domain%20adaptation%20for%20sentiment%20classification%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blitzer%2C%20John%20Dredze%2C%20Mark%20Biographies%2C%20Fernando%20Pereira%20bollywood%20boom-boxes%20and%20blenders%3A%20Domain%20adaptation%20for%20sentiment%20classification%202007"
        },
        {
            "id": "4",
            "entry": "[4] Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao, Xinying Song, and Li Deng. End-to-end learning of LDA by mirror-descent back propagation over a deep architecture. In Advances in Neural Information Processing Systems, pages 1765\u20131773, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chen%2C%20Jianshu%20He%2C%20Ji%20Shen%2C%20Yelong%20Xiao%2C%20Lin%20End-to-end%20learning%20of%20LDA%20by%20mirror-descent%20back%20propagation%20over%20a%20deep%20architecture%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chen%2C%20Jianshu%20He%2C%20Ji%20Shen%2C%20Yelong%20Xiao%2C%20Lin%20End-to-end%20learning%20of%20LDA%20by%20mirror-descent%20back%20propagation%20over%20a%20deep%20architecture%202015"
        },
        {
            "id": "5",
            "entry": "[5] Jen-Tzung Chien and Chao-Hsi Lee. Deep unfolding for topic models. IEEE transactions on pattern analysis and machine intelligence, 40(2):318\u2013331, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chien%2C%20Jen-Tzung%20Lee%2C%20Chao-Hsi%20Deep%20unfolding%20for%20topic%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chien%2C%20Jen-Tzung%20Lee%2C%20Chao-Hsi%20Deep%20unfolding%20for%20topic%20models%202018"
        },
        {
            "id": "6",
            "entry": "[6] Wang Chong, David Blei, and Fei-Fei Li. Simultaneous image classification and annotation. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1903\u20131910. IEEE, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chong%2C%20Wang%20Blei%2C%20David%20Li%2C%20Fei-Fei%20Simultaneous%20image%20classification%20and%20annotation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chong%2C%20Wang%20Blei%2C%20David%20Li%2C%20Fei-Fei%20Simultaneous%20image%20classification%20and%20annotation%202009"
        },
        {
            "id": "7",
            "entry": "[7] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In International Conference on Machine Learning, pages 2702\u20132711, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Dai%2C%20Hanjun%20Dai%2C%20Bo%20Song%2C%20Le%20Discriminative%20embeddings%20of%20latent%20variable%20models%20for%20structured%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Dai%2C%20Hanjun%20Dai%2C%20Bo%20Song%2C%20Le%20Discriminative%20embeddings%20of%20latent%20variable%20models%20for%20structured%20data%202016"
        },
        {
            "id": "8",
            "entry": "[8] Adji Dieng. TopicRNN: A recurrent neural network with long-range semantic dependency. In arXiv preprint arXiv:1611.01702, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01702"
        },
        {
            "id": "9",
            "entry": "[9] Zhe Gan, Changyou Chen, Ricardo Henao, David Carlson, and Lawrence Carin. Scalable deep poisson factor analysis for topic modeling. In International Conference on Machine Learning, pages 1823\u20131832, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gan%2C%20Zhe%20Chen%2C%20Changyou%20Henao%2C%20Ricardo%20Carlson%2C%20David%20Scalable%20deep%20poisson%20factor%20analysis%20for%20topic%20modeling%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gan%2C%20Zhe%20Chen%2C%20Changyou%20Henao%2C%20Ricardo%20Carlson%2C%20David%20Scalable%20deep%20poisson%20factor%20analysis%20for%20topic%20modeling%202015"
        },
        {
            "id": "10",
            "entry": "[10] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Kavukcuoglu%2C%20Koray%20Spatial%20transformer%20networks%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Jaderberg%2C%20Max%20Simonyan%2C%20Karen%20Zisserman%2C%20Andrew%20Kavukcuoglu%2C%20Koray%20Spatial%20transformer%20networks%202015"
        },
        {
            "id": "11",
            "entry": "[11] Thorsten Joachims. Text categorization with support vector machines: Learning with many relevant features. In ECML, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Thorsten%20Joachims.%20Text%20categorization%20with%20support%20vector%20machines%3A%20Learning%20with%20many%20relevant%20features%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Thorsten%20Joachims.%20Text%20categorization%20with%20support%20vector%20machines%3A%20Learning%20with%20many%20relevant%20features%201998"
        },
        {
            "id": "12",
            "entry": "[12] Yoon Kim. Convolutional neural networks for sentence classification. In arXiv, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kim%2C%20Yoon%20Convolutional%20neural%20networks%20for%20sentence%20classification%202014"
        },
        {
            "id": "13",
            "entry": "[13] Simon Lacoste-Julien, Fei Sha, and Michael I Jordan. DiscLDA: Discriminative learning for dimensionality reduction and classification. In Advances in neural information processing systems, pages 897\u2013904, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lacoste-Julien%2C%20Simon%20Sha%2C%20Fei%20Jordan%2C%20Michael%20I.%20DiscLDA%3A%20Discriminative%20learning%20for%20dimensionality%20reduction%20and%20classification%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lacoste-Julien%2C%20Simon%20Sha%2C%20Fei%20Jordan%2C%20Michael%20I.%20DiscLDA%3A%20Discriminative%20learning%20for%20dimensionality%20reduction%20and%20classification%202009"
        },
        {
            "id": "14",
            "entry": "[14] Hugo Larochelle and Stanislas Lauly. A neural autoregressive topic model. In Advances in Neural Information Processing Systems, pages 2708\u20132716, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Larochelle%2C%20Hugo%20Lauly%2C%20Stanislas%20A%20neural%20autoregressive%20topic%20model%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Larochelle%2C%20Hugo%20Lauly%2C%20Stanislas%20A%20neural%20autoregressive%20topic%20model%202012"
        },
        {
            "id": "15",
            "entry": "[15] Wei Li and Andrew McCallum. Pachinko allocation:dag-structured mixture models of topic correlations. 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Wei%20McCallum%2C%20Andrew%20Pachinko%20allocation%3Adag-structured%20mixture%20models%20of%20topic%20correlations%202006"
        },
        {
            "id": "16",
            "entry": "[16] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=van%20der%20Maaten%2C%20Laurens%20Hinton%2C%20Geoffrey%20Visualizing%20data%20using%20t-sne%202008"
        },
        {
            "id": "17",
            "entry": "[17] Jon D Mcauliffe and David M Blei. Supervised topic models. In Advances in neural information processing systems, pages 121\u2013128, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mcauliffe%2C%20Jon%20D.%20Blei%2C%20David%20M.%20Supervised%20topic%20models%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mcauliffe%2C%20Jon%20D.%20Blei%2C%20David%20M.%20Supervised%20topic%20models%202008"
        },
        {
            "id": "18",
            "entry": "[18] Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In International Conference on Machine Learning, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miao%2C%20Yishu%20Yu%2C%20Lei%20Blunsom%2C%20Phil%20Neural%20variational%20inference%20for%20text%20processing%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miao%2C%20Yishu%20Yu%2C%20Lei%20Blunsom%2C%20Phil%20Neural%20variational%20inference%20for%20text%20processing%202016"
        },
        {
            "id": "19",
            "entry": "[19] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mikolov%2C%20Tomas%20Sutskever%2C%20Ilya%20Chen%2C%20Kai%20Corrado%2C%20Greg%20S.%20Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality%202013"
        },
        {
            "id": "20",
            "entry": "[20] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1402.0030"
        },
        {
            "id": "21",
            "entry": "[21] Hojjat S. Mousavi, Tiantong Guo, and Vishal Monga. Deep image super resolution via natural image priors. In arxiv, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mousavi%2C%20Hojjat%20S.%20Guo%2C%20Tiantong%20Monga%2C%20Vishal%20Deep%20image%20super%20resolution%20via%20natural%20image%20priors%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mousavi%2C%20Hojjat%20S.%20Guo%2C%20Tiantong%20Monga%2C%20Vishal%20Deep%20image%20super%20resolution%20via%20natural%20image%20priors%202018"
        },
        {
            "id": "22",
            "entry": "[22] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.",
            "arxiv_url": "https://arxiv.org/pdf/1401.4082"
        },
        {
            "id": "23",
            "entry": "[23] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in Neural Information Processing Systems, pages 3856\u20133866, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sabour%2C%20Sara%20Frosst%2C%20Nicholas%20Hinton%2C%20Geoffrey%20E.%20Dynamic%20routing%20between%20capsules%202017"
        },
        {
            "id": "24",
            "entry": "[24] Alex Smola, Arthur Gretton, Le Song, and Bernhard Sch\u00f6lkopf. A hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory, pages 13\u201331.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Smola%2C%20Alex%20Gretton%2C%20Arthur%20Song%2C%20Le%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20hilbert%20space%20embedding%20for%20distributions",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Smola%2C%20Alex%20Gretton%2C%20Arthur%20Song%2C%20Le%20Sch%C3%B6lkopf%2C%20Bernhard%20A%20hilbert%20space%20embedding%20for%20distributions"
        },
        {
            "id": "25",
            "entry": "[25] Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Gert Lanckriet, and Bernhard Sch\u00f6lkopf. Injective hilbert space embeddings of probability measures. 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sriperumbudur%2C%20Bharath%20K.%20Gretton%2C%20Arthur%20Fukumizu%2C%20Kenji%20Lanckriet%2C%20Gert%20Injective%20hilbert%20space%20embeddings%20of%20probability%20measures%202008"
        },
        {
            "id": "26",
            "entry": "[26] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Srivastava%2C%20Nitish%20Hinton%2C%20Geoffrey%20Krizhevsky%2C%20Alex%20Sutskever%2C%20Ilya%20Dropout%3A%20A%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting%202014"
        },
        {
            "id": "27",
            "entry": "[27] Nitish Srivastava, Ruslan R Salakhutdinov, and Geoffrey E Hinton. Modeling documents with deep boltzmann machines. arXiv preprint arXiv:1309.6865, 2013.",
            "arxiv_url": "https://arxiv.org/pdf/1309.6865"
        },
        {
            "id": "28",
            "entry": "[28] Yichuan Tang and Ruslan R Salakhutdinov. Learning stochastic feedforward neural networks. In Advances in Neural Information Processing Systems, pages 530\u2013538, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tang%2C%20Yichuan%20Salakhutdinov%2C%20Ruslan%20R.%20Learning%20stochastic%20feedforward%20neural%20networks%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tang%2C%20Yichuan%20Salakhutdinov%2C%20Ruslan%20R.%20Learning%20stochastic%20feedforward%20neural%20networks%202013"
        },
        {
            "id": "29",
            "entry": "[29] Martin J Wainwright, Tommi S Jaakkola, and Alan S Willsky. Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching. In AISTATS, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wainwright%2C%20Martin%20J.%20Jaakkola%2C%20Tommi%20S.%20Willsky%2C%20Alan%20S.%20Tree-reweighted%20belief%20propagation%20algorithms%20and%20approximate%20ML%20estimation%20by%20pseudo-moment%20matching%202003"
        },
        {
            "id": "30",
            "entry": "[30] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1\u20132):1\u2013305, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=-%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=-%20Graphical%20models%2C%20exponential%20families%2C%20and%20variational%20inference%202008"
        },
        {
            "id": "31",
            "entry": "[31] Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational autoencoders for text modeling using dilated convolutions. arXiv preprint arXiv:1702.08139, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.08139"
        },
        {
            "id": "32",
            "entry": "[32] Yin Zheng, Yu-Jin Zhang, and Hugo Larochelle. A deep and autoregressive approach for topic modeling of multimodal data. IEEE transactions on pattern analysis and machine intelligence, 38(6):1056\u20131069, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20Yin%20Zhang%2C%20Yu-Jin%20Larochelle%2C%20Hugo%20A%20deep%20and%20autoregressive%20approach%20for%20topic%20modeling%20of%20multimodal%20data%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20Yin%20Zhang%2C%20Yu-Jin%20Larochelle%2C%20Hugo%20A%20deep%20and%20autoregressive%20approach%20for%20topic%20modeling%20of%20multimodal%20data%202016"
        },
        {
            "id": "33",
            "entry": "[33] Jun Zhu, Amr Ahmed, and Eric P Xing. MedLDA: maximum margin supervised topic models for regression and classification. In Proceedings of the 26th annual international conference on machine learning, pages 1257\u20131264. ACM, 2009. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20Jun%20Ahmed%2C%20Amr%20and%20Eric%20P%20Xing.%20MedLDA%3A%20maximum%20margin%20supervised%20topic%20models%20for%20regression%20and%20classification%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20Jun%20Ahmed%2C%20Amr%20and%20Eric%20P%20Xing.%20MedLDA%3A%20maximum%20margin%20supervised%20topic%20models%20for%20regression%20and%20classification%202009"
        }
    ]
}
