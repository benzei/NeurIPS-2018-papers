{
    "filename": "7679-inexact-trust-region-algorithms-on-riemannian-manifolds.pdf",
    "metadata": {
        "title": "Inexact trust-region algorithms on Riemannian manifolds",
        "author": "Hiroyuki Kasai, Bamdev Mishra",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7679-inexact-trust-region-algorithms-on-riemannian-manifolds.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We consider an inexact variant of the popular Riemannian trust-region algorithm for structured big-data minimization problems. The proposed algorithm approximates the gradient and the Hessian in addition to the solution of a trust-region sub-problem. Addressing large-scale finite-sum problems, we specifically propose sub-sampled algorithms with a fixed bound on sub-sampled Hessian and gradient sizes, where the gradient and Hessian are computed by a random sampling technique. Numerical evaluations demonstrate that the proposed algorithms outperform state-of-the-art Riemannian deterministic and stochastic gradient algorithms across different applications."
    },
    "keywords": [
        {
            "term": "principal component analysis",
            "url": "https://en.wikipedia.org/wiki/principal_component_analysis"
        },
        {
            "term": "machine learning",
            "url": "https://en.wikipedia.org/wiki/machine_learning"
        },
        {
            "term": "component analysis",
            "url": "https://en.wikipedia.org/wiki/component_analysis"
        },
        {
            "term": "optimization problem",
            "url": "https://en.wikipedia.org/wiki/optimization_problem"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "condition number",
            "url": "https://en.wikipedia.org/wiki/condition_number"
        },
        {
            "term": "riemannian manifold",
            "url": "https://en.wikipedia.org/wiki/riemannian_manifold"
        },
        {
            "term": "variance reduction",
            "url": "https://en.wikipedia.org/wiki/variance_reduction"
        },
        {
            "term": "matrix completion",
            "url": "https://en.wikipedia.org/wiki/matrix_completion"
        },
        {
            "term": "independent component analysis",
            "url": "https://en.wikipedia.org/wiki/independent_component_analysis"
        }
    ],
    "highlights": [
        "We consider the optimization problem min f (x), (1)<br/><br/>x\u2208M where f : M \u2192 R is a smooth real-valued function on a Riemannian manifold M [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "The principal component analysis (PCA) and subspace tracking problems are defined on the Grassmann manifold [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "The independent component analysis (ICA) problem requires a whitening step that is posed as a joint diagonalization problem on the Stiefel manifold [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "We address the independent component analysis (ICA) problem on the Stiefel manifold and two problems on the Grassmann manifold, namely the principal component analysis (PCA) and the lowrank matrix completion (MC) problems",
        "We show the mean squares error (MSE) on a test set, which is different from the training set",
        "We have proposed an inexact trust-region algorithm in the Riemannian setting with a worst case total complexity bound"
    ],
    "key_statements": [
        "We consider the optimization problem min f (x), (1)<br/><br/>x\u2208M where f : M \u2192 R is a smooth real-valued function on a Riemannian manifold M [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>]",
        "The focus on the paper is when f has a finite-sum structure, which frequently arises as big-data problems in machine learning applications",
        "The principal component analysis (PCA) and subspace tracking problems are defined on the Grassmann manifold [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]",
        "The independent component analysis (ICA) problem requires a whitening step that is posed as a joint diagonalization problem on the Stiefel manifold [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>, <a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>]",
        "We propose an inexact Riemannian trust-region algorithm, inexact Riemannian trust-region algorithm, for (1)",
        "In comparison with the Euclidean case, in Riemannian trust-region algorithm, the approximation model mx of fx around x is obtained from the Taylor expansion of the pullback of the function fx fx \u25e6 Rx defined on the tangent space, where Rx is the retraction operator that maps a tangent vector onto the manifold with a local rigidity condition that preserves the gradients at x [1, Chap. 4]",
        "We address the independent component analysis (ICA) problem on the Stiefel manifold and two problems on the Grassmann manifold, namely the principal component analysis (PCA) and the lowrank matrix completion (MC) problems",
        "We show the mean squares error (MSE) on a test set, which is different from the training set",
        "We have proposed an inexact trust-region algorithm in the Riemannian setting with a worst case total complexity bound"
    ],
    "summary": [
        "We consider the optimization problem min f (x), (1)<br/><br/>x\u2208M where f : M \u2192 R is a smooth real-valued function on a Riemannian manifold M [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>].",
        "We propose a sub-sampled trust-region algorithm, Sub-RTR, as a practical but efficient variant of inexact RTR for finite-sum problems.",
        "In Section 4, we propose sub-sampled trust-region algorithms as its practical variants.",
        "Building upon the results in the Euclidean space [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>, <a class=\"ref-link\" id=\"c36\" href=\"#r36\">36</a>] and that of the RTR algorithm [<a class=\"ref-link\" id=\"c30\" href=\"#r30\">30</a>], we derive the bounds of the sample size of sub-sampled gradients and Hessians in Theorem 4.1, which only requires a fixed sample size [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>].",
        "In comparison with the Euclidean case, in RTR, the approximation model mx of fx around x is obtained from the Taylor expansion of the pullback of the function fx fx \u25e6 Rx defined on the tangent space, where Rx is the retraction operator that maps a tangent vector onto the manifold with a local rigidity condition that preserves the gradients at x [1, Chap.",
        "We provide essential assumptions on the bounds for approximation error of the inexact Riemannian gradient Gk and the inexact Riemannian Hessian Hk at iteration k.",
        "It should be noted that the condition (5) requires that the sizes of the sub-sampled Hessian and gradient need to be increased towards the convergence, whereas our new condition (4) allows the size to be fixed, as seen later in Section 4 [<a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>, <a class=\"ref-link\" id=\"c35\" href=\"#r35\">35</a>].",
        "This section proposes an inexact variant of the Riemannian trust-region algorithm, i.e., inexact RTR, which approximates gradient and Hessian as well as the solution of a sub-problem.",
        "We assume an additional approximation condition on the inexact gradient and Hessian for the constants in Assumption 4 [35, Cond.",
        "Addressing large-scale finite-sum minimization problems, we propose an inexact gradient and Hessian trust-region algorithm, Sub-RTR, by exploiting a sub-sampling technique to generate inexact gradient and Hessian.",
        "This section evaluates the performance of our two proposed inexact RTR algorithms: the subsampled Hessian RTR (Sub-H-RTR) and the sub-sampled Hessian and gradient RTR (Sub-HGRTR).",
        "Rs is the number of iterations required for solving the trust-region sub-problem approximately.",
        "The results on the synthetic dataset same as Case P2 show that all the proposed algorithms except Sub-HG-RTR with fixed sample size outperform the original RTR.",
        "Figure 3(c) shows the comparable or superior performance of the sub-sampled RTR on the test sets against state-of-the-art algorithms.",
        "We have proposed an inexact trust-region algorithm in the Riemannian setting with a worst case total complexity bound.",
        "We have proposed sub-sampled trust-region algorithms for finite-sum problems, which need only fixed sample bounds of sub-sampled gradient and Hessian.",
        "The numerical comparisons show the benefits of our proposed inexact RTR algorithms on a number of applications"
    ],
    "headline": "Addressing large-scale finite-sum problems, we specifically propose sub-sampled algorithms with a fixed bound on sub-sampled Hessian and gradient sizes, where the gradient and Hessian are computed by a random sampling technique",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Absil%2C%20P.-A.%20Mahony%2C%20R.%20Sepulchre%2C%20R.%20Optimization%20Algorithms%20on%20Matrix%20Manifolds%202008"
        },
        {
            "id": "2",
            "entry": "[2] L. Balzano, R. Nowak, and B. Recht. Online identification and tracking of subspaces from highly incomplete information. In Allerton, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Balzano%2C%20L.%20Nowak%2C%20R.%20Recht%2C%20B.%20Online%20identification%20and%20tracking%20of%20subspaces%20from%20highly%20incomplete%20information%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Balzano%2C%20L.%20Nowak%2C%20R.%20Recht%2C%20B.%20Online%20identification%20and%20tracking%20of%20subspaces%20from%20highly%20incomplete%20information%202010"
        },
        {
            "id": "3",
            "entry": "[3] B. Mishra and R. Sepulchre. R3MC: A Riemannian three-factor algorithm for low-rank matrix completion. In IEEE CDC, pages 1137\u20131142, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Mishra%2C%20B.%20R.%20Sepulchre.%20R3MC%3A%20A%20Riemannian%20three-factor%20algorithm%20for%20low-rank%20matrix%20completion%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Mishra%2C%20B.%20R.%20Sepulchre.%20R3MC%3A%20A%20Riemannian%20three-factor%20algorithm%20for%20low-rank%20matrix%20completion%202014"
        },
        {
            "id": "4",
            "entry": "[4] H. Kasai and B. Mishra. Low-rank tensor completion: a Riemannian manifold preconditioning approach. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kasai%2C%20H.%20Mishra%2C%20B.%20Low-rank%20tensor%20completion%3A%20a%20Riemannian%20manifold%20preconditioning%20approach%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kasai%2C%20H.%20Mishra%2C%20B.%20Low-rank%20tensor%20completion%3A%20a%20Riemannian%20manifold%20preconditioning%20approach%202016"
        },
        {
            "id": "5",
            "entry": "[5] D. Kressner, M. Steinlechner, and B. Vandereycken. Low-rank tensor completion by Riemannian optimization. BIT Numer. Math., 54(2):447\u2013468, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kressner%2C%20D.%20Steinlechner%2C%20M.%20Vandereycken%2C%20B.%20Low-rank%20tensor%20completion%20by%20Riemannian%20optimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kressner%2C%20D.%20Steinlechner%2C%20M.%20Vandereycken%2C%20B.%20Low-rank%20tensor%20completion%20by%20Riemannian%20optimization%202014"
        },
        {
            "id": "6",
            "entry": "[6] B. Vandereycken. Low-rank matrix completion by Riemannian optimization. SIAM J. Optim., 23(2):1214\u20131236, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Vandereycken%2C%20B.%20Low-rank%20matrix%20completion%20by%20Riemannian%20optimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Vandereycken%2C%20B.%20Low-rank%20matrix%20completion%20by%20Riemannian%20optimization%202013"
        },
        {
            "id": "7",
            "entry": "[7] C. Da Silva and F. J. Herrmann. Optimization on the hierarchical tucker manifold\u2013applications to tensor completion. Linear Algebra Its Appl., 481:131\u2013173, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Silva%2C%20C.Da%20Herrmann%2C%20F.J.%20Optimization%20on%20the%20hierarchical%20tucker%20manifold%E2%80%93applications%20to%20tensor%20completion%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Silva%2C%20C.Da%20Herrmann%2C%20F.J.%20Optimization%20on%20the%20hierarchical%20tucker%20manifold%E2%80%93applications%20to%20tensor%20completion%202015"
        },
        {
            "id": "8",
            "entry": "[8] N. Boumal and P.-A Absil. Low-rank matrix completion via preconditioned optimization on the Grassmann manifold. Linear Algebra Its Appl., 475(15):200\u2013239, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boumal%2C%20N.%20Absil%2C%20P.-A.%20Low-rank%20matrix%20completion%20via%20preconditioned%20optimization%20on%20the%20Grassmann%20manifold%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boumal%2C%20N.%20Absil%2C%20P.-A.%20Low-rank%20matrix%20completion%20via%20preconditioned%20optimization%20on%20the%20Grassmann%20manifold%202015"
        },
        {
            "id": "9",
            "entry": "[9] G. Meyer, S. Bonnabel, and R. Sepulchre. Linear regression under fixed-rank constraints: a Riemannian approach. In ICML, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Meyer%2C%20G.%20Bonnabel%2C%20S.%20Sepulchre%2C%20R.%20Linear%20regression%20under%20fixed-rank%20constraints%3A%20a%20Riemannian%20approach%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Meyer%2C%20G.%20Bonnabel%2C%20S.%20Sepulchre%2C%20R.%20Linear%20regression%20under%20fixed-rank%20constraints%3A%20a%20Riemannian%20approach%202011"
        },
        {
            "id": "10",
            "entry": "[10] U. Shalit, D. Weinshall, and G. Chechik. Online learning in the embedded manifold of lowrank matrices. J. Mach. Learn. Res., 13(Feb):429\u2013458, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalit%2C%20U.%20Weinshall%2C%20D.%20Chechik%2C%20G.%20Online%20learning%20in%20the%20embedded%20manifold%20of%20lowrank%20matrices%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalit%2C%20U.%20Weinshall%2C%20D.%20Chechik%2C%20G.%20Online%20learning%20in%20the%20embedded%20manifold%20of%20lowrank%20matrices%202012"
        },
        {
            "id": "11",
            "entry": "[11] F. J. Theis, T. P. Cason, and P.-A. Absil. Soft dimension reduction for ICA by joint diagonalization on the Stiefel manifold. In ICA, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Theis%2C%20F.J.%20Cason%2C%20T.P.%20Absil%2C%20P.-A.%20Soft%20dimension%20reduction%20for%20ICA%20by%20joint%20diagonalization%20on%20the%20Stiefel%20manifold%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Theis%2C%20F.J.%20Cason%2C%20T.P.%20Absil%2C%20P.-A.%20Soft%20dimension%20reduction%20for%20ICA%20by%20joint%20diagonalization%20on%20the%20Stiefel%20manifold%202009"
        },
        {
            "id": "12",
            "entry": "[12] W. Huang, P.-A. Absil, and K. A. Gallivan. A Riemannian BFGS method for nonconvex optimization problems. In ENUMATH 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20W.%20Absil%2C%20P.-A.%20Gallivan%2C%20K.A.%20A%20Riemannian%20BFGS%20method%20for%20nonconvex%20optimization%20problems%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20W.%20Absil%2C%20P.-A.%20Gallivan%2C%20K.A.%20A%20Riemannian%20BFGS%20method%20for%20nonconvex%20optimization%20problems%202015"
        },
        {
            "id": "13",
            "entry": "[13] D. G. Luenberger. The gradient projection method along geodesics. Manag. Sci., 18(11):620\u2013 631, 1972.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Luenberger%2C%20D.G.%20The%20gradient%20projection%20method%20along%20geodesics%201972",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Luenberger%2C%20D.G.%20The%20gradient%20projection%20method%20along%20geodesics%201972"
        },
        {
            "id": "14",
            "entry": "[14] S. Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Trans. on Automatic Control, 58(9):2217\u20132229, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bonnabel%2C%20S.%20Stochastic%20gradient%20descent%20on%20Riemannian%20manifolds%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bonnabel%2C%20S.%20Stochastic%20gradient%20descent%20on%20Riemannian%20manifolds%202013"
        },
        {
            "id": "15",
            "entry": "[15] H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statistics, pages 400\u2013407, 1951.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Robbins%2C%20H.%20Monro%2C%20S.%20A%20stochastic%20approximation%20method%201951"
        },
        {
            "id": "16",
            "entry": "[16] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization mehtods for large-scale machine learning. SIAM Rev., 60(2):223\u2013311, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20mehtods%20for%20large-scale%20machine%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bottou%2C%20L.%20Curtis%2C%20F.E.%20Nocedal%2C%20J.%20Optimization%20mehtods%20for%20large-scale%20machine%20learning%202018"
        },
        {
            "id": "17",
            "entry": "[17] H. Sato, H. Kasai, and B. Mishra. Riemannian stochastic variance reduced gradient. arXiv preprint: arXiv:1702.05594, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1702.05594"
        },
        {
            "id": "18",
            "entry": "[18] H. Zhang, S. J. Reddi, and S. Sra. Riemannian SVRG: fast stochastic optimization on Riemannian manifolds. In NIPS, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhang%2C%20H.%20Reddi%2C%20S.J.%20Sra%2C%20S.%20Riemannian%20SVRG%3A%20fast%20stochastic%20optimization%20on%20Riemannian%20manifolds%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhang%2C%20H.%20Reddi%2C%20S.J.%20Sra%2C%20S.%20Riemannian%20SVRG%3A%20fast%20stochastic%20optimization%20on%20Riemannian%20manifolds%202016"
        },
        {
            "id": "19",
            "entry": "[19] H. Kasai, H. Sato, and B. Mishra. Riemannian stochastic recursive gradient algorithm. In ICML, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kasai%2C%20H.%20Sato%2C%20H.%20Mishra%2C%20B.%20Riemannian%20stochastic%20recursive%20gradient%20algorithm%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kasai%2C%20H.%20Sato%2C%20H.%20Mishra%2C%20B.%20Riemannian%20stochastic%20recursive%20gradient%20algorithm%202018"
        },
        {
            "id": "20",
            "entry": "[20] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In NIPS, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Johnson%2C%20R.%20Zhang%2C%20T.%20Accelerating%20stochastic%20gradient%20descent%20using%20predictive%20variance%20reduction%202013"
        },
        {
            "id": "21",
            "entry": "[21] N. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In NIPS, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Roux%2C%20N.L.%20Schmidt%2C%20M.%20Bach%2C%20F.R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Roux%2C%20N.L.%20Schmidt%2C%20M.%20Bach%2C%20F.R.%20A%20stochastic%20gradient%20method%20with%20an%20exponential%20convergence%20rate%20for%20finite%20training%20sets%202012"
        },
        {
            "id": "22",
            "entry": "[22] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. JMLR, 14:567\u2013599, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Stochastic%20dual%20coordinate%20ascent%20methods%20for%20regularized%20loss%20minimization%202013"
        },
        {
            "id": "23",
            "entry": "[23] A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: a fast incremental gradient method with support for non-strongly convex composite objectives. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20a%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Defazio%2C%20A.%20Bach%2C%20F.%20Lacoste-Julien%2C%20S.%20SAGA%3A%20a%20fast%20incremental%20gradient%20method%20with%20support%20for%20non-strongly%20convex%20composite%20objectives%202014"
        },
        {
            "id": "24",
            "entry": "[24] S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex optimization. In ICML, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Reddi%2C%20S.J.%20Hefny%2C%20A.%20Sra%2C%20S.%20Poczos%2C%20B.%20Stochastic%20variance%20reduction%20for%20nonconvex%20optimization%202016"
        },
        {
            "id": "25",
            "entry": "[25] L. M. Nguyen, J. Liu, K. Scheinberg, and M. Takac. SARAH: a novel method for machine learning problems using stochastic recursive gradient. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nguyen%2C%20L.M.%20Liu%2C%20J.%20Scheinberg%2C%20K.%20Takac%2C%20M.%20SARAH%3A%20a%20novel%20method%20for%20machine%20learning%20problems%20using%20stochastic%20recursive%20gradient%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nguyen%2C%20L.M.%20Liu%2C%20J.%20Scheinberg%2C%20K.%20Takac%2C%20M.%20SARAH%3A%20a%20novel%20method%20for%20machine%20learning%20problems%20using%20stochastic%20recursive%20gradient%202017"
        },
        {
            "id": "26",
            "entry": "[26] W. H. Yang, L.-H. Zhang, and R. Song. Optimality conditions for the nonlinear programming problems on riemannian manifolds. Pac. J. Optim., 10(2):415\u2013434, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yang%2C%20W.H.%20Zhang%2C%20L.-H.%20Song%2C%20R.%20Optimality%20conditions%20for%20the%20nonlinear%20programming%20problems%20on%20riemannian%20manifolds%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yang%2C%20W.H.%20Zhang%2C%20L.-H.%20Song%2C%20R.%20Optimality%20conditions%20for%20the%20nonlinear%20programming%20problems%20on%20riemannian%20manifolds%202014"
        },
        {
            "id": "27",
            "entry": "[27] W. Huang, K. A. Gallivan, and P.-A. Absil. A Broyden class of quasi-Newton methods for Riemannian optimization. SIAM J. Optim., 25(3):1660\u20131685, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huang%2C%20W.%20Gallivan%2C%20K.A.%20Absil%2C%20P.-A.%20A%20Broyden%20class%20of%20quasi-Newton%20methods%20for%20Riemannian%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huang%2C%20W.%20Gallivan%2C%20K.A.%20Absil%2C%20P.-A.%20A%20Broyden%20class%20of%20quasi-Newton%20methods%20for%20Riemannian%20optimization%202015"
        },
        {
            "id": "28",
            "entry": "[28] D. Gabay. Minimizing a differentiable function over a differential manifold. J. Optim. Theory Appl., 37(2):177\u2013219, 1982.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gabay%2C%20D.%20Minimizing%20a%20differentiable%20function%20over%20a%20differential%20manifold%201982",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gabay%2C%20D.%20Minimizing%20a%20differentiable%20function%20over%20a%20differential%20manifold%201982"
        },
        {
            "id": "29",
            "entry": "[29] W. Ring and B. Wirth. Optimization methods on Riemannian manifolds and their application to shape space. SIAM J. Optim., 22(2):596\u2013627, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ring%2C%20W.%20Wirth%2C%20B.%20Optimization%20methods%20on%20Riemannian%20manifolds%20and%20their%20application%20to%20shape%20space%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ring%2C%20W.%20Wirth%2C%20B.%20Optimization%20methods%20on%20Riemannian%20manifolds%20and%20their%20application%20to%20shape%20space%202012"
        },
        {
            "id": "30",
            "entry": "[30] N. Boumal, P.-A. Absil, and C. Cartis. Global rates of convergence for nonconvex optimization on manifolds. IMA J. Numer. Anal., 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boumal%2C%20N.%20Absil%2C%20P.-A.%20Cartis%2C%20C.%20Global%20rates%20of%20convergence%20for%20nonconvex%20optimization%20on%20manifolds%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boumal%2C%20N.%20Absil%2C%20P.-A.%20Cartis%2C%20C.%20Global%20rates%20of%20convergence%20for%20nonconvex%20optimization%20on%20manifolds%202018"
        },
        {
            "id": "31",
            "entry": "[31] H. Kasai, H. Sato, and B. Mishra. Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis. In AISTATS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kasai%2C%20H.%20Sato%2C%20H.%20Mishra%2C%20B.%20Riemannian%20stochastic%20quasi-Newton%20algorithm%20with%20variance%20reduction%20and%20its%20convergence%20analysis%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kasai%2C%20H.%20Sato%2C%20H.%20Mishra%2C%20B.%20Riemannian%20stochastic%20quasi-Newton%20algorithm%20with%20variance%20reduction%20and%20its%20convergence%20analysis%202018"
        },
        {
            "id": "32",
            "entry": "[32] R. H. Byrd, G. M. Chin, W. Neveitt, and J. Nocedal. On the use of stochastic Hessian information in optimization methods for machine learning. SIAM J. Optim., 21(3):977\u2013995, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Byrd%2C%20R.H.%20Chin%2C%20G.M.%20Neveitt%2C%20W.%20Nocedal%2C%20J.%20On%20the%20use%20of%20stochastic%20Hessian%20information%20in%20optimization%20methods%20for%20machine%20learning%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Byrd%2C%20R.H.%20Chin%2C%20G.M.%20Neveitt%2C%20W.%20Nocedal%2C%20J.%20On%20the%20use%20of%20stochastic%20Hessian%20information%20in%20optimization%20methods%20for%20machine%20learning%202011"
        },
        {
            "id": "33",
            "entry": "[33] M. A. Erdogdu and A. Montanari. Convergence rates of sub-sampled Newton methods. In NIPS, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Erdogdu%2C%20M.A.%20Montanari%2C%20A.%20Convergence%20rates%20of%20sub-sampled%20Newton%20methods%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Erdogdu%2C%20M.A.%20Montanari%2C%20A.%20Convergence%20rates%20of%20sub-sampled%20Newton%20methods%202015"
        },
        {
            "id": "34",
            "entry": "[34] P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Newton-type methods for non-convex optimization under inexact Hessian information. arXiv preprint arXiv:1708.07164, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.07164"
        },
        {
            "id": "35",
            "entry": "[35] Z. Yao, P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Inexact non-convex Newton-type methods. arXiv preprint arXiv:1802.06925, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06925"
        },
        {
            "id": "36",
            "entry": "[36] J. M. Kohler and A. Lucchi. Sub-sampled cubic regularization for non-convex optimization. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kohler%2C%20J.M.%20Lucchi%2C%20A.%20Sub-sampled%20cubic%20regularization%20for%20non-convex%20optimization%202017"
        },
        {
            "id": "37",
            "entry": "[37] N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a Matlab toolbox for optimization on manifolds. J. Mach. Learn. Res., 15(1):1455\u20131459, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Boumal%2C%20N.%20Mishra%2C%20B.%20Absil%2C%20P.-A.%20Sepulchre%2C%20R.%20Manopt%2C%20a%20Matlab%20toolbox%20for%20optimization%20on%20manifolds%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Boumal%2C%20N.%20Mishra%2C%20B.%20Absil%2C%20P.-A.%20Sepulchre%2C%20R.%20Manopt%2C%20a%20Matlab%20toolbox%20for%20optimization%20on%20manifolds%202014"
        },
        {
            "id": "38",
            "entry": "[38] A. R. Conn, N. I. M. Gould, and P. L. Toint. Trust Region Methods. MOS-SIAM Series on Optimization. SIAM, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Conn%2C%20A.R.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Trust%20Region%20Methods%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Conn%2C%20A.R.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Trust%20Region%20Methods%202000"
        },
        {
            "id": "39",
            "entry": "[39] J. Nocedal and Wright S.J. Numerical Optimization. Springer, New York, USA, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nocedal%2C%20J.%20J%2C%20Wright%20S.%20Numerical%20Optimization%202006"
        },
        {
            "id": "40",
            "entry": "[40] C. Cartis, N. I. M. Gould, and P. L. Toint. Adaptive cubic regularisation methods for unconstrained optimization. part I: motivation, convergence and numerical results. Math. Program., 127(2):245\u2013295, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cartis%2C%20C.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20I%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cartis%2C%20C.%20Gould%2C%20N.I.M.%20Toint%2C%20P.L.%20Adaptive%20cubic%20regularisation%20methods%20for%20unconstrained%20optimization.%20part%20I%3A%20motivation%2C%20convergence%20and%20numerical%20results%202011"
        },
        {
            "id": "41",
            "entry": "[41] P. L. Toint. Towards an efficient sparsity exploiting Newton method for minimization. Sparse matrices and their uses, page 1981, 1981.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Toint%2C%20P.L.%20Towards%20an%20efficient%20sparsity%20exploiting%20Newton%20method%20for%20minimization.%20Sparse%20matrices%20and%20their%20uses%201981"
        },
        {
            "id": "42",
            "entry": "[42] A. Hyv\u00e4rinen and E. Oja. Independent component analysis: algorithms and applications. Neural networks, 13(4-5):411\u2013430, 2000.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hyv%C3%A4rinen%2C%20A.%20Oja%2C%20E.%20Independent%20component%20analysis%3A%20algorithms%20and%20applications%202000",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hyv%C3%A4rinen%2C%20A.%20Oja%2C%20E.%20Independent%20component%20analysis%3A%20algorithms%20and%20applications%202000"
        },
        {
            "id": "43",
            "entry": "[43] The extended Yale Face Database b. http://vision.ucsd.edu/leekc/ExtYaleDatabase/ExtYaleB.html.",
            "url": "http://vision.ucsd.edu/leekc/ExtYaleDatabase/ExtYaleB.html"
        },
        {
            "id": "44",
            "entry": "[44] Columbia university image library (COIL-100). http://www1.cs.columbia.edu/CAVE/software/softlib/coil-",
            "url": "http://www1.cs.columbia.edu/CAVE/software/softlib/coil-"
        },
        {
            "id": "45",
            "entry": "[45] The CIFAR-100 dataset. http://www.cs.toronto.edu/kriz/cifar.html.",
            "url": "http://www.cs.toronto.edu/kriz/cifar.html"
        },
        {
            "id": "46",
            "entry": "[46] F. Porikli and O. Tuzel. Fast construction of covariance matrices for arbitrary size image windows. In ICIP, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Porikli%2C%20F.%20Tuzel%2C%20O.%20Fast%20construction%20of%20covariance%20matrices%20for%20arbitrary%20size%20image%20windows%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Porikli%2C%20F.%20Tuzel%2C%20O.%20Fast%20construction%20of%20covariance%20matrices%20for%20arbitrary%20size%20image%20windows%202006"
        },
        {
            "id": "47",
            "entry": "[47] O. Tuzel, F. Porikli, and P. Meer. Region covariance: a fast descriptor for detection and classification. In ECCV, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tuzel%2C%20O.%20Porikli%2C%20F.%20Meer%2C%20P.%20Region%20covariance%3A%20a%20fast%20descriptor%20for%20detection%20and%20classification%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tuzel%2C%20O.%20Porikli%2C%20F.%20Meer%2C%20P.%20Region%20covariance%3A%20a%20fast%20descriptor%20for%20detection%20and%20classification%202006"
        },
        {
            "id": "48",
            "entry": "[48] Y. Pang, Y. Yuan, and X. Li. Gabor-based region covariance matrices for face recognition. IEEE Trans. Circuits Syst. Video Technol., 18(7):989\u2013993, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pang%2C%20Y.%20Yuan%2C%20Y.%20Li%2C%20X.%20Gabor-based%20region%20covariance%20matrices%20for%20face%20recognition%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pang%2C%20Y.%20Yuan%2C%20Y.%20Li%2C%20X.%20Gabor-based%20region%20covariance%20matrices%20for%20face%20recognition%202008"
        },
        {
            "id": "49",
            "entry": "[49] The MNIST database. http://yann.lecun.com/exdb/mnist/.",
            "url": "http://yann.lecun.com/exdb/mnist/"
        },
        {
            "id": "50",
            "entry": "[50] Covertype dataset. https://archive.ics.uci.edu/ml/datasets/covertype.",
            "url": "https://archive.ics.uci.edu/ml/datasets/covertype"
        },
        {
            "id": "51",
            "entry": "[51] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigentaste: a constant time collaborative filtering algorithm. Inform. Retrieval, 4(2):133\u2013151, 2001. Inf. Theory, 57(3):1548\u20131566, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goldberg%2C%20K.%20Roeder%2C%20T.%20Gupta%2C%20D.%20Perkins%2C%20C.%20Eigentaste%3A%20a%20constant%20time%20collaborative%20filtering%20algorithm%202001",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goldberg%2C%20K.%20Roeder%2C%20T.%20Gupta%2C%20D.%20Perkins%2C%20C.%20Eigentaste%3A%20a%20constant%20time%20collaborative%20filtering%20algorithm%202001"
        }
    ]
}
