{
    "filename": "7885-online-adaptive-methods-universality-and-acceleration.pdf",
    "metadata": {
        "title": "Online Adaptive Methods, Universality and Acceleration",
        "author": "Yehuda Kfir Levy, Alp Yurtsever, Volkan Cevher",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7885-online-adaptive-methods-universality-and-acceleration.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a novel method for convex unconstrained optimization that, without any modifications, ensures: (i) accelerated convergence rate for smooth objectives, (ii) standard convergence rate in the general (non-smooth) setting, and (iii) standard convergence rate in the stochastic optimization setting. To the best of our knowledge, this is the first method that simultaneously applies to all of the above settings. At the heart of our method is an adaptive learning rate rule that employs importance weights, in the spirit of adaptive online learning algorithms [<a class=\"ref-link\" id=\"c12\" href=\"#r12\">12</a>, <a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>], combined with an update that linearly couples two sequences, in the spirit of [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>]. An empirical examination of our method demonstrates its applicability to the above mentioned scenarios and corroborates our theoretical findings."
    },
    "keywords": [
        {
            "term": "stochastic optimization",
            "url": "https://en.wikipedia.org/wiki/stochastic_optimization"
        },
        {
            "term": "convergence rate",
            "url": "https://en.wikipedia.org/wiki/convergence_rate"
        },
        {
            "term": "European Research Council",
            "url": "https://en.wikipedia.org/wiki/European_Research_Council"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        }
    ],
    "highlights": [
        "The accelerated gradient method of Nesterov [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] is one of the cornerstones of modern optimization",
        "Accelerated methods are less prevalent in Machine Learning due to two major issues: (i) acceleration is inappropriate for handling noisy feedback, and acceleration requires the knowledge of the objective\u2019s smoothness",
        "Lan [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], Hu et al [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], and Xiao [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], devise acceler\u221aated methods that are able to handle noisy feedback and obtain a convergence rate of O(1/T 2 + \u03c3/ T ), where \u03c3 is the variance of the gradients",
        "We show that in the cas\u221ae of stochastic optimization with a smooth expected loss, AdaGrad ensures an O(1/T + \u03c3/ T ) convergence rate, where \u03c3 is the variance of the gradients",
        "We have presented a novel universal method that may exploit smoothness in order to accelerate while still being able to successfully handle noisy feedback",
        "Our current analysis only applies to unconstrained optimization problems"
    ],
    "key_statements": [
        "The accelerated gradient method of Nesterov [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] is one of the cornerstones of modern optimization",
        "Accelerated methods are less prevalent in Machine Learning due to two major issues: (i) acceleration is inappropriate for handling noisy feedback, and acceleration requires the knowledge of the objective\u2019s smoothness",
        "Lan [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>], Hu et al [<a class=\"ref-link\" id=\"c16\" href=\"#r16\">16</a>], and Xiao [<a class=\"ref-link\" id=\"c33\" href=\"#r33\">33</a>], devise acceler\u221aated methods that are able to handle noisy feedback and obtain a convergence rate of O(1/T 2 + \u03c3/ T ), where \u03c3 is the variance of the gradients",
        "We show that in the cas\u221ae of stochastic optimization with a smooth expected loss, AdaGrad ensures an O(1/T + \u03c3/ T ) convergence rate, where \u03c3 is the variance of the gradients",
        "As a related result we show that the AdaGrad algorithm (Alg. 1) is universal and is able to exploit small variance in order to ensure fast rates in the case of stochastic optimization with smooth expected loss (Thm. 4.2)",
        "We have presented a novel universal method that may exploit smoothness in order to accelerate while still being able to successfully handle noisy feedback",
        "Our current analysis only applies to unconstrained optimization problems"
    ],
    "summary": [
        "The accelerated gradient method of Nesterov [<a class=\"ref-link\" id=\"c23\" href=\"#r23\">23</a>] is one of the cornerstones of modern optimization.",
        "We suggest AcceleGrad (Alg. 2), a novel universal method which employs an accelerated-gradient-like update rule together with an adaptive learning rate \u00e0 la AdaGrad.",
        "We show that in the cas\u221ae of stochastic optimization with a smooth expected loss, AdaGrad ensures an O(1/T + \u03c3/ T ) convergence rate, where \u03c3 is the variance of the gradients.",
        "We present our method in Algorithm 2, and substantiate its universality by providing O(1/T 2) rate in the smooth case (Thm. 3.1), and a rate of O( log T /T ) in the general convex case (Thm. 3.2).",
        "Algorithm 2 Accelerated Adaptive Gradient Method (AcceleGrad)",
        "For any sequence of non-negative weights {\u03b1t}t\u22650, and learning rates {\u03b7t}t\u22650, Algorithm 2 ensures the following to hold, \u03b1t(f",
        "The universal gradient methods presented in [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>] rely on a line search procedure, which requires exact gradients and function values, and are inappropriate for stochastic optimization.",
        "As a related result we show that the AdaGrad algorithm (Alg. 1) is universal and is able to exploit small variance in order to ensure fast rates in the case of stochastic optimization with smooth expected loss (Thm. 4.2).",
        "We assume that our feedback is a first order noisy oracle such that upon querying this oracle with a point x, we receive a bounded and unbiased gradient estimate, g, such",
        "The lemma demonstrates that: (i) AdaGrad is universal, and AdaGrad implicitly make use of smoothness and small variance in the stochastic setting.",
        "Let K be a convex set with bounded diameter D, and assume there exists a global minimizer for f in K.",
        "We compare AcceleGrad against AdaGrad (Alg. 1) and universal gradient methods [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>], focusing on the effect of tuning parameters and the level of adaptivity.",
        "Universal gradient methods are based on an inexact line-search technique that requires an input parameter .",
        "Universal line search methods fail in this case, while AcceleGrad converges and performs to AdaGrad.",
        "We have presented a novel universal method that may exploit smoothness in order to accelerate while still being able to successfully handle noisy feedback.",
        "Another direction is to implicitly adapt the parameter D, this might be possible using ideas in the spirit of scale-free online algorithms, [<a class=\"ref-link\" id=\"c27\" href=\"#r27\">27</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>]"
    ],
    "headline": "We present a novel method for convex unconstrained optimization that, without any modifications, ensures:  accelerated convergence rate for smooth objectives,  standard convergence rate in the general  setting, and  standard convergence rate in the stochastic optimization setting",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Z. Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC, 2017. Full version available at http://arxiv.org/abs/1603.05953.",
            "url": "http://arxiv.org/abs/1603.05953",
            "arxiv_url": "https://arxiv.org/pdf/1603.05953"
        },
        {
            "id": "2",
            "entry": "[2] Z. Allen-Zhu and L. Orecchia. Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent. In Proceedings of the 8th Innovations in Theoretical Computer Science, ITCS \u201917, 2017. Full version available at http://arxiv.org/abs/1407.1537.",
            "url": "http://arxiv.org/abs/1407.1537",
            "arxiv_url": "https://arxiv.org/pdf/1407.1537"
        },
        {
            "id": "3",
            "entry": "[3] Y. Arjevani, S. Shalev-Shwartz, and O. Shamir. On lower and upper bounds in smooth and strongly convex optimization. The Journal of Machine Learning Research, 17(1):4303\u20134353, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Arjevani%2C%20Y.%20Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20On%20lower%20and%20upper%20bounds%20in%20smooth%20and%20strongly%20convex%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Arjevani%2C%20Y.%20Shalev-Shwartz%2C%20S.%20Shamir%2C%20O.%20On%20lower%20and%20upper%20bounds%20in%20smooth%20and%20strongly%20convex%20optimization%202016"
        },
        {
            "id": "4",
            "entry": "[4] H. Attouch and Z. Chbani. Fast inertial dynamics and fista algorithms in convex optimization. perturbation aspects. arXiv preprint arXiv:1507.01367, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1507.01367"
        },
        {
            "id": "5",
            "entry": "[5] J. Aujol and C. Dossal. Optimal rate of convergence of an ode associated to the fast gradient descent schemes for b> 0. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Aujol%2C%20J.%20Dossal%2C%20C.%20Optimal%20rate%20of%20convergence%20of%20an%20ode%20associated%20to%20the%20fast%20gradient%20descent%20schemes%20for%20b%3E%200%202017"
        },
        {
            "id": "6",
            "entry": "[6] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM journal on imaging sciences, 2(1):183\u2013202, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Beck%2C%20A.%20Teboulle%2C%20M.%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Beck%2C%20A.%20Teboulle%2C%20M.%20A%20fast%20iterative%20shrinkage-thresholding%20algorithm%20for%20linear%20inverse%20problems%202009"
        },
        {
            "id": "7",
            "entry": "[7] S. Bubeck, Y. T. Lee, and M. Singh. A geometric alternative to nesterov\u2019s accelerated gradient descent. arXiv preprint arXiv:1506.08187, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1506.08187"
        },
        {
            "id": "8",
            "entry": "[8] A. Chambolle and T. Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of mathematical imaging and vision, 40(1):120\u2013145, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chambolle%2C%20A.%20Pock%2C%20T.%20A%20first-order%20primal-dual%20algorithm%20for%20convex%20problems%20with%20applications%20to%20imaging%202011"
        },
        {
            "id": "9",
            "entry": "[9] M. B. Cohen, J. Diakonikolas, and L. Orecchia. On acceleration with noise-corrupted gradients. arXiv preprint arXiv:1805.12591, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1805.12591"
        },
        {
            "id": "10",
            "entry": "[10] A. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in banach spaces. arXiv preprint arXiv:1802.06293, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1802.06293"
        },
        {
            "id": "11",
            "entry": "[11] J. Diakonikolas and L. Orecchia. Accelerated extra-gradient descent: A novel accelerated first-order method. arXiv preprint arXiv:1706.04680, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1706.04680"
        },
        {
            "id": "12",
            "entry": "[12] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121\u20132159, 2011.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Duchi%2C%20J.%20Hazan%2C%20E.%20Singer%2C%20Y.%20Adaptive%20subgradient%20methods%20for%20online%20learning%20and%20stochastic%20optimization%202011"
        },
        {
            "id": "13",
            "entry": "[13] N. Flammarion and F. Bach. From averaging to acceleration, there is only a step-size. In Conference on Learning Theory, pages 658\u2013695, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Flammarion%2C%20N.%20Bach%2C%20F.%20From%20averaging%20to%20acceleration%2C%20there%20is%20only%20a%20step-size%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Flammarion%2C%20N.%20Bach%2C%20F.%20From%20averaging%20to%20acceleration%2C%20there%20is%20only%20a%20step-size%202015"
        },
        {
            "id": "14",
            "entry": "[14] S. Foucart and H. Rauhut. A mathematical introduction to compressive sensing, volume 1. Birkh\u00e4user Basel, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Foucart%2C%20S.%20Rauhut%2C%20H.%20A%20mathematical%20introduction%20to%20compressive%20sensing%2C%20volume%201%202013"
        },
        {
            "id": "15",
            "entry": "[15] R. Frostig, R. Ge, S. Kakade, and A. Sidford. Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization. In International Conference on Machine Learning, pages 2540\u20132548, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Frostig%2C%20R.%20Ge%2C%20R.%20Kakade%2C%20S.%20Sidford%2C%20A.%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Frostig%2C%20R.%20Ge%2C%20R.%20Kakade%2C%20S.%20Sidford%2C%20A.%20Un-regularizing%3A%20approximate%20proximal%20point%20and%20faster%20stochastic%20algorithms%20for%20empirical%20risk%20minimization%202015"
        },
        {
            "id": "16",
            "entry": "[16] C. Hu, W. Pan, and J. T. Kwok. Accelerated gradient methods for stochastic optimization and online learning. In Advances in Neural Information Processing Systems, pages 781\u2013789, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hu%2C%20C.%20Pan%2C%20W.%20Kwok%2C%20J.T.%20Accelerated%20gradient%20methods%20for%20stochastic%20optimization%20and%20online%20learning%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hu%2C%20C.%20Pan%2C%20W.%20Kwok%2C%20J.T.%20Accelerated%20gradient%20methods%20for%20stochastic%20optimization%20and%20online%20learning%202009"
        },
        {
            "id": "17",
            "entry": "[17] G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2):365\u2013397, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20G.%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20G.%20An%20optimal%20method%20for%20stochastic%20composite%20optimization%202012"
        },
        {
            "id": "18",
            "entry": "[18] G. Lan. Bundle-level type methods uniformly optimal for smooth and nonsmooth convex optimization. Mathematical Programming, 149(1-2):1\u201345, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lan%2C%20G.%20Bundle-level%20type%20methods%20uniformly%20optimal%20for%20smooth%20and%20nonsmooth%20convex%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lan%2C%20G.%20Bundle-level%20type%20methods%20uniformly%20optimal%20for%20smooth%20and%20nonsmooth%20convex%20optimization%202015"
        },
        {
            "id": "19",
            "entry": "[19] L. Lessard, B. Recht, and A. Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57\u201395, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lessard%2C%20L.%20Recht%2C%20B.%20Packard%2C%20A.%20Analysis%20and%20design%20of%20optimization%20algorithms%20via%20integral%20quadratic%20constraints%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lessard%2C%20L.%20Recht%2C%20B.%20Packard%2C%20A.%20Analysis%20and%20design%20of%20optimization%20algorithms%20via%20integral%20quadratic%20constraints%202016"
        },
        {
            "id": "20",
            "entry": "[20] K. Levy. Online to offline conversions, universality and adaptive minibatch sizes. In Advances in Neural Information Processing Systems, pages 1612\u20131621, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Levy%2C%20K.%20Online%20to%20offline%20conversions%2C%20universality%20and%20adaptive%20minibatch%20sizes%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Levy%2C%20K.%20Online%20to%20offline%20conversions%2C%20universality%20and%20adaptive%20minibatch%20sizes%202017"
        },
        {
            "id": "21",
            "entry": "[21] H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In Advances in Neural Information Processing Systems, pages 3384\u20133392, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Lin%2C%20H.%20Mairal%2C%20J.%20Harchaoui%2C%20Z.%20A%20universal%20catalyst%20for%20first-order%20optimization%202015"
        },
        {
            "id": "22",
            "entry": "[22] A. Nemirovskii, D. B. Yudin, and E. Dawson. Problem complexity and method efficiency in optimization. 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nemirovskii%2C%20A.%20Yudin%2C%20D.B.%20Dawson%2C%20E.%20Problem%20complexity%20and%20method%20efficiency%20in%20optimization%201983"
        },
        {
            "id": "23",
            "entry": "[23] Y. Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372\u2013376, 1983.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Nesterov%2C%20Y.%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%201983",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Nesterov%2C%20Y.%20A%20method%20of%20solving%20a%20convex%20programming%20problem%20with%20convergence%20rate%201983"
        },
        {
            "id": "24",
            "entry": "[24] Y. Nesterov. Introductory lectures on convex optimization. 2004, 2003. Programming, 152(1-2):381\u2013404, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Y%20Nesterov%20Introductory%20lectures%20on%20convex%20optimization%202004%202003%20Programming%2015212381404%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Y%20Nesterov%20Introductory%20lectures%20on%20convex%20optimization%202004%202003%20Programming%2015212381404%202015"
        },
        {
            "id": "Programming_2016_a",
            "entry": "Programming, 158(1-2):1\u201321, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Programming%2015812121%202016"
        },
        {
            "id": "27",
            "entry": "[27] F. Orabona and D. P\u00e1l. Scale-free algorithms for online linear optimization. In International",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Orabona%2C%20F.%20P%C3%A1l%2C%20D.%20Scale-free%20algorithms%20for%20online%20linear%20optimization",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Orabona%2C%20F.%20P%C3%A1l%2C%20D.%20Scale-free%20algorithms%20for%20online%20linear%20optimization"
        },
        {
            "id": "28",
            "entry": "[28] D. Scieur, A. d\u2019Aspremont, and F. Bach. Regularized nonlinear acceleration. In Advances In Neural Information Processing Systems, pages 712\u2013720, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Scieur%2C%20D.%20d%E2%80%99Aspremont%2C%20A.%20Bach%2C%20F.%20Regularized%20nonlinear%20acceleration%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Scieur%2C%20D.%20d%E2%80%99Aspremont%2C%20A.%20Bach%2C%20F.%20Regularized%20nonlinear%20acceleration%202016"
        },
        {
            "id": "29",
            "entry": "[29] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In International Conference on Machine Learning, pages 64\u201372, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Shalev-Shwartz%2C%20S.%20Zhang%2C%20T.%20Accelerated%20proximal%20stochastic%20dual%20coordinate%20ascent%20for%20regularized%20loss%20minimization%202014"
        },
        {
            "id": "30",
            "entry": "[30] W. Su, S. Boyd, and E. Candes. A differential equation for modeling nesterov\u2019s accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pages 2510\u20132518, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Su%2C%20W.%20Boyd%2C%20S.%20Candes%2C%20E.%20A%20differential%20equation%20for%20modeling%20nesterov%E2%80%99s%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Su%2C%20W.%20Boyd%2C%20S.%20Candes%2C%20E.%20A%20differential%20equation%20for%20modeling%20nesterov%E2%80%99s%20accelerated%20gradient%20method%3A%20Theory%20and%20insights%202014"
        },
        {
            "id": "31",
            "entry": "[31] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u2013 1147, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sutskever%2C%20I.%20Martens%2C%20J.%20Dahl%2C%20G.%20Hinton%2C%20G.%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sutskever%2C%20I.%20Martens%2C%20J.%20Dahl%2C%20G.%20Hinton%2C%20G.%20On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning%202013"
        },
        {
            "id": "32",
            "entry": "[32] A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351\u2013E7358, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Wibisono%2C%20A.%20Wilson%2C%20A.C.%20Jordan%2C%20M.I.%20A%20variational%20perspective%20on%20accelerated%20methods%20in%20optimization%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Wibisono%2C%20A.%20Wilson%2C%20A.C.%20Jordan%2C%20M.I.%20A%20variational%20perspective%20on%20accelerated%20methods%20in%20optimization%202016"
        },
        {
            "id": "33",
            "entry": "[33] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11(Oct):2543\u20132596, 2010.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xiao%2C%20L.%20Dual%20averaging%20methods%20for%20regularized%20stochastic%20learning%20and%20online%20optimization%202010"
        },
        {
            "id": "34",
            "entry": "[34] A. Yurtsever, Q. T. Dinh, and V. Cevher. A universal primal-dual convex optimization framework. In Advances in Neural Information Processing Systems, pages 3150\u20133158, 2015. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yurtsever%2C%20A.%20Dinh%2C%20Q.T.%20Cevher%2C%20V.%20A%20universal%20primal-dual%20convex%20optimization%20framework%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yurtsever%2C%20A.%20Dinh%2C%20Q.T.%20Cevher%2C%20V.%20A%20universal%20primal-dual%20convex%20optimization%20framework%202015"
        }
    ]
}
