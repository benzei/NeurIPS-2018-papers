{
    "filename": "8086-houdini-lifelong-learning-as-program-synthesis.pdf",
    "metadata": {
        "title": "HOUDINI: Lifelong Learning as Program Synthesis",
        "author": "Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, Swarat Chaudhuri",
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8086-houdini-lifelong-learning-as-program-synthesis.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks significantly accelerates the search."
    },
    "keywords": [
        {
            "term": "neural architecture search",
            "url": "https://en.wikipedia.org/wiki/neural_architecture_search"
        },
        {
            "term": "convolutional neural networks",
            "url": "https://en.wikipedia.org/wiki/convolutional_neural_networks"
        },
        {
            "term": "lifelong learning",
            "url": "https://en.wikipedia.org/wiki/lifelong_learning"
        },
        {
            "term": "library function",
            "url": "https://en.wikipedia.org/wiki/library_function"
        },
        {
            "term": "program synthesis",
            "url": "https://en.wikipedia.org/wiki/program_synthesis"
        },
        {
            "term": "programming language",
            "url": "https://en.wikipedia.org/wiki/programming_language"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "functional program",
            "url": "https://en.wikipedia.org/wiki/functional_program"
        },
        {
            "term": "gradient descent",
            "url": "https://en.wikipedia.org/wiki/gradient_descent"
        },
        {
            "term": "stochastic gradient descent",
            "url": "https://en.wikipedia.org/wiki/stochastic_gradient_descent"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "recurrent neural networks",
            "url": "https://en.wikipedia.org/wiki/recurrent_neural_networks"
        },
        {
            "term": "abstract data types",
            "url": "https://en.wikipedia.org/wiki/abstract_data_types"
        },
        {
            "term": "deep network",
            "url": "https://en.wikipedia.org/wiki/deep_network"
        },
        {
            "term": "multi-layer perceptrons",
            "url": "https://en.wikipedia.org/wiki/multi-layer_perceptrons"
        }
    ],
    "highlights": [
        "Differentiable programming languages [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] have recently emerged as a powerful approach to the task of engineering deep learning systems",
        "We evaluate HOUDINI in the setting of lifelong learning [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>], in which a model is trained on a series of tasks, and each training round is expected to benefit from previous rounds of learning",
        "Function names with prefix \u201cnn \u201d denote fresh neural modules trained during the corresponding tasks",
        "We have presented HOUDINI, the first neurosymbolic approach to the synthesis of differentiable functional programs",
        "Deep networks can be naturally specified as differentiable programs, and functional programs can compactly represent popular deep architectures [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "On several lifelong learning tasks that combine perceptual and algorithmic reasoning, we showed that HOUDINI can accelerate learning by transferring high-level concepts"
    ],
    "key_statements": [
        "Differentiable programming languages [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] have recently emerged as a powerful approach to the task of engineering deep learning systems",
        "Programs in HOUDINI specify the architecture of the network, by using functional combinators to express the network\u2019s connections, and can facilitate learning transfer, by letting the synthesizer choose among library functions",
        "We evaluate HOUDINI in the setting of lifelong learning [<a class=\"ref-link\" id=\"c38\" href=\"#r38\">38</a>], in which a model is trained on a series of tasks, and each training round is expected to benefit from previous rounds of learning",
        "We demonstrate that HOUDINI offers greater transfer than progressive neural networks [<a class=\"ref-link\" id=\"c32\" href=\"#r32\">32</a>] and traditional \u201clow-level\u201d transfer [<a class=\"ref-link\" id=\"c40\" href=\"#r40\">40</a>], in which early network layers are inherited from previous tasks",
        "We show that the use of a higher-level, typed language is critical to scaling the search for programs",
        "We propose the use of symbolic program synthesis in transfer and lifelong learning",
        "We introduce a specific representation of neural networks as typed functional programs, whose types contain rich information such as tensor dimensions, and show how to leverage this representation in program synthesis",
        "We report error, while for the regression tasks \u2014 counting, summing, regress speed and shortest path \u2014 we report root mean-squared error (RMSE)",
        "Function names with prefix \u201cnn \u201d denote fresh neural modules trained during the corresponding tasks",
        "Terms with prefix \u201clib.\u201d denote pretrained neural modules selected from the library",
        "First we evaluate the performance of the methods on the counting sequences (Figure 5)",
        "In all cases where there is an opportunity to transfer from previous tasks, we see that HOUDINI has much lower error than any of the other transfer learning methods",
        "The actual programs generated by HOUDINI are listed in the Appendix",
        "Let the size of a program be the number of occurrences of library functions and combinators in the program",
        "Table 1 shows the number of programs of different sizes generated for the tasks in the sequence CS1",
        "When the type system is disabled, the only constraint that GENERATE has while composing programs is the arity of the library functions",
        "The timed out runs are not included in the plots",
        "We have presented HOUDINI, the first neurosymbolic approach to the synthesis of differentiable functional programs",
        "Deep networks can be naturally specified as differentiable programs, and functional programs can compactly represent popular deep architectures [<a class=\"ref-link\" id=\"c26\" href=\"#r26\">26</a>]",
        "On several lifelong learning tasks that combine perceptual and algorithmic reasoning, we showed that HOUDINI can accelerate learning by transferring high-level concepts"
    ],
    "summary": [
        "Differentiable programming languages [<a class=\"ref-link\" id=\"c25\" href=\"#r25\">25</a>, <a class=\"ref-link\" id=\"c29\" href=\"#r29\">29</a>, <a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>, <a class=\"ref-link\" id=\"c15\" href=\"#r15\">15</a>, <a class=\"ref-link\" id=\"c10\" href=\"#r10\">10</a>, <a class=\"ref-link\" id=\"c39\" href=\"#r39\">39</a>, <a class=\"ref-link\" id=\"c34\" href=\"#r34\">34</a>] have recently emerged as a powerful approach to the task of engineering deep learning systems.",
        "Programs in HOUDINI specify the architecture of the network, by using functional combinators to express the network\u2019s connections, and can facilitate learning transfer, by letting the synthesizer choose among library functions.",
        "On a task of computing least-cost paths in a grid of images, HOUDINI discovers an algorithm that has the structure of the Bellman-Ford shortest path algorithm [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>], but uses a learned neural function that approximates the algorithm\u2019s \u201crelaxation\u201d step.",
        "Given a task specified by a set of training examples, the symbolic module enumerates parameterized programs in the HOUDINI language.",
        "The graph task sequence GS1 demonstrates that the graph convolution combinator in HOUDINI allows learning of complex graph algorithms and GS2 tests if high-level transfer can be performed with this more complex task.",
        "We list in Figure 4 the top 3 programs for each task in the graph sequence GS2, and the corresponding RMSEs. Here, function names with prefix \u201cnn \u201d denote fresh neural modules trained during the corresponding tasks.",
        "The kernel for the graph convolution combinator conv g is a function lib.nn gs2 3, originally learned in Task 2, that implements the relaxation operation used in shortest-path algorithms.",
        "We see in Figure 5c that the higher-level network still successfully transfers across tasks, learning an effective network for counting the number of toys of type t1, even though the network has not previously seen any toy images at all.",
        "It can be seen that HOUDINI outperforms all the baselines even under the limited data setting, confirming the successful selective transfer of both low-level and high-level perceptual information.",
        "For the graph-based tasks (Table 2), we see that the graph convolutional program learned by HOUDINI on the graph tasks has significantly less error than a simple sequence model, a standalone baseline and the evolutionary-algorithm-based version of HOUDINI.",
        "In the shortest path street task in the graph sequence GS2, HOUDINI learns a program that uses newly learned regress functions for the street signs, along with a \u201crelaxation\u201d function already learned from the earlier task shortest path mnist.",
        "Symbolic search through a space of differentiable functional programs is particularly appealing, because it can at the same time select both which pretrained neural library functions should be reused, and what deep architecture should be used to combine them.",
        "On several lifelong learning tasks that combine perceptual and algorithmic reasoning, we showed that HOUDINI can accelerate learning by transferring high-level concepts."
    ],
    "headline": "We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Houdini code repository. https://github.com/capergroup/houdini.",
            "url": "https://github.com/capergroup/houdini"
        },
        {
            "id": "2",
            "entry": "[2] Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles Sutton. Learning continuous semantic representations of symbolic expressions. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Allamanis%2C%20Miltiadis%20Chanthirasegaran%2C%20Pankajan%20Kohli%2C%20Pushmeet%20Sutton%2C%20Charles%20Learning%20continuous%20semantic%20representations%20of%20symbolic%20expressions%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Allamanis%2C%20Miltiadis%20Chanthirasegaran%2C%20Pankajan%20Kohli%2C%20Pushmeet%20Sutton%2C%20Charles%20Learning%20continuous%20semantic%20representations%20of%20symbolic%20expressions%202017"
        },
        {
            "id": "3",
            "entry": "[3] Rajeev Alur, Rastislav Bod\u0131k, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax-guided synthesis. In Formal Methods in Computer-Aided Design, FMCAD, pages 1\u201317, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Alur%2C%20Rajeev%20Bod%C4%B1k%2C%20Rastislav%20Juniwal%2C%20Garvit%20Martin%2C%20Milo%20M.K.%20Syntax-guided%20synthesis%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Alur%2C%20Rajeev%20Bod%C4%B1k%2C%20Rastislav%20Juniwal%2C%20Garvit%20Martin%2C%20Milo%20M.K.%20Syntax-guided%20synthesis%202013"
        },
        {
            "id": "4",
            "entry": "[4] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39\u201348, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Andreas%2C%20Jacob%20Rohrbach%2C%20Marcus%20Darrell%2C%20Trevor%20Klein%2C%20Dan%20Neural%20module%20networks%202016"
        },
        {
            "id": "5",
            "entry": "[5] John Backus. Can programming be liberated from the von Neumann style?: A functional style and its algebra of programs. Commun. ACM, 21(8):613\u2013641, August 1978.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Backus%2C%20John%20Can%20programming%20be%20liberated%20from%20the%20von%20Neumann%20style%3F%3A%20A%20functional%20style%20and%20its%20algebra%20of%20programs%201978-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Backus%2C%20John%20Can%20programming%20be%20liberated%20from%20the%20von%20Neumann%20style%3F%3A%20A%20functional%20style%20and%20its%20algebra%20of%20programs%201978-08"
        },
        {
            "id": "6",
            "entry": "[6] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. In International Conference on Learning Representations (ICLR), 2017. arXiv:1611.01989.",
            "arxiv_url": "https://arxiv.org/pdf/1611.01989"
        },
        {
            "id": "7",
            "entry": "[7] Richard Bellman. On a routing problem. Quarterly of Applied Mathematics, 16(1):87\u201390, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bellman%2C%20Richard%20On%20a%20routing%20problem%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bellman%2C%20Richard%20On%20a%20routing%20problem%201958"
        },
        {
            "id": "8",
            "entry": "[8] Matko Bosnjak, Tim Rocktaschel, Jason Naradowsky, and Sebastian Riedel. Programming with a differentiable Forth interpreter. In International Conference on Machine Learning (ICML), pages 547\u2013556, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bosnjak%2C%20Matko%20Rocktaschel%2C%20Tim%20Naradowsky%2C%20Jason%20Riedel%2C%20Sebastian%20Programming%20with%20a%20differentiable%20Forth%20interpreter%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bosnjak%2C%20Matko%20Rocktaschel%2C%20Tim%20Naradowsky%2C%20Jason%20Riedel%2C%20Sebastian%20Programming%20with%20a%20differentiable%20Forth%20interpreter%202017"
        },
        {
            "id": "9",
            "entry": "[9] Forrest Briggs and Melissa O\u2019Neill. Functional genetic programming with combinators. In Proceedings of the Third Asian-Pacific Workshop on Genetic Programming (ASPGP), pages 110\u2013127, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Briggs%2C%20Forrest%20O%E2%80%99Neill%2C%20Melissa%20Functional%20genetic%20programming%20with%20combinators%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Briggs%2C%20Forrest%20O%E2%80%99Neill%2C%20Melissa%20Functional%20genetic%20programming%20with%20combinators%202006"
        },
        {
            "id": "10",
            "entry": "[10] Rudy R. Bunel, Alban Desmaison, Pawan Kumar Mudigonda, Pushmeet Kohli, and Philip H. S. Torr. Adaptive neural compilation. In Advances in Neural Information Processing Systems 29, pages 1444\u20131452, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bunel%2C%20Rudy%20R.%20Desmaison%2C%20Alban%20Mudigonda%2C%20Pawan%20Kumar%20Kohli%2C%20Pushmeet%20Adaptive%20neural%20compilation%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bunel%2C%20Rudy%20R.%20Desmaison%2C%20Alban%20Mudigonda%2C%20Pawan%20Kumar%20Kohli%2C%20Pushmeet%20Adaptive%20neural%20compilation%202016"
        },
        {
            "id": "11",
            "entry": "[11] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20Robustfill%3A%20Neural%20program%20learning%20under%20noisy%20I/O%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Devlin%2C%20Jacob%20Uesato%2C%20Jonathan%20Bhupatiraju%2C%20Surya%20Singh%2C%20Rishabh%20Robustfill%3A%20Neural%20program%20learning%20under%20noisy%20I/O%202017"
        },
        {
            "id": "12",
            "entry": "[12] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Joshua B. Tenenbaum. Learning to infer graphics programs from hand-drawn images. CoRR, abs/1707.09627, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1707.09627"
        },
        {
            "id": "13",
            "entry": "[13] John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from input-output examples. In ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 229\u2013239, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Feser%2C%20John%20K.%20Chaudhuri%2C%20Swarat%20Dillig%2C%20Isil%20Synthesizing%20data%20structure%20transformations%20from%20input-output%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Feser%2C%20John%20K.%20Chaudhuri%2C%20Swarat%20Dillig%2C%20Isil%20Synthesizing%20data%20structure%20transformations%20from%20input-output%20examples%202015"
        },
        {
            "id": "14",
            "entry": "[14] Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, and Daniel Tarlow. Differentiable programs with neural libraries. In International Conference on Machine Learning (ICML), pages 1213\u20131222, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gaunt%2C%20Alexander%20L.%20Brockschmidt%2C%20Marc%20Kushman%2C%20Nate%20Tarlow%2C%20Daniel%20Differentiable%20programs%20with%20neural%20libraries%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gaunt%2C%20Alexander%20L.%20Brockschmidt%2C%20Marc%20Kushman%2C%20Nate%20Tarlow%2C%20Daniel%20Differentiable%20programs%20with%20neural%20libraries%202017"
        },
        {
            "id": "15",
            "entry": "[15] Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction. CoRR, abs/1608.04428, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1608.04428"
        },
        {
            "id": "16",
            "entry": "[16] Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. Foundations and Trends in Programming Languages, 4(1-2):1\u2013119, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulwani%2C%20Sumit%20Polozov%2C%20Oleksandr%20Singh%2C%20Rishabh%20Program%20synthesis%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulwani%2C%20Sumit%20Polozov%2C%20Oleksandr%20Singh%2C%20Rishabh%20Program%20synthesis%202017"
        },
        {
            "id": "17",
            "entry": "[17] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In International Conference on Computer Vision (ICCV), 2017. CoRR, abs/1704.05526.",
            "arxiv_url": "https://arxiv.org/pdf/1704.05526"
        },
        {
            "id": "18",
            "entry": "[18] Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani. Neural-guided deductive search for real-time program synthesis from examples. In International Conference on Learning Representations (ICLR), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kalyan%2C%20Ashwin%20Mohta%2C%20Abhishek%20Polozov%2C%20Oleksandr%20Batra%2C%20Dhruv%20Neural-guided%20deductive%20search%20for%20real-time%20program%20synthesis%20from%20examples%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kalyan%2C%20Ashwin%20Mohta%2C%20Abhishek%20Polozov%2C%20Oleksandr%20Batra%2C%20Dhruv%20Neural-guided%20deductive%20search%20for%20real-time%20program%20synthesis%20from%20examples%202018"
        },
        {
            "id": "19",
            "entry": "[19] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kipf%2C%20Thomas%20N.%20Welling%2C%20Max%20Semi-supervised%20classification%20with%20graph%20convolutional%20networks%202017"
        },
        {
            "id": "20",
            "entry": "[20] Vu Le and Sumit Gulwani. FlashExtract: A framework for data extraction by examples. In ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pages 542\u2013553, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Le%2C%20Vu%20Gulwani%2C%20Sumit%20FlashExtract%3A%20A%20framework%20for%20data%20extraction%20by%20examples%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Le%2C%20Vu%20Gulwani%2C%20Sumit%20FlashExtract%3A%20A%20framework%20for%20data%20extraction%20by%20examples%202014"
        },
        {
            "id": "21",
            "entry": "[21] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Bottou%2C%20Leon%20Bengio%2C%20Yoshua%20Haffner%2C%20Patrick%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998"
        },
        {
            "id": "22",
            "entry": "[22] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition (CVPR), 2004.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=LeCun%2C%20Yann%20Huang%2C%20Fu%20Jie%20Bottou%2C%20Leon%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004",
            "oa_query": "https://api.scholarcy.com/oa_version?query=LeCun%2C%20Yann%20Huang%2C%20Fu%20Jie%20Bottou%2C%20Leon%20Learning%20methods%20for%20generic%20object%20recognition%20with%20invariance%20to%20pose%20and%20lighting%202004"
        },
        {
            "id": "23",
            "entry": "[23] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Li%2C%20Yujia%20Tarlow%2C%20Daniel%20Brockschmidt%2C%20Marc%20Zemel%2C%20Richard%20Gated%20graph%20sequence%20neural%20networks%202016"
        },
        {
            "id": "24",
            "entry": "[24] Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In European Conference on Computer Vision (ECCV), 2018. arXiv:1712.00559.",
            "arxiv_url": "https://arxiv.org/pdf/1712.00559"
        },
        {
            "id": "25",
            "entry": "[25] Dougal Maclaurin, David Duvenaud, Matthew Johnson, and Ryan P. Adams. Autograd: Reversemode differentiation of native Python, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Johnson%2C%20Matthew%20Adams%2C%20Ryan%20P.%20Autograd%3A%20Reversemode%20differentiation%20of%20native%20Python%202015"
        },
        {
            "id": "26",
            "entry": "[26] Christopher Olah. Neural networks, types, and functional programming, 2015. http://colah.github.io/posts/2015-09-NN-Types-FP/.",
            "url": "http://colah.github.io/posts/2015-09-NN-Types-FP/"
        },
        {
            "id": "27",
            "entry": "[27] Peter-Michael Osera and Steve Zdancewic. Type-and-example-directed program synthesis. In PLDI, volume 50, pages 619\u2013630. ACM, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Osera%2C%20Peter-Michael%20Zdancewic%2C%20Steve%20Type-and-example-directed%20program%20synthesis%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Osera%2C%20Peter-Michael%20Zdancewic%2C%20Steve%20Type-and-example-directed%20program%20synthesis%202015"
        },
        {
            "id": "28",
            "entry": "[28] Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. In International Conference on Learning Representations (ICLR), 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Parisotto%2C%20Emilio%20rahman%20Mohamed%2C%20Abdel%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Parisotto%2C%20Emilio%20rahman%20Mohamed%2C%20Abdel%20Singh%2C%20Rishabh%20Li%2C%20Lihong%20Neuro-symbolic%20program%20synthesis%202016"
        },
        {
            "id": "29",
            "entry": "[29] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Paszke%2C%20Adam%20Gross%2C%20Sam%20Chintala%2C%20Soumith%20Chanan%2C%20Gregory%20Automatic%20differentiation%20in%20pytorch%202017"
        },
        {
            "id": "30",
            "entry": "[30] Benjamin C. Pierce. Types and programming languages. MIT Press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pierce%2C%20Benjamin%20C.%20Types%20and%20programming%20languages%202002"
        },
        {
            "id": "31",
            "entry": "[31] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning (ICML), 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Real%2C%20Esteban%20Moore%2C%20Sherry%20Selle%2C%20Andrew%20Saxena%2C%20Saurabh%20Large-scale%20evolution%20of%20image%20classifiers%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Real%2C%20Esteban%20Moore%2C%20Sherry%20Selle%2C%20Andrew%20Saxena%2C%20Saurabh%20Large-scale%20evolution%20of%20image%20classifiers%202017"
        },
        {
            "id": "32",
            "entry": "[32] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1606.04671"
        },
        {
            "id": "33",
            "entry": "[33] Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International Conference on Machine Learning (ICML), 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Schwarz%2C%20Jonathan%20Luketina%2C%20Jelena%20Czarnecki%2C%20Wojciech%20M.%20Grabska-Barwinska%2C%20Agnieszka%20Progress%20%26%20compress%3A%20A%20scalable%20framework%20for%20continual%20learning%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Schwarz%2C%20Jonathan%20Luketina%2C%20Jelena%20Czarnecki%2C%20Wojciech%20M.%20Grabska-Barwinska%2C%20Agnieszka%20Progress%20%26%20compress%3A%20A%20scalable%20framework%20for%20continual%20learning%202018"
        },
        {
            "id": "34",
            "entry": "[34] Asim Shankar and Wolff Dobson. Eager execution: An imperative, defineby-run interface to tensorflow. https://research.googleblog.com/2017/",
            "url": "https://research.googleblog.com/2017/"
        },
        {
            "id": "Eager-Execution-Imperative-Define-Byhtml_2017_a",
            "entry": "10/eager-execution-imperative-define-by.html, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=eagerexecutionimperativedefinebyhtml%202017"
        },
        {
            "id": "35",
            "entry": "[35] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Socher%2C%20Richard%20Perelygin%2C%20Alex%20Wu%2C%20Jean%20Y.%20Chuang%2C%20Jason%20Andrew%20Y%20Ng%2C%20and%20Christopher%20Potts.%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%202013"
        },
        {
            "id": "36",
            "entry": "[36] Armando Solar-Lezama. Program sketching. STTT, 15(5-6):475\u2013495, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Solar-Lezama%2C%20Armando%20Program%20sketching%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Solar-Lezama%2C%20Armando%20Program%20sketching%202013"
        },
        {
            "id": "37",
            "entry": "[37] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural Networks, (0), 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Stallkamp%2C%20J.%20Schlipsing%2C%20M.%20Salmen%2C%20J.%20Igel%2C%20C.%20Man%20vs.%20computer%3A%20Benchmarking%20machine%20learning%20algorithms%20for%20traffic%20sign%20recognition%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Stallkamp%2C%20J.%20Schlipsing%2C%20M.%20Salmen%2C%20J.%20Igel%2C%20C.%20Man%20vs.%20computer%3A%20Benchmarking%20machine%20learning%20algorithms%20for%20traffic%20sign%20recognition%202012"
        },
        {
            "id": "38",
            "entry": "[38] Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics and Autonomous Systems, 15(1-2):25\u201346, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Thrun%2C%20Sebastian%20Mitchell%2C%20Tom%20M.%20Lifelong%20robot%20learning%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Thrun%2C%20Sebastian%20Mitchell%2C%20Tom%20M.%20Lifelong%20robot%20learning%201995"
        },
        {
            "id": "39",
            "entry": "[39] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of NIPS Workshop on Machine Learning Systems (LearningSys), 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tokui%2C%20Seiya%20Oono%2C%20Kenta%20Hido%2C%20Shohei%20Clayton%2C%20Justin%20Chainer%3A%20a%20next-generation%20open%20source%20framework%20for%20deep%20learning%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tokui%2C%20Seiya%20Oono%2C%20Kenta%20Hido%2C%20Shohei%20Clayton%2C%20Justin%20Chainer%3A%20a%20next-generation%20open%20source%20framework%20for%20deep%20learning%202015"
        },
        {
            "id": "40",
            "entry": "[40] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems 27 (NIPS), 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Yosinski%2C%20Jason%20Clune%2C%20Jeff%20Bengio%2C%20Yoshua%20Lipson%2C%20Hod%20How%20transferable%20are%20features%20in%20deep%20neural%20networks%3F%202014"
        },
        {
            "id": "41",
            "entry": "[41] Zhao Zhong, Junjie Yan, and Cheng-Lin Liu. Practical network blocks design with q-learning. CoRR, abs/1708.05552, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1708.05552"
        },
        {
            "id": "42",
            "entry": "[42] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations (ICLR), 2017. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zoph%2C%20Barret%20Le%2C%20Quoc%20V.%20Neural%20architecture%20search%20with%20reinforcement%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zoph%2C%20Barret%20Le%2C%20Quoc%20V.%20Neural%20architecture%20search%20with%20reinforcement%20learning%202017"
        }
    ]
}
