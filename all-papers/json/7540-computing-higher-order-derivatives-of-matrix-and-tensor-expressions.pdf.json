{
    "filename": "7540-computing-higher-order-derivatives-of-matrix-and-tensor-expressions.pdf",
    "metadata": {
        "title": "Computing Higher Order Derivatives of Matrix and Tensor Expressions",
        "author": "Soeren Laue, Matthias Mitterreiter, Joachim Giesen",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/7540-computing-higher-order-derivatives-of-matrix-and-tensor-expressions.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup of up to two orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives on CPUs and a speedup of about three orders of magnitude on GPUs."
    },
    "keywords": [
        {
            "term": "automatic differentiation",
            "url": "https://en.wikipedia.org/wiki/automatic_differentiation"
        },
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "second order derivative",
            "url": "https://en.wikipedia.org/wiki/second_order_derivative"
        },
        {
            "term": "Deutsche Forschungsgemeinschaft",
            "url": "https://en.wikipedia.org/wiki/Deutsche_Forschungsgemeinschaft"
        },
        {
            "term": "directed acyclic graph",
            "url": "https://en.wikipedia.org/wiki/directed_acyclic_graph"
        },
        {
            "term": "tensor derivative",
            "url": "https://en.wikipedia.org/wiki/tensor_derivative"
        }
    ],
    "highlights": [
        "Automatic differentiation has become popular in the machine learning community due to its genericity, flexibility, and efficiency",
        "We provide an algorithmic framework for computing higher order derivatives of matrix and tensor expressions efficiently, which fully operates on tensors, i.e., all variables are allowed to be tensors of any order, including the output variables",
        "We have presented the first algorithmic framework for computing matrix and tensor derivatives that naturally extends to higher order derivatives",
        "Experiments show that our approach achieves state-of-the-art performance for the evaluation of first order derivatives",
        "We showed that the difficulties in computing matrix derivatives can be attributed to the predominantly used standard matrix language that, in contrast to Ricci calculus, is not well suited for matrix calculus"
    ],
    "key_statements": [
        "Automatic differentiation has become popular in the machine learning community due to its genericity, flexibility, and efficiency",
        "We provide an algorithmic framework for computing higher order derivatives of matrix and tensor expressions efficiently, which fully operates on tensors, i.e., all variables are allowed to be tensors of any order, including the output variables",
        "We have presented the first algorithmic framework for computing matrix and tensor derivatives that naturally extends to higher order derivatives",
        "Experiments show that our approach achieves state-of-the-art performance for the evaluation of first order derivatives",
        "We showed that the difficulties in computing matrix derivatives can be attributed to the predominantly used standard matrix language that, in contrast to Ricci calculus, is not well suited for matrix calculus"
    ],
    "summary": [
        "Automatic differentiation has become popular in the machine learning community due to its genericity, flexibility, and efficiency.",
        "To implement matrix calculus, we first translate linear algebra matrix expressions into a tensor representation, compute derivatives in this representation and translate the result back into the standard matrix language.",
        "Pearlmutter [<a class=\"ref-link\" id=\"c17\" href=\"#r17\">17</a>] discusses the following approach for computing higher order derivatives, like the Hessian of a function f : Rn \u2192 R, by automatic differentiation: First, an expression for the gradient \u2207f is computed.",
        "The problem is avoided by turning to a different language for encoding matrix expressions, namely Ricci calculus [<a class=\"ref-link\" id=\"c20\" href=\"#r20\">20</a>].",
        "Ricci calculus lacks the simplicity of the standard language for matrix expressions, but is more precise and can distinguish between linear maps and bilinear maps through the use of indices.",
        "The resulting derivative is again an expression in Ricci calculus that can be translated back into the standard matrix calculus language.",
        "The trace of Aij does not need a special symbol in Ricci calculus, because it can be encoded by the expression Aji \u03b4ji .",
        "In forward mode for computing derivatives with respect to the input variable x[j], each node v[i]",
        "The edges of the expression DAG are labeled by the corresponding tensor derivatives v[i] in forward mode and v[i] in reverse mode, respectively.",
        "To the best of our knowledge, this is the first approach for applying automatic differentiation to matrix and tensor expressions that treats forward and reverse mode .",
        "We want to point out that the equations from [<a class=\"ref-link\" id=\"c8\" href=\"#r8\">8</a>] that we used here in conjunction with standard matrix language are only valid for scalar expressions, i.e., scalar output functions.",
        "While standard matrix language is the preferred notation in linear algebra, it is not well suited for computing derivatives.",
        "To illustrate the tensor calculus algorithm we demonstrate it on the rather simple example f = (x A)x and compute derivatives of f with respect to x.",
        "We provide an outline of the steps of the algorithm in computing the first and the second order derivatives through forward mode automatic differentiation.",
        "All these frameworks support reverse mode automatic differentiation for computing first order derivatives.",
        "We have presented the first algorithmic framework for computing matrix and tensor derivatives that naturally extends to higher order derivatives.",
        "In the case of second order derivatives it is up to two orders of magnitude more efficient on CPUs and up to three orders of magnitude more efficient on GPUs. Our framework operates directly on tensors using Ricci calculus to specify tensor expressions.",
        "We showed that the difficulties in computing matrix derivatives can be attributed to the predominantly used standard matrix language that, in contrast to Ricci calculus, is not well suited for matrix calculus"
    ],
    "headline": "We provide an algorithmic framework for computing higher order derivatives of matrix and tensor expressions efficiently, which fully operates on tensors, i.e., all variables are allowed to be tensors of any order, including the output variables",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In USENIX Conference on Operating Systems Design and Implementation (OSDI), pages 265\u2013283. USENIX Association, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Abadi%2C%20Mart%C3%ADn%20Barham%2C%20Paul%20Chen%2C%20Jianmin%20Chen%2C%20Zhifeng%20Tensorflow%3A%20A%20system%20for%20large-scale%20machine%20learning%202016"
        },
        {
            "id": "2",
            "entry": "[2] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of Machine Learning Research, 18(153):1\u201343, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baydin%2C%20Atilim%20Gunes%20Pearlmutter%2C%20Barak%20A.%20Radul%2C%20Alexey%20Andreyevich%20Siskind%2C%20Jeffrey%20Mark%20Automatic%20differentiation%20in%20machine%20learning%3A%20a%20survey%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baydin%2C%20Atilim%20Gunes%20Pearlmutter%2C%20Barak%20A.%20Radul%2C%20Alexey%20Andreyevich%20Siskind%2C%20Jeffrey%20Mark%20Automatic%20differentiation%20in%20machine%20learning%3A%20a%20survey%202018"
        },
        {
            "id": "3",
            "entry": "[3] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993\u20131022, 2003.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20dirichlet%20allocation%202003",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Blei%2C%20David%20M.%20Ng%2C%20Andrew%20Y.%20Jordan%2C%20Michael%20I.%20Latent%20dirichlet%20allocation%202003"
        },
        {
            "id": "4",
            "entry": "[4] D.S. Broomhead and David Lowe. Multivariable functional interpolation and adaptive networks. Complex Systems, 2:321\u2013355, 1988.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Broomhead%2C%20D.S.%20Lowe%2C%20David%20Multivariable%20functional%20interpolation%20and%20adaptive%20networks%201988",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Broomhead%2C%20D.S.%20Lowe%2C%20David%20Multivariable%20functional%20interpolation%20and%20adaptive%20networks%201988"
        },
        {
            "id": "5",
            "entry": "[5] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273\u2013 297, 1995.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cortes%2C%20Corinna%20Vapnik%2C%20Vladimir%20Support-vector%20networks%201995",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cortes%2C%20Corinna%20Vapnik%2C%20Vladimir%20Support-vector%20networks%201995"
        },
        {
            "id": "6",
            "entry": "[6] David R. Cox. The regression analysis of binary sequences. Journal of the Royal Statistical Society. Series B (Methodological), 20(2):215\u2013242, 1958.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Cox%2C%20David%20R.%20The%20regression%20analysis%20of%20binary%20sequences%201958",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Cox%2C%20David%20R.%20The%20regression%20analysis%20of%20binary%20sequences%201958"
        },
        {
            "id": "7",
            "entry": "[7] Assefaw Hadish Gebremedhin, Arijit Tarafdar, Alex Pothen, and Andrea Walther. Efficient computation of sparse hessians using coloring and automatic differentiation. INFORMS Journal on Computing, 21(2):209\u2013223, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gebremedhin%2C%20Assefaw%20Hadish%20Tarafdar%2C%20Arijit%20Pothen%2C%20Alex%20Walther%2C%20Andrea%20Efficient%20computation%20of%20sparse%20hessians%20using%20coloring%20and%20automatic%20differentiation%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gebremedhin%2C%20Assefaw%20Hadish%20Tarafdar%2C%20Arijit%20Pothen%2C%20Alex%20Walther%2C%20Andrea%20Efficient%20computation%20of%20sparse%20hessians%20using%20coloring%20and%20automatic%20differentiation%202009"
        },
        {
            "id": "8",
            "entry": "[8] Mike B. Giles. Collected matrix derivative results for forward and reverse mode algorithmic differentiation. In Advances in Automatic Differentiation, pages 35\u201344. Springer Berlin Heidelberg, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Giles%2C%20Mike%20B.%20Collected%20matrix%20derivative%20results%20for%20forward%20and%20reverse%20mode%20algorithmic%20differentiation%202008",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Giles%2C%20Mike%20B.%20Collected%20matrix%20derivative%20results%20for%20forward%20and%20reverse%20mode%20algorithmic%20differentiation%202008"
        },
        {
            "id": "9",
            "entry": "[9] Andreas Griewank and Andrea Walther. Evaluating derivatives - principles and techniques of algorithmic differentiation (2. ed.). SIAM, 2008.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Griewank%2C%20Andreas%20Walther%2C%20Andrea%20Evaluating%20derivatives%20-%20principles%20and%20techniques%20of%20algorithmic%20differentiation%202008"
        },
        {
            "id": "10",
            "entry": "[10] Laurent Hasco\u00ebt and Val\u00e9rie Pascual. The Tapenade automatic differentiation tool: Principles, model, and specification. ACM Transactions on Mathematical Software, 39(3):20:1\u201320:43, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hasco%C3%ABt%2C%20Laurent%20Pascual%2C%20Val%C3%A9rie%20The%20Tapenade%20automatic%20differentiation%20tool%3A%20Principles%2C%20model%2C%20and%20specification%202013",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hasco%C3%ABt%2C%20Laurent%20Pascual%2C%20Val%C3%A9rie%20The%20Tapenade%20automatic%20differentiation%20tool%3A%20Principles%2C%20model%2C%20and%20specification%202013"
        },
        {
            "id": "11",
            "entry": "[11] Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527\u20131554, 2006.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee%20Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20Geoffrey%20E.%20Osindero%2C%20Simon%20Teh%2C%20Yee%20Whye%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets%202006"
        },
        {
            "id": "12",
            "entry": "[12] Thomas Hofmann. Probabilistic latent semantic analysis. In Conference on Uncertainty in Artificial Intelligence (UAI), pages 289\u2013296, 1999.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hofmann%2C%20Thomas%20Probabilistic%20latent%20semantic%20analysis%201999",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hofmann%2C%20Thomas%20Probabilistic%20latent%20semantic%20analysis%201999"
        },
        {
            "id": "13",
            "entry": "[13] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337, August 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Koren%2C%20Yehuda%20Bell%2C%20Robert%20Volinsky%2C%20Chris%20Matrix%20factorization%20techniques%20for%20recommender%20systems%202009-08",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Koren%2C%20Yehuda%20Bell%2C%20Robert%20Volinsky%2C%20Chris%20Matrix%20factorization%20techniques%20for%20recommender%20systems%202009-08"
        },
        {
            "id": "14",
            "entry": "[14] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Autograd: Effortless gradients in numpy. In ICML AutoML workshop, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20P.%20Autograd%3A%20Effortless%20gradients%20in%20numpy%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Maclaurin%2C%20Dougal%20Duvenaud%2C%20David%20Adams%2C%20Ryan%20P.%20Autograd%3A%20Effortless%20gradients%20in%20numpy%202015"
        },
        {
            "id": "15",
            "entry": "[15] Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statistics and Econometrics. John Wiley and Sons, third edition, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Magnus%2C%20Jan%20R.%20Neudecker%2C%20Heinz%20Matrix%20Differential%20Calculus%20with%20Applications%20in%20Statistics%20and%20Econometrics%202007"
        },
        {
            "id": "16",
            "entry": "[16] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS Autodiff workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPS%20Autodiff%20workshop%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20pytorch%20In%20NIPS%20Autodiff%20workshop%202017"
        },
        {
            "id": "17",
            "entry": "[17] Barak A. Pearlmutter. Fast exact multiplication by the hessian. Neural Computation, 6(1):147\u2013 160, 1994.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Pearlmutter%2C%20Barak%20A.%20Fast%20exact%20multiplication%20by%20the%20hessian%201994",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Pearlmutter%2C%20Barak%20A.%20Fast%20exact%20multiplication%20by%20the%20hessian%201994"
        },
        {
            "id": "18",
            "entry": "[18] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NIPS), pages 1177\u20131184, 2007.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Rahimi%2C%20Ali%20Recht%2C%20Benjamin%20Random%20features%20for%20large-scale%20kernel%20machines%202007"
        },
        {
            "id": "19",
            "entry": "[19] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2005.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Rasmussen%2C%20Carl%20Edward%20Williams%2C%20Christopher%20K.I.%20Gaussian%20Processes%20for%20Machine%20Learning%202005"
        },
        {
            "id": "20",
            "entry": "[20] Gregorio Ricci and Tullio Levi-Civita. M\u00e9thodes de calcul diff\u00e9rentiel absolu et leurs applications. Mathematische Annalen, 54(1-2):125\u2013201, 1900.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ricci%2C%20Gregorio%20Levi-Civita%2C%20Tullio%20M%C3%A9thodes%20de%20calcul%20diff%C3%A9rentiel%20absolu%20et%20leurs%20applications%201900",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ricci%2C%20Gregorio%20Levi-Civita%2C%20Tullio%20M%C3%A9thodes%20de%20calcul%20diff%C3%A9rentiel%20absolu%20et%20leurs%20applications%201900"
        },
        {
            "id": "21",
            "entry": "[21] Bernhard Sch\u00f6lkopf and Alexander Johannes Smola. Learning with Kernels: support vector machines, regularization, optimization, and beyond. Adaptive computation and machine learning series. MIT Press, 2002.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sch%C3%B6lkopf%2C%20Bernhard%20Smola%2C%20Alexander%20Johannes%20Learning%20with%20Kernels%3A%20support%20vector%20machines%2C%20regularization%2C%20optimization%2C%20and%20beyond.%20Adaptive%20computation%20and%20machine%20learning%20series%202002"
        },
        {
            "id": "22",
            "entry": "[22] Matthias W. Seeger, Asmus Hetzel, Zhenwen Dai, and Neil D. Lawrence. Auto-differentiating linear algebra. In NIPS Autodiff workshop, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Seeger%2C%20Matthias%20W.%20Hetzel%2C%20Asmus%20Dai%2C%20Zhenwen%20Lawrence%2C%20Neil%20D.%20Auto-differentiating%20linear%20algebra%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Seeger%2C%20Matthias%20W.%20Hetzel%2C%20Asmus%20Dai%2C%20Zhenwen%20Lawrence%2C%20Neil%20D.%20Auto-differentiating%20linear%20algebra%202017"
        },
        {
            "id": "23",
            "entry": "[23] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1605.02688"
        },
        {
            "id": "24",
            "entry": "[24] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267\u2013288, 1996.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tibshirani%2C%20Robert%20Regression%20shrinkage%20and%20selection%20via%20the%20lasso%201996"
        },
        {
            "id": "25",
            "entry": "[25] Andrea Walther and Andreas Griewank. Getting started with adol-c. In Combinatorial Scientific Computing, pages 181\u2013202. Chapman-Hall CRC Computational Science, 2012. ",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Walther%2C%20Andrea%20Griewank%2C%20Andreas%20Getting%20started%20with%20adol-c%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Walther%2C%20Andrea%20Griewank%2C%20Andreas%20Getting%20started%20with%20adol-c%202012"
        }
    ]
}
