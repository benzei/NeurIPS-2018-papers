{
    "filename": "8257-improved-network-robustness-with-adversary-critic.pdf",
    "metadata": {
        "title": "Improved Network Robustness with Adversary Critic",
        "author": "Alexander Matyasko, Lap-Pui Chau",
        "date": 2018,
        "identifiers": {
            "url": "https://papers.nips.cc/paper/8257-improved-network-robustness-with-adversary-critic.pdf"
        },
        "journal": "Conference on Neural Information Processing Systems",
        "abstract": "Ideally, what confuses neural network should be confusing to humans. However, recent experiments have shown that small, imperceptible perturbations can change the network prediction. To address this gap in perception, we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN), where the adversarial attack on classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, we introduce adversarial cycleconsistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments, we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally, we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing."
    },
    "keywords": [
        {
            "term": "deep learning",
            "url": "https://en.wikipedia.org/wiki/deep_learning"
        },
        {
            "term": "Nanyang Technological University",
            "url": "https://en.wikipedia.org/wiki/Nanyang_Technological_University"
        },
        {
            "term": "speech recognition",
            "url": "https://en.wikipedia.org/wiki/speech_recognition"
        },
        {
            "term": "Generative Adversarial Networks",
            "url": "https://en.wikipedia.org/wiki/Generative_Adversarial_Networks"
        },
        {
            "term": "neural network",
            "url": "https://en.wikipedia.org/wiki/neural_network"
        },
        {
            "term": "robust optimization",
            "url": "https://en.wikipedia.org/wiki/robust_optimization"
        },
        {
            "term": "deep neural network",
            "url": "https://en.wikipedia.org/wiki/deep_neural_network"
        }
    ],
    "highlights": [
        "Deep neural networks are powerful representation learning models which achieve near-human performance in image [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and speech [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] recognition tasks",
        "Our method surpasses in terms of robustness networks trained with adversarial training",
        "As we have argued in the previous section, adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target",
        "Our defense is based on the intuition that adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target",
        "Unlike prior work based on robust optimization, our method does not put any prior constraints on adversarial noise",
        "In experiments with human annotators, we show that adversarial examples for our defense are visually confusing"
    ],
    "key_statements": [
        "Deep neural networks are powerful representation learning models which achieve near-human performance in image [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and speech [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] recognition tasks",
        "State-of-the-art networks are sensitive to small input perturbations. [<a class=\"ref-link\" id=\"c3\" href=\"#r3\">3</a>] showed that adding adversarial noise to inputs produces images which are visually similar to the original inputs but which the network misclassifies with high confidence",
        "[<a class=\"ref-link\" id=\"c4\" href=\"#r4\">4</a>] introduced an adversarial attack, which can change any audio waveform, such that the corrupted signal is over 99.9% similar to the original but transcribes to any targeted phrase",
        "The existence of adversarial examples puts into question generalization ability of deep neural networks, reduces model interpretability, and limits applications of deep learning in safety and security-critical environments [<a class=\"ref-link\" id=\"c5\" href=\"#r5\">5</a>, <a class=\"ref-link\" id=\"c6\" href=\"#r6\">6</a>]",
        "To address this gap in perception, we propose a novel approach for learning robust classifier",
        "To improve the stability of the adversarial mapping, we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original",
        "Our method surpasses in terms of robustness networks trained with adversarial training",
        "Compare with defenses based on robust optimization, we do not put any prior constraint on the adversarial attack",
        "Adversarial training with Projected Gradient Descent attack increases the robustness of the regularized models compare to the original defense",
        "We propose a novel approach for learning a robust classifier which is orthogonal to prior robust optimization methods",
        "Based on the above intuition, we develop a novel formulation for learning a robust classifier",
        "As we have argued in the previous section, adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target",
        "The objective for the classifier f is to minimize the number of mistakes subject to that its adversarial examples generated by the attack as follows: 1 \u03c1adv fool the adversary critic D: L(f, D\u2217)",
        "To improve stability of the adversarial mapping during training, we introduce adversarial cycle-consistency constraint which ensures that adversarial mapping as follows: 1 \u03c1adv of the adversarial examples should be close to the original: Lcycle = Ex\u223cpdata(x|ys) as follows: 1 \u03c1adv (Af (x, yt), ys) \u2212 x 2 \u2200ys = yt",
        "To generate high-confidence adversarial examples, we propose a novel adversarial attack which iteratively maximizes the confidence of the adversarial target",
        "If the classifier f \u2217 is optimal and robust, its adversarial examples generated by the attack as follows: 1 \u03c1adv should fool the adversary critic D",
        "The attack as follows: 1 \u03c1adv to fool the critic D should generate adversarial examples with the confidence C equal to the confidence of the classifier f on the regular examples",
        "We study the robustness of two networks with rectified activation: 1) a fully-connected neural network with three hidden layers of size 1200 units each; 2) Lenet-5 convolutional neural network",
        "We introduce a novel approach for learning a robust classifier",
        "Our defense is based on the intuition that adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target",
        "Unlike prior work based on robust optimization, our method does not put any prior constraints on adversarial noise",
        "In experiments with human annotators, we show that adversarial examples for our defense are visually confusing"
    ],
    "summary": [
        "Deep neural networks are powerful representation learning models which achieve near-human performance in image [<a class=\"ref-link\" id=\"c1\" href=\"#r1\">1</a>] and speech [<a class=\"ref-link\" id=\"c2\" href=\"#r2\">2</a>] recognition tasks.",
        "We formulate the problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN) [<a class=\"ref-link\" id=\"c11\" href=\"#r11\">11</a>].",
        "The adversarial attack on the classifier acts as a generator, and the critic network learns to distinguish between natural and adversarial images.",
        "Compare with defenses based on robust optimization, we do not put any prior constraint on the adversarial attack.",
        "We require that adversarial noise for robust classifier should change the \u201ctrue\u201d label of the input to confuse the critic.",
        "Adversarial training [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] is a popular regularization method to improve neural network robustness.",
        "AT assumes that adversarial noise is label non-changing and trains neural network on the mixture of original and adversarial images: N",
        "Adversarial training with PGD attack increases the robustness of the regularized models compare to the original defense.",
        "To ensure that the changes of the network prediction are fooling examples, Goodfellow et al [<a class=\"ref-link\" id=\"c7\" href=\"#r7\">7</a>] argue in favor of a max-norm perturbation constraint for image classification problems.",
        "We require that adversarial noise for the robust classifier should be visually confusing and, it should change the underlying label of the input.",
        "The objective for the classifier f is to minimize the number of mistakes subject to that its adversarial examples generated by the attack Af fool the adversary critic D: L(f, D\u2217)",
        "Adversarial examples generated by the attack Af should be misclassified by the network f with high confidence.",
        "Because we use the approximation of the non-convex decision boundary, we iteratively update perturbation r for Nmax steps using eq (7) until the adversarial input xadv is misclassified as the target k with the confidence C.",
        "If the classifier f \u2217 is optimal and robust, its adversarial examples generated by the attack Af should fool the adversary critic D.",
        "The attack Af to fool the critic D should generate adversarial examples with the confidence C equal to the confidence of the classifier f on the regular examples.",
        "For the model trained without any defense, adversarial noise does not change the label of the input.",
        "When the model is trained with our defense, the high-confidence adversarial noise changes the label of the input.",
        "Our defense is based on the intuition that adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target.",
        "We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks.",
        "We plan to scale our defense to more complex datasets and apply it to the classification tasks in other domains, such as audio or text"
    ],
    "headline": "To address this gap in perception, we propose a novel approach for learning robust classifier",
    "reference_links": [
        {
            "id": "1",
            "entry": "[1] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016"
        },
        {
            "id": "2",
            "entry": "[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. In IEEE Signal Processing Magazine, 2012.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Hinton%2C%20G.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%202012",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Hinton%2C%20G.%20Deng%2C%20L.%20Yu%2C%20D.%20Dahl%2C%20G.E.%20Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20The%20shared%20views%202012"
        },
        {
            "id": "3",
            "entry": "[3] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2013.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Szegedy%2C%20C.%20Zaremba%2C%20W.%20Sutskever%2C%20I.%20Bruna%2C%20J.%20Intriguing%20properties%20of%20neural%20networks%202013"
        },
        {
            "id": "4",
            "entry": "[4] N. Carlini and D. Wagner. Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. arXiv preprint arXiv:1801.01944, 2018.",
            "arxiv_url": "https://arxiv.org/pdf/1801.01944"
        },
        {
            "id": "5",
            "entry": "[5] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Sharif%2C%20Mahmood%20Bhagavatula%2C%20Sruti%20Bauer%2C%20Lujo%20Reiter%2C%20Michael%20K.%20Accessorize%20to%20a%20crime%3A%20Real%20and%20stealthy%20attacks%20on%20state-of-the-art%20face%20recognition%202016"
        },
        {
            "id": "6",
            "entry": "[6] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20D.%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20D.%20Jha%2C%20Somesh%20Matt%20Fredrikson%2C%20Z.Berkay%20Celik%20The%20limitations%20of%20deep%20learning%20in%20adversarial%20settings%202016"
        },
        {
            "id": "7",
            "entry": "[7] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Goodfellow%2C%20Ian%20J.%20Shlens%2C%20Jonathon%20Szegedy%2C%20Christian%20Explaining%20and%20harnessing%20adversarial%20examples%202015"
        },
        {
            "id": "8",
            "entry": "[8] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial Machine Learning at Scale. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Kurakin%2C%20A.%20Goodfellow%2C%20I.%20Bengio%2C%20S.%20Adversarial%20Machine%20Learning%20at%20Scale%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Kurakin%2C%20A.%20Goodfellow%2C%20I.%20Bengio%2C%20S.%20Adversarial%20Machine%20Learning%20at%20Scale%202017"
        },
        {
            "id": "9",
            "entry": "[9] Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Tram%C3%A8r%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Goodfellow%2C%20Ian%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Tram%C3%A8r%2C%20Florian%20Kurakin%2C%20Alexey%20Papernot%2C%20Nicolas%20Goodfellow%2C%20Ian%20Ensemble%20adversarial%20training%3A%20Attacks%20and%20defenses%202018"
        },
        {
            "id": "10",
            "entry": "[10] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Towards%20evaluating%20the%20robustness%20of%20neural%20networks%202017"
        },
        {
            "id": "11",
            "entry": "[11] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ian%20J%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20C%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ian%20J%20Goodfellow%20Jean%20PougetAbadie%20Mehdi%20Mirza%20Bing%20Xu%20David%20WardeFarley%20Sherjil%20Ozair%20Aaron%20C%20Courville%20and%20Yoshua%20Bengio%20Generative%20adversarial%20nets%20In%20NIPS%202014"
        },
        {
            "id": "12",
            "entry": "[12] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.",
            "arxiv_url": "https://arxiv.org/pdf/1607.02533"
        },
        {
            "id": "13",
            "entry": "[13] S. M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Moosavi-Dezfooli%2C%20S.M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20A%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Moosavi-Dezfooli%2C%20S.M.%20Fawzi%2C%20A.%20Frossard%2C%20P.%20Deepfool%3A%20A%20simple%20and%20accurate%20method%20to%20fool%20deep%20neural%20networks%202016"
        },
        {
            "id": "14",
            "entry": "[14] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20D.%20Goodfellow%2C%20Ian%20J.%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Papernot%2C%20Nicolas%20McDaniel%2C%20Patrick%20D.%20Goodfellow%2C%20Ian%20J.%20Somesh%20Jha%2C%20Z.Berkay%20Celik%20Practical%20black-box%20attacks%20against%20machine%20learning%202017"
        },
        {
            "id": "15",
            "entry": "[15] Shumeet Baluja and Ian Fischer. Learning to attack: Adversarial transformation networks. In AAAI, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Baluja%2C%20Shumeet%20Fischer%2C%20Ian%20Learning%20to%20attack%3A%20Adversarial%20transformation%20networks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Baluja%2C%20Shumeet%20Fischer%2C%20Ian%20Learning%20to%20attack%3A%20Adversarial%20transformation%20networks%202018"
        },
        {
            "id": "16",
            "entry": "[16] T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii. Distributional Smoothing with Virtual Adversarial Training. In ICLR, 2015.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Miyato%2C%20T.%20Maeda%2C%20S.-i%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20Smoothing%20with%20Virtual%20Adversarial%20Training%202015",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Miyato%2C%20T.%20Maeda%2C%20S.-i%20Koyama%2C%20M.%20Nakae%2C%20K.%20Distributional%20Smoothing%20with%20Virtual%20Adversarial%20Training%202015"
        },
        {
            "id": "17",
            "entry": "[17] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20Deep%20Learning%20Models%20Resistant%20to%20Adversarial%20Attacks%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Madry%2C%20A.%20Makelov%2C%20A.%20Schmidt%2C%20L.%20Tsipras%2C%20D.%20Towards%20Deep%20Learning%20Models%20Resistant%20to%20Adversarial%20Attacks%202018"
        },
        {
            "id": "18",
            "entry": "[18] S. Zheng, Y. Song, T. Leung, and I. Goodfellow. Improving the robustness of deep neural networks via stability training. In CVPR, 2016.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zheng%2C%20S.%20Song%2C%20Y.%20Leung%2C%20T.%20Goodfellow%2C%20I.%20Improving%20the%20robustness%20of%20deep%20neural%20networks%20via%20stability%20training%202016",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zheng%2C%20S.%20Song%2C%20Y.%20Leung%2C%20T.%20Goodfellow%2C%20I.%20Improving%20the%20robustness%20of%20deep%20neural%20networks%20via%20stability%20training%202016"
        },
        {
            "id": "19",
            "entry": "[19] Alexander Matyasko and Lap-Pui Chau. Margin maximization for robust classification using deep learning. In IJCNN, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Matyasko%2C%20Alexander%20Chau%2C%20Lap-Pui%20Margin%20maximization%20for%20robust%20classification%20using%20deep%20learning%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Matyasko%2C%20Alexander%20Chau%2C%20Lap-Pui%20Margin%20maximization%20for%20robust%20classification%20using%20deep%20learning%202017"
        },
        {
            "id": "20",
            "entry": "[20] Gamaleldin Fathy Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large margin deep networks for classification. In NIPS, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Elsayed%2C%20Gamaleldin%20Fathy%20Krishnan%2C%20Dilip%20Mobahi%2C%20Hossein%20Regan%2C%20Kevin%20Large%20margin%20deep%20networks%20for%20classification%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Elsayed%2C%20Gamaleldin%20Fathy%20Krishnan%2C%20Dilip%20Mobahi%2C%20Hossein%20Regan%2C%20Kevin%20Large%20margin%20deep%20networks%20for%20classification%202018"
        },
        {
            "id": "21",
            "entry": "[21] Moustapha Ciss\u00e9, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In ICML, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Ciss%C3%A9%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Ciss%C3%A9%2C%20Moustapha%20Bojanowski%2C%20Piotr%20Grave%2C%20Edouard%20Dauphin%2C%20Yann%20Parseval%20networks%3A%20Improving%20robustness%20to%20adversarial%20examples%202017"
        },
        {
            "id": "22",
            "entry": "[22] J. Hendrik Metzen, T. Genewein, V. Fischer, and B. Bischoff. On Detecting Adversarial Perturbations. In ICLR, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Metzen%2C%20J.Hendrik%20Genewein%2C%20T.%20Fischer%2C%20V.%20Bischoff%2C%20B.%20On%20Detecting%20Adversarial%20Perturbations%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Metzen%2C%20J.Hendrik%20Genewein%2C%20T.%20Fischer%2C%20V.%20Bischoff%2C%20B.%20On%20Detecting%20Adversarial%20Perturbations%202017"
        },
        {
            "id": "23",
            "entry": "[23] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner. Detecting Adversarial Samples from Artifacts. arXiv preprint arXiv:1703.00410, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1703.00410"
        },
        {
            "id": "24",
            "entry": "[24] Nicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Carlini%2C%20Nicholas%20Wagner%2C%20David%20A.%20Adversarial%20examples%20are%20not%20easily%20detected%3A%20Bypassing%20ten%20detection%20methods%202017"
        },
        {
            "id": "25",
            "entry": "[25] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. In ICLR, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Samangouei%2C%20Pouya%20Kabkab%2C%20Maya%20Chellappa%2C%20Rama%20Defense-GAN%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Samangouei%2C%20Pouya%20Kabkab%2C%20Maya%20Chellappa%2C%20Rama%20Defense-GAN%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%202018"
        },
        {
            "id": "26",
            "entry": "[26] H. Lee, S. Han, and J. Lee. Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN. arXiv preprint arXiv:1705.03387, 2017.",
            "arxiv_url": "https://arxiv.org/pdf/1705.03387"
        },
        {
            "id": "27",
            "entry": "[27] Chongxuan LI, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Chongxuan%2C%20L.I.%20Xu%2C%20Taufik%20Zhu%2C%20Jun%20Zhang%2C%20Bo%20Triple%20generative%20adversarial%20nets%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Chongxuan%2C%20L.I.%20Xu%2C%20Taufik%20Zhu%2C%20Jun%20Zhang%2C%20Bo%20Triple%20generative%20adversarial%20nets%202017"
        },
        {
            "id": "28",
            "entry": "[28] Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. In NIPS, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Huan%20Xu%20Constantine%20Caramanis%20and%20Shie%20Mannor%20Robust%20regression%20and%20lasso%20In%20NIPS%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Huan%20Xu%20Constantine%20Caramanis%20and%20Shie%20Mannor%20Robust%20regression%20and%20lasso%20In%20NIPS%202009"
        },
        {
            "id": "29",
            "entry": "[29] Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. In Journal of Machine Learning Research, 2009.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Xu%2C%20Huan%20Caramanis%2C%20Constantine%20Mannor%2C%20Shie%20Robustness%20and%20regularization%20of%20support%20vector%20machines%202009",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Xu%2C%20Huan%20Caramanis%2C%20Constantine%20Mannor%2C%20Shie%20Robustness%20and%20regularization%20of%20support%20vector%20machines%202009"
        },
        {
            "id": "30",
            "entry": "[30] U. Shaham, Y. Yamada, and S. Negahban. Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization. arXiv preprint arXiv:1511.05432, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1511.05432"
        },
        {
            "id": "31",
            "entry": "[31] Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. In Mathematical Programming, 2018.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Bertsimas%2C%20Dimitris%20Gupta%2C%20Vishal%20Kallus%2C%20Nathan%20Data-driven%20robust%20optimization%202018",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Bertsimas%2C%20Dimitris%20Gupta%2C%20Vishal%20Kallus%2C%20Nathan%20Data-driven%20robust%20optimization%202018"
        },
        {
            "id": "32",
            "entry": "[32] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Gulrajani%2C%20Ishaan%20Ahmed%2C%20Faruk%20Arjovsky%2C%20Martin%20Dumoulin%2C%20Vincent%20Improved%20training%20of%20wasserstein%20gans%202017"
        },
        {
            "id": "33",
            "entry": "[33] J. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, 2017.",
            "scholar_url": "https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycleconsistent%20adversarial%20networks%202017",
            "oa_query": "https://api.scholarcy.com/oa_version?query=Zhu%2C%20J.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycleconsistent%20adversarial%20networks%202017"
        },
        {
            "id": "34",
            "entry": "[34] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1308.3432"
        },
        {
            "id": "35",
            "entry": "[35] The Tensorflow Development Team. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv preprint arXiv:1603.04467, 2015.",
            "arxiv_url": "https://arxiv.org/pdf/1603.04467"
        },
        {
            "id": "36",
            "entry": "[36] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980, 2015. ",
            "arxiv_url": "https://arxiv.org/pdf/1412.6980"
        }
    ]
}
